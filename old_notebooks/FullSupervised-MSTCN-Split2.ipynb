{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mstcn_model import *\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 8, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 2, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split2/', 'project_name': 'breakfast-split-2', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split2.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split2.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split2_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=2,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split2/\",\n",
    "    project_name=\"breakfast-split-2\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1261\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 451\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", allow_pickle=True).item()\n",
    "# loaded_vidid_selected_frames\n",
    "video_id_boundary_frames = pickle.load(open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"rb\"))\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_var_actions = pickle.load(open(\"dump_dir/mean_var_actions.pkl\", \"rb\"))\n",
    "def get_possion_prob(cur_len, cur_class):\n",
    "    mean_class, std_class = loaded_mean_var_actions[cur_class]\n",
    "    mean_class = mean_class * 10\n",
    "    prob = cur_len * torch.log(torch.tensor(mean_class, device=device) + 1e-5)\n",
    "    prob = prob - mean_class\n",
    "    factorials = torch.cumsum(torch.log(torch.arange(1, torch.max(cur_len)+1, 1).type(torch.float).to(device)), \n",
    "                              dim=0)[min(cur_len)-1:]\n",
    "    prob = prob - factorials\n",
    "    return prob\n",
    "\n",
    "# get_possion_prob(torch.arange(10, 21), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_video_each_segment_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vals_per_segment(selected_frames, cur_vid_feat, labels):\n",
    "    prob_each_segment = []\n",
    "    \n",
    "    log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "    cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "    cur_boundary = 0\n",
    "    for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "\n",
    "        next_ele = selected_frames[i + 1]\n",
    "        label_cur_ele = labels[cur_ele]\n",
    "        label_next_ele = labels[next_ele]\n",
    "\n",
    "        indices = torch.arange(cur_ele, next_ele).to(cumsum_feat.device)\n",
    "        strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "        end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "        left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "        right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "\n",
    "        cur_boundary_len = indices - cur_boundary\n",
    "        len_prob = get_possion_prob(cur_boundary_len, label_cur_ele.item())\n",
    "\n",
    "        prob = torch.softmax((left_sum + right_sum + len_prob), dim=0)\n",
    "#         prob = torch.softmax((left_sum + right_sum), dim=0)\n",
    "\n",
    "        cur_boundary = round(np.sum(np.arange(cur_ele, next_ele, 1) * prob.detach().cpu().numpy()).item())\n",
    "        prob_each_segment.append(prob)\n",
    "    \n",
    "    return prob_each_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_element_probb(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_video = prob_vals_per_segment(selected_frames, cur_vid_feat, labels)\n",
    "        prob_video_each_segment_dict[cur_vidid] = prob_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_loss(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    loss_arr = []\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_each_video = prob_video_each_segment_dict[cur_vidid]\n",
    "        \n",
    "        log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "        cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "        \n",
    "        \n",
    "        current_vid_prob_arr = []\n",
    "        # Start segment log probability\n",
    "        start_ele_idx = selected_frames[0]\n",
    "        if start_ele_idx > 0:\n",
    "            start_ele_label_idx = labels[selected_frames[0]]\n",
    "            first_segment_prob = (cumsum_feat[start_ele_idx - 1, :])[start_ele_label_idx]\n",
    "            current_vid_prob_arr.append(first_segment_prob.unsqueeze(0))\n",
    "        \n",
    "        for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "            next_ele = selected_frames[i + 1]\n",
    "            label_cur_ele = labels[cur_ele]\n",
    "            label_next_ele = labels[next_ele]\n",
    "\n",
    "            indices = torch.arange(cur_ele, next_ele)\n",
    "            strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "            end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "            left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "            right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "            current_vid_prob_arr.append((left_sum + right_sum)  * (prob_each_video[i]))\n",
    "        \n",
    "        # End segment log probability\n",
    "        end_index = cur_vid_count.item() - 1\n",
    "        last_ele_label_idx = labels[selected_frames[-1]]\n",
    "        last_segment_sum_prob = (cumsum_feat[end_index, :] - cumsum_feat[selected_frames[-1] - 1, :])[last_ele_label_idx]\n",
    "        current_vid_prob_arr.append(last_segment_sum_prob.unsqueeze(0))\n",
    "            \n",
    "        loss_arr.append(torch.cat(current_vid_prob_arr))\n",
    "    return -torch.mean(torch.cat(loss_arr)), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_estimated_boundaries():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames, video_id_boundary_frames\n",
    "    estimated_boundary_dict = {}\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        selected_ele_list = loaded_vidid_selected_frames[ele + \".txt\"]\n",
    "        boundary_list = []\n",
    "        for i, segment in enumerate(estimated_boundary_probs):\n",
    "            estimated_boundary = np.sum(np.arange(selected_ele_list[i], selected_ele_list[i + 1], 1) \\\n",
    "                                        * segment.detach().cpu().numpy())\n",
    "            estimated_boundary = round(estimated_boundary.item())\n",
    "            \n",
    "            if (estimated_boundary < selected_ele_list[i]) or (estimated_boundary > selected_ele_list[i + 1]):\n",
    "                print(\"Estimated value wrong\")\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            boundary_list.append(estimated_boundary)\n",
    "\n",
    "        estimated_boundary_dict[ele] = boundary_list\n",
    "    return estimated_boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_boundary_err():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    err_list = []\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    estimated_boundar_dict = get_estimated_boundaries()\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        estimated_boundary_list = estimated_boundar_dict[ele]\n",
    "        \n",
    "        estimated = np.array(estimated_boundary_list) #.detach().cpu().numpy()\n",
    "        actual = np.array(video_id_boundary_frames[ele][:-1])\n",
    "        if len(actual) != len(estimated):\n",
    "            print(ele)\n",
    "            continue\n",
    "        \n",
    "        mse_err = (actual - estimated)**2\n",
    "        err_list.append(mse_err)\n",
    "        \n",
    "        estimated_labels = []\n",
    "        actual_labels = []\n",
    "        start_v1 = 0\n",
    "        start_v2 = 0\n",
    "        for i, v1 in enumerate(estimated):\n",
    "            estimated_labels.extend([i] * (v1 - start_v1))\n",
    "            start_v1 = v1\n",
    "            v2 = actual[i]\n",
    "            actual_labels.extend([i] * (v2 - start_v2))\n",
    "            start_v2 = v2\n",
    "            \n",
    "        last_ele = video_id_boundary_frames[ele][-1]\n",
    "        estimated_labels.extend([i+1] * (last_ele - start_v1))\n",
    "        actual_labels.extend([i+1] * (last_ele - start_v2))\n",
    "        \n",
    "        correct += np.sum(np.array(actual_labels) == np.array(estimated_labels))\n",
    "        total += len(actual_labels)\n",
    "        \n",
    "    print(f\"Train Boundary avergage error = {np.sqrt(np.mean(np.concatenate(err_list))):.3f}\")\n",
    "    print(f\"Train From boundary avergage accuracy = {correct * 100.0 / total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = MultiStageModel(num_stages=4, num_layers=10, num_f_maps=64, dim=2048, num_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(labels_all, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((labels_all.shape[0], labels_all.shape[1]), dtype=torch.long, device=labels_all.device) * (-100)\n",
    "    for iter_num, labels in enumerate(labels_all):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(loaded_vidid_selected_frames[cur_vidid + \".txt\"]))\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = labels[frame_idx_tensor]\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Training:: Epoch 0, Iteration 0, Current loss 15.632051467895508 Accuracy 1.0729173785279846\n",
      "Training:: Epoch 0, Iteration 10, Current loss 14.708296775817871 Accuracy 6.625744719263405\n",
      "Training:: Epoch 0, Iteration 20, Current loss 13.606067657470703 Accuracy 11.623159960745829\n",
      "Training:: Epoch 0, Iteration 30, Current loss 15.668014526367188 Accuracy 2.6079019428869477\n",
      "Training:: Epoch 0, Iteration 40, Current loss 12.57455825805664 Accuracy 25.899855224402177\n",
      "Training:: Epoch 0, Iteration 50, Current loss 11.26087474822998 Accuracy 23.480308960548648\n",
      "Training:: Epoch 0, Iteration 60, Current loss 12.612722396850586 Accuracy 17.954012724634445\n",
      "Training:: Epoch 0, Iteration 70, Current loss 12.3187837600708 Accuracy 4.792892711634224\n",
      "Training:: Epoch 0, Iteration 80, Current loss 13.801706314086914 Accuracy 7.953004970628107\n",
      "Training:: Epoch 0, Iteration 90, Current loss 12.562198638916016 Accuracy 4.76417089678511\n",
      "Training:: Epoch 0, Iteration 100, Current loss 11.246192932128906 Accuracy 26.15114598389428\n",
      "Training:: Epoch 0, Iteration 110, Current loss 11.180292129516602 Accuracy 20.892479021253234\n",
      "Training:: Epoch 0, Iteration 120, Current loss 11.652303695678711 Accuracy 9.286214680470149\n",
      "Training:: Epoch 0, Iteration 130, Current loss 11.597734451293945 Accuracy 10.803606428851431\n",
      "Training:: Epoch 0, Iteration 140, Current loss 10.711905479431152 Accuracy 13.721433273770337\n",
      "Training:: Epoch 0, Iteration 150, Current loss 10.625932693481445 Accuracy 15.551323307283116\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 0, Probability Accuracy 17.480734971206033\n",
      "Starting Training\n",
      "Training:: Epoch 1, Iteration 0, Current loss 11.30415153503418 Accuracy 32.87288530187801\n",
      "Training:: Epoch 1, Iteration 10, Current loss 10.660992622375488 Accuracy 14.205556315861502\n",
      "Training:: Epoch 1, Iteration 20, Current loss 10.435012817382812 Accuracy 20.045045045045047\n",
      "Training:: Epoch 1, Iteration 30, Current loss 8.806392669677734 Accuracy 26.453320500481233\n",
      "Training:: Epoch 1, Iteration 40, Current loss 9.171517372131348 Accuracy 22.564132737114615\n",
      "Training:: Epoch 1, Iteration 50, Current loss 8.630814552307129 Accuracy 27.224445812807883\n",
      "Training:: Epoch 1, Iteration 60, Current loss 9.248812675476074 Accuracy 31.33066818960594\n",
      "Training:: Epoch 1, Iteration 70, Current loss 10.353009223937988 Accuracy 29.27289896128423\n",
      "Training:: Epoch 1, Iteration 80, Current loss 8.311851501464844 Accuracy 27.484848484848484\n",
      "Training:: Epoch 1, Iteration 90, Current loss 8.812114715576172 Accuracy 25.318853974121996\n",
      "Training:: Epoch 1, Iteration 100, Current loss 10.294219017028809 Accuracy 20.902612826603324\n",
      "Training:: Epoch 1, Iteration 110, Current loss 8.309198379516602 Accuracy 20.88630806845966\n",
      "Training:: Epoch 1, Iteration 120, Current loss 8.735559463500977 Accuracy 18.83058786379664\n",
      "Training:: Epoch 1, Iteration 130, Current loss 9.230245590209961 Accuracy 23.06157112526539\n",
      "Training:: Epoch 1, Iteration 140, Current loss 7.58941125869751 Accuracy 31.112841568738496\n",
      "Training:: Epoch 1, Iteration 150, Current loss 9.407613754272461 Accuracy 22.60475432759535\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 1, Probability Accuracy 36.22964742925799\n",
      "Starting Training\n",
      "Training:: Epoch 2, Iteration 0, Current loss 8.75422477722168 Accuracy 33.352576129311586\n",
      "Training:: Epoch 2, Iteration 10, Current loss 6.076831817626953 Accuracy 34.61469694260683\n",
      "Training:: Epoch 2, Iteration 20, Current loss 9.523605346679688 Accuracy 20.537853909647644\n",
      "Training:: Epoch 2, Iteration 30, Current loss 7.882345676422119 Accuracy 28.63739917492765\n",
      "Training:: Epoch 2, Iteration 40, Current loss 7.657584190368652 Accuracy 34.048655063291136\n",
      "Training:: Epoch 2, Iteration 50, Current loss 7.279068946838379 Accuracy 35.81172550244715\n",
      "Training:: Epoch 2, Iteration 60, Current loss 6.406201362609863 Accuracy 41.05527638190955\n",
      "Training:: Epoch 2, Iteration 70, Current loss 6.7015533447265625 Accuracy 44.14828651483584\n",
      "Training:: Epoch 2, Iteration 80, Current loss 7.681519031524658 Accuracy 20.886678658343033\n",
      "Training:: Epoch 2, Iteration 90, Current loss 7.498733997344971 Accuracy 23.388979687135187\n",
      "Training:: Epoch 2, Iteration 100, Current loss 6.194490432739258 Accuracy 44.93005351610177\n",
      "Training:: Epoch 2, Iteration 110, Current loss 7.0427165031433105 Accuracy 32.88042090207897\n",
      "Training:: Epoch 2, Iteration 120, Current loss 10.919892311096191 Accuracy 28.79152097902098\n",
      "Training:: Epoch 2, Iteration 130, Current loss 11.162458419799805 Accuracy 17.761130339955432\n",
      "Training:: Epoch 2, Iteration 140, Current loss 7.385791778564453 Accuracy 28.59414460761388\n",
      "Training:: Epoch 2, Iteration 150, Current loss 6.650737285614014 Accuracy 45.81504036255488\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 2, Probability Accuracy 36.78201516344202\n",
      "Starting Training\n",
      "Training:: Epoch 3, Iteration 0, Current loss 8.515308380126953 Accuracy 29.02113626143007\n",
      "Training:: Epoch 3, Iteration 10, Current loss 4.751305103302002 Accuracy 64.47046371634583\n",
      "Training:: Epoch 3, Iteration 20, Current loss 6.485409259796143 Accuracy 42.54935312612752\n",
      "Training:: Epoch 3, Iteration 30, Current loss 5.606677532196045 Accuracy 41.22050544975446\n",
      "Training:: Epoch 3, Iteration 40, Current loss 4.875309944152832 Accuracy 66.55776386241946\n",
      "Training:: Epoch 3, Iteration 50, Current loss 6.563748359680176 Accuracy 39.887263454569855\n",
      "Training:: Epoch 3, Iteration 60, Current loss 7.085235118865967 Accuracy 44.13397005678885\n",
      "Training:: Epoch 3, Iteration 70, Current loss 6.6066670417785645 Accuracy 52.617222058389856\n",
      "Training:: Epoch 3, Iteration 80, Current loss 5.124607563018799 Accuracy 52.63090513378569\n",
      "Training:: Epoch 3, Iteration 90, Current loss 6.172400951385498 Accuracy 43.58105949261934\n",
      "Training:: Epoch 3, Iteration 100, Current loss 6.735019683837891 Accuracy 45.900048520135854\n",
      "Training:: Epoch 3, Iteration 110, Current loss 5.536215782165527 Accuracy 54.41272868124665\n",
      "Training:: Epoch 3, Iteration 120, Current loss 5.999378681182861 Accuracy 49.47711190431461\n",
      "Training:: Epoch 3, Iteration 130, Current loss 8.96112060546875 Accuracy 25.3225978457929\n",
      "Training:: Epoch 3, Iteration 140, Current loss 6.864901542663574 Accuracy 43.227598981245656\n",
      "Training:: Epoch 3, Iteration 150, Current loss 7.212412357330322 Accuracy 29.23808942841457\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 3, Probability Accuracy 43.55419066163981\n",
      "Starting Training\n",
      "Training:: Epoch 4, Iteration 0, Current loss 4.864506244659424 Accuracy 52.99911268855368\n",
      "Training:: Epoch 4, Iteration 10, Current loss 5.781623363494873 Accuracy 53.87049145749842\n",
      "Training:: Epoch 4, Iteration 20, Current loss 6.908537864685059 Accuracy 38.833333333333336\n",
      "Training:: Epoch 4, Iteration 30, Current loss 5.050318241119385 Accuracy 46.06978445123263\n",
      "Training:: Epoch 4, Iteration 40, Current loss 6.410890102386475 Accuracy 49.611639864347445\n",
      "Training:: Epoch 4, Iteration 50, Current loss 5.519848823547363 Accuracy 39.64932960070723\n",
      "Training:: Epoch 4, Iteration 60, Current loss 5.757153511047363 Accuracy 48.007979165512275\n",
      "Training:: Epoch 4, Iteration 70, Current loss 8.368172645568848 Accuracy 39.064174306465056\n",
      "Training:: Epoch 4, Iteration 80, Current loss 5.198120594024658 Accuracy 61.97072514160731\n",
      "Training:: Epoch 4, Iteration 90, Current loss 6.376357555389404 Accuracy 44.81578947368421\n",
      "Training:: Epoch 4, Iteration 100, Current loss 5.551620006561279 Accuracy 53.332948910160304\n",
      "Training:: Epoch 4, Iteration 110, Current loss 6.054718017578125 Accuracy 40.82541762201114\n",
      "Training:: Epoch 4, Iteration 120, Current loss 5.080145359039307 Accuracy 56.610370051635115\n",
      "Training:: Epoch 4, Iteration 130, Current loss 6.575667381286621 Accuracy 45.7225277201839\n",
      "Training:: Epoch 4, Iteration 140, Current loss 5.766263961791992 Accuracy 57.504946763403375\n",
      "Training:: Epoch 4, Iteration 150, Current loss 5.043367862701416 Accuracy 56.03317577811469\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 4, Probability Accuracy 42.632162240543565\n",
      "Starting Training\n",
      "Training:: Epoch 5, Iteration 0, Current loss 6.081158638000488 Accuracy 50.1953125\n",
      "Training:: Epoch 5, Iteration 10, Current loss 5.562331199645996 Accuracy 46.88955528616091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 5, Iteration 20, Current loss 5.026158809661865 Accuracy 64.92052306005795\n",
      "Training:: Epoch 5, Iteration 30, Current loss 5.31523323059082 Accuracy 53.79978782910599\n",
      "Training:: Epoch 5, Iteration 40, Current loss 6.597119331359863 Accuracy 51.29882264997197\n",
      "Training:: Epoch 5, Iteration 50, Current loss 5.104883193969727 Accuracy 49.24928180389181\n",
      "Training:: Epoch 5, Iteration 60, Current loss 4.9304094314575195 Accuracy 43.90351360281969\n",
      "Training:: Epoch 5, Iteration 70, Current loss 5.053787708282471 Accuracy 43.80183941770581\n",
      "Training:: Epoch 5, Iteration 80, Current loss 5.562042713165283 Accuracy 52.52219531880549\n",
      "Training:: Epoch 5, Iteration 90, Current loss 5.7333292961120605 Accuracy 51.581929796946326\n",
      "Training:: Epoch 5, Iteration 100, Current loss 6.853933334350586 Accuracy 36.489329726480314\n",
      "Training:: Epoch 5, Iteration 110, Current loss 4.002933502197266 Accuracy 69.85866942433644\n",
      "Training:: Epoch 5, Iteration 120, Current loss 6.191177845001221 Accuracy 53.19743377483444\n",
      "Training:: Epoch 5, Iteration 130, Current loss 4.764451503753662 Accuracy 62.213241412100324\n",
      "Training:: Epoch 5, Iteration 140, Current loss 5.958820819854736 Accuracy 57.51188845006851\n",
      "Training:: Epoch 5, Iteration 150, Current loss 7.199516773223877 Accuracy 38.33205814843152\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 5, Probability Accuracy 46.18707793014873\n",
      "Starting Training\n",
      "Training:: Epoch 6, Iteration 0, Current loss 4.759457588195801 Accuracy 60.87899651042158\n",
      "Training:: Epoch 6, Iteration 10, Current loss 4.5983476638793945 Accuracy 51.297195607472545\n",
      "Training:: Epoch 6, Iteration 20, Current loss 5.085122108459473 Accuracy 58.42544114795424\n",
      "Training:: Epoch 6, Iteration 30, Current loss 4.427644729614258 Accuracy 60.355434039644564\n",
      "Training:: Epoch 6, Iteration 40, Current loss 5.553287029266357 Accuracy 42.65920476598818\n",
      "Training:: Epoch 6, Iteration 50, Current loss 3.732940435409546 Accuracy 72.27847569429856\n",
      "Training:: Epoch 6, Iteration 60, Current loss 5.703725337982178 Accuracy 55.23461708720673\n",
      "Training:: Epoch 6, Iteration 70, Current loss 3.960444927215576 Accuracy 68.92426133999167\n",
      "Training:: Epoch 6, Iteration 80, Current loss 4.48642110824585 Accuracy 57.321239982605455\n",
      "Training:: Epoch 6, Iteration 90, Current loss 6.311727046966553 Accuracy 43.622977222179614\n",
      "Training:: Epoch 6, Iteration 100, Current loss 5.480003356933594 Accuracy 53.21167296406463\n",
      "Training:: Epoch 6, Iteration 110, Current loss 4.83963680267334 Accuracy 58.09775034839737\n",
      "Training:: Epoch 6, Iteration 120, Current loss 6.736610412597656 Accuracy 46.50875284343784\n",
      "Training:: Epoch 6, Iteration 130, Current loss 7.146843433380127 Accuracy 42.40609496810772\n",
      "Training:: Epoch 6, Iteration 140, Current loss 5.197903156280518 Accuracy 60.65421350688868\n",
      "Training:: Epoch 6, Iteration 150, Current loss 4.230105876922607 Accuracy 61.30978543173373\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 6, Probability Accuracy 51.8035588515557\n",
      "Starting Training\n",
      "Training:: Epoch 7, Iteration 0, Current loss 5.582113742828369 Accuracy 53.92354124748491\n",
      "Training:: Epoch 7, Iteration 10, Current loss 7.850892543792725 Accuracy 38.552397389569066\n",
      "Training:: Epoch 7, Iteration 20, Current loss 5.060877799987793 Accuracy 64.63034380897678\n",
      "Training:: Epoch 7, Iteration 30, Current loss 3.924732208251953 Accuracy 75.16387034698418\n",
      "Training:: Epoch 7, Iteration 40, Current loss 5.495273590087891 Accuracy 54.388970468265626\n",
      "Training:: Epoch 7, Iteration 50, Current loss 4.775222301483154 Accuracy 56.828665952192345\n",
      "Training:: Epoch 7, Iteration 60, Current loss 5.860288143157959 Accuracy 51.53598856938739\n",
      "Training:: Epoch 7, Iteration 70, Current loss 4.656691551208496 Accuracy 58.34063221594076\n",
      "Training:: Epoch 7, Iteration 80, Current loss 4.924538612365723 Accuracy 53.12142693802881\n",
      "Training:: Epoch 7, Iteration 90, Current loss 4.600790977478027 Accuracy 56.39193863738908\n",
      "Training:: Epoch 7, Iteration 100, Current loss 4.19791316986084 Accuracy 68.29656342808656\n",
      "Training:: Epoch 7, Iteration 110, Current loss 4.529592514038086 Accuracy 66.91058769723303\n",
      "Training:: Epoch 7, Iteration 120, Current loss 4.855474472045898 Accuracy 61.51748078353583\n",
      "Training:: Epoch 7, Iteration 130, Current loss 3.859973907470703 Accuracy 63.87450882554731\n",
      "Training:: Epoch 7, Iteration 140, Current loss 4.627878665924072 Accuracy 66.44472506026862\n",
      "Training:: Epoch 7, Iteration 150, Current loss 5.818519592285156 Accuracy 43.14385775862069\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 7, Probability Accuracy 54.68647719269172\n",
      "Starting Training\n",
      "Training:: Epoch 8, Iteration 0, Current loss 4.686216831207275 Accuracy 68.51862161108946\n",
      "Training:: Epoch 8, Iteration 10, Current loss 4.447962760925293 Accuracy 68.96587348049175\n",
      "Training:: Epoch 8, Iteration 20, Current loss 4.1697001457214355 Accuracy 71.23556968378786\n",
      "Training:: Epoch 8, Iteration 30, Current loss 4.463049411773682 Accuracy 65.15616999487968\n",
      "Training:: Epoch 8, Iteration 40, Current loss 5.353493690490723 Accuracy 53.36491196711056\n",
      "Training:: Epoch 8, Iteration 50, Current loss 4.828783988952637 Accuracy 58.00876966726851\n",
      "Training:: Epoch 8, Iteration 60, Current loss 4.183289527893066 Accuracy 69.67459222591901\n",
      "Training:: Epoch 8, Iteration 70, Current loss 3.465562343597412 Accuracy 73.6586808636498\n",
      "Training:: Epoch 8, Iteration 80, Current loss 4.940934181213379 Accuracy 67.1586108520327\n",
      "Training:: Epoch 8, Iteration 90, Current loss 4.676018238067627 Accuracy 59.21774931381519\n",
      "Training:: Epoch 8, Iteration 100, Current loss 3.7203145027160645 Accuracy 77.34709997743172\n",
      "Training:: Epoch 8, Iteration 110, Current loss 3.548429012298584 Accuracy 70.11415914125064\n",
      "Training:: Epoch 8, Iteration 120, Current loss 3.850208044052124 Accuracy 74.75552336110106\n",
      "Training:: Epoch 8, Iteration 130, Current loss 4.2715864181518555 Accuracy 60.944283646888564\n",
      "Training:: Epoch 8, Iteration 140, Current loss 5.516023635864258 Accuracy 55.38586139072081\n",
      "Training:: Epoch 8, Iteration 150, Current loss 6.62207555770874 Accuracy 46.21758569299553\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 8, Probability Accuracy 59.45674690309483\n",
      "Starting Training\n",
      "Training:: Epoch 9, Iteration 0, Current loss 5.192655086517334 Accuracy 57.0430733410943\n",
      "Training:: Epoch 9, Iteration 10, Current loss 6.020023822784424 Accuracy 59.45737713939794\n",
      "Training:: Epoch 9, Iteration 20, Current loss 3.1664278507232666 Accuracy 83.0756264384583\n",
      "Training:: Epoch 9, Iteration 30, Current loss 4.380615234375 Accuracy 73.01594553844748\n",
      "Training:: Epoch 9, Iteration 40, Current loss 3.483855724334717 Accuracy 75.21697203471552\n",
      "Training:: Epoch 9, Iteration 50, Current loss 4.0760722160339355 Accuracy 71.86980535149338\n",
      "Training:: Epoch 9, Iteration 60, Current loss 5.294158458709717 Accuracy 67.37990350325153\n",
      "Training:: Epoch 9, Iteration 70, Current loss 6.869118690490723 Accuracy 50.087393489185054\n",
      "Training:: Epoch 9, Iteration 80, Current loss 4.757233142852783 Accuracy 67.83818770226537\n",
      "Training:: Epoch 9, Iteration 90, Current loss 3.5763041973114014 Accuracy 75.47090832984513\n",
      "Training:: Epoch 9, Iteration 100, Current loss 4.897348880767822 Accuracy 65.75186846038864\n",
      "Training:: Epoch 9, Iteration 110, Current loss 3.209176778793335 Accuracy 80.11804785295516\n",
      "Training:: Epoch 9, Iteration 120, Current loss 5.32143497467041 Accuracy 62.801402893467774\n",
      "Training:: Epoch 9, Iteration 130, Current loss 3.498595714569092 Accuracy 70.70698319182118\n",
      "Training:: Epoch 9, Iteration 140, Current loss 4.3119306564331055 Accuracy 72.65632621207688\n",
      "Training:: Epoch 9, Iteration 150, Current loss 3.570586919784546 Accuracy 78.59080004172317\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 9, Probability Accuracy 59.63065003935866\n",
      "Starting Training\n",
      "Training:: Epoch 10, Iteration 0, Current loss 3.414245128631592 Accuracy 80.64577397910732\n",
      "Training:: Epoch 10, Iteration 10, Current loss 2.8063366413116455 Accuracy 83.9262099973552\n",
      "Training:: Epoch 10, Iteration 20, Current loss 4.160850524902344 Accuracy 72.47049191512588\n",
      "Training:: Epoch 10, Iteration 30, Current loss 3.323068857192993 Accuracy 73.72028141678797\n",
      "Training:: Epoch 10, Iteration 40, Current loss 3.875584363937378 Accuracy 73.16263844670642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 10, Iteration 50, Current loss 4.4660162925720215 Accuracy 71.98132088058706\n",
      "Training:: Epoch 10, Iteration 60, Current loss 3.9149045944213867 Accuracy 73.08219407026105\n",
      "Training:: Epoch 10, Iteration 70, Current loss 3.143449306488037 Accuracy 76.67334669338678\n",
      "Training:: Epoch 10, Iteration 80, Current loss 2.9630675315856934 Accuracy 82.8207171314741\n",
      "Training:: Epoch 10, Iteration 90, Current loss 4.375743389129639 Accuracy 69.70775095298602\n",
      "Training:: Epoch 10, Iteration 100, Current loss 3.856971502304077 Accuracy 73.22700296735906\n",
      "Training:: Epoch 10, Iteration 110, Current loss 3.360086441040039 Accuracy 79.7449108877439\n",
      "Training:: Epoch 10, Iteration 120, Current loss 3.3046953678131104 Accuracy 80.80345106497708\n",
      "Training:: Epoch 10, Iteration 130, Current loss 5.9185943603515625 Accuracy 59.94313430339522\n",
      "Training:: Epoch 10, Iteration 140, Current loss 3.0173850059509277 Accuracy 80.06136650046572\n",
      "Training:: Epoch 10, Iteration 150, Current loss 4.057679176330566 Accuracy 70.43693212185997\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 10, Probability Accuracy 56.6518208559473\n",
      "Starting Training\n",
      "Training:: Epoch 11, Iteration 0, Current loss 4.933083534240723 Accuracy 61.53846153846154\n",
      "Training:: Epoch 11, Iteration 10, Current loss 3.2213077545166016 Accuracy 67.34663191659983\n",
      "Training:: Epoch 11, Iteration 20, Current loss 2.937201738357544 Accuracy 82.61251372118551\n",
      "Training:: Epoch 11, Iteration 30, Current loss 3.6666064262390137 Accuracy 72.63117184478536\n",
      "Training:: Epoch 11, Iteration 40, Current loss 2.953731060028076 Accuracy 80.99438231742532\n",
      "Training:: Epoch 11, Iteration 50, Current loss 7.851138114929199 Accuracy 59.426907550790816\n",
      "Training:: Epoch 11, Iteration 60, Current loss 3.4585859775543213 Accuracy 73.76626354100992\n",
      "Training:: Epoch 11, Iteration 70, Current loss 3.692903757095337 Accuracy 78.59836398445205\n",
      "Training:: Epoch 11, Iteration 80, Current loss 2.9486775398254395 Accuracy 80.87779790777863\n",
      "Training:: Epoch 11, Iteration 90, Current loss 4.767363548278809 Accuracy 68.89791593076652\n",
      "Training:: Epoch 11, Iteration 100, Current loss 3.7675867080688477 Accuracy 78.81669020971422\n",
      "Training:: Epoch 11, Iteration 110, Current loss 2.4102203845977783 Accuracy 86.42873042962084\n",
      "Training:: Epoch 11, Iteration 120, Current loss 4.111140251159668 Accuracy 78.93141608514837\n",
      "Training:: Epoch 11, Iteration 130, Current loss 3.3865444660186768 Accuracy 71.64312078401426\n",
      "Training:: Epoch 11, Iteration 140, Current loss 4.608407020568848 Accuracy 71.37454201631013\n",
      "Training:: Epoch 11, Iteration 150, Current loss 5.449453830718994 Accuracy 63.30994835817987\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 11, Probability Accuracy 53.606185524298795\n",
      "Starting Training\n",
      "Training:: Epoch 12, Iteration 0, Current loss 3.0853023529052734 Accuracy 79.52562094428284\n",
      "Training:: Epoch 12, Iteration 10, Current loss 3.986044406890869 Accuracy 71.78254832852718\n",
      "Training:: Epoch 12, Iteration 20, Current loss 4.231564998626709 Accuracy 67.12619300106044\n",
      "Training:: Epoch 12, Iteration 30, Current loss 2.39211368560791 Accuracy 83.43362509117432\n",
      "Training:: Epoch 12, Iteration 40, Current loss 3.3715410232543945 Accuracy 78.45766974015088\n",
      "Training:: Epoch 12, Iteration 50, Current loss 3.552903175354004 Accuracy 70.53817271589487\n",
      "Training:: Epoch 12, Iteration 60, Current loss 3.571977138519287 Accuracy 80.47018168490497\n",
      "Training:: Epoch 12, Iteration 70, Current loss 4.410235404968262 Accuracy 71.15409166347504\n",
      "Training:: Epoch 12, Iteration 80, Current loss 3.5695793628692627 Accuracy 77.30053766763488\n",
      "Training:: Epoch 12, Iteration 90, Current loss 5.08102560043335 Accuracy 69.6510999293051\n",
      "Training:: Epoch 12, Iteration 100, Current loss 8.369621276855469 Accuracy 48.865300146412885\n",
      "Training:: Epoch 12, Iteration 110, Current loss 3.4566221237182617 Accuracy 78.78363569950079\n",
      "Training:: Epoch 12, Iteration 120, Current loss 3.0488860607147217 Accuracy 80.42970574497897\n",
      "Training:: Epoch 12, Iteration 130, Current loss 3.8128914833068848 Accuracy 79.08051568961324\n",
      "Training:: Epoch 12, Iteration 140, Current loss 5.693077087402344 Accuracy 60.91259145167501\n",
      "Training:: Epoch 12, Iteration 150, Current loss 4.34245491027832 Accuracy 71.27292940522513\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 12, Probability Accuracy 62.64583419646186\n",
      "Starting Training\n",
      "Training:: Epoch 13, Iteration 0, Current loss 3.2760722637176514 Accuracy 81.44032327897243\n",
      "Training:: Epoch 13, Iteration 10, Current loss 2.95914363861084 Accuracy 84.06017579445572\n",
      "Training:: Epoch 13, Iteration 20, Current loss 3.3074843883514404 Accuracy 78.45089613248088\n",
      "Training:: Epoch 13, Iteration 30, Current loss 4.045900344848633 Accuracy 77.07421669875723\n",
      "Training:: Epoch 13, Iteration 40, Current loss 3.2789390087127686 Accuracy 80.10457165839925\n",
      "Training:: Epoch 13, Iteration 50, Current loss 4.448359489440918 Accuracy 71.52385838891739\n",
      "Training:: Epoch 13, Iteration 60, Current loss 3.879920482635498 Accuracy 79.74538144921988\n",
      "Training:: Epoch 13, Iteration 70, Current loss 4.63723611831665 Accuracy 66.79121479996354\n",
      "Training:: Epoch 13, Iteration 80, Current loss 2.636255979537964 Accuracy 85.13226491600695\n",
      "Training:: Epoch 13, Iteration 90, Current loss 3.1581337451934814 Accuracy 78.77514259981987\n",
      "Training:: Epoch 13, Iteration 100, Current loss 3.4326560497283936 Accuracy 81.42458100558659\n",
      "Training:: Epoch 13, Iteration 110, Current loss 3.4407436847686768 Accuracy 76.31312666727992\n",
      "Training:: Epoch 13, Iteration 120, Current loss 2.9953911304473877 Accuracy 84.88267516965088\n",
      "Training:: Epoch 13, Iteration 130, Current loss 3.5268239974975586 Accuracy 79.92959399202066\n",
      "Training:: Epoch 13, Iteration 140, Current loss 3.984748125076294 Accuracy 70.18171131899888\n",
      "Training:: Epoch 13, Iteration 150, Current loss 2.302746057510376 Accuracy 85.31332607681543\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 13, Probability Accuracy 62.167315739321374\n",
      "Starting Training\n",
      "Training:: Epoch 14, Iteration 0, Current loss 3.537923812866211 Accuracy 73.35403726708074\n",
      "Training:: Epoch 14, Iteration 10, Current loss 4.1693854331970215 Accuracy 68.51246775580395\n",
      "Training:: Epoch 14, Iteration 20, Current loss 2.602438449859619 Accuracy 85.63945875163684\n",
      "Training:: Epoch 14, Iteration 30, Current loss 2.2211852073669434 Accuracy 87.24229710831416\n",
      "Training:: Epoch 14, Iteration 40, Current loss 3.1494991779327393 Accuracy 84.83449409186947\n",
      "Training:: Epoch 14, Iteration 50, Current loss 2.7886245250701904 Accuracy 86.59256468224065\n",
      "Training:: Epoch 14, Iteration 60, Current loss 2.887805938720703 Accuracy 82.36414237586055\n",
      "Training:: Epoch 14, Iteration 70, Current loss 2.641864061355591 Accuracy 83.01067486882576\n",
      "Training:: Epoch 14, Iteration 80, Current loss 3.022228717803955 Accuracy 79.14014990807523\n",
      "Training:: Epoch 14, Iteration 90, Current loss 2.3007404804229736 Accuracy 88.17994505494505\n",
      "Training:: Epoch 14, Iteration 100, Current loss 2.5704729557037354 Accuracy 85.34311887287727\n",
      "Training:: Epoch 14, Iteration 110, Current loss 3.4895200729370117 Accuracy 79.73923393204934\n",
      "Training:: Epoch 14, Iteration 120, Current loss 2.585336208343506 Accuracy 85.50160717306716\n",
      "Training:: Epoch 14, Iteration 130, Current loss 3.2750537395477295 Accuracy 82.62160310573191\n",
      "Training:: Epoch 14, Iteration 140, Current loss 2.7225747108459473 Accuracy 82.53822429293261\n",
      "Training:: Epoch 14, Iteration 150, Current loss 3.0799362659454346 Accuracy 83.8832972353166\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 14, Probability Accuracy 66.228818825869\n",
      "Starting Training\n",
      "Training:: Epoch 15, Iteration 0, Current loss 1.5503920316696167 Accuracy 91.12014329016257\n",
      "Training:: Epoch 15, Iteration 10, Current loss 2.3900132179260254 Accuracy 85.85606168784807\n",
      "Training:: Epoch 15, Iteration 20, Current loss 2.49558687210083 Accuracy 88.37604658081517\n",
      "Training:: Epoch 15, Iteration 30, Current loss 2.126387596130371 Accuracy 88.5213335636552\n",
      "Training:: Epoch 15, Iteration 40, Current loss 2.3994498252868652 Accuracy 86.25984251968504\n",
      "Training:: Epoch 15, Iteration 50, Current loss 2.537182569503784 Accuracy 87.05068005181347\n",
      "Training:: Epoch 15, Iteration 60, Current loss 2.40765380859375 Accuracy 88.03363518758086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 15, Iteration 70, Current loss 2.04842209815979 Accuracy 88.16414299793925\n",
      "Training:: Epoch 15, Iteration 80, Current loss 2.523913860321045 Accuracy 85.51383135106241\n",
      "Training:: Epoch 15, Iteration 90, Current loss 2.3061838150024414 Accuracy 88.28726861148637\n",
      "Training:: Epoch 15, Iteration 100, Current loss 2.7886152267456055 Accuracy 85.71087501492181\n",
      "Training:: Epoch 15, Iteration 110, Current loss 2.063171148300171 Accuracy 89.68286237040049\n",
      "Training:: Epoch 15, Iteration 120, Current loss 2.7274885177612305 Accuracy 86.06392694063926\n",
      "Training:: Epoch 15, Iteration 130, Current loss 2.3826112747192383 Accuracy 86.21142481943532\n",
      "Training:: Epoch 15, Iteration 140, Current loss 2.636629104614258 Accuracy 80.96756307180482\n",
      "Training:: Epoch 15, Iteration 150, Current loss 3.5015339851379395 Accuracy 79.09604519774011\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 63.99977213406803\n",
      "Starting Training\n",
      "Training:: Epoch 16, Iteration 0, Current loss 2.3168563842773438 Accuracy 88.73554838037704\n",
      "Training:: Epoch 16, Iteration 10, Current loss 2.890742540359497 Accuracy 78.78486954951701\n",
      "Training:: Epoch 16, Iteration 20, Current loss 2.1269569396972656 Accuracy 88.18824537599086\n",
      "Training:: Epoch 16, Iteration 30, Current loss 2.27755069732666 Accuracy 88.31929258529007\n",
      "Training:: Epoch 16, Iteration 40, Current loss 2.4172916412353516 Accuracy 87.25800908778494\n",
      "Training:: Epoch 16, Iteration 50, Current loss 1.936059594154358 Accuracy 89.48486909214553\n",
      "Training:: Epoch 16, Iteration 60, Current loss 1.982704997062683 Accuracy 90.7360818049018\n",
      "Training:: Epoch 16, Iteration 70, Current loss 2.2042787075042725 Accuracy 87.99182689788861\n",
      "Training:: Epoch 16, Iteration 80, Current loss 1.7353334426879883 Accuracy 91.29969929836285\n",
      "Training:: Epoch 16, Iteration 90, Current loss 2.6503026485443115 Accuracy 84.17546807489198\n",
      "Training:: Epoch 16, Iteration 100, Current loss 3.083956003189087 Accuracy 81.4127892911862\n",
      "Training:: Epoch 16, Iteration 110, Current loss 2.248007297515869 Accuracy 86.27582249426167\n",
      "Training:: Epoch 16, Iteration 120, Current loss 4.144292831420898 Accuracy 73.07302231237323\n",
      "Training:: Epoch 16, Iteration 130, Current loss 2.488741397857666 Accuracy 84.31585344677896\n",
      "Training:: Epoch 16, Iteration 140, Current loss 2.71231746673584 Accuracy 85.48741217798595\n",
      "Training:: Epoch 16, Iteration 150, Current loss 2.0548086166381836 Accuracy 91.1374254128313\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 16, Probability Accuracy 64.85903384844843\n",
      "Starting Training\n",
      "Training:: Epoch 17, Iteration 0, Current loss 2.1521782875061035 Accuracy 88.25334937160396\n",
      "Training:: Epoch 17, Iteration 10, Current loss 3.3905935287475586 Accuracy 83.6719223771983\n",
      "Training:: Epoch 17, Iteration 20, Current loss 1.7796543836593628 Accuracy 88.71259289013858\n",
      "Training:: Epoch 17, Iteration 30, Current loss 2.6826677322387695 Accuracy 83.13375969519663\n",
      "Training:: Epoch 17, Iteration 40, Current loss 2.6598005294799805 Accuracy 85.76434311343674\n",
      "Training:: Epoch 17, Iteration 50, Current loss 1.970056176185608 Accuracy 90.28059583312714\n",
      "Training:: Epoch 17, Iteration 60, Current loss 1.6605873107910156 Accuracy 93.26898814029775\n",
      "Training:: Epoch 17, Iteration 70, Current loss 2.605670928955078 Accuracy 88.60502838605028\n",
      "Training:: Epoch 17, Iteration 80, Current loss 2.1955313682556152 Accuracy 90.88588007736944\n",
      "Training:: Epoch 17, Iteration 90, Current loss 2.2613942623138428 Accuracy 88.88425732944282\n",
      "Training:: Epoch 17, Iteration 100, Current loss 2.6583335399627686 Accuracy 86.58250676284942\n",
      "Training:: Epoch 17, Iteration 110, Current loss 2.1499345302581787 Accuracy 91.26073811009262\n",
      "Training:: Epoch 17, Iteration 120, Current loss 2.3082187175750732 Accuracy 90.40890957446808\n",
      "Training:: Epoch 17, Iteration 130, Current loss 2.627617597579956 Accuracy 84.44907110826394\n",
      "Training:: Epoch 17, Iteration 140, Current loss 1.9294447898864746 Accuracy 92.79872331936964\n",
      "Training:: Epoch 17, Iteration 150, Current loss 2.0113370418548584 Accuracy 91.85344386449698\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 17, Probability Accuracy 66.59972241786468\n",
      "Starting Training\n",
      "Training:: Epoch 18, Iteration 0, Current loss 1.9332644939422607 Accuracy 90.3720696824145\n",
      "Training:: Epoch 18, Iteration 10, Current loss 2.310049057006836 Accuracy 87.30004323389538\n",
      "Training:: Epoch 18, Iteration 20, Current loss 1.491390347480774 Accuracy 91.25750786538278\n",
      "Training:: Epoch 18, Iteration 30, Current loss 1.8083856105804443 Accuracy 91.57501697216564\n",
      "Training:: Epoch 18, Iteration 40, Current loss 2.3707685470581055 Accuracy 91.57379263762242\n",
      "Training:: Epoch 18, Iteration 50, Current loss 2.251145839691162 Accuracy 88.54291280719144\n",
      "Training:: Epoch 18, Iteration 60, Current loss 1.933438777923584 Accuracy 92.03552231829867\n",
      "Training:: Epoch 18, Iteration 70, Current loss 2.8363120555877686 Accuracy 89.61722488038278\n",
      "Training:: Epoch 18, Iteration 80, Current loss 2.361410140991211 Accuracy 87.25112395632627\n",
      "Training:: Epoch 18, Iteration 90, Current loss 2.68605637550354 Accuracy 82.1344493852995\n",
      "Training:: Epoch 18, Iteration 100, Current loss 3.448685884475708 Accuracy 78.75588981515041\n",
      "Training:: Epoch 18, Iteration 110, Current loss 2.3790714740753174 Accuracy 85.98888636879111\n",
      "Training:: Epoch 18, Iteration 120, Current loss 2.522300958633423 Accuracy 87.28127259580621\n",
      "Training:: Epoch 18, Iteration 130, Current loss 1.4991037845611572 Accuracy 93.45309381237524\n",
      "Training:: Epoch 18, Iteration 140, Current loss 3.302898406982422 Accuracy 84.3312597200622\n",
      "Training:: Epoch 18, Iteration 150, Current loss 2.3679823875427246 Accuracy 87.92095416276894\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 18, Probability Accuracy 68.0733935451796\n",
      "Starting Training\n",
      "Training:: Epoch 19, Iteration 0, Current loss 1.5580674409866333 Accuracy 92.54505886948273\n",
      "Training:: Epoch 19, Iteration 10, Current loss 2.6504828929901123 Accuracy 85.52560159218382\n",
      "Training:: Epoch 19, Iteration 20, Current loss 1.5074437856674194 Accuracy 94.20962885027029\n",
      "Training:: Epoch 19, Iteration 30, Current loss 2.299132823944092 Accuracy 89.6490963097216\n",
      "Training:: Epoch 19, Iteration 40, Current loss 2.048813819885254 Accuracy 88.36885509264218\n",
      "Training:: Epoch 19, Iteration 50, Current loss 2.3772666454315186 Accuracy 87.65103225192655\n",
      "Training:: Epoch 19, Iteration 60, Current loss 2.3260107040405273 Accuracy 87.97479683528444\n",
      "Training:: Epoch 19, Iteration 70, Current loss 1.8366559743881226 Accuracy 90.76556110858701\n",
      "Training:: Epoch 19, Iteration 80, Current loss 3.2328081130981445 Accuracy 83.02200850749028\n",
      "Training:: Epoch 19, Iteration 90, Current loss 2.7302489280700684 Accuracy 87.73013469355224\n",
      "Training:: Epoch 19, Iteration 100, Current loss 2.0872299671173096 Accuracy 89.39796822799482\n",
      "Training:: Epoch 19, Iteration 110, Current loss 3.1159541606903076 Accuracy 85.07083950095158\n",
      "Training:: Epoch 19, Iteration 120, Current loss 4.17674446105957 Accuracy 76.29264861062258\n",
      "Training:: Epoch 19, Iteration 130, Current loss 4.649946689605713 Accuracy 66.68168252716933\n",
      "Training:: Epoch 19, Iteration 140, Current loss 5.545023441314697 Accuracy 59.5685177786656\n",
      "Training:: Epoch 19, Iteration 150, Current loss 2.592459201812744 Accuracy 85.22419720576403\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 19, Probability Accuracy 63.4981563574595\n",
      "Starting Training\n",
      "Training:: Epoch 20, Iteration 0, Current loss 3.1526665687561035 Accuracy 79.25058548009368\n",
      "Training:: Epoch 20, Iteration 10, Current loss 2.4397780895233154 Accuracy 87.87666433076384\n",
      "Training:: Epoch 20, Iteration 150, Current loss 3.4945449829101562 Accuracy 79.45548092039445\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 20, Probability Accuracy 61.007063843891125\n",
      "Starting Training\n",
      "Training:: Epoch 21, Iteration 0, Current loss 2.9263503551483154 Accuracy 85.5900136532085\n",
      "Training:: Epoch 21, Iteration 10, Current loss 2.6951138973236084 Accuracy 89.195599515543\n",
      "Training:: Epoch 21, Iteration 20, Current loss 1.7067723274230957 Accuracy 93.28058379796654\n",
      "Training:: Epoch 21, Iteration 30, Current loss 2.64640736579895 Accuracy 85.36005785721666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 21, Iteration 40, Current loss 1.5087285041809082 Accuracy 93.49525571402995\n",
      "Training:: Epoch 21, Iteration 50, Current loss 2.181457042694092 Accuracy 91.50714158715107\n",
      "Training:: Epoch 21, Iteration 60, Current loss 2.300534248352051 Accuracy 90.20157839688125\n",
      "Training:: Epoch 21, Iteration 70, Current loss 1.9381202459335327 Accuracy 90.81270059605686\n",
      "Training:: Epoch 21, Iteration 80, Current loss 1.975787878036499 Accuracy 91.81028724114897\n",
      "Training:: Epoch 21, Iteration 90, Current loss 3.4070851802825928 Accuracy 77.56653992395437\n",
      "Training:: Epoch 21, Iteration 100, Current loss 2.3476622104644775 Accuracy 86.30280080955801\n",
      "Training:: Epoch 21, Iteration 110, Current loss 3.3066775798797607 Accuracy 80.07033997655334\n",
      "Training:: Epoch 21, Iteration 120, Current loss 2.1751110553741455 Accuracy 89.25819672131148\n",
      "Training:: Epoch 21, Iteration 130, Current loss 2.0425570011138916 Accuracy 92.74444030314325\n",
      "Training:: Epoch 21, Iteration 140, Current loss 1.6102135181427002 Accuracy 93.10165375274661\n",
      "Training:: Epoch 21, Iteration 150, Current loss 3.5037381649017334 Accuracy 81.07012750455374\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 21, Probability Accuracy 68.93400174006712\n",
      "Starting Training\n",
      "Training:: Epoch 22, Iteration 0, Current loss 1.7655810117721558 Accuracy 92.65770875141786\n",
      "Training:: Epoch 22, Iteration 10, Current loss 1.7468760013580322 Accuracy 93.02020202020202\n",
      "Training:: Epoch 22, Iteration 20, Current loss 2.2474308013916016 Accuracy 88.85055783910745\n",
      "Training:: Epoch 22, Iteration 30, Current loss 2.157100200653076 Accuracy 91.68534116374315\n",
      "Training:: Epoch 22, Iteration 40, Current loss 2.409989356994629 Accuracy 90.64672943203495\n",
      "Training:: Epoch 22, Iteration 50, Current loss 2.05137300491333 Accuracy 92.04446028711234\n",
      "Training:: Epoch 22, Iteration 60, Current loss 1.763221025466919 Accuracy 92.67047161309702\n",
      "Training:: Epoch 22, Iteration 70, Current loss 1.424781322479248 Accuracy 94.25274785366653\n",
      "Training:: Epoch 22, Iteration 80, Current loss 2.183467388153076 Accuracy 90.56491575817641\n",
      "Training:: Epoch 22, Iteration 90, Current loss 1.4385573863983154 Accuracy 94.58751636839808\n",
      "Training:: Epoch 22, Iteration 100, Current loss 1.6204345226287842 Accuracy 92.47691640329677\n",
      "Training:: Epoch 22, Iteration 110, Current loss 2.6712839603424072 Accuracy 88.02839835862697\n",
      "Training:: Epoch 22, Iteration 120, Current loss 2.214538097381592 Accuracy 89.94509546832747\n",
      "Training:: Epoch 22, Iteration 130, Current loss 1.8778066635131836 Accuracy 90.59279024747553\n",
      "Training:: Epoch 22, Iteration 140, Current loss 1.8104939460754395 Accuracy 92.48345555356823\n",
      "Training:: Epoch 22, Iteration 150, Current loss 1.7868314981460571 Accuracy 90.45285469681762\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 22, Probability Accuracy 64.97855988730994\n",
      "Starting Training\n",
      "Training:: Epoch 23, Iteration 0, Current loss 1.6512278318405151 Accuracy 94.27362238728624\n",
      "Training:: Epoch 23, Iteration 10, Current loss 1.9119588136672974 Accuracy 92.3274031773922\n",
      "Training:: Epoch 23, Iteration 20, Current loss 1.7460665702819824 Accuracy 92.55740841990159\n",
      "Training:: Epoch 23, Iteration 30, Current loss 1.930018663406372 Accuracy 90.16045131511466\n",
      "Training:: Epoch 23, Iteration 40, Current loss 1.6771039962768555 Accuracy 93.85397901875596\n",
      "Training:: Epoch 23, Iteration 50, Current loss 1.7832746505737305 Accuracy 90.54235984229253\n",
      "Training:: Epoch 23, Iteration 60, Current loss 1.7345845699310303 Accuracy 93.59235668789809\n",
      "Training:: Epoch 23, Iteration 70, Current loss 1.6293364763259888 Accuracy 94.71533056073426\n",
      "Training:: Epoch 23, Iteration 80, Current loss 1.2937064170837402 Accuracy 95.62424099972579\n",
      "Training:: Epoch 23, Iteration 90, Current loss 1.5112193822860718 Accuracy 94.67556311853477\n",
      "Training:: Epoch 23, Iteration 100, Current loss 2.2569868564605713 Accuracy 88.998414558489\n",
      "Training:: Epoch 23, Iteration 110, Current loss 2.1910998821258545 Accuracy 91.20025119711123\n",
      "Training:: Epoch 23, Iteration 120, Current loss 1.8091782331466675 Accuracy 92.31681602530311\n",
      "Training:: Epoch 23, Iteration 130, Current loss 1.3737636804580688 Accuracy 93.49809031678275\n",
      "Training:: Epoch 23, Iteration 140, Current loss 1.6482867002487183 Accuracy 92.864783910997\n",
      "Training:: Epoch 23, Iteration 150, Current loss 2.5772008895874023 Accuracy 84.00187295145935\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 23, Probability Accuracy 61.2182541326594\n",
      "Starting Training\n",
      "Training:: Epoch 24, Iteration 0, Current loss 1.7722079753875732 Accuracy 94.33698187690652\n",
      "Training:: Epoch 24, Iteration 10, Current loss 2.98968505859375 Accuracy 82.13144365707934\n",
      "Training:: Epoch 24, Iteration 20, Current loss 1.6068257093429565 Accuracy 94.00453999198825\n",
      "Training:: Epoch 24, Iteration 30, Current loss 2.352083206176758 Accuracy 90.3860816524572\n",
      "Training:: Epoch 24, Iteration 40, Current loss 2.558699369430542 Accuracy 81.68563300142247\n",
      "Training:: Epoch 24, Iteration 50, Current loss 1.5953748226165771 Accuracy 94.01755626742106\n",
      "Training:: Epoch 24, Iteration 60, Current loss 2.2239527702331543 Accuracy 89.21315777566626\n",
      "Training:: Epoch 24, Iteration 70, Current loss 2.6029815673828125 Accuracy 83.71086790531942\n",
      "Training:: Epoch 24, Iteration 80, Current loss 1.926507592201233 Accuracy 89.44962194478636\n",
      "Training:: Epoch 24, Iteration 90, Current loss 1.6971759796142578 Accuracy 93.67258761401824\n",
      "Training:: Epoch 24, Iteration 100, Current loss 2.1101293563842773 Accuracy 90.68754254594963\n",
      "Training:: Epoch 24, Iteration 110, Current loss 2.6086513996124268 Accuracy 85.55661274976214\n",
      "Training:: Epoch 24, Iteration 120, Current loss 1.9649015665054321 Accuracy 91.02801431568555\n",
      "Training:: Epoch 24, Iteration 130, Current loss 2.019801616668701 Accuracy 91.88376753507013\n",
      "Training:: Epoch 24, Iteration 140, Current loss 1.4286541938781738 Accuracy 94.3813749783899\n",
      "Training:: Epoch 24, Iteration 150, Current loss 1.547115445137024 Accuracy 93.15793089235419\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 24, Probability Accuracy 66.9160417616108\n",
      "Starting Training\n",
      "Training:: Epoch 25, Iteration 0, Current loss 1.0968166589736938 Accuracy 96.5233049380151\n",
      "Training:: Epoch 25, Iteration 10, Current loss 2.1455321311950684 Accuracy 91.75479476608712\n",
      "Training:: Epoch 25, Iteration 20, Current loss 1.5129611492156982 Accuracy 92.94524753027851\n",
      "Training:: Epoch 25, Iteration 30, Current loss 1.86979341506958 Accuracy 93.16929133858268\n",
      "Training:: Epoch 25, Iteration 40, Current loss 1.7295113801956177 Accuracy 92.78337037521244\n",
      "Training:: Epoch 25, Iteration 50, Current loss 1.6604567766189575 Accuracy 92.37214677838774\n",
      "Training:: Epoch 25, Iteration 60, Current loss 1.523918867111206 Accuracy 93.17443120260022\n",
      "Training:: Epoch 25, Iteration 70, Current loss 1.4154882431030273 Accuracy 94.23819423819424\n",
      "Training:: Epoch 25, Iteration 80, Current loss 1.6573843955993652 Accuracy 94.89784710017575\n",
      "Training:: Epoch 25, Iteration 90, Current loss 1.7776793241500854 Accuracy 91.90185532120626\n",
      "Training:: Epoch 25, Iteration 100, Current loss 1.759878158569336 Accuracy 93.22415398944428\n",
      "Training:: Epoch 25, Iteration 110, Current loss 2.1922545433044434 Accuracy 91.37372620270163\n",
      "Training:: Epoch 25, Iteration 120, Current loss 1.6177881956100464 Accuracy 95.50187000909735\n",
      "Training:: Epoch 25, Iteration 130, Current loss 2.3713185787200928 Accuracy 87.24557229711868\n",
      "Training:: Epoch 25, Iteration 140, Current loss 3.2757115364074707 Accuracy 82.35781336278299\n",
      "Training:: Epoch 25, Iteration 150, Current loss 2.673060178756714 Accuracy 87.90412191121223\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 25, Probability Accuracy 63.95171313750673\n",
      "Starting Training\n",
      "Training:: Epoch 26, Iteration 0, Current loss 2.511725902557373 Accuracy 85.69502234550778\n",
      "Training:: Epoch 26, Iteration 10, Current loss 2.1992692947387695 Accuracy 86.22837877455292\n",
      "Training:: Epoch 26, Iteration 20, Current loss 2.1031880378723145 Accuracy 90.64122351620598\n",
      "Training:: Epoch 26, Iteration 30, Current loss 1.4991424083709717 Accuracy 92.95635411002917\n",
      "Training:: Epoch 26, Iteration 40, Current loss 2.1524078845977783 Accuracy 91.82379625075149\n",
      "Training:: Epoch 26, Iteration 50, Current loss 1.8472981452941895 Accuracy 91.89358372456964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 26, Iteration 60, Current loss 1.9523587226867676 Accuracy 89.57998816868081\n",
      "Training:: Epoch 26, Iteration 70, Current loss 1.5693767070770264 Accuracy 93.49499296532318\n",
      "Training:: Epoch 26, Iteration 80, Current loss 1.598554253578186 Accuracy 92.91808364656046\n",
      "Training:: Epoch 26, Iteration 90, Current loss 1.884065866470337 Accuracy 91.54382067851373\n",
      "Training:: Epoch 26, Iteration 100, Current loss 2.4044203758239746 Accuracy 87.92894017613119\n",
      "Training:: Epoch 26, Iteration 110, Current loss 2.097999095916748 Accuracy 90.333787765513\n",
      "Training:: Epoch 26, Iteration 120, Current loss 1.2150784730911255 Accuracy 94.38567422890645\n",
      "Training:: Epoch 26, Iteration 130, Current loss 1.6684937477111816 Accuracy 91.92961566448369\n",
      "Training:: Epoch 26, Iteration 140, Current loss 1.902451515197754 Accuracy 93.13636774952494\n",
      "Training:: Epoch 26, Iteration 150, Current loss 2.013932943344116 Accuracy 92.36396432508268\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 26, Probability Accuracy 66.86487550234081\n",
      "Starting Training\n",
      "Training:: Epoch 27, Iteration 0, Current loss 1.5537488460540771 Accuracy 95.28916124090387\n",
      "Training:: Epoch 27, Iteration 10, Current loss 1.8586010932922363 Accuracy 93.2559238506717\n",
      "Training:: Epoch 27, Iteration 20, Current loss 1.3609815835952759 Accuracy 93.89918655820776\n",
      "Training:: Epoch 27, Iteration 30, Current loss 1.5919889211654663 Accuracy 94.31747992588018\n",
      "Training:: Epoch 27, Iteration 40, Current loss 1.3976454734802246 Accuracy 94.97320860957224\n",
      "Training:: Epoch 27, Iteration 50, Current loss 1.5581992864608765 Accuracy 93.45582163501238\n",
      "Training:: Epoch 27, Iteration 60, Current loss 1.6477605104446411 Accuracy 93.99310684391925\n",
      "Training:: Epoch 27, Iteration 70, Current loss 1.365018606185913 Accuracy 96.04493971501644\n",
      "Training:: Epoch 27, Iteration 80, Current loss 1.681636929512024 Accuracy 94.08522180418234\n",
      "Training:: Epoch 27, Iteration 90, Current loss 1.7339569330215454 Accuracy 94.51014863353045\n",
      "Training:: Epoch 27, Iteration 100, Current loss 1.2981520891189575 Accuracy 94.68958543983823\n",
      "Training:: Epoch 27, Iteration 110, Current loss 1.4249389171600342 Accuracy 95.74111568805418\n",
      "Training:: Epoch 27, Iteration 120, Current loss 1.1296969652175903 Accuracy 96.23746967547032\n",
      "Training:: Epoch 27, Iteration 130, Current loss 1.594833254814148 Accuracy 95.24322287129513\n",
      "Training:: Epoch 27, Iteration 140, Current loss 1.8353623151779175 Accuracy 93.20346691839012\n",
      "Training:: Epoch 27, Iteration 150, Current loss 1.6627193689346313 Accuracy 93.09100283578242\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 27, Probability Accuracy 67.34225462982144\n",
      "Starting Training\n",
      "Training:: Epoch 28, Iteration 0, Current loss 1.2899478673934937 Accuracy 94.83355116475744\n",
      "Training:: Epoch 28, Iteration 10, Current loss 1.336613655090332 Accuracy 96.92544196771713\n",
      "Training:: Epoch 28, Iteration 20, Current loss 1.02546226978302 Accuracy 96.21577502411516\n",
      "Training:: Epoch 28, Iteration 30, Current loss 1.3156726360321045 Accuracy 95.93354525280394\n",
      "Training:: Epoch 28, Iteration 40, Current loss 1.486676573753357 Accuracy 94.78377173428444\n",
      "Training:: Epoch 28, Iteration 50, Current loss 1.2540711164474487 Accuracy 96.10321680760178\n",
      "Training:: Epoch 28, Iteration 60, Current loss 1.3065886497497559 Accuracy 95.1092611862643\n",
      "Training:: Epoch 28, Iteration 70, Current loss 1.077940821647644 Accuracy 95.27127924340468\n",
      "Training:: Epoch 28, Iteration 80, Current loss 1.3898624181747437 Accuracy 94.84719946013689\n",
      "Training:: Epoch 28, Iteration 90, Current loss 1.6467357873916626 Accuracy 93.51810460438087\n",
      "Training:: Epoch 28, Iteration 100, Current loss 1.1212071180343628 Accuracy 96.38704318936877\n",
      "Training:: Epoch 28, Iteration 110, Current loss 1.307342290878296 Accuracy 95.34385451505017\n",
      "Training:: Epoch 28, Iteration 120, Current loss 1.2494271993637085 Accuracy 95.47106219390173\n",
      "Training:: Epoch 28, Iteration 130, Current loss 1.6888478994369507 Accuracy 94.68098874077909\n",
      "Training:: Epoch 28, Iteration 140, Current loss 1.418259859085083 Accuracy 95.9294532627866\n",
      "Training:: Epoch 28, Iteration 150, Current loss 1.4802144765853882 Accuracy 94.66279785456784\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 28, Probability Accuracy 66.78916186767204\n",
      "Starting Training\n",
      "Training:: Epoch 29, Iteration 0, Current loss 1.8762086629867554 Accuracy 89.36784626693269\n",
      "Training:: Epoch 29, Iteration 10, Current loss 1.4953172206878662 Accuracy 94.770819883796\n",
      "Training:: Epoch 29, Iteration 20, Current loss 1.472141146659851 Accuracy 94.53384207033842\n",
      "Training:: Epoch 29, Iteration 30, Current loss 1.196527123451233 Accuracy 96.00406120820944\n",
      "Training:: Epoch 29, Iteration 40, Current loss 1.4832745790481567 Accuracy 94.5077149155033\n",
      "Training:: Epoch 29, Iteration 50, Current loss 1.2992711067199707 Accuracy 95.95075874883865\n",
      "Training:: Epoch 29, Iteration 60, Current loss 1.109494686126709 Accuracy 96.18548534519529\n",
      "Training:: Epoch 29, Iteration 70, Current loss 1.7740123271942139 Accuracy 93.14711359404097\n",
      "Training:: Epoch 29, Iteration 80, Current loss 1.3251739740371704 Accuracy 96.27852151216638\n",
      "Training:: Epoch 29, Iteration 90, Current loss 1.0395845174789429 Accuracy 96.5244004695623\n",
      "Training:: Epoch 29, Iteration 100, Current loss 1.029634714126587 Accuracy 96.08833432362844\n",
      "Training:: Epoch 29, Iteration 110, Current loss 1.3385287523269653 Accuracy 94.83143785540211\n",
      "Training:: Epoch 29, Iteration 120, Current loss 1.4830421209335327 Accuracy 95.08689525598872\n",
      "Training:: Epoch 29, Iteration 130, Current loss 1.390405535697937 Accuracy 94.42682126449354\n",
      "Training:: Epoch 29, Iteration 140, Current loss 1.141474962234497 Accuracy 95.5211187753945\n",
      "Training:: Epoch 29, Iteration 150, Current loss 1.1695070266723633 Accuracy 95.92092989883045\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 29, Probability Accuracy 68.10974851887144\n",
      "Starting Training\n",
      "Training:: Epoch 30, Iteration 0, Current loss 1.4626511335372925 Accuracy 93.82771619872841\n",
      "Training:: Epoch 30, Iteration 10, Current loss 1.2667440176010132 Accuracy 96.33600377002827\n",
      "Training:: Epoch 30, Iteration 20, Current loss 1.1802706718444824 Accuracy 96.7996308879661\n",
      "Training:: Epoch 30, Iteration 30, Current loss 1.5155673027038574 Accuracy 94.06469002695418\n",
      "Training:: Epoch 30, Iteration 40, Current loss 1.526698350906372 Accuracy 92.98776642703999\n",
      "Training:: Epoch 30, Iteration 50, Current loss 1.442165493965149 Accuracy 93.62478587919863\n",
      "Training:: Epoch 30, Iteration 60, Current loss 1.1446468830108643 Accuracy 95.39679372048161\n",
      "Training:: Epoch 30, Iteration 70, Current loss 1.223507046699524 Accuracy 96.08708799516178\n",
      "Training:: Epoch 30, Iteration 80, Current loss 0.9802472591400146 Accuracy 95.72661534872871\n",
      "Training:: Epoch 30, Iteration 90, Current loss 1.3549449443817139 Accuracy 94.88082901554404\n",
      "Training:: Epoch 30, Iteration 100, Current loss 1.4006303548812866 Accuracy 95.76423409499144\n",
      "Training:: Epoch 30, Iteration 110, Current loss 1.7598576545715332 Accuracy 94.59952914044106\n",
      "Training:: Epoch 30, Iteration 120, Current loss 2.6614694595336914 Accuracy 86.56449267255846\n",
      "Training:: Epoch 30, Iteration 130, Current loss 2.371763229370117 Accuracy 87.77817025785941\n",
      "Training:: Epoch 30, Iteration 140, Current loss 2.2898154258728027 Accuracy 88.45996851128346\n",
      "Training:: Epoch 30, Iteration 150, Current loss 1.9525827169418335 Accuracy 91.7625712804562\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 30, Probability Accuracy 65.86247255251274\n",
      "Starting Training\n",
      "Training:: Epoch 31, Iteration 0, Current loss 1.6387218236923218 Accuracy 93.0019305019305\n",
      "Training:: Epoch 31, Iteration 10, Current loss 1.9937463998794556 Accuracy 89.63002254342925\n",
      "Training:: Epoch 31, Iteration 20, Current loss 1.356515645980835 Accuracy 92.57621951219512\n",
      "Training:: Epoch 31, Iteration 30, Current loss 1.0456233024597168 Accuracy 95.48200048320851\n",
      "Training:: Epoch 31, Iteration 40, Current loss 2.9224512577056885 Accuracy 88.40150167950998\n",
      "Training:: Epoch 31, Iteration 50, Current loss 1.635166049003601 Accuracy 91.26425217974514\n",
      "Training:: Epoch 31, Iteration 60, Current loss 1.4403587579727173 Accuracy 92.42817135151955\n",
      "Training:: Epoch 31, Iteration 70, Current loss 3.3630590438842773 Accuracy 83.40998126383921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 31, Iteration 80, Current loss 2.0931222438812256 Accuracy 90.96611203066469\n",
      "Training:: Epoch 31, Iteration 90, Current loss 9.149024963378906 Accuracy 66.03568138035011\n",
      "Training:: Epoch 31, Iteration 100, Current loss 5.305654048919678 Accuracy 66.91842900302115\n",
      "Training:: Epoch 31, Iteration 110, Current loss 4.635118007659912 Accuracy 76.18089489316853\n",
      "Training:: Epoch 31, Iteration 120, Current loss 4.900609493255615 Accuracy 73.37612974068756\n",
      "Training:: Epoch 31, Iteration 130, Current loss 4.872030735015869 Accuracy 69.39103684541705\n",
      "Training:: Epoch 31, Iteration 140, Current loss 3.188941478729248 Accuracy 83.999101073094\n",
      "Training:: Epoch 31, Iteration 150, Current loss 3.5420823097229004 Accuracy 74.13600891861762\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 31, Probability Accuracy 54.91713966110122\n",
      "Starting Training\n",
      "Training:: Epoch 32, Iteration 0, Current loss 2.423250913619995 Accuracy 87.84213098729228\n",
      "Training:: Epoch 32, Iteration 10, Current loss 2.593733072280884 Accuracy 82.55540166204986\n",
      "Training:: Epoch 32, Iteration 20, Current loss 2.4962284564971924 Accuracy 88.58665171060011\n",
      "Training:: Epoch 32, Iteration 30, Current loss 5.11668062210083 Accuracy 78.4904821865047\n",
      "Training:: Epoch 32, Iteration 40, Current loss 1.8715870380401611 Accuracy 92.66378809266794\n",
      "Training:: Epoch 32, Iteration 50, Current loss 2.1004703044891357 Accuracy 90.75526506899055\n",
      "Training:: Epoch 32, Iteration 60, Current loss 2.2963764667510986 Accuracy 90.24268297373177\n",
      "Training:: Epoch 32, Iteration 70, Current loss 2.201402187347412 Accuracy 88.4947267497603\n",
      "Training:: Epoch 32, Iteration 80, Current loss 2.004577398300171 Accuracy 92.23617680639374\n",
      "Training:: Epoch 32, Iteration 90, Current loss 3.410869598388672 Accuracy 78.83930841596745\n",
      "Training:: Epoch 32, Iteration 100, Current loss 1.5813088417053223 Accuracy 94.44043693322341\n",
      "Training:: Epoch 32, Iteration 110, Current loss 1.8181942701339722 Accuracy 92.40154830023562\n",
      "Training:: Epoch 32, Iteration 120, Current loss 1.94865083694458 Accuracy 91.6202825860909\n",
      "Training:: Epoch 32, Iteration 130, Current loss 1.7054985761642456 Accuracy 92.3754280522748\n",
      "Training:: Epoch 32, Iteration 140, Current loss 1.455794095993042 Accuracy 95.02945127622196\n",
      "Training:: Epoch 32, Iteration 150, Current loss 1.5922985076904297 Accuracy 92.73853452820242\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 32, Probability Accuracy 65.34614906574969\n",
      "Starting Training\n",
      "Training:: Epoch 33, Iteration 0, Current loss 1.9714277982711792 Accuracy 93.92594323093097\n",
      "Training:: Epoch 33, Iteration 10, Current loss 2.5250210762023926 Accuracy 83.61843429261421\n",
      "Training:: Epoch 33, Iteration 20, Current loss 1.4776668548583984 Accuracy 93.12152501985703\n",
      "Training:: Epoch 33, Iteration 30, Current loss 2.1037726402282715 Accuracy 91.78887031287317\n",
      "Training:: Epoch 33, Iteration 40, Current loss 3.100497007369995 Accuracy 89.22766788459116\n",
      "Training:: Epoch 33, Iteration 50, Current loss 2.636868715286255 Accuracy 88.77309840996992\n",
      "Training:: Epoch 33, Iteration 60, Current loss 1.5464328527450562 Accuracy 92.41364232619152\n",
      "Training:: Epoch 33, Iteration 70, Current loss 2.118788480758667 Accuracy 92.4312318469161\n",
      "Training:: Epoch 33, Iteration 80, Current loss 1.299721121788025 Accuracy 95.63597386688444\n",
      "Training:: Epoch 33, Iteration 90, Current loss 1.5418055057525635 Accuracy 95.06558825528444\n",
      "Training:: Epoch 33, Iteration 100, Current loss 1.6544814109802246 Accuracy 94.37778359968561\n",
      "Training:: Epoch 33, Iteration 110, Current loss 1.447766900062561 Accuracy 95.52444694221631\n",
      "Training:: Epoch 33, Iteration 120, Current loss 1.5432988405227661 Accuracy 94.57856840321898\n",
      "Training:: Epoch 33, Iteration 130, Current loss 1.270568609237671 Accuracy 95.23648648648648\n",
      "Training:: Epoch 33, Iteration 140, Current loss 1.5857614278793335 Accuracy 95.18105960793707\n",
      "Training:: Epoch 33, Iteration 150, Current loss 1.4171770811080933 Accuracy 96.34564643799472\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 33, Probability Accuracy 67.8356879479637\n",
      "Starting Training\n",
      "Training:: Epoch 34, Iteration 0, Current loss 1.556925892829895 Accuracy 95.47411730934839\n",
      "Training:: Epoch 34, Iteration 10, Current loss 1.1730194091796875 Accuracy 96.26701231367466\n",
      "Training:: Epoch 34, Iteration 20, Current loss 1.2126694917678833 Accuracy 96.24837239583333\n",
      "Training:: Epoch 34, Iteration 30, Current loss 1.200544834136963 Accuracy 96.61425456214852\n",
      "Training:: Epoch 34, Iteration 40, Current loss 1.4651856422424316 Accuracy 96.94936226917952\n",
      "Training:: Epoch 34, Iteration 50, Current loss 1.2175449132919312 Accuracy 94.44708115804461\n",
      "Training:: Epoch 34, Iteration 60, Current loss 1.4736732244491577 Accuracy 95.42060726729716\n",
      "Training:: Epoch 34, Iteration 70, Current loss 2.3219356536865234 Accuracy 88.78158699501046\n",
      "Training:: Epoch 34, Iteration 80, Current loss 1.0063085556030273 Accuracy 96.9215452011151\n",
      "Training:: Epoch 34, Iteration 90, Current loss 1.529927372932434 Accuracy 96.50552901479972\n",
      "Training:: Epoch 34, Iteration 100, Current loss 1.771885633468628 Accuracy 91.51245735552568\n",
      "Training:: Epoch 34, Iteration 110, Current loss 1.1277899742126465 Accuracy 97.07303974221267\n",
      "Training:: Epoch 34, Iteration 120, Current loss 1.2313593626022339 Accuracy 94.38322248652847\n",
      "Training:: Epoch 34, Iteration 130, Current loss 1.1480016708374023 Accuracy 96.11379342989682\n",
      "Training:: Epoch 34, Iteration 140, Current loss 1.175642967224121 Accuracy 95.31734837799718\n",
      "Training:: Epoch 34, Iteration 150, Current loss 1.104224681854248 Accuracy 95.94805819406035\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 34, Probability Accuracy 69.38610846418362\n",
      "Starting Training\n",
      "Training:: Epoch 35, Iteration 0, Current loss 1.1259524822235107 Accuracy 96.81684858393038\n",
      "Training:: Epoch 35, Iteration 10, Current loss 1.1616837978363037 Accuracy 96.30071599045345\n",
      "Training:: Epoch 35, Iteration 20, Current loss 1.0662989616394043 Accuracy 95.99589980139663\n",
      "Training:: Epoch 35, Iteration 30, Current loss 1.3868181705474854 Accuracy 95.28437724982001\n",
      "Training:: Epoch 35, Iteration 40, Current loss 1.1473045349121094 Accuracy 96.78512396694215\n",
      "Training:: Epoch 35, Iteration 50, Current loss 1.2581499814987183 Accuracy 94.62035541195476\n",
      "Training:: Epoch 35, Iteration 60, Current loss 0.9721916913986206 Accuracy 96.83921948072891\n",
      "Training:: Epoch 35, Iteration 70, Current loss 1.6986348628997803 Accuracy 92.85148200982724\n",
      "Training:: Epoch 35, Iteration 80, Current loss 1.4103081226348877 Accuracy 96.8233977050098\n",
      "Training:: Epoch 35, Iteration 90, Current loss 0.9405766129493713 Accuracy 96.85074295852739\n",
      "Training:: Epoch 35, Iteration 100, Current loss 1.2870299816131592 Accuracy 96.08066742692516\n",
      "Training:: Epoch 35, Iteration 110, Current loss 1.2178220748901367 Accuracy 96.61188773876854\n",
      "Training:: Epoch 35, Iteration 120, Current loss 1.4732911586761475 Accuracy 95.26127284415465\n",
      "Training:: Epoch 35, Iteration 130, Current loss 1.1093077659606934 Accuracy 96.49134355868607\n",
      "Training:: Epoch 35, Iteration 140, Current loss 1.4247701168060303 Accuracy 93.50108181745333\n",
      "Training:: Epoch 35, Iteration 150, Current loss 1.344464898109436 Accuracy 95.60920193126952\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 35, Probability Accuracy 68.85963458590545\n",
      "Starting Training\n",
      "Training:: Epoch 36, Iteration 0, Current loss 1.0615642070770264 Accuracy 96.89256376999568\n",
      "Training:: Epoch 36, Iteration 10, Current loss 1.434817910194397 Accuracy 95.55580558055806\n",
      "Training:: Epoch 36, Iteration 20, Current loss 1.2114452123641968 Accuracy 96.4943172722907\n",
      "Training:: Epoch 36, Iteration 30, Current loss 1.0435564517974854 Accuracy 95.8889782783588\n",
      "Training:: Epoch 36, Iteration 40, Current loss 1.4598758220672607 Accuracy 95.14362115069439\n",
      "Training:: Epoch 36, Iteration 50, Current loss 1.076293706893921 Accuracy 97.06574084443946\n",
      "Training:: Epoch 36, Iteration 60, Current loss 1.3764218091964722 Accuracy 94.0531136831641\n",
      "Training:: Epoch 36, Iteration 70, Current loss 0.9347091317176819 Accuracy 97.17461306741457\n",
      "Training:: Epoch 36, Iteration 80, Current loss 1.208387851715088 Accuracy 95.75210456357998\n",
      "Training:: Epoch 36, Iteration 90, Current loss 1.2456002235412598 Accuracy 96.44049542272482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 36, Iteration 100, Current loss 0.8151689171791077 Accuracy 97.77964899840454\n",
      "Training:: Epoch 36, Iteration 110, Current loss 1.1485605239868164 Accuracy 96.31486880466473\n",
      "Training:: Epoch 36, Iteration 120, Current loss 1.1633328199386597 Accuracy 96.73139294890696\n",
      "Training:: Epoch 36, Iteration 130, Current loss 1.4982331991195679 Accuracy 95.69961489088575\n",
      "Training:: Epoch 36, Iteration 140, Current loss 0.9552005529403687 Accuracy 97.06510973257572\n",
      "Training:: Epoch 36, Iteration 150, Current loss 1.0838342905044556 Accuracy 97.0839636913767\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 36, Probability Accuracy 67.80316526494593\n",
      "Starting Training\n",
      "Training:: Epoch 37, Iteration 0, Current loss 1.0137690305709839 Accuracy 96.99547863289416\n",
      "Training:: Epoch 37, Iteration 10, Current loss 0.7869246602058411 Accuracy 96.91249400095985\n",
      "Training:: Epoch 37, Iteration 20, Current loss 1.0722153186798096 Accuracy 96.19466735697644\n",
      "Training:: Epoch 37, Iteration 30, Current loss 1.0353202819824219 Accuracy 95.96539883782356\n",
      "Training:: Epoch 37, Iteration 40, Current loss 0.8844124674797058 Accuracy 97.36464061029375\n",
      "Training:: Epoch 37, Iteration 50, Current loss 1.2646803855895996 Accuracy 94.8438155764734\n",
      "Training:: Epoch 37, Iteration 60, Current loss 0.8645823001861572 Accuracy 98.18003826814399\n",
      "Training:: Epoch 37, Iteration 70, Current loss 0.9806737303733826 Accuracy 98.08180894308943\n",
      "Training:: Epoch 37, Iteration 80, Current loss 0.7279787659645081 Accuracy 97.39873417721519\n",
      "Training:: Epoch 37, Iteration 90, Current loss 0.9843133091926575 Accuracy 97.49972584713237\n",
      "Training:: Epoch 37, Iteration 100, Current loss 1.0123509168624878 Accuracy 97.0720223644523\n",
      "Training:: Epoch 37, Iteration 110, Current loss 1.101689338684082 Accuracy 96.04637584452415\n",
      "Training:: Epoch 37, Iteration 120, Current loss 1.1157375574111938 Accuracy 95.6432387998356\n",
      "Training:: Epoch 37, Iteration 130, Current loss 1.0229734182357788 Accuracy 96.24158157723224\n",
      "Training:: Epoch 37, Iteration 140, Current loss 1.1908705234527588 Accuracy 96.09028617492946\n",
      "Training:: Epoch 37, Iteration 150, Current loss 0.9862433671951294 Accuracy 97.30859879875314\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 37, Probability Accuracy 67.8974189004433\n",
      "Starting Training\n",
      "Training:: Epoch 38, Iteration 0, Current loss 1.1964890956878662 Accuracy 96.4019636153624\n",
      "Training:: Epoch 38, Iteration 10, Current loss 1.3020236492156982 Accuracy 96.02158404975306\n",
      "Training:: Epoch 38, Iteration 20, Current loss 0.9621574878692627 Accuracy 97.5627147209438\n",
      "Training:: Epoch 38, Iteration 30, Current loss 1.1354107856750488 Accuracy 97.96902396259497\n",
      "Training:: Epoch 38, Iteration 40, Current loss 0.9058648347854614 Accuracy 97.49804063423163\n",
      "Training:: Epoch 38, Iteration 50, Current loss 1.2242846488952637 Accuracy 96.30128680185946\n",
      "Training:: Epoch 38, Iteration 60, Current loss 1.1257119178771973 Accuracy 97.58201839028267\n",
      "Training:: Epoch 38, Iteration 70, Current loss 1.1225059032440186 Accuracy 96.76717671767177\n",
      "Training:: Epoch 38, Iteration 80, Current loss 0.9460790157318115 Accuracy 95.92986850344396\n",
      "Training:: Epoch 38, Iteration 90, Current loss 0.8661434650421143 Accuracy 97.71867115222877\n",
      "Training:: Epoch 38, Iteration 100, Current loss 1.359756350517273 Accuracy 96.50106538073344\n",
      "Training:: Epoch 38, Iteration 110, Current loss 1.035050868988037 Accuracy 96.77946447396162\n",
      "Training:: Epoch 38, Iteration 120, Current loss 1.22223699092865 Accuracy 96.47969807391983\n",
      "Training:: Epoch 38, Iteration 130, Current loss 1.0634208917617798 Accuracy 97.42014742014742\n",
      "Training:: Epoch 38, Iteration 140, Current loss 1.3013052940368652 Accuracy 96.28235726087755\n",
      "Training:: Epoch 38, Iteration 150, Current loss 0.9438895583152771 Accuracy 97.35346358792185\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 38, Probability Accuracy 67.37177362555413\n",
      "Starting Training\n",
      "Training:: Epoch 39, Iteration 0, Current loss 1.1335819959640503 Accuracy 96.76869221793376\n",
      "Training:: Epoch 39, Iteration 10, Current loss 0.9988740682601929 Accuracy 97.70142068249399\n",
      "Training:: Epoch 39, Iteration 20, Current loss 1.00593101978302 Accuracy 97.17257935449453\n",
      "Training:: Epoch 39, Iteration 30, Current loss 0.9361279606819153 Accuracy 97.0263753927084\n",
      "Training:: Epoch 39, Iteration 40, Current loss 1.038152813911438 Accuracy 96.87793226989679\n",
      "Training:: Epoch 39, Iteration 50, Current loss 1.4074867963790894 Accuracy 93.50077109495484\n",
      "Training:: Epoch 39, Iteration 60, Current loss 1.130562424659729 Accuracy 96.86539643515673\n",
      "Training:: Epoch 39, Iteration 70, Current loss 1.0083022117614746 Accuracy 96.58170914542728\n",
      "Training:: Epoch 39, Iteration 80, Current loss 1.0822606086730957 Accuracy 97.93311036789298\n",
      "Training:: Epoch 39, Iteration 90, Current loss 1.1570930480957031 Accuracy 96.83763645815283\n",
      "Training:: Epoch 39, Iteration 100, Current loss 1.1249548196792603 Accuracy 97.1651334246754\n",
      "Training:: Epoch 39, Iteration 110, Current loss 0.950717568397522 Accuracy 97.14814503238833\n",
      "Training:: Epoch 39, Iteration 120, Current loss 1.242760181427002 Accuracy 95.8690397714788\n",
      "Training:: Epoch 39, Iteration 130, Current loss 0.9977697134017944 Accuracy 96.4934423530703\n",
      "Training:: Epoch 39, Iteration 140, Current loss 1.357323408126831 Accuracy 94.21417053969458\n",
      "Training:: Epoch 39, Iteration 150, Current loss 1.9534534215927124 Accuracy 91.77901002187515\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 39, Probability Accuracy 63.859738161329076\n",
      "Starting Training\n",
      "Training:: Epoch 40, Iteration 0, Current loss 1.7596625089645386 Accuracy 90.70894116380471\n",
      "Training:: Epoch 40, Iteration 10, Current loss 1.5059237480163574 Accuracy 92.69201520912547\n",
      "Training:: Epoch 40, Iteration 20, Current loss 1.4839082956314087 Accuracy 93.97119522262228\n",
      "Training:: Epoch 40, Iteration 30, Current loss 1.0444658994674683 Accuracy 97.34630045938813\n",
      "Training:: Epoch 40, Iteration 40, Current loss 1.0382148027420044 Accuracy 97.181762753201\n",
      "Training:: Epoch 40, Iteration 50, Current loss 1.0863134860992432 Accuracy 97.59767219048295\n",
      "Training:: Epoch 40, Iteration 60, Current loss 0.8414377570152283 Accuracy 97.02198719732814\n",
      "Training:: Epoch 40, Iteration 70, Current loss 1.057541847229004 Accuracy 97.60720035123664\n",
      "Training:: Epoch 40, Iteration 80, Current loss 0.9842875599861145 Accuracy 97.12336164154927\n",
      "Training:: Epoch 40, Iteration 90, Current loss 0.9830308556556702 Accuracy 96.63333333333334\n",
      "Training:: Epoch 40, Iteration 100, Current loss 0.7153093814849854 Accuracy 98.21618428824571\n",
      "Training:: Epoch 40, Iteration 110, Current loss 0.7883147597312927 Accuracy 97.6602086438152\n",
      "Training:: Epoch 40, Iteration 120, Current loss 0.9467558860778809 Accuracy 96.76097669010575\n",
      "Training:: Epoch 40, Iteration 130, Current loss 0.7888208627700806 Accuracy 97.90715372907154\n",
      "Training:: Epoch 40, Iteration 140, Current loss 1.2977259159088135 Accuracy 95.16670974411993\n",
      "Training:: Epoch 40, Iteration 150, Current loss 0.902367353439331 Accuracy 96.87392253763934\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 40, Probability Accuracy 55.06028089654887\n",
      "Starting Training\n",
      "Training:: Epoch 41, Iteration 0, Current loss 2.6101765632629395 Accuracy 75.43190850839484\n",
      "Training:: Epoch 41, Iteration 10, Current loss 1.7613426446914673 Accuracy 89.78266300274794\n",
      "Training:: Epoch 41, Iteration 20, Current loss 2.2687342166900635 Accuracy 92.49347258485639\n",
      "Training:: Epoch 41, Iteration 30, Current loss 1.6472632884979248 Accuracy 94.70088744174168\n",
      "Training:: Epoch 41, Iteration 40, Current loss 1.660710096359253 Accuracy 92.8227823744468\n",
      "Training:: Epoch 41, Iteration 50, Current loss 1.041100263595581 Accuracy 95.96570352381424\n",
      "Training:: Epoch 41, Iteration 60, Current loss 1.0203889608383179 Accuracy 97.04504504504504\n",
      "Training:: Epoch 41, Iteration 70, Current loss 2.543180227279663 Accuracy 88.85466399712512\n",
      "Training:: Epoch 41, Iteration 80, Current loss 1.7053295373916626 Accuracy 91.69907374672984\n",
      "Training:: Epoch 41, Iteration 90, Current loss 1.5884547233581543 Accuracy 93.05899490167516\n",
      "Training:: Epoch 41, Iteration 100, Current loss 2.379009485244751 Accuracy 88.75804967801288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 41, Iteration 110, Current loss 1.3613922595977783 Accuracy 96.73606399314359\n",
      "Training:: Epoch 41, Iteration 120, Current loss 1.4401979446411133 Accuracy 95.13907355209275\n",
      "Training:: Epoch 41, Iteration 130, Current loss 1.514148473739624 Accuracy 94.03843551224793\n",
      "Training:: Epoch 41, Iteration 140, Current loss 1.4576221704483032 Accuracy 94.56122410332159\n",
      "Training:: Epoch 41, Iteration 150, Current loss 1.5603399276733398 Accuracy 94.57466228550565\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 41, Probability Accuracy 65.606019803621\n",
      "Starting Training\n",
      "Training:: Epoch 42, Iteration 0, Current loss 1.2148123979568481 Accuracy 96.2643886194165\n",
      "Training:: Epoch 42, Iteration 10, Current loss 1.1664865016937256 Accuracy 96.53812241867372\n",
      "Training:: Epoch 42, Iteration 20, Current loss 1.4263757467269897 Accuracy 92.07805634530887\n",
      "Training:: Epoch 42, Iteration 30, Current loss 1.3755484819412231 Accuracy 92.78868813825609\n",
      "Training:: Epoch 42, Iteration 40, Current loss 1.2558177709579468 Accuracy 96.33480104370516\n",
      "Training:: Epoch 42, Iteration 50, Current loss 1.31536865234375 Accuracy 94.64495703301674\n",
      "Training:: Epoch 42, Iteration 60, Current loss 0.9736737012863159 Accuracy 96.31223516949153\n",
      "Training:: Epoch 42, Iteration 70, Current loss 1.0295835733413696 Accuracy 96.59498983256177\n",
      "Training:: Epoch 42, Iteration 80, Current loss 1.1722924709320068 Accuracy 95.56077259630776\n",
      "Training:: Epoch 42, Iteration 90, Current loss 1.02437424659729 Accuracy 97.76670468871946\n",
      "Training:: Epoch 42, Iteration 100, Current loss 1.3368775844573975 Accuracy 95.90120058117306\n",
      "Training:: Epoch 42, Iteration 110, Current loss 2.4114267826080322 Accuracy 87.26575809199319\n",
      "Training:: Epoch 42, Iteration 120, Current loss 5.537125110626221 Accuracy 66.81848852901484\n",
      "Training:: Epoch 42, Iteration 130, Current loss 3.987799644470215 Accuracy 68.13733905579399\n",
      "Training:: Epoch 42, Iteration 140, Current loss 3.6541495323181152 Accuracy 72.77414602706934\n",
      "Training:: Epoch 42, Iteration 150, Current loss 3.484198570251465 Accuracy 74.85727335007992\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 42, Probability Accuracy 58.640676140365414\n",
      "Starting Training\n",
      "Training:: Epoch 43, Iteration 0, Current loss 4.865924835205078 Accuracy 79.3529003928819\n",
      "Training:: Epoch 43, Iteration 10, Current loss 1.587760329246521 Accuracy 92.93115201090662\n",
      "Training:: Epoch 43, Iteration 20, Current loss 2.0949039459228516 Accuracy 88.30579850988015\n",
      "Training:: Epoch 43, Iteration 30, Current loss 2.027940034866333 Accuracy 94.07325602797718\n",
      "Training:: Epoch 43, Iteration 40, Current loss 3.29665207862854 Accuracy 83.68178596739901\n",
      "Training:: Epoch 43, Iteration 50, Current loss 1.7921302318572998 Accuracy 90.57061340941512\n",
      "Training:: Epoch 43, Iteration 60, Current loss 1.6632603406906128 Accuracy 93.16486385676986\n",
      "Training:: Epoch 43, Iteration 70, Current loss 1.7375686168670654 Accuracy 88.60776031006401\n",
      "Training:: Epoch 43, Iteration 80, Current loss 1.5166521072387695 Accuracy 96.72035294742997\n",
      "Training:: Epoch 43, Iteration 90, Current loss 1.6085126399993896 Accuracy 95.96049847166705\n",
      "Training:: Epoch 43, Iteration 100, Current loss 2.2780661582946777 Accuracy 84.21586657365718\n",
      "Training:: Epoch 43, Iteration 110, Current loss 1.4323046207427979 Accuracy 94.98112737981667\n",
      "Training:: Epoch 43, Iteration 120, Current loss 1.2962945699691772 Accuracy 95.71457723964356\n",
      "Training:: Epoch 43, Iteration 130, Current loss 1.4371743202209473 Accuracy 94.59404490725676\n",
      "Training:: Epoch 43, Iteration 140, Current loss 1.604200005531311 Accuracy 94.04672192916352\n",
      "Training:: Epoch 43, Iteration 150, Current loss 0.9573919177055359 Accuracy 97.24818547334343\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 43, Probability Accuracy 66.15952686746489\n",
      "Starting Training\n",
      "Training:: Epoch 44, Iteration 0, Current loss 0.9706339836120605 Accuracy 96.87350997888427\n",
      "Training:: Epoch 44, Iteration 10, Current loss 1.2078087329864502 Accuracy 96.40569395017793\n",
      "Training:: Epoch 44, Iteration 20, Current loss 1.3594486713409424 Accuracy 95.19191506737444\n",
      "Training:: Epoch 44, Iteration 30, Current loss 1.0570082664489746 Accuracy 96.74924719408705\n",
      "Training:: Epoch 44, Iteration 40, Current loss 1.4980533123016357 Accuracy 94.34823813216163\n",
      "Training:: Epoch 44, Iteration 50, Current loss 1.475645899772644 Accuracy 94.07170923379175\n",
      "Training:: Epoch 44, Iteration 60, Current loss 1.0058907270431519 Accuracy 96.34375167533372\n",
      "Training:: Epoch 44, Iteration 70, Current loss 1.1871882677078247 Accuracy 96.53554915231521\n",
      "Training:: Epoch 44, Iteration 80, Current loss 0.9578419923782349 Accuracy 97.47344705503701\n",
      "Training:: Epoch 44, Iteration 90, Current loss 1.0148972272872925 Accuracy 97.19894133215703\n",
      "Training:: Epoch 44, Iteration 100, Current loss 0.9150602221488953 Accuracy 96.73624624093402\n",
      "Training:: Epoch 44, Iteration 110, Current loss 0.9982510805130005 Accuracy 96.72093817602186\n",
      "Training:: Epoch 44, Iteration 120, Current loss 1.126183032989502 Accuracy 96.72417558418869\n",
      "Training:: Epoch 44, Iteration 130, Current loss 1.1139857769012451 Accuracy 96.10048349800294\n",
      "Training:: Epoch 44, Iteration 140, Current loss 1.169746994972229 Accuracy 96.86550574286773\n",
      "Training:: Epoch 44, Iteration 150, Current loss 1.205038070678711 Accuracy 96.73309886674483\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 44, Probability Accuracy 66.94276422090566\n",
      "Starting Training\n",
      "Training:: Epoch 45, Iteration 0, Current loss 0.8794944882392883 Accuracy 97.69769190721351\n",
      "Training:: Epoch 45, Iteration 10, Current loss 0.7861416339874268 Accuracy 97.3963133640553\n",
      "Training:: Epoch 45, Iteration 20, Current loss 0.9385979771614075 Accuracy 97.40137167306361\n",
      "Training:: Epoch 45, Iteration 30, Current loss 0.9735706448554993 Accuracy 97.86069874871059\n",
      "Training:: Epoch 45, Iteration 40, Current loss 0.7457626461982727 Accuracy 98.46941170946992\n",
      "Training:: Epoch 45, Iteration 50, Current loss 1.0900741815567017 Accuracy 97.59004923555325\n",
      "Training:: Epoch 45, Iteration 60, Current loss 0.9800916910171509 Accuracy 97.42997264925022\n",
      "Training:: Epoch 45, Iteration 70, Current loss 0.8584054708480835 Accuracy 97.10396442323336\n",
      "Training:: Epoch 45, Iteration 80, Current loss 1.205231785774231 Accuracy 97.07223546481501\n",
      "Training:: Epoch 45, Iteration 90, Current loss 1.2157728672027588 Accuracy 97.41882449650637\n",
      "Training:: Epoch 45, Iteration 100, Current loss 0.9475056529045105 Accuracy 97.65974230870366\n",
      "Training:: Epoch 45, Iteration 110, Current loss 1.113563060760498 Accuracy 97.62812605136257\n",
      "Training:: Epoch 45, Iteration 120, Current loss 1.4659099578857422 Accuracy 96.59603632417614\n",
      "Training:: Epoch 45, Iteration 130, Current loss 1.33340322971344 Accuracy 96.87164179104478\n",
      "Training:: Epoch 45, Iteration 140, Current loss 0.9779040813446045 Accuracy 97.77302213602621\n",
      "Training:: Epoch 45, Iteration 150, Current loss 1.2842215299606323 Accuracy 93.92333709131906\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 45, Probability Accuracy 67.63464805071052\n",
      "Starting Training\n",
      "Training:: Epoch 46, Iteration 0, Current loss 0.8839110136032104 Accuracy 97.43272830002118\n",
      "Training:: Epoch 46, Iteration 10, Current loss 0.9575081467628479 Accuracy 97.18518518518519\n",
      "Training:: Epoch 46, Iteration 20, Current loss 2.734318256378174 Accuracy 89.47368421052632\n",
      "Training:: Epoch 46, Iteration 30, Current loss 1.1512867212295532 Accuracy 96.07612781954887\n",
      "Training:: Epoch 46, Iteration 40, Current loss 1.9239226579666138 Accuracy 93.38168631006346\n",
      "Training:: Epoch 46, Iteration 50, Current loss 2.5130157470703125 Accuracy 83.77515614156836\n",
      "Training:: Epoch 46, Iteration 60, Current loss 1.6166894435882568 Accuracy 92.35374828489459\n",
      "Training:: Epoch 46, Iteration 70, Current loss 1.759102702140808 Accuracy 89.91148296350188\n",
      "Training:: Epoch 46, Iteration 80, Current loss 2.2103848457336426 Accuracy 87.04548503958138\n",
      "Training:: Epoch 46, Iteration 90, Current loss 2.6999330520629883 Accuracy 85.16003879728419\n",
      "Training:: Epoch 46, Iteration 100, Current loss 2.2229530811309814 Accuracy 87.96563326123528\n",
      "Training:: Epoch 46, Iteration 110, Current loss 1.0512404441833496 Accuracy 97.23401068450747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 46, Iteration 120, Current loss 2.0630908012390137 Accuracy 89.85697719366061\n",
      "Training:: Epoch 46, Iteration 130, Current loss 1.6409587860107422 Accuracy 92.90785386777104\n",
      "Training:: Epoch 46, Iteration 140, Current loss 1.7690619230270386 Accuracy 90.35022606113773\n",
      "Training:: Epoch 46, Iteration 150, Current loss 1.6159924268722534 Accuracy 93.0220459127268\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 46, Probability Accuracy 66.42706218668434\n",
      "Starting Training\n",
      "Training:: Epoch 47, Iteration 0, Current loss 1.4684734344482422 Accuracy 94.77797513321492\n",
      "Training:: Epoch 47, Iteration 10, Current loss 1.8024810552597046 Accuracy 92.0320152908852\n",
      "Training:: Epoch 47, Iteration 20, Current loss 1.2420254945755005 Accuracy 95.76858157266749\n",
      "Training:: Epoch 47, Iteration 30, Current loss 1.6047927141189575 Accuracy 90.36911143068811\n",
      "Training:: Epoch 47, Iteration 40, Current loss 2.024425745010376 Accuracy 91.676738377246\n",
      "Training:: Epoch 47, Iteration 50, Current loss 1.2980871200561523 Accuracy 94.06478438736504\n",
      "Training:: Epoch 47, Iteration 60, Current loss 1.5542407035827637 Accuracy 93.10635042081101\n",
      "Training:: Epoch 47, Iteration 70, Current loss 1.0157610177993774 Accuracy 96.67060302464668\n",
      "Training:: Epoch 47, Iteration 80, Current loss 1.49196195602417 Accuracy 94.63509665817473\n",
      "Training:: Epoch 47, Iteration 90, Current loss 0.8518893122673035 Accuracy 97.81985325124398\n",
      "Training:: Epoch 47, Iteration 100, Current loss 1.0268363952636719 Accuracy 97.54376514947482\n",
      "Training:: Epoch 47, Iteration 110, Current loss 1.207760214805603 Accuracy 96.35766459140154\n",
      "Training:: Epoch 47, Iteration 120, Current loss 1.4398382902145386 Accuracy 94.89350443180315\n",
      "Training:: Epoch 47, Iteration 130, Current loss 1.0424296855926514 Accuracy 97.52382503869023\n",
      "Training:: Epoch 47, Iteration 140, Current loss 0.8277732133865356 Accuracy 97.63006374665844\n",
      "Training:: Epoch 47, Iteration 150, Current loss 1.4861118793487549 Accuracy 95.06867671691792\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 47, Probability Accuracy 66.2014749140324\n",
      "Starting Training\n",
      "Training:: Epoch 48, Iteration 0, Current loss 1.4956697225570679 Accuracy 94.20342912929404\n",
      "Training:: Epoch 48, Iteration 10, Current loss 0.8081400394439697 Accuracy 97.2614146273766\n",
      "Training:: Epoch 48, Iteration 20, Current loss 0.96921306848526 Accuracy 96.36935579002557\n",
      "Training:: Epoch 48, Iteration 30, Current loss 0.8882108330726624 Accuracy 97.02481486293361\n",
      "Training:: Epoch 48, Iteration 40, Current loss 0.9790776371955872 Accuracy 97.38858225554\n",
      "Training:: Epoch 48, Iteration 50, Current loss 1.3219748735427856 Accuracy 95.43572759263093\n",
      "Training:: Epoch 48, Iteration 60, Current loss 0.7570494413375854 Accuracy 98.70480835777609\n",
      "Training:: Epoch 48, Iteration 70, Current loss 1.2785247564315796 Accuracy 97.1295818275684\n",
      "Training:: Epoch 48, Iteration 80, Current loss 1.1106292009353638 Accuracy 95.75762769969147\n",
      "Training:: Epoch 48, Iteration 90, Current loss 0.8798025846481323 Accuracy 98.38051571110083\n",
      "Training:: Epoch 48, Iteration 100, Current loss 0.7977503538131714 Accuracy 98.40736152893294\n",
      "Training:: Epoch 48, Iteration 110, Current loss 0.9625815153121948 Accuracy 97.82747603833866\n",
      "Training:: Epoch 48, Iteration 120, Current loss 0.8254971504211426 Accuracy 97.8742684538418\n",
      "Training:: Epoch 48, Iteration 130, Current loss 1.170828938484192 Accuracy 97.06552706552706\n",
      "Training:: Epoch 48, Iteration 140, Current loss 0.7527605295181274 Accuracy 98.45418054488\n",
      "Training:: Epoch 48, Iteration 150, Current loss 0.9060453176498413 Accuracy 97.46741461804983\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 48, Probability Accuracy 65.4794506359531\n",
      "Starting Training\n",
      "Training:: Epoch 49, Iteration 0, Current loss 1.622723937034607 Accuracy 92.30086184382345\n",
      "Training:: Epoch 49, Iteration 10, Current loss 0.9764418601989746 Accuracy 96.16602767127854\n",
      "Training:: Epoch 49, Iteration 20, Current loss 1.1201555728912354 Accuracy 97.2196241816172\n",
      "Training:: Epoch 49, Iteration 30, Current loss 1.0767229795455933 Accuracy 97.50652140555471\n",
      "Training:: Epoch 49, Iteration 40, Current loss 0.8244441151618958 Accuracy 97.225218158643\n",
      "Training:: Epoch 49, Iteration 50, Current loss 0.7858171463012695 Accuracy 98.16510706317227\n",
      "Training:: Epoch 49, Iteration 60, Current loss 1.2717429399490356 Accuracy 97.50429094957555\n",
      "Training:: Epoch 49, Iteration 70, Current loss 0.8722332119941711 Accuracy 97.83472044756017\n",
      "Training:: Epoch 49, Iteration 80, Current loss 0.979019045829773 Accuracy 98.15921004510545\n",
      "Training:: Epoch 49, Iteration 90, Current loss 0.9144359827041626 Accuracy 98.30415754923413\n",
      "Training:: Epoch 49, Iteration 100, Current loss 0.8646355271339417 Accuracy 98.3685729430473\n",
      "Training:: Epoch 49, Iteration 110, Current loss 0.8968483209609985 Accuracy 98.26388888888889\n",
      "Training:: Epoch 49, Iteration 120, Current loss 0.7560147643089294 Accuracy 97.94504943728072\n",
      "Training:: Epoch 49, Iteration 130, Current loss 0.9403597712516785 Accuracy 97.6080769751999\n",
      "Training:: Epoch 49, Iteration 140, Current loss 0.9395518898963928 Accuracy 98.01677713338857\n",
      "Training:: Epoch 49, Iteration 150, Current loss 1.0407514572143555 Accuracy 97.99656126186738\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 49, Probability Accuracy 68.5391722252144\n",
      "Starting Training\n",
      "Training:: Epoch 50, Iteration 0, Current loss 0.9119448065757751 Accuracy 98.03763595557346\n",
      "Training:: Epoch 50, Iteration 10, Current loss 0.8204987049102783 Accuracy 98.37751790852874\n",
      "Training:: Epoch 50, Iteration 20, Current loss 1.2931865453720093 Accuracy 96.29895280447569\n",
      "Training:: Epoch 50, Iteration 30, Current loss 0.8649022579193115 Accuracy 97.61108456760631\n",
      "Training:: Epoch 50, Iteration 40, Current loss 1.1534062623977661 Accuracy 97.25397267911904\n",
      "Training:: Epoch 50, Iteration 50, Current loss 0.8486266136169434 Accuracy 98.452664854972\n",
      "Training:: Epoch 50, Iteration 60, Current loss 0.9171615839004517 Accuracy 97.98348171189531\n",
      "Training:: Epoch 50, Iteration 70, Current loss 0.9193022847175598 Accuracy 98.34634818557649\n",
      "Training:: Epoch 50, Iteration 80, Current loss 0.8950086832046509 Accuracy 97.039600153787\n",
      "Training:: Epoch 50, Iteration 90, Current loss 0.8642116189002991 Accuracy 97.6819248826291\n",
      "Training:: Epoch 50, Iteration 100, Current loss 1.0105587244033813 Accuracy 96.76671816376076\n",
      "Training:: Epoch 50, Iteration 110, Current loss 0.7529363632202148 Accuracy 98.2632360217714\n",
      "Training:: Epoch 50, Iteration 120, Current loss 0.9803593158721924 Accuracy 98.49027959347194\n",
      "Training:: Epoch 50, Iteration 130, Current loss 0.920554518699646 Accuracy 97.59011881586896\n",
      "Training:: Epoch 50, Iteration 140, Current loss 0.8881513476371765 Accuracy 98.22798684691267\n",
      "Training:: Epoch 50, Iteration 150, Current loss 0.6464072465896606 Accuracy 98.02309307207838\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 50, Probability Accuracy 67.86986783775946\n",
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 1.0337800979614258 Accuracy 98.39667745206935\n",
      "Training:: Epoch 51, Iteration 10, Current loss 0.6956489682197571 Accuracy 98.55478377772799\n",
      "Training:: Epoch 51, Iteration 20, Current loss 0.6957494020462036 Accuracy 97.37712483655103\n",
      "Training:: Epoch 51, Iteration 30, Current loss 0.7983571887016296 Accuracy 97.68235716852318\n",
      "Training:: Epoch 51, Iteration 40, Current loss 0.8859297633171082 Accuracy 98.40565734490518\n",
      "Training:: Epoch 51, Iteration 50, Current loss 0.6618918776512146 Accuracy 98.51323223312518\n",
      "Training:: Epoch 51, Iteration 60, Current loss 0.9936563968658447 Accuracy 98.0018850141376\n",
      "Training:: Epoch 51, Iteration 70, Current loss 0.9834253787994385 Accuracy 98.11533169328274\n",
      "Training:: Epoch 51, Iteration 80, Current loss 0.7791900038719177 Accuracy 98.49217088729944\n",
      "Training:: Epoch 51, Iteration 90, Current loss 0.7640165686607361 Accuracy 97.80436607892527\n",
      "Training:: Epoch 51, Iteration 100, Current loss 0.8285685777664185 Accuracy 97.70405360870043\n",
      "Training:: Epoch 51, Iteration 110, Current loss 0.9700190424919128 Accuracy 97.35888140859659\n",
      "Training:: Epoch 51, Iteration 120, Current loss 0.9116708636283875 Accuracy 96.94556561579579\n",
      "Training:: Epoch 51, Iteration 130, Current loss 0.7635955810546875 Accuracy 98.35407466880771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 51, Iteration 140, Current loss 0.8480674624443054 Accuracy 97.72167810192217\n",
      "Training:: Epoch 51, Iteration 150, Current loss 0.7521646618843079 Accuracy 98.47186287192756\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 51, Probability Accuracy 67.94164560633052\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 0.704993724822998 Accuracy 98.49976110845677\n",
      "Training:: Epoch 52, Iteration 10, Current loss 0.6175462603569031 Accuracy 98.66016452391507\n",
      "Training:: Epoch 52, Iteration 20, Current loss 0.7228475213050842 Accuracy 98.48797250859107\n",
      "Training:: Epoch 52, Iteration 30, Current loss 0.8874333500862122 Accuracy 98.56603453396123\n",
      "Training:: Epoch 52, Iteration 40, Current loss 0.702734649181366 Accuracy 98.96077561624739\n",
      "Training:: Epoch 52, Iteration 50, Current loss 0.703070342540741 Accuracy 98.28858070227206\n",
      "Training:: Epoch 52, Iteration 60, Current loss 0.9073598384857178 Accuracy 97.35596429537463\n",
      "Training:: Epoch 52, Iteration 70, Current loss 0.6777166724205017 Accuracy 98.64241708783709\n",
      "Training:: Epoch 52, Iteration 80, Current loss 0.7709757089614868 Accuracy 98.39344778705308\n",
      "Training:: Epoch 52, Iteration 90, Current loss 0.697467029094696 Accuracy 98.23969293891867\n",
      "Training:: Epoch 52, Iteration 100, Current loss 0.9376188516616821 Accuracy 96.988118264714\n",
      "Training:: Epoch 52, Iteration 110, Current loss 0.9111749529838562 Accuracy 97.59355142386444\n",
      "Training:: Epoch 52, Iteration 120, Current loss 0.872687041759491 Accuracy 98.43877829288246\n",
      "Training:: Epoch 52, Iteration 130, Current loss 0.893811047077179 Accuracy 98.08438124338936\n",
      "Training:: Epoch 52, Iteration 140, Current loss 0.989870548248291 Accuracy 98.06064502303654\n",
      "Training:: Epoch 52, Iteration 150, Current loss 0.7327601909637451 Accuracy 98.36877214925995\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 52, Probability Accuracy 67.41102871110743\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 0.8096189498901367 Accuracy 98.94832372410848\n",
      "Training:: Epoch 53, Iteration 10, Current loss 0.7303087115287781 Accuracy 98.79670677644079\n",
      "Training:: Epoch 53, Iteration 20, Current loss 0.9856151938438416 Accuracy 98.1307901907357\n",
      "Training:: Epoch 53, Iteration 30, Current loss 0.5667687654495239 Accuracy 98.96250116311529\n",
      "Training:: Epoch 53, Iteration 40, Current loss 0.7651999592781067 Accuracy 98.70104464180915\n",
      "Training:: Epoch 53, Iteration 50, Current loss 0.7353243231773376 Accuracy 98.48707530042361\n",
      "Training:: Epoch 53, Iteration 60, Current loss 0.7154395580291748 Accuracy 98.29924327564247\n",
      "Training:: Epoch 53, Iteration 70, Current loss 0.8206156492233276 Accuracy 98.49573183427025\n",
      "Training:: Epoch 53, Iteration 80, Current loss 0.6606347560882568 Accuracy 98.1952211489578\n",
      "Training:: Epoch 53, Iteration 90, Current loss 0.9547868967056274 Accuracy 97.85017583438392\n",
      "Training:: Epoch 53, Iteration 100, Current loss 0.9310576319694519 Accuracy 96.01645782681878\n",
      "Training:: Epoch 53, Iteration 110, Current loss 0.7015033960342407 Accuracy 98.05373386926169\n",
      "Training:: Epoch 53, Iteration 120, Current loss 0.7553390860557556 Accuracy 98.23179271708683\n",
      "Training:: Epoch 53, Iteration 130, Current loss 0.9687672257423401 Accuracy 96.65168007572173\n",
      "Training:: Epoch 53, Iteration 140, Current loss 0.9165424704551697 Accuracy 98.29113134588998\n",
      "Training:: Epoch 53, Iteration 150, Current loss 0.9271754026412964 Accuracy 97.12407991587803\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 53, Probability Accuracy 67.08269461822098\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 0.9214756488800049 Accuracy 97.79161205766711\n",
      "Training:: Epoch 54, Iteration 10, Current loss 0.8040352463722229 Accuracy 98.02997858672377\n",
      "Training:: Epoch 54, Iteration 20, Current loss 0.9611644744873047 Accuracy 97.72433667892278\n",
      "Training:: Epoch 54, Iteration 30, Current loss 0.818507969379425 Accuracy 98.68447889047522\n",
      "Training:: Epoch 54, Iteration 40, Current loss 0.8894625902175903 Accuracy 98.2150420523991\n",
      "Training:: Epoch 54, Iteration 50, Current loss 0.8964022994041443 Accuracy 97.4436271554323\n",
      "Training:: Epoch 54, Iteration 60, Current loss 0.9592281579971313 Accuracy 97.43647402743423\n",
      "Training:: Epoch 54, Iteration 70, Current loss 0.621849000453949 Accuracy 98.51140456182473\n",
      "Training:: Epoch 54, Iteration 80, Current loss 0.7925946116447449 Accuracy 97.45702238079792\n",
      "Training:: Epoch 54, Iteration 90, Current loss 0.8784183263778687 Accuracy 98.70017331022531\n",
      "Training:: Epoch 54, Iteration 100, Current loss 1.1258741617202759 Accuracy 96.64023372287146\n",
      "Training:: Epoch 54, Iteration 110, Current loss 0.7596681714057922 Accuracy 98.09953513809133\n",
      "Training:: Epoch 54, Iteration 120, Current loss 0.8203904032707214 Accuracy 98.32394056942107\n",
      "Training:: Epoch 54, Iteration 130, Current loss 0.7793260216712952 Accuracy 97.99762959266421\n",
      "Training:: Epoch 54, Iteration 140, Current loss 0.8466050624847412 Accuracy 98.39745588326994\n",
      "Training:: Epoch 54, Iteration 150, Current loss 0.7780272960662842 Accuracy 98.64249098158345\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 54, Probability Accuracy 65.81120271781911\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 0.7993345260620117 Accuracy 98.35564610011642\n",
      "Training:: Epoch 55, Iteration 10, Current loss 0.9557737112045288 Accuracy 97.64699547706225\n",
      "Training:: Epoch 55, Iteration 20, Current loss 0.777611494064331 Accuracy 98.5317066250532\n",
      "Training:: Epoch 55, Iteration 30, Current loss 0.9586632251739502 Accuracy 97.52933320614328\n",
      "Training:: Epoch 55, Iteration 40, Current loss 0.8620150089263916 Accuracy 98.52875363616022\n",
      "Training:: Epoch 55, Iteration 50, Current loss 0.6921279430389404 Accuracy 98.52808198533106\n",
      "Training:: Epoch 55, Iteration 60, Current loss 1.197977900505066 Accuracy 97.59825327510917\n",
      "Training:: Epoch 55, Iteration 70, Current loss 0.7112522721290588 Accuracy 98.2080137624543\n",
      "Training:: Epoch 55, Iteration 80, Current loss 0.7433267831802368 Accuracy 98.32984633143352\n",
      "Training:: Epoch 55, Iteration 90, Current loss 0.9081286191940308 Accuracy 97.16312056737588\n",
      "Training:: Epoch 55, Iteration 100, Current loss 0.6904388070106506 Accuracy 98.28235510241572\n",
      "Training:: Epoch 55, Iteration 110, Current loss 0.8184976577758789 Accuracy 98.43234323432343\n",
      "Training:: Epoch 55, Iteration 120, Current loss 0.8077733516693115 Accuracy 97.51084560130859\n",
      "Training:: Epoch 55, Iteration 130, Current loss 0.7880293130874634 Accuracy 98.32343167858221\n",
      "Training:: Epoch 55, Iteration 140, Current loss 0.6836009621620178 Accuracy 98.39233652644731\n",
      "Training:: Epoch 55, Iteration 150, Current loss 0.9634288549423218 Accuracy 97.69666055630829\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 65.6823548908315\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 0.7349421381950378 Accuracy 98.63746958637469\n",
      "Training:: Epoch 56, Iteration 10, Current loss 0.9228411316871643 Accuracy 96.23407790289828\n",
      "Training:: Epoch 56, Iteration 20, Current loss 0.7769135236740112 Accuracy 97.14830146527368\n",
      "Training:: Epoch 56, Iteration 30, Current loss 0.8683739900588989 Accuracy 96.92451071761417\n",
      "Training:: Epoch 56, Iteration 40, Current loss 0.9268680214881897 Accuracy 98.23576158940398\n",
      "Training:: Epoch 56, Iteration 50, Current loss 0.4680173695087433 Accuracy 98.70773686848995\n",
      "Training:: Epoch 56, Iteration 60, Current loss 0.6840871572494507 Accuracy 98.73379710725189\n",
      "Training:: Epoch 56, Iteration 70, Current loss 0.7352734804153442 Accuracy 98.54267487901451\n",
      "Training:: Epoch 56, Iteration 80, Current loss 0.7082500457763672 Accuracy 98.21266968325791\n",
      "Training:: Epoch 56, Iteration 90, Current loss 0.8077062964439392 Accuracy 98.51505407775504\n",
      "Training:: Epoch 56, Iteration 100, Current loss 0.6235696077346802 Accuracy 98.13767550702028\n",
      "Training:: Epoch 56, Iteration 110, Current loss 0.8356459736824036 Accuracy 99.10203768371771\n",
      "Training:: Epoch 56, Iteration 120, Current loss 0.8763856291770935 Accuracy 98.22131007354199\n",
      "Training:: Epoch 56, Iteration 130, Current loss 1.1105027198791504 Accuracy 96.98122780314561\n",
      "Training:: Epoch 56, Iteration 140, Current loss 0.7582091093063354 Accuracy 98.27489817107259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 56, Iteration 150, Current loss 1.1819450855255127 Accuracy 95.59346870420767\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 56, Probability Accuracy 64.14633135849526\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 0.9365888833999634 Accuracy 95.78516805975458\n",
      "Training:: Epoch 57, Iteration 10, Current loss 1.1304616928100586 Accuracy 95.38028169014085\n",
      "Training:: Epoch 57, Iteration 20, Current loss 0.754297137260437 Accuracy 98.18535320803629\n",
      "Training:: Epoch 57, Iteration 30, Current loss 0.9668518304824829 Accuracy 97.44884400744087\n",
      "Training:: Epoch 57, Iteration 40, Current loss 0.7458757758140564 Accuracy 97.30163829103759\n",
      "Training:: Epoch 57, Iteration 50, Current loss 0.7209593653678894 Accuracy 98.09102318515093\n",
      "Training:: Epoch 57, Iteration 60, Current loss 1.0031192302703857 Accuracy 96.37984909686762\n",
      "Training:: Epoch 57, Iteration 70, Current loss 0.9290242195129395 Accuracy 97.19403444034441\n",
      "Training:: Epoch 57, Iteration 80, Current loss 0.8660778403282166 Accuracy 97.26913002285013\n",
      "Training:: Epoch 57, Iteration 90, Current loss 1.033780813217163 Accuracy 95.48911164880747\n",
      "Training:: Epoch 57, Iteration 100, Current loss 1.0523171424865723 Accuracy 96.61544850498339\n",
      "Training:: Epoch 57, Iteration 110, Current loss 0.6066142320632935 Accuracy 98.56703367875647\n",
      "Training:: Epoch 57, Iteration 120, Current loss 1.1475049257278442 Accuracy 95.20948860135552\n",
      "Training:: Epoch 57, Iteration 130, Current loss 1.3426783084869385 Accuracy 95.774231678487\n",
      "Training:: Epoch 57, Iteration 140, Current loss 0.845685601234436 Accuracy 97.42132678978862\n",
      "Training:: Epoch 57, Iteration 150, Current loss 0.9995067715644836 Accuracy 96.13428280773144\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 57, Probability Accuracy 65.60488047396115\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 0.6895782351493835 Accuracy 97.71929824561404\n",
      "Training:: Epoch 58, Iteration 10, Current loss 0.9760635495185852 Accuracy 97.32592203109742\n",
      "Training:: Epoch 58, Iteration 20, Current loss 0.8119575381278992 Accuracy 96.99579921076081\n",
      "Training:: Epoch 58, Iteration 30, Current loss 0.8917582035064697 Accuracy 98.00501413559503\n",
      "Training:: Epoch 58, Iteration 40, Current loss 0.9174055457115173 Accuracy 96.93726072349402\n",
      "Training:: Epoch 58, Iteration 50, Current loss 0.9063809514045715 Accuracy 96.86988346335356\n",
      "Training:: Epoch 58, Iteration 60, Current loss 0.9258865714073181 Accuracy 97.76306370794559\n",
      "Training:: Epoch 58, Iteration 70, Current loss 0.8792636394500732 Accuracy 98.29291640076579\n",
      "Training:: Epoch 58, Iteration 80, Current loss 0.6838032603263855 Accuracy 98.10201053870159\n",
      "Training:: Epoch 58, Iteration 90, Current loss 0.8760368824005127 Accuracy 97.0129409679135\n",
      "Training:: Epoch 58, Iteration 100, Current loss 0.9507548213005066 Accuracy 98.28342798141456\n",
      "Training:: Epoch 58, Iteration 110, Current loss 1.0173124074935913 Accuracy 94.52584452584452\n",
      "Training:: Epoch 58, Iteration 120, Current loss 1.347147822380066 Accuracy 95.55837563451777\n",
      "Training:: Epoch 58, Iteration 130, Current loss 0.9003923535346985 Accuracy 97.73280389394134\n",
      "Training:: Epoch 58, Iteration 140, Current loss 1.454888939857483 Accuracy 90.72545612510861\n",
      "Training:: Epoch 58, Iteration 150, Current loss 1.1224360466003418 Accuracy 95.45281763755582\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 58, Probability Accuracy 64.03478062725277\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 0.922730565071106 Accuracy 96.66188655194391\n",
      "Training:: Epoch 59, Iteration 10, Current loss 0.9039284586906433 Accuracy 97.03535669586984\n",
      "Training:: Epoch 59, Iteration 20, Current loss 1.0131313800811768 Accuracy 97.40631578947368\n",
      "Training:: Epoch 59, Iteration 30, Current loss 0.8639944195747375 Accuracy 96.75114188890527\n",
      "Training:: Epoch 59, Iteration 40, Current loss 1.0154467821121216 Accuracy 95.87837837837837\n",
      "Training:: Epoch 59, Iteration 50, Current loss 0.759075939655304 Accuracy 97.83097064063831\n",
      "Training:: Epoch 59, Iteration 60, Current loss 0.7136005759239197 Accuracy 98.68197703444832\n",
      "Training:: Epoch 59, Iteration 70, Current loss 0.5460730195045471 Accuracy 98.27345844504022\n",
      "Training:: Epoch 59, Iteration 80, Current loss 0.6572936177253723 Accuracy 98.10069168840005\n",
      "Training:: Epoch 59, Iteration 90, Current loss 0.7580519318580627 Accuracy 96.90202270847432\n",
      "Training:: Epoch 59, Iteration 100, Current loss 0.7852368354797363 Accuracy 97.09063469439087\n",
      "Training:: Epoch 59, Iteration 110, Current loss 0.6013619303703308 Accuracy 98.55603333864204\n",
      "Training:: Epoch 59, Iteration 120, Current loss 0.8276496529579163 Accuracy 96.74850696748507\n",
      "Training:: Epoch 59, Iteration 130, Current loss 0.8574554324150085 Accuracy 98.45858538855282\n",
      "Training:: Epoch 59, Iteration 140, Current loss 0.519685685634613 Accuracy 98.4766050054407\n",
      "Training:: Epoch 59, Iteration 150, Current loss 0.8877056241035461 Accuracy 98.64675483927476\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 59, Probability Accuracy 64.92687575092182\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 0.7257946133613586 Accuracy 98.45614035087719\n",
      "Training:: Epoch 60, Iteration 10, Current loss 0.7732767462730408 Accuracy 98.08942625755675\n",
      "Training:: Epoch 60, Iteration 20, Current loss 0.6942523717880249 Accuracy 98.78834355828221\n",
      "Training:: Epoch 60, Iteration 30, Current loss 0.7712962031364441 Accuracy 98.32422855455485\n",
      "Training:: Epoch 60, Iteration 40, Current loss 0.6578484177589417 Accuracy 98.1174892980556\n",
      "Training:: Epoch 60, Iteration 50, Current loss 0.8911884427070618 Accuracy 97.42394504416094\n",
      "Training:: Epoch 60, Iteration 60, Current loss 0.728938102722168 Accuracy 98.11141591052964\n",
      "Training:: Epoch 60, Iteration 70, Current loss 1.1899107694625854 Accuracy 95.92248333917924\n",
      "Training:: Epoch 60, Iteration 80, Current loss 1.3407238721847534 Accuracy 95.35935667616016\n",
      "Training:: Epoch 60, Iteration 90, Current loss 1.0565741062164307 Accuracy 96.79158110882958\n",
      "Training:: Epoch 60, Iteration 100, Current loss 0.7012208104133606 Accuracy 97.19524229990131\n",
      "Training:: Epoch 60, Iteration 110, Current loss 0.6785854697227478 Accuracy 98.14082030023239\n",
      "Training:: Epoch 60, Iteration 120, Current loss 0.8152140974998474 Accuracy 98.00358905338717\n",
      "Training:: Epoch 60, Iteration 130, Current loss 0.8101649284362793 Accuracy 98.29386674510957\n",
      "Training:: Epoch 60, Iteration 140, Current loss 0.7629108428955078 Accuracy 98.11032168936254\n",
      "Training:: Epoch 60, Iteration 150, Current loss 0.8816754817962646 Accuracy 98.181143281366\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 66.97746198781952\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 0.6578248143196106 Accuracy 98.49537037037037\n",
      "Training:: Epoch 61, Iteration 10, Current loss 0.6100203990936279 Accuracy 99.00125266614755\n",
      "Training:: Epoch 61, Iteration 20, Current loss 0.6154628396034241 Accuracy 98.3422978573318\n",
      "Training:: Epoch 61, Iteration 30, Current loss 0.6656307578086853 Accuracy 97.92676927401786\n",
      "Training:: Epoch 61, Iteration 40, Current loss 0.6065826416015625 Accuracy 98.66847491061522\n",
      "Training:: Epoch 61, Iteration 50, Current loss 0.7729759216308594 Accuracy 98.47754953644792\n",
      "Training:: Epoch 61, Iteration 60, Current loss 0.5882678627967834 Accuracy 99.05794392523364\n",
      "Training:: Epoch 61, Iteration 70, Current loss 0.8031757473945618 Accuracy 98.54710889345465\n",
      "Training:: Epoch 61, Iteration 80, Current loss 0.7649025917053223 Accuracy 98.29882632203613\n",
      "Training:: Epoch 61, Iteration 90, Current loss 1.0802204608917236 Accuracy 97.93117368049248\n",
      "Training:: Epoch 61, Iteration 100, Current loss 0.7111621499061584 Accuracy 98.1982497283075\n",
      "Training:: Epoch 61, Iteration 110, Current loss 0.694210410118103 Accuracy 98.25214999477734\n",
      "Training:: Epoch 61, Iteration 120, Current loss 0.7088867425918579 Accuracy 98.1124399967456\n",
      "Training:: Epoch 61, Iteration 130, Current loss 0.5998417139053345 Accuracy 98.77155308532811\n",
      "Training:: Epoch 61, Iteration 140, Current loss 0.6184621453285217 Accuracy 98.27537857543466\n",
      "Training:: Epoch 61, Iteration 150, Current loss 0.8575350046157837 Accuracy 98.38816662244628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 61, Probability Accuracy 66.71520901520488\n",
      "Starting Training\n",
      "Training:: Epoch 62, Iteration 0, Current loss 0.8565094470977783 Accuracy 97.33463578007256\n",
      "Training:: Epoch 62, Iteration 10, Current loss 0.5887295007705688 Accuracy 98.12961011591149\n",
      "Training:: Epoch 62, Iteration 20, Current loss 0.735126793384552 Accuracy 98.49637791440921\n",
      "Training:: Epoch 62, Iteration 30, Current loss 0.7322562336921692 Accuracy 98.29935603230511\n",
      "Training:: Epoch 62, Iteration 40, Current loss 0.8445252180099487 Accuracy 97.89781161710482\n",
      "Training:: Epoch 62, Iteration 50, Current loss 1.058663010597229 Accuracy 97.23560449644414\n",
      "Training:: Epoch 62, Iteration 60, Current loss 0.6420655846595764 Accuracy 98.84009208429255\n",
      "Training:: Epoch 62, Iteration 70, Current loss 10.268743515014648 Accuracy 68.95595718547114\n",
      "Training:: Epoch 62, Iteration 80, Current loss 1.9524760246276855 Accuracy 91.07444722428832\n",
      "Training:: Epoch 62, Iteration 90, Current loss 1.7267615795135498 Accuracy 87.58521952549768\n",
      "Training:: Epoch 62, Iteration 100, Current loss 1.6486097574234009 Accuracy 93.75870069605568\n",
      "Training:: Epoch 62, Iteration 110, Current loss 2.745345115661621 Accuracy 80.12217194570135\n",
      "Training:: Epoch 62, Iteration 120, Current loss 5.882538795471191 Accuracy 61.5244899194769\n",
      "Training:: Epoch 62, Iteration 130, Current loss 3.804884433746338 Accuracy 70.33226259787192\n",
      "Training:: Epoch 62, Iteration 140, Current loss 3.6989848613739014 Accuracy 80.24034809033773\n",
      "Training:: Epoch 62, Iteration 150, Current loss 1.7840995788574219 Accuracy 90.81683168316832\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 62, Probability Accuracy 51.264034469900984\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 3.2508819103240967 Accuracy 87.29751086527064\n",
      "Training:: Epoch 63, Iteration 10, Current loss 2.6754698753356934 Accuracy 87.95656465942744\n",
      "Training:: Epoch 63, Iteration 20, Current loss 6.0869951248168945 Accuracy 63.60404191616767\n",
      "Training:: Epoch 63, Iteration 30, Current loss 4.69357442855835 Accuracy 65.47704191102846\n",
      "Training:: Epoch 63, Iteration 40, Current loss 2.4172818660736084 Accuracy 83.43961478396669\n",
      "Training:: Epoch 63, Iteration 50, Current loss 2.792712688446045 Accuracy 87.64128679667122\n",
      "Training:: Epoch 63, Iteration 60, Current loss 3.80787992477417 Accuracy 73.99800669182032\n",
      "Training:: Epoch 63, Iteration 70, Current loss 2.1738364696502686 Accuracy 90.15623454958964\n",
      "Training:: Epoch 63, Iteration 80, Current loss 2.871629238128662 Accuracy 87.86957318190144\n",
      "Training:: Epoch 63, Iteration 90, Current loss 2.039594888687134 Accuracy 90.74480003896926\n",
      "Training:: Epoch 63, Iteration 100, Current loss 2.3731327056884766 Accuracy 87.8070973612375\n",
      "Training:: Epoch 63, Iteration 110, Current loss 3.784851312637329 Accuracy 81.57791699295223\n",
      "Training:: Epoch 63, Iteration 120, Current loss 1.7158739566802979 Accuracy 91.48606083399935\n",
      "Training:: Epoch 63, Iteration 130, Current loss 2.1379315853118896 Accuracy 92.12361447551935\n",
      "Training:: Epoch 63, Iteration 140, Current loss 1.6701315641403198 Accuracy 93.65366467442593\n",
      "Training:: Epoch 63, Iteration 150, Current loss 1.4030799865722656 Accuracy 95.73573138008558\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 63, Probability Accuracy 65.33910593694328\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 1.9474669694900513 Accuracy 92.19184809297758\n",
      "Training:: Epoch 64, Iteration 10, Current loss 2.02459454536438 Accuracy 92.61149497487438\n",
      "Training:: Epoch 64, Iteration 20, Current loss 1.1286104917526245 Accuracy 95.5421800056428\n",
      "Training:: Epoch 64, Iteration 30, Current loss 1.205027461051941 Accuracy 96.0238751147842\n",
      "Training:: Epoch 64, Iteration 40, Current loss 1.546893835067749 Accuracy 92.18615149704574\n",
      "Training:: Epoch 64, Iteration 50, Current loss 0.9897258281707764 Accuracy 97.04819740179876\n",
      "Training:: Epoch 64, Iteration 60, Current loss 1.2455496788024902 Accuracy 96.44873000940734\n",
      "Training:: Epoch 64, Iteration 70, Current loss 1.487118124961853 Accuracy 96.83137064465218\n",
      "Training:: Epoch 64, Iteration 80, Current loss 0.8627941608428955 Accuracy 97.76649160270098\n",
      "Training:: Epoch 64, Iteration 90, Current loss 1.7648169994354248 Accuracy 92.23975389500843\n",
      "Training:: Epoch 64, Iteration 100, Current loss 1.2527141571044922 Accuracy 96.86572939507288\n",
      "Training:: Epoch 64, Iteration 110, Current loss 1.2392709255218506 Accuracy 96.50497775894938\n",
      "Training:: Epoch 64, Iteration 120, Current loss 1.1228998899459839 Accuracy 98.00664451827242\n",
      "Training:: Epoch 64, Iteration 130, Current loss 1.0436102151870728 Accuracy 97.1588446230839\n",
      "Training:: Epoch 64, Iteration 140, Current loss 0.7742269039154053 Accuracy 97.49943681009236\n",
      "Training:: Epoch 64, Iteration 150, Current loss 0.9679921865463257 Accuracy 97.68756795940558\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 64, Probability Accuracy 68.96662799850851\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 1.307671308517456 Accuracy 96.83685220729366\n",
      "Training:: Epoch 65, Iteration 10, Current loss 1.082969307899475 Accuracy 97.66755240625923\n",
      "Training:: Epoch 65, Iteration 20, Current loss 1.1511410474777222 Accuracy 97.54696025882467\n",
      "Training:: Epoch 65, Iteration 30, Current loss 0.7983500957489014 Accuracy 97.76739767239332\n",
      "Training:: Epoch 65, Iteration 40, Current loss 0.8666020631790161 Accuracy 98.40186272953379\n",
      "Training:: Epoch 65, Iteration 50, Current loss 0.921822190284729 Accuracy 98.34741094381197\n",
      "Training:: Epoch 65, Iteration 60, Current loss 0.6810587048530579 Accuracy 98.15834612259637\n",
      "Training:: Epoch 65, Iteration 70, Current loss 1.1191596984863281 Accuracy 97.72242940863079\n",
      "Training:: Epoch 65, Iteration 80, Current loss 0.9477921724319458 Accuracy 97.81414165811243\n",
      "Training:: Epoch 65, Iteration 90, Current loss 1.1631500720977783 Accuracy 96.47456887142545\n",
      "Training:: Epoch 65, Iteration 100, Current loss 0.7455804944038391 Accuracy 97.23582060030658\n",
      "Training:: Epoch 65, Iteration 110, Current loss 1.1403251886367798 Accuracy 96.82472837530531\n",
      "Training:: Epoch 65, Iteration 120, Current loss 0.5822850465774536 Accuracy 99.02653390443345\n",
      "Training:: Epoch 65, Iteration 130, Current loss 0.9496867656707764 Accuracy 96.73995451099317\n",
      "Training:: Epoch 65, Iteration 140, Current loss 0.8406479954719543 Accuracy 97.38759367194005\n",
      "Training:: Epoch 65, Iteration 150, Current loss 0.7784444689750671 Accuracy 98.29586455731335\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 66.79527281766582\n",
      "Starting Training\n",
      "Training:: Epoch 66, Iteration 0, Current loss 0.9525086879730225 Accuracy 97.78421433743664\n",
      "Training:: Epoch 66, Iteration 10, Current loss 0.7688977718353271 Accuracy 98.2218859358744\n",
      "Training:: Epoch 66, Iteration 20, Current loss 0.727687656879425 Accuracy 98.37297654443343\n",
      "Training:: Epoch 66, Iteration 30, Current loss 0.6947720646858215 Accuracy 97.89778566756983\n",
      "Training:: Epoch 66, Iteration 40, Current loss 0.8680781722068787 Accuracy 98.13489736070382\n",
      "Training:: Epoch 66, Iteration 50, Current loss 0.6849565505981445 Accuracy 98.43598526465456\n",
      "Training:: Epoch 66, Iteration 60, Current loss 0.6380872130393982 Accuracy 98.41258692140198\n",
      "Training:: Epoch 66, Iteration 70, Current loss 0.8005603551864624 Accuracy 98.4271186440678\n",
      "Training:: Epoch 66, Iteration 80, Current loss 0.6799365282058716 Accuracy 98.21466638243444\n",
      "Training:: Epoch 66, Iteration 90, Current loss 1.2469029426574707 Accuracy 98.26710215931773\n",
      "Training:: Epoch 66, Iteration 100, Current loss 0.7812951803207397 Accuracy 98.23797400509179\n",
      "Training:: Epoch 66, Iteration 110, Current loss 0.7907670140266418 Accuracy 98.55322824443965\n",
      "Training:: Epoch 66, Iteration 120, Current loss 0.8653965592384338 Accuracy 98.48719271101943\n",
      "Training:: Epoch 66, Iteration 130, Current loss 0.8525160551071167 Accuracy 98.88193395208289\n",
      "Training:: Epoch 66, Iteration 140, Current loss 0.5359682440757751 Accuracy 99.14805574149577\n",
      "Training:: Epoch 66, Iteration 150, Current loss 0.7844359278678894 Accuracy 98.20478723404256\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 66, Probability Accuracy 69.2144839872395\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 67, Iteration 0, Current loss 0.7119669914245605 Accuracy 98.14431408634307\n",
      "Training:: Epoch 67, Iteration 10, Current loss 0.7330781817436218 Accuracy 98.67868540395274\n",
      "Training:: Epoch 67, Iteration 20, Current loss 0.5922732949256897 Accuracy 98.98297389330307\n",
      "Training:: Epoch 67, Iteration 30, Current loss 0.8744380474090576 Accuracy 98.63397250975734\n",
      "Training:: Epoch 67, Iteration 40, Current loss 0.7651288509368896 Accuracy 98.93551688843398\n",
      "Training:: Epoch 67, Iteration 50, Current loss 0.7213298678398132 Accuracy 98.621103117506\n",
      "Training:: Epoch 67, Iteration 60, Current loss 0.8325714468955994 Accuracy 97.76367837700231\n",
      "Training:: Epoch 67, Iteration 70, Current loss 0.8096498847007751 Accuracy 98.7936886524172\n",
      "Training:: Epoch 67, Iteration 80, Current loss 0.7492349743843079 Accuracy 98.37369895916733\n",
      "Training:: Epoch 67, Iteration 90, Current loss 0.6929189562797546 Accuracy 98.34692028985508\n",
      "Training:: Epoch 67, Iteration 100, Current loss 0.7881622314453125 Accuracy 97.8902109789021\n",
      "Training:: Epoch 67, Iteration 110, Current loss 0.8599923849105835 Accuracy 98.00588738011585\n",
      "Training:: Epoch 67, Iteration 120, Current loss 0.8478621244430542 Accuracy 98.491626519844\n",
      "Training:: Epoch 67, Iteration 130, Current loss 0.6438211798667908 Accuracy 98.99259868421052\n",
      "Training:: Epoch 67, Iteration 140, Current loss 0.6302313804626465 Accuracy 98.4073848650842\n",
      "Training:: Epoch 67, Iteration 150, Current loss 0.7891355156898499 Accuracy 98.22695035460993\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 67, Probability Accuracy 68.60918921158388\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 0.5229105949401855 Accuracy 98.974799252563\n",
      "Training:: Epoch 68, Iteration 10, Current loss 0.7309374213218689 Accuracy 98.72172590762213\n",
      "Training:: Epoch 68, Iteration 20, Current loss 0.6910476088523865 Accuracy 99.04967055245818\n",
      "Training:: Epoch 68, Iteration 30, Current loss 0.5388232469558716 Accuracy 98.84141095497563\n",
      "Training:: Epoch 68, Iteration 40, Current loss 1.066449522972107 Accuracy 98.04472178887156\n",
      "Training:: Epoch 68, Iteration 50, Current loss 0.6744499802589417 Accuracy 99.09458775923116\n",
      "Training:: Epoch 68, Iteration 60, Current loss 0.744141161441803 Accuracy 98.6492702714005\n",
      "Training:: Epoch 68, Iteration 70, Current loss 0.6299347877502441 Accuracy 98.85161817439064\n",
      "Training:: Epoch 68, Iteration 80, Current loss 0.718288242816925 Accuracy 98.5569214323891\n",
      "Training:: Epoch 68, Iteration 90, Current loss 0.78654944896698 Accuracy 98.74908380161251\n",
      "Training:: Epoch 68, Iteration 100, Current loss 0.672562301158905 Accuracy 98.56987805656642\n",
      "Training:: Epoch 68, Iteration 110, Current loss 0.615418553352356 Accuracy 98.82203792720934\n",
      "Training:: Epoch 68, Iteration 120, Current loss 0.9009853601455688 Accuracy 98.54962411724505\n",
      "Training:: Epoch 68, Iteration 130, Current loss 0.545440137386322 Accuracy 98.81480218281037\n",
      "Training:: Epoch 68, Iteration 140, Current loss 0.9103637337684631 Accuracy 98.68553128435543\n",
      "Training:: Epoch 68, Iteration 150, Current loss 0.7737973928451538 Accuracy 98.52954607726733\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 68, Probability Accuracy 68.64751211832457\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 0.8182947039604187 Accuracy 98.68512110726644\n",
      "Training:: Epoch 69, Iteration 10, Current loss 0.5350133180618286 Accuracy 98.98612291135656\n",
      "Training:: Epoch 69, Iteration 20, Current loss 0.7018148303031921 Accuracy 98.72990051930176\n",
      "Training:: Epoch 69, Iteration 30, Current loss 0.7334235906600952 Accuracy 98.64820669355719\n",
      "Training:: Epoch 69, Iteration 40, Current loss 1.0360194444656372 Accuracy 98.7105566621239\n",
      "Training:: Epoch 69, Iteration 50, Current loss 0.6824716925621033 Accuracy 98.74668727915194\n",
      "Training:: Epoch 69, Iteration 60, Current loss 0.7867333292961121 Accuracy 98.64061172472387\n",
      "Training:: Epoch 69, Iteration 70, Current loss 0.6477568745613098 Accuracy 98.15961776676694\n",
      "Training:: Epoch 69, Iteration 80, Current loss 0.5362287759780884 Accuracy 98.97610921501706\n",
      "Training:: Epoch 69, Iteration 90, Current loss 0.7993891835212708 Accuracy 98.46941468306328\n",
      "Training:: Epoch 69, Iteration 100, Current loss 0.6931978464126587 Accuracy 98.8206301707446\n",
      "Training:: Epoch 69, Iteration 110, Current loss 0.8673256635665894 Accuracy 98.30249533186216\n",
      "Training:: Epoch 69, Iteration 120, Current loss 0.9196410775184631 Accuracy 98.10322370563334\n",
      "Training:: Epoch 69, Iteration 130, Current loss 0.7157189249992371 Accuracy 98.82205843643064\n",
      "Training:: Epoch 69, Iteration 140, Current loss 0.6652827262878418 Accuracy 98.24842263866654\n",
      "Training:: Epoch 69, Iteration 150, Current loss 0.6030240058898926 Accuracy 98.77663546034117\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 69, Probability Accuracy 68.13243153664499\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 0.7079133987426758 Accuracy 99.13675353767142\n",
      "Training:: Epoch 70, Iteration 10, Current loss 0.4867849349975586 Accuracy 99.02307346230478\n",
      "Training:: Epoch 70, Iteration 20, Current loss 0.6767700910568237 Accuracy 98.51988899167438\n",
      "Training:: Epoch 70, Iteration 30, Current loss 0.5530534982681274 Accuracy 98.72040456669987\n",
      "Training:: Epoch 70, Iteration 40, Current loss 0.8102287650108337 Accuracy 98.47630355199367\n",
      "Training:: Epoch 70, Iteration 50, Current loss 0.8602909445762634 Accuracy 98.25746024831192\n",
      "Training:: Epoch 70, Iteration 60, Current loss 0.6947262287139893 Accuracy 99.13511435710167\n",
      "Training:: Epoch 70, Iteration 70, Current loss 0.7890843749046326 Accuracy 98.61826063397453\n",
      "Training:: Epoch 70, Iteration 80, Current loss 0.7989844679832458 Accuracy 98.66076309172406\n",
      "Training:: Epoch 70, Iteration 90, Current loss 0.8890916109085083 Accuracy 98.48231107389574\n",
      "Training:: Epoch 70, Iteration 100, Current loss 0.762816846370697 Accuracy 98.95277414157688\n",
      "Training:: Epoch 70, Iteration 110, Current loss 0.6273800730705261 Accuracy 98.92356071601354\n",
      "Training:: Epoch 70, Iteration 120, Current loss 0.7191337943077087 Accuracy 97.87183870777575\n",
      "Training:: Epoch 70, Iteration 130, Current loss 0.5914811491966248 Accuracy 98.72611464968153\n",
      "Training:: Epoch 70, Iteration 140, Current loss 0.7146729230880737 Accuracy 98.91080027516625\n",
      "Training:: Epoch 70, Iteration 150, Current loss 0.5867716670036316 Accuracy 99.1510063913002\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 68.08913700957036\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 0.5752094388008118 Accuracy 98.95695607462906\n",
      "Training:: Epoch 71, Iteration 10, Current loss 0.49223533272743225 Accuracy 99.2693298197281\n",
      "Training:: Epoch 71, Iteration 20, Current loss 0.8335262537002563 Accuracy 98.56861457649376\n",
      "Training:: Epoch 71, Iteration 30, Current loss 0.6413581967353821 Accuracy 98.99036968002486\n",
      "Training:: Epoch 71, Iteration 40, Current loss 0.690608024597168 Accuracy 99.03437942229252\n",
      "Training:: Epoch 71, Iteration 50, Current loss 0.7864179015159607 Accuracy 98.52542372881356\n",
      "Training:: Epoch 71, Iteration 60, Current loss 0.6357380747795105 Accuracy 98.87015177065767\n",
      "Training:: Epoch 71, Iteration 70, Current loss 0.7362400889396667 Accuracy 98.48546315077755\n",
      "Training:: Epoch 71, Iteration 80, Current loss 0.6168903708457947 Accuracy 99.24457513688907\n",
      "Training:: Epoch 71, Iteration 90, Current loss 0.8397908210754395 Accuracy 98.3641975308642\n",
      "Training:: Epoch 71, Iteration 100, Current loss 0.7405827641487122 Accuracy 98.91927143988447\n",
      "Training:: Epoch 71, Iteration 110, Current loss 0.5406919121742249 Accuracy 99.10390530571416\n",
      "Training:: Epoch 71, Iteration 120, Current loss 0.7489191889762878 Accuracy 98.99253536566268\n",
      "Training:: Epoch 71, Iteration 130, Current loss 0.7406853437423706 Accuracy 98.85291868468009\n",
      "Training:: Epoch 71, Iteration 140, Current loss 0.6898090839385986 Accuracy 98.78010264508488\n",
      "Training:: Epoch 71, Iteration 150, Current loss 0.6305241584777832 Accuracy 98.64032837352488\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 71, Probability Accuracy 67.27544848158429\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 0.8161222338676453 Accuracy 97.7251480211904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 72, Iteration 10, Current loss 0.6575028300285339 Accuracy 98.78168706293707\n",
      "Training:: Epoch 72, Iteration 20, Current loss 0.618395984172821 Accuracy 98.88245033112582\n",
      "Training:: Epoch 72, Iteration 30, Current loss 0.8367652297019958 Accuracy 98.53915953407555\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.6092157363891602 Accuracy 98.90613313388182\n",
      "Training:: Epoch 72, Iteration 50, Current loss 0.483883798122406 Accuracy 98.99672235990087\n",
      "Training:: Epoch 72, Iteration 60, Current loss 0.7622624635696411 Accuracy 99.06818181818181\n",
      "Training:: Epoch 72, Iteration 70, Current loss 0.6402557492256165 Accuracy 99.06881784682801\n",
      "Training:: Epoch 72, Iteration 80, Current loss 0.5611394047737122 Accuracy 99.39794556683114\n",
      "Training:: Epoch 72, Iteration 90, Current loss 0.630693793296814 Accuracy 98.68597254770673\n",
      "Training:: Epoch 72, Iteration 100, Current loss 0.4910218119621277 Accuracy 99.1207757435415\n",
      "Training:: Epoch 72, Iteration 110, Current loss 0.5842450261116028 Accuracy 99.10579976600367\n",
      "Training:: Epoch 72, Iteration 120, Current loss 0.4956732392311096 Accuracy 99.06188925081433\n",
      "Training:: Epoch 72, Iteration 130, Current loss 0.6472238302230835 Accuracy 98.80301667473881\n",
      "Training:: Epoch 72, Iteration 140, Current loss 0.6533834338188171 Accuracy 98.97026984210804\n",
      "Training:: Epoch 72, Iteration 150, Current loss 0.6572953462600708 Accuracy 98.7962382445141\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 72, Probability Accuracy 67.23432903840576\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 0.7316901683807373 Accuracy 99.11672337084534\n",
      "Training:: Epoch 73, Iteration 10, Current loss 0.6661903262138367 Accuracy 98.59192776911915\n",
      "Training:: Epoch 73, Iteration 20, Current loss 0.5446357727050781 Accuracy 98.92717625794414\n",
      "Training:: Epoch 73, Iteration 30, Current loss 0.6137226223945618 Accuracy 99.13895167366269\n",
      "Training:: Epoch 73, Iteration 40, Current loss 0.6812841892242432 Accuracy 98.62838544693712\n",
      "Training:: Epoch 73, Iteration 50, Current loss 0.4992135763168335 Accuracy 98.96483169945841\n",
      "Training:: Epoch 73, Iteration 60, Current loss 0.5908446311950684 Accuracy 99.04509549045096\n",
      "Training:: Epoch 73, Iteration 70, Current loss 0.44061392545700073 Accuracy 99.1484859172671\n",
      "Training:: Epoch 73, Iteration 80, Current loss 0.4891127943992615 Accuracy 99.02156834737895\n",
      "Training:: Epoch 73, Iteration 90, Current loss 0.6505644917488098 Accuracy 98.81379310344828\n",
      "Training:: Epoch 73, Iteration 100, Current loss 0.7347129583358765 Accuracy 98.92694610778443\n",
      "Training:: Epoch 73, Iteration 110, Current loss 0.6149404048919678 Accuracy 99.29729729729729\n",
      "Training:: Epoch 73, Iteration 120, Current loss 0.43520796298980713 Accuracy 99.25073252406865\n",
      "Training:: Epoch 73, Iteration 130, Current loss 0.4470601975917816 Accuracy 99.01419558359622\n",
      "Training:: Epoch 73, Iteration 140, Current loss 1.1179437637329102 Accuracy 97.79878859506574\n",
      "Training:: Epoch 73, Iteration 150, Current loss 0.863606333732605 Accuracy 98.09967510574388\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 73, Probability Accuracy 60.59162281973733\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 2.0677378177642822 Accuracy 90.95300198038932\n",
      "Training:: Epoch 74, Iteration 10, Current loss 1.734944462776184 Accuracy 93.6820809248555\n",
      "Training:: Epoch 74, Iteration 20, Current loss 1.8884631395339966 Accuracy 90.37822111388196\n",
      "Training:: Epoch 74, Iteration 30, Current loss 1.8462178707122803 Accuracy 89.51554591467824\n",
      "Training:: Epoch 74, Iteration 40, Current loss 1.4195513725280762 Accuracy 94.33879300868418\n",
      "Training:: Epoch 74, Iteration 50, Current loss 2.5105342864990234 Accuracy 86.29009993926343\n",
      "Training:: Epoch 74, Iteration 60, Current loss 1.225411295890808 Accuracy 95.34933409301331\n",
      "Training:: Epoch 74, Iteration 70, Current loss 1.403691291809082 Accuracy 94.52332657200812\n",
      "Training:: Epoch 74, Iteration 80, Current loss 1.3037320375442505 Accuracy 94.38287153652394\n",
      "Training:: Epoch 74, Iteration 90, Current loss 1.1048915386199951 Accuracy 96.93269161698154\n",
      "Training:: Epoch 74, Iteration 100, Current loss 0.9293832182884216 Accuracy 98.12886079586266\n",
      "Training:: Epoch 74, Iteration 110, Current loss 1.312072992324829 Accuracy 97.08145282613367\n",
      "Training:: Epoch 74, Iteration 120, Current loss 1.4708482027053833 Accuracy 92.19339266857746\n",
      "Training:: Epoch 74, Iteration 130, Current loss 1.927262783050537 Accuracy 79.69403043923981\n",
      "Training:: Epoch 74, Iteration 140, Current loss 1.2900618314743042 Accuracy 96.02081702206051\n",
      "Training:: Epoch 74, Iteration 150, Current loss 1.986361026763916 Accuracy 88.98714373171893\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 74, Probability Accuracy 63.87019927911505\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 1.3109869956970215 Accuracy 95.74531985183702\n",
      "Training:: Epoch 75, Iteration 10, Current loss 1.7189879417419434 Accuracy 91.50399631109745\n",
      "Training:: Epoch 75, Iteration 20, Current loss 1.0820369720458984 Accuracy 95.91181805497419\n",
      "Training:: Epoch 75, Iteration 30, Current loss 0.8978541493415833 Accuracy 96.95725852005272\n",
      "Training:: Epoch 75, Iteration 40, Current loss 1.0291681289672852 Accuracy 96.7456449896376\n",
      "Training:: Epoch 75, Iteration 50, Current loss 1.0827475786209106 Accuracy 97.15156332803392\n",
      "Training:: Epoch 75, Iteration 60, Current loss 0.8165342807769775 Accuracy 97.5397489539749\n",
      "Training:: Epoch 75, Iteration 70, Current loss 0.9865589737892151 Accuracy 97.390518707483\n",
      "Training:: Epoch 75, Iteration 80, Current loss 0.9137751460075378 Accuracy 97.59687457411529\n",
      "Training:: Epoch 75, Iteration 90, Current loss 0.5083624124526978 Accuracy 98.4713599625161\n",
      "Training:: Epoch 75, Iteration 100, Current loss 0.8411146998405457 Accuracy 97.43969048702776\n",
      "Training:: Epoch 75, Iteration 110, Current loss 0.7640959024429321 Accuracy 97.48450825204\n",
      "Training:: Epoch 75, Iteration 120, Current loss 1.2566382884979248 Accuracy 97.38018098536476\n",
      "Training:: Epoch 75, Iteration 130, Current loss 0.9658983945846558 Accuracy 96.51343705799151\n",
      "Training:: Epoch 75, Iteration 140, Current loss 0.7399995923042297 Accuracy 97.36963240612876\n",
      "Training:: Epoch 75, Iteration 150, Current loss 0.6754038333892822 Accuracy 98.127379896719\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 68.07432572399222\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 0.765334963798523 Accuracy 98.68642640619737\n",
      "Training:: Epoch 76, Iteration 10, Current loss 0.699475884437561 Accuracy 98.08159813973153\n",
      "Training:: Epoch 76, Iteration 20, Current loss 0.8424807190895081 Accuracy 98.09641678453559\n",
      "Training:: Epoch 76, Iteration 30, Current loss 0.8438106179237366 Accuracy 97.55238598344779\n",
      "Training:: Epoch 76, Iteration 40, Current loss 0.750878095626831 Accuracy 98.36234371169404\n",
      "Training:: Epoch 76, Iteration 50, Current loss 0.7331081032752991 Accuracy 98.43479978274823\n",
      "Training:: Epoch 76, Iteration 60, Current loss 0.6340121626853943 Accuracy 98.5709265549959\n",
      "Training:: Epoch 76, Iteration 70, Current loss 0.9282004833221436 Accuracy 97.51301726500411\n",
      "Training:: Epoch 76, Iteration 80, Current loss 1.2376518249511719 Accuracy 96.07894053492599\n",
      "Training:: Epoch 76, Iteration 90, Current loss 0.7475137114524841 Accuracy 98.88860695255012\n",
      "Training:: Epoch 76, Iteration 100, Current loss 0.7096651792526245 Accuracy 98.47334791371843\n",
      "Training:: Epoch 76, Iteration 110, Current loss 0.8781993985176086 Accuracy 98.09605817068504\n",
      "Training:: Epoch 76, Iteration 120, Current loss 0.623246967792511 Accuracy 98.99219140670077\n",
      "Training:: Epoch 76, Iteration 130, Current loss 0.6318737864494324 Accuracy 99.08905506458191\n",
      "Training:: Epoch 76, Iteration 140, Current loss 0.6563953757286072 Accuracy 98.43365589634246\n",
      "Training:: Epoch 76, Iteration 150, Current loss 0.708397388458252 Accuracy 97.80570142535633\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 76, Probability Accuracy 67.33075775779923\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 0.6344848871231079 Accuracy 98.21845794392523\n",
      "Training:: Epoch 77, Iteration 10, Current loss 0.640868604183197 Accuracy 98.76427635274293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 77, Iteration 20, Current loss 0.6195254921913147 Accuracy 98.85701845098787\n",
      "Training:: Epoch 77, Iteration 30, Current loss 0.44466161727905273 Accuracy 99.00548696844993\n",
      "Training:: Epoch 77, Iteration 40, Current loss 0.7609408497810364 Accuracy 98.24682814302192\n",
      "Training:: Epoch 77, Iteration 50, Current loss 0.6585091948509216 Accuracy 98.50483729111697\n",
      "Training:: Epoch 77, Iteration 60, Current loss 0.6245153546333313 Accuracy 97.89013049933578\n",
      "Training:: Epoch 77, Iteration 70, Current loss 0.8145466446876526 Accuracy 98.52968092245744\n",
      "Training:: Epoch 77, Iteration 80, Current loss 0.5966436862945557 Accuracy 98.44259038537942\n",
      "Training:: Epoch 77, Iteration 90, Current loss 0.5706877708435059 Accuracy 99.15629039940124\n",
      "Training:: Epoch 77, Iteration 100, Current loss 0.9448069930076599 Accuracy 98.09663250366032\n",
      "Training:: Epoch 77, Iteration 110, Current loss 0.6735072731971741 Accuracy 99.06447534766119\n",
      "Training:: Epoch 77, Iteration 120, Current loss 0.6613139510154724 Accuracy 98.51227374163155\n",
      "Training:: Epoch 77, Iteration 130, Current loss 0.6442238092422485 Accuracy 98.74551971326164\n",
      "Training:: Epoch 77, Iteration 140, Current loss 0.7400473356246948 Accuracy 98.99865490958004\n",
      "Training:: Epoch 77, Iteration 150, Current loss 0.666745662689209 Accuracy 98.59729407083168\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 77, Probability Accuracy 67.00874176575383\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 0.5878767967224121 Accuracy 98.9804469273743\n",
      "Training:: Epoch 78, Iteration 10, Current loss 0.5380706191062927 Accuracy 99.35675050592657\n",
      "Training:: Epoch 78, Iteration 20, Current loss 0.5631119608879089 Accuracy 99.00740812472765\n",
      "Training:: Epoch 78, Iteration 30, Current loss 0.740699827671051 Accuracy 98.94601716100263\n",
      "Training:: Epoch 78, Iteration 40, Current loss 0.5402024984359741 Accuracy 99.07104540102054\n",
      "Training:: Epoch 78, Iteration 50, Current loss 0.5494513511657715 Accuracy 98.9184523205037\n",
      "Training:: Epoch 78, Iteration 60, Current loss 0.5554755330085754 Accuracy 98.84518828451883\n",
      "Training:: Epoch 78, Iteration 70, Current loss 0.6775013208389282 Accuracy 98.8750817527796\n",
      "Training:: Epoch 78, Iteration 80, Current loss 0.6948646903038025 Accuracy 98.6323581247797\n",
      "Training:: Epoch 78, Iteration 90, Current loss 0.7177227735519409 Accuracy 98.57918447499134\n",
      "Training:: Epoch 78, Iteration 100, Current loss 0.7251931428909302 Accuracy 98.09151256519678\n",
      "Training:: Epoch 78, Iteration 110, Current loss 0.6449575424194336 Accuracy 98.47642577722874\n",
      "Training:: Epoch 78, Iteration 120, Current loss 0.7578318119049072 Accuracy 98.68770764119601\n",
      "Training:: Epoch 78, Iteration 130, Current loss 0.567827582359314 Accuracy 98.94254113838683\n",
      "Training:: Epoch 78, Iteration 140, Current loss 0.7395662069320679 Accuracy 98.93526974651401\n",
      "Training:: Epoch 78, Iteration 150, Current loss 0.7786362171173096 Accuracy 98.53470652264924\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 78, Probability Accuracy 66.59920454074657\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 0.598751962184906 Accuracy 98.48259830825855\n",
      "Training:: Epoch 79, Iteration 10, Current loss 0.6027769446372986 Accuracy 99.21368193434245\n",
      "Training:: Epoch 79, Iteration 20, Current loss 0.8371259570121765 Accuracy 98.504729935917\n",
      "Training:: Epoch 79, Iteration 30, Current loss 0.5463541746139526 Accuracy 99.08149976009322\n",
      "Training:: Epoch 79, Iteration 40, Current loss 0.5791255831718445 Accuracy 99.09527622097679\n",
      "Training:: Epoch 79, Iteration 50, Current loss 0.6152399182319641 Accuracy 98.56938483547926\n",
      "Training:: Epoch 79, Iteration 60, Current loss 0.984843373298645 Accuracy 98.56507389869422\n",
      "Training:: Epoch 79, Iteration 70, Current loss 0.6334474086761475 Accuracy 98.61699754628597\n",
      "Training:: Epoch 79, Iteration 80, Current loss 0.4798220694065094 Accuracy 99.36321795599268\n",
      "Training:: Epoch 79, Iteration 90, Current loss 0.7761604189872742 Accuracy 98.95788102475032\n",
      "Training:: Epoch 79, Iteration 100, Current loss 0.5370368957519531 Accuracy 99.15572952644102\n",
      "Training:: Epoch 79, Iteration 110, Current loss 0.6452813148498535 Accuracy 98.93834837339804\n",
      "Training:: Epoch 79, Iteration 120, Current loss 0.5286803245544434 Accuracy 99.1858098284385\n",
      "Training:: Epoch 79, Iteration 130, Current loss 0.7101905941963196 Accuracy 98.50016041065126\n",
      "Training:: Epoch 79, Iteration 140, Current loss 0.6065719127655029 Accuracy 98.97151898734177\n",
      "Training:: Epoch 79, Iteration 150, Current loss 0.5502118468284607 Accuracy 99.08559703822651\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 79, Probability Accuracy 66.25668061482371\n",
      "Starting Training\n",
      "Training:: Epoch 80, Iteration 0, Current loss 0.4531835615634918 Accuracy 98.820738732633\n",
      "Training:: Epoch 80, Iteration 10, Current loss 0.7388105988502502 Accuracy 98.91658458974165\n",
      "Training:: Epoch 80, Iteration 20, Current loss 0.5291787385940552 Accuracy 99.20537270583411\n",
      "Training:: Epoch 80, Iteration 30, Current loss 0.6062190532684326 Accuracy 99.03577472549097\n",
      "Training:: Epoch 80, Iteration 40, Current loss 0.6573111414909363 Accuracy 98.5766401531009\n",
      "Training:: Epoch 80, Iteration 50, Current loss 0.6184127926826477 Accuracy 98.8367304920733\n",
      "Training:: Epoch 80, Iteration 60, Current loss 0.5538756847381592 Accuracy 98.55031799476244\n",
      "Training:: Epoch 80, Iteration 70, Current loss 0.7724977731704712 Accuracy 98.44568634299685\n",
      "Training:: Epoch 80, Iteration 80, Current loss 0.5088297724723816 Accuracy 99.28507092997073\n",
      "Training:: Epoch 80, Iteration 90, Current loss 0.6030441522598267 Accuracy 98.62637362637362\n",
      "Training:: Epoch 80, Iteration 100, Current loss 0.4097411036491394 Accuracy 99.23962633065392\n",
      "Training:: Epoch 80, Iteration 110, Current loss 0.441811203956604 Accuracy 98.97300855826201\n",
      "Training:: Epoch 80, Iteration 120, Current loss 0.6250365376472473 Accuracy 98.97683295340084\n",
      "Training:: Epoch 80, Iteration 130, Current loss 0.5042348504066467 Accuracy 99.38825018457969\n",
      "Training:: Epoch 80, Iteration 140, Current loss 0.7445419430732727 Accuracy 98.9269234896448\n",
      "Training:: Epoch 80, Iteration 150, Current loss 0.627854585647583 Accuracy 98.77525384695907\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 80, Probability Accuracy 65.50586236897709\n",
      "Starting Training\n",
      "Training:: Epoch 81, Iteration 0, Current loss 0.4937857389450073 Accuracy 99.12955787909347\n",
      "Training:: Epoch 81, Iteration 10, Current loss 0.43866074085235596 Accuracy 99.17086171157833\n",
      "Training:: Epoch 81, Iteration 20, Current loss 0.43080344796180725 Accuracy 99.20395454837261\n",
      "Training:: Epoch 81, Iteration 30, Current loss 0.5357885360717773 Accuracy 98.97191574724172\n",
      "Training:: Epoch 81, Iteration 40, Current loss 0.8475525379180908 Accuracy 98.77593844719048\n",
      "Training:: Epoch 81, Iteration 50, Current loss 0.4270155429840088 Accuracy 98.98918107873332\n",
      "Training:: Epoch 81, Iteration 60, Current loss 0.5994272232055664 Accuracy 99.04603438713255\n",
      "Training:: Epoch 81, Iteration 70, Current loss 0.47307997941970825 Accuracy 99.33064516129032\n",
      "Training:: Epoch 81, Iteration 80, Current loss 0.4707345962524414 Accuracy 98.92701332390048\n",
      "Training:: Epoch 81, Iteration 90, Current loss 0.5060831308364868 Accuracy 99.34210526315789\n",
      "Training:: Epoch 81, Iteration 100, Current loss 0.49311399459838867 Accuracy 99.2024202420242\n",
      "Training:: Epoch 81, Iteration 110, Current loss 0.6270043253898621 Accuracy 99.08286930887651\n",
      "Training:: Epoch 81, Iteration 120, Current loss 0.48721086978912354 Accuracy 99.08262381580525\n",
      "Training:: Epoch 81, Iteration 130, Current loss 0.5167348384857178 Accuracy 99.11206170198976\n",
      "Training:: Epoch 81, Iteration 140, Current loss 0.5937969088554382 Accuracy 98.79504504504504\n",
      "Training:: Epoch 81, Iteration 150, Current loss 0.6912422776222229 Accuracy 98.8085666252467\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 81, Probability Accuracy 66.54586319758047\n",
      "Starting Training\n",
      "Training:: Epoch 82, Iteration 0, Current loss 0.4562920033931732 Accuracy 99.17387691254827\n",
      "Training:: Epoch 82, Iteration 10, Current loss 0.5608239769935608 Accuracy 99.21371284793206\n",
      "Training:: Epoch 82, Iteration 20, Current loss 0.4216316044330597 Accuracy 99.24277648621721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 82, Iteration 30, Current loss 0.6225008964538574 Accuracy 99.13128523760211\n",
      "Training:: Epoch 82, Iteration 40, Current loss 0.6229621171951294 Accuracy 99.17659694164578\n",
      "Training:: Epoch 82, Iteration 50, Current loss 0.5248212218284607 Accuracy 99.14156737956043\n",
      "Training:: Epoch 82, Iteration 60, Current loss 0.5984808802604675 Accuracy 98.81963810882677\n",
      "Training:: Epoch 82, Iteration 70, Current loss 0.5858986973762512 Accuracy 99.15008872700103\n",
      "Training:: Epoch 82, Iteration 80, Current loss 0.6777439713478088 Accuracy 98.77147966721095\n",
      "Training:: Epoch 82, Iteration 90, Current loss 0.50229412317276 Accuracy 98.89593130239214\n",
      "Training:: Epoch 82, Iteration 100, Current loss 0.5705402493476868 Accuracy 98.7617452108675\n",
      "Training:: Epoch 82, Iteration 110, Current loss 0.5171273946762085 Accuracy 99.15629839269504\n",
      "Training:: Epoch 82, Iteration 120, Current loss 0.5654721260070801 Accuracy 99.15998472699503\n",
      "Training:: Epoch 82, Iteration 130, Current loss 0.8301554918289185 Accuracy 98.65355344807399\n",
      "Training:: Epoch 82, Iteration 140, Current loss 0.4715634286403656 Accuracy 99.07262686256122\n",
      "Training:: Epoch 82, Iteration 150, Current loss 0.6100696921348572 Accuracy 99.09262759924385\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 82, Probability Accuracy 65.71570617723826\n",
      "Starting Training\n",
      "Training:: Epoch 83, Iteration 0, Current loss 0.6138127446174622 Accuracy 99.18084436042848\n",
      "Training:: Epoch 83, Iteration 10, Current loss 0.4525102972984314 Accuracy 99.3671336130271\n",
      "Training:: Epoch 83, Iteration 20, Current loss 0.46851128339767456 Accuracy 99.15183605432172\n",
      "Training:: Epoch 83, Iteration 30, Current loss 0.6742327809333801 Accuracy 99.14834546409544\n",
      "Training:: Epoch 83, Iteration 40, Current loss 0.6728991866111755 Accuracy 98.55737704918033\n",
      "Training:: Epoch 83, Iteration 50, Current loss 0.4574527442455292 Accuracy 99.16621352184158\n",
      "Training:: Epoch 83, Iteration 60, Current loss 0.5246080756187439 Accuracy 99.28353256155964\n",
      "Training:: Epoch 83, Iteration 70, Current loss 0.7664270997047424 Accuracy 99.06412913761699\n",
      "Training:: Epoch 83, Iteration 80, Current loss 0.4121209681034088 Accuracy 99.13433458489779\n",
      "Training:: Epoch 83, Iteration 90, Current loss 0.47034329175949097 Accuracy 99.34210526315789\n",
      "Training:: Epoch 83, Iteration 100, Current loss 0.5739449858665466 Accuracy 98.79395120141015\n",
      "Training:: Epoch 83, Iteration 110, Current loss 0.40706828236579895 Accuracy 99.33961872326286\n",
      "Training:: Epoch 83, Iteration 120, Current loss 0.46107298135757446 Accuracy 98.9916865585645\n",
      "Training:: Epoch 83, Iteration 130, Current loss 0.5319523215293884 Accuracy 98.9575402635432\n",
      "Training:: Epoch 83, Iteration 140, Current loss 0.50883948802948 Accuracy 99.19418098837625\n",
      "Training:: Epoch 83, Iteration 150, Current loss 0.4285595715045929 Accuracy 99.35346496322106\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 83, Probability Accuracy 66.17982765049508\n",
      "Starting Training\n",
      "Training:: Epoch 84, Iteration 0, Current loss 0.46932199597358704 Accuracy 98.83287985106688\n",
      "Training:: Epoch 84, Iteration 10, Current loss 0.6734644770622253 Accuracy 98.74363695440269\n",
      "Training:: Epoch 84, Iteration 20, Current loss 0.5225048065185547 Accuracy 98.80425155004428\n",
      "Training:: Epoch 84, Iteration 30, Current loss 0.7937242388725281 Accuracy 97.30379529452172\n",
      "Training:: Epoch 84, Iteration 40, Current loss 0.7564372420310974 Accuracy 98.16372193503588\n",
      "Training:: Epoch 84, Iteration 50, Current loss 0.5732782483100891 Accuracy 97.95993674222457\n",
      "Training:: Epoch 84, Iteration 60, Current loss 1.0799585580825806 Accuracy 95.78039578039578\n",
      "Training:: Epoch 84, Iteration 70, Current loss 0.5507838726043701 Accuracy 98.47663450070476\n",
      "Training:: Epoch 84, Iteration 80, Current loss 0.7281970381736755 Accuracy 97.96545298887042\n",
      "Training:: Epoch 84, Iteration 90, Current loss 1.0125690698623657 Accuracy 96.46145313366611\n",
      "Training:: Epoch 84, Iteration 100, Current loss 0.8126035928726196 Accuracy 97.95468369829683\n",
      "Training:: Epoch 84, Iteration 110, Current loss 0.587656557559967 Accuracy 98.42721874498476\n",
      "Training:: Epoch 84, Iteration 120, Current loss 0.516239583492279 Accuracy 98.21335517010348\n",
      "Training:: Epoch 84, Iteration 130, Current loss 0.8008704781532288 Accuracy 97.6104620414443\n",
      "Training:: Epoch 84, Iteration 140, Current loss 1.0099985599517822 Accuracy 96.414845408134\n",
      "Training:: Epoch 84, Iteration 150, Current loss 0.7260944247245789 Accuracy 97.83751314011113\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 84, Probability Accuracy 66.02166797862203\n",
      "Starting Training\n",
      "Training:: Epoch 85, Iteration 0, Current loss 0.7987260818481445 Accuracy 97.41592247767433\n",
      "Training:: Epoch 85, Iteration 10, Current loss 1.2264297008514404 Accuracy 97.02887431998884\n",
      "Training:: Epoch 85, Iteration 20, Current loss 0.9336434602737427 Accuracy 97.5532546329882\n",
      "Training:: Epoch 85, Iteration 30, Current loss 0.629608690738678 Accuracy 98.61234261393864\n",
      "Training:: Epoch 85, Iteration 40, Current loss 0.5986983776092529 Accuracy 98.49183940499965\n",
      "Training:: Epoch 85, Iteration 50, Current loss 0.6862139701843262 Accuracy 98.21223773864939\n",
      "Training:: Epoch 85, Iteration 60, Current loss 0.8930913805961609 Accuracy 98.37591414189382\n",
      "Training:: Epoch 85, Iteration 70, Current loss 0.7445522546768188 Accuracy 98.40836929088985\n",
      "Training:: Epoch 85, Iteration 80, Current loss 0.8047018051147461 Accuracy 97.559280812994\n",
      "Training:: Epoch 85, Iteration 90, Current loss 1.0463064908981323 Accuracy 96.02764696946682\n",
      "Training:: Epoch 85, Iteration 100, Current loss 0.8081684112548828 Accuracy 97.55100849411834\n",
      "Training:: Epoch 85, Iteration 110, Current loss 0.4702968895435333 Accuracy 98.8317191283293\n",
      "Training:: Epoch 85, Iteration 120, Current loss 0.6437665820121765 Accuracy 98.34684368417693\n",
      "Training:: Epoch 85, Iteration 130, Current loss 0.7677218317985535 Accuracy 98.5770001054074\n",
      "Training:: Epoch 85, Iteration 140, Current loss 0.7945781350135803 Accuracy 98.59183953839813\n",
      "Training:: Epoch 85, Iteration 150, Current loss 0.6723446249961853 Accuracy 98.44201312910285\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 85, Probability Accuracy 67.22573227824502\n",
      "Starting Training\n",
      "Training:: Epoch 86, Iteration 0, Current loss 0.8048712611198425 Accuracy 98.29127234490011\n",
      "Training:: Epoch 86, Iteration 10, Current loss 0.8019541501998901 Accuracy 96.92409558779269\n",
      "Training:: Epoch 86, Iteration 20, Current loss 0.711057722568512 Accuracy 97.72727272727273\n",
      "Training:: Epoch 86, Iteration 30, Current loss 0.5973573923110962 Accuracy 98.0689892275957\n",
      "Training:: Epoch 86, Iteration 40, Current loss 0.8470167517662048 Accuracy 98.51172449597647\n",
      "Training:: Epoch 86, Iteration 50, Current loss 0.6060137152671814 Accuracy 98.79184861717613\n",
      "Training:: Epoch 86, Iteration 60, Current loss 0.6409559845924377 Accuracy 98.98366819410175\n",
      "Training:: Epoch 86, Iteration 70, Current loss 0.6792304515838623 Accuracy 98.3640272973731\n",
      "Training:: Epoch 86, Iteration 80, Current loss 0.7019624710083008 Accuracy 98.6797162375197\n",
      "Training:: Epoch 86, Iteration 90, Current loss 0.7600665092468262 Accuracy 98.0400806654903\n",
      "Training:: Epoch 86, Iteration 100, Current loss 0.6367499828338623 Accuracy 97.91749463476671\n",
      "Training:: Epoch 86, Iteration 110, Current loss 0.503259539604187 Accuracy 99.20237132848288\n",
      "Training:: Epoch 86, Iteration 120, Current loss 0.5761321187019348 Accuracy 98.85479584440687\n",
      "Training:: Epoch 86, Iteration 130, Current loss 0.6780248880386353 Accuracy 98.2385357213147\n",
      "Training:: Epoch 86, Iteration 140, Current loss 0.6488301157951355 Accuracy 99.13741702110744\n",
      "Training:: Epoch 86, Iteration 150, Current loss 0.8310350179672241 Accuracy 97.345580180332\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 86, Probability Accuracy 65.70493433318143\n",
      "Starting Training\n",
      "Training:: Epoch 87, Iteration 0, Current loss 0.7061111927032471 Accuracy 97.9383041388932\n",
      "Training:: Epoch 87, Iteration 10, Current loss 0.6438105702400208 Accuracy 98.32196817756385\n",
      "Training:: Epoch 87, Iteration 20, Current loss 0.8653967976570129 Accuracy 96.92721014188565\n",
      "Training:: Epoch 87, Iteration 30, Current loss 0.5501112937927246 Accuracy 98.25451418744626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 87, Iteration 40, Current loss 0.5562096238136292 Accuracy 98.62679824039948\n",
      "Training:: Epoch 87, Iteration 50, Current loss 0.4451475143432617 Accuracy 98.74817799879962\n",
      "Training:: Epoch 87, Iteration 60, Current loss 0.6912320256233215 Accuracy 98.81622989631806\n",
      "Training:: Epoch 87, Iteration 70, Current loss 0.544395387172699 Accuracy 99.0977970481321\n",
      "Training:: Epoch 87, Iteration 80, Current loss 0.41809263825416565 Accuracy 98.77513049770013\n",
      "Training:: Epoch 87, Iteration 90, Current loss 0.6471987366676331 Accuracy 99.0185387131952\n",
      "Training:: Epoch 87, Iteration 100, Current loss 0.465128630399704 Accuracy 98.80812317765614\n",
      "Training:: Epoch 87, Iteration 110, Current loss 0.7134866118431091 Accuracy 98.26343175976427\n",
      "Training:: Epoch 87, Iteration 120, Current loss 0.5675780177116394 Accuracy 98.53811500750817\n",
      "Training:: Epoch 87, Iteration 130, Current loss 0.9629478454589844 Accuracy 96.62412515438452\n",
      "Training:: Epoch 87, Iteration 140, Current loss 0.7655029296875 Accuracy 96.57835727929574\n",
      "Training:: Epoch 87, Iteration 150, Current loss 1.0259404182434082 Accuracy 96.28252788104089\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 87, Probability Accuracy 61.95198243360815\n",
      "Starting Training\n",
      "Training:: Epoch 88, Iteration 0, Current loss 0.8256931304931641 Accuracy 97.41577233167814\n",
      "Training:: Epoch 88, Iteration 10, Current loss 1.5802831649780273 Accuracy 92.18785796105384\n",
      "Training:: Epoch 88, Iteration 20, Current loss 0.9530024528503418 Accuracy 96.8661626363479\n",
      "Training:: Epoch 88, Iteration 30, Current loss 1.1869986057281494 Accuracy 96.05645851154833\n",
      "Training:: Epoch 88, Iteration 40, Current loss 1.465455412864685 Accuracy 93.5947983675728\n",
      "Training:: Epoch 88, Iteration 50, Current loss 1.8146342039108276 Accuracy 92.29129157585521\n",
      "Training:: Epoch 88, Iteration 60, Current loss 2.167081117630005 Accuracy 88.44934851508393\n",
      "Training:: Epoch 88, Iteration 70, Current loss 1.5128378868103027 Accuracy 90.14267185473412\n",
      "Training:: Epoch 88, Iteration 80, Current loss 1.2899914979934692 Accuracy 94.03219837157661\n",
      "Training:: Epoch 88, Iteration 90, Current loss 1.945370078086853 Accuracy 88.87400844751211\n",
      "Training:: Epoch 88, Iteration 100, Current loss 1.2569184303283691 Accuracy 94.5320143493858\n",
      "Training:: Epoch 88, Iteration 110, Current loss 1.0988715887069702 Accuracy 96.0042339243186\n",
      "Training:: Epoch 88, Iteration 120, Current loss 1.1923304796218872 Accuracy 96.84813753581662\n",
      "Training:: Epoch 88, Iteration 130, Current loss 6.173525333404541 Accuracy 67.46389370306181\n",
      "Training:: Epoch 88, Iteration 140, Current loss 2.0936784744262695 Accuracy 62.43036930059831\n",
      "Training:: Epoch 88, Iteration 150, Current loss 1.5434376001358032 Accuracy 92.69024777823553\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 88, Probability Accuracy 45.91974976177652\n",
      "Starting Training\n",
      "Training:: Epoch 89, Iteration 0, Current loss 3.0115411281585693 Accuracy 88.98611750773077\n",
      "Training:: Epoch 89, Iteration 10, Current loss 2.17299485206604 Accuracy 88.52563591420142\n",
      "Training:: Epoch 89, Iteration 20, Current loss 2.829231023788452 Accuracy 78.11715266745247\n",
      "Training:: Epoch 89, Iteration 30, Current loss 1.661546230316162 Accuracy 94.63554463554463\n",
      "Training:: Epoch 89, Iteration 40, Current loss 2.2206594944000244 Accuracy 90.77110569465987\n",
      "Training:: Epoch 89, Iteration 50, Current loss 5.860971450805664 Accuracy 67.95527761985977\n",
      "Training:: Epoch 89, Iteration 60, Current loss 2.626343250274658 Accuracy 89.4372223807001\n",
      "Training:: Epoch 89, Iteration 70, Current loss 4.38862419128418 Accuracy 76.82847293394205\n",
      "Training:: Epoch 89, Iteration 80, Current loss 2.3997981548309326 Accuracy 91.3339183852567\n",
      "Training:: Epoch 89, Iteration 90, Current loss 4.444971561431885 Accuracy 72.06347771046207\n",
      "Training:: Epoch 89, Iteration 100, Current loss 1.7569894790649414 Accuracy 93.18435062523214\n",
      "Training:: Epoch 89, Iteration 110, Current loss 2.7164103984832764 Accuracy 88.26299924642049\n",
      "Training:: Epoch 89, Iteration 120, Current loss 2.006622314453125 Accuracy 91.81466571947273\n",
      "Training:: Epoch 89, Iteration 130, Current loss 1.5153428316116333 Accuracy 92.6333615580017\n",
      "Training:: Epoch 89, Iteration 140, Current loss 1.9677865505218506 Accuracy 91.12182669124947\n",
      "Training:: Epoch 89, Iteration 150, Current loss 0.9581372737884521 Accuracy 97.1696662051992\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 89, Probability Accuracy 67.77996437005427\n",
      "Starting Training\n",
      "Training:: Epoch 90, Iteration 0, Current loss 1.14645254611969 Accuracy 96.71465485418973\n",
      "Training:: Epoch 90, Iteration 10, Current loss 1.6932631731033325 Accuracy 94.96157961287813\n",
      "Training:: Epoch 90, Iteration 20, Current loss 1.0638782978057861 Accuracy 96.27863920388799\n",
      "Training:: Epoch 90, Iteration 30, Current loss 1.0801219940185547 Accuracy 97.22314932247514\n",
      "Training:: Epoch 90, Iteration 40, Current loss 1.1300759315490723 Accuracy 97.67359261232463\n",
      "Training:: Epoch 90, Iteration 50, Current loss 1.0918521881103516 Accuracy 97.95561881348256\n",
      "Training:: Epoch 90, Iteration 60, Current loss 0.9472264647483826 Accuracy 97.73678781627328\n",
      "Training:: Epoch 90, Iteration 70, Current loss 0.9412702918052673 Accuracy 97.69263724830161\n",
      "Training:: Epoch 90, Iteration 80, Current loss 0.9573520421981812 Accuracy 97.81203381402287\n",
      "Training:: Epoch 90, Iteration 90, Current loss 0.8338209390640259 Accuracy 98.10479840053316\n",
      "Training:: Epoch 90, Iteration 100, Current loss 1.1862620115280151 Accuracy 97.24630290668027\n",
      "Training:: Epoch 90, Iteration 110, Current loss 0.8437241315841675 Accuracy 97.70281614897351\n",
      "Training:: Epoch 90, Iteration 120, Current loss 1.0217384099960327 Accuracy 97.68662692410356\n",
      "Training:: Epoch 90, Iteration 130, Current loss 1.092994213104248 Accuracy 96.14236509758898\n",
      "Training:: Epoch 90, Iteration 140, Current loss 0.7645795345306396 Accuracy 97.60890151515152\n",
      "Training:: Epoch 90, Iteration 150, Current loss 1.0045957565307617 Accuracy 97.76636269190811\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 90, Probability Accuracy 68.86874922318432\n",
      "Starting Training\n",
      "Training:: Epoch 91, Iteration 0, Current loss 0.7612552046775818 Accuracy 98.42576676015561\n",
      "Training:: Epoch 91, Iteration 10, Current loss 0.8563408255577087 Accuracy 98.8328505707957\n",
      "Training:: Epoch 91, Iteration 20, Current loss 0.6064275503158569 Accuracy 98.58823529411765\n",
      "Training:: Epoch 91, Iteration 30, Current loss 1.0084846019744873 Accuracy 98.746195982958\n",
      "Training:: Epoch 91, Iteration 40, Current loss 0.7856574058532715 Accuracy 98.57761286332715\n",
      "Training:: Epoch 91, Iteration 50, Current loss 0.6833281517028809 Accuracy 99.03388539881972\n",
      "Training:: Epoch 91, Iteration 60, Current loss 0.892288088798523 Accuracy 98.1977342945417\n",
      "Training:: Epoch 91, Iteration 70, Current loss 1.1464180946350098 Accuracy 98.55996482356821\n",
      "Training:: Epoch 91, Iteration 80, Current loss 0.5444669127464294 Accuracy 98.86792452830188\n",
      "Training:: Epoch 91, Iteration 90, Current loss 0.6921905875205994 Accuracy 98.77752027809966\n",
      "Training:: Epoch 91, Iteration 100, Current loss 0.8631716370582581 Accuracy 98.40772014475272\n",
      "Training:: Epoch 91, Iteration 110, Current loss 0.7260016202926636 Accuracy 98.28815977175464\n",
      "Training:: Epoch 91, Iteration 120, Current loss 0.49183225631713867 Accuracy 98.9401317674019\n",
      "Training:: Epoch 91, Iteration 130, Current loss 0.8314705491065979 Accuracy 98.8274441451434\n",
      "Training:: Epoch 91, Iteration 140, Current loss 0.8690468072891235 Accuracy 98.51370181142592\n",
      "Training:: Epoch 91, Iteration 150, Current loss 0.7381737232208252 Accuracy 98.7989518124909\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 91, Probability Accuracy 68.84720553507064\n",
      "Starting Training\n",
      "Training:: Epoch 92, Iteration 0, Current loss 0.7996094822883606 Accuracy 98.70509200662059\n",
      "Training:: Epoch 92, Iteration 10, Current loss 0.7125762104988098 Accuracy 98.65102962228829\n",
      "Training:: Epoch 92, Iteration 20, Current loss 0.5200382471084595 Accuracy 99.0657100087751\n",
      "Training:: Epoch 92, Iteration 30, Current loss 0.7870312333106995 Accuracy 98.46370226117061\n",
      "Training:: Epoch 92, Iteration 40, Current loss 0.6682757139205933 Accuracy 98.84045044040585\n",
      "Training:: Epoch 92, Iteration 50, Current loss 0.6273515224456787 Accuracy 98.83783448447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 92, Iteration 60, Current loss 0.7670016288757324 Accuracy 98.5934711871523\n",
      "Training:: Epoch 92, Iteration 70, Current loss 0.7449213862419128 Accuracy 98.96914117410805\n",
      "Training:: Epoch 92, Iteration 80, Current loss 0.6176260709762573 Accuracy 98.96937865424636\n",
      "Training:: Epoch 92, Iteration 90, Current loss 0.6866841316223145 Accuracy 99.06381276255252\n",
      "Training:: Epoch 92, Iteration 100, Current loss 0.7336186766624451 Accuracy 98.35689721054642\n",
      "Training:: Epoch 92, Iteration 110, Current loss 0.564253568649292 Accuracy 99.06531839300355\n",
      "Training:: Epoch 92, Iteration 120, Current loss 0.6059260368347168 Accuracy 99.15824915824916\n",
      "Training:: Epoch 92, Iteration 130, Current loss 0.5919122695922852 Accuracy 98.67248590652846\n",
      "Training:: Epoch 92, Iteration 140, Current loss 0.8408187627792358 Accuracy 98.6796728741748\n",
      "Training:: Epoch 92, Iteration 150, Current loss 0.5985859036445618 Accuracy 98.52983157293748\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 92, Probability Accuracy 67.73459833450718\n",
      "Starting Training\n",
      "Training:: Epoch 93, Iteration 0, Current loss 0.5322079062461853 Accuracy 99.26128590971273\n",
      "Training:: Epoch 93, Iteration 10, Current loss 0.584538459777832 Accuracy 98.9762097665169\n",
      "Training:: Epoch 93, Iteration 20, Current loss 0.41063496470451355 Accuracy 99.10110277082754\n",
      "Training:: Epoch 93, Iteration 30, Current loss 0.5970089435577393 Accuracy 99.13006029285098\n",
      "Training:: Epoch 93, Iteration 40, Current loss 0.6385694742202759 Accuracy 99.31635388739946\n",
      "Training:: Epoch 93, Iteration 50, Current loss 0.5710375905036926 Accuracy 98.89604415823366\n",
      "Training:: Epoch 93, Iteration 60, Current loss 0.5644677877426147 Accuracy 99.04682653755957\n",
      "Training:: Epoch 93, Iteration 70, Current loss 0.6522157788276672 Accuracy 99.0472524299875\n",
      "Training:: Epoch 93, Iteration 80, Current loss 0.560023307800293 Accuracy 98.94259818731118\n",
      "Training:: Epoch 93, Iteration 90, Current loss 0.6181575655937195 Accuracy 99.02618480848301\n",
      "Training:: Epoch 93, Iteration 100, Current loss 0.5930218696594238 Accuracy 98.96607431340873\n",
      "Training:: Epoch 93, Iteration 110, Current loss 0.6263807415962219 Accuracy 99.09044686740862\n",
      "Training:: Epoch 93, Iteration 120, Current loss 0.4367890954017639 Accuracy 98.9808298956564\n",
      "Training:: Epoch 93, Iteration 130, Current loss 0.6160816550254822 Accuracy 98.54066419499881\n",
      "Training:: Epoch 93, Iteration 140, Current loss 0.505202054977417 Accuracy 99.04079444789257\n",
      "Training:: Epoch 93, Iteration 150, Current loss 0.4561302363872528 Accuracy 99.0933389235365\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 93, Probability Accuracy 67.37985250859676\n",
      "Starting Training\n",
      "Training:: Epoch 94, Iteration 0, Current loss 0.5385538339614868 Accuracy 99.30674916924487\n",
      "Training:: Epoch 94, Iteration 10, Current loss 0.7168207168579102 Accuracy 98.98074745186862\n",
      "Training:: Epoch 94, Iteration 20, Current loss 0.5274522304534912 Accuracy 99.41118743866535\n",
      "Training:: Epoch 94, Iteration 30, Current loss 0.5994574427604675 Accuracy 98.73417721518987\n",
      "Training:: Epoch 94, Iteration 40, Current loss 0.5478385090827942 Accuracy 98.85990208570854\n",
      "Training:: Epoch 94, Iteration 50, Current loss 0.7747865915298462 Accuracy 98.7546699875467\n",
      "Training:: Epoch 94, Iteration 60, Current loss 0.6425942778587341 Accuracy 99.17601683029453\n",
      "Training:: Epoch 94, Iteration 70, Current loss 0.7282084822654724 Accuracy 99.0673339399454\n",
      "Training:: Epoch 94, Iteration 80, Current loss 0.6677030324935913 Accuracy 98.65973341145452\n",
      "Training:: Epoch 94, Iteration 90, Current loss 0.5667461156845093 Accuracy 98.83694016896193\n",
      "Training:: Epoch 94, Iteration 100, Current loss 0.5517566800117493 Accuracy 99.25684761837356\n",
      "Training:: Epoch 94, Iteration 110, Current loss 0.6617319583892822 Accuracy 98.644935089548\n",
      "Training:: Epoch 94, Iteration 120, Current loss 0.5683163404464722 Accuracy 99.11155454199924\n",
      "Training:: Epoch 94, Iteration 130, Current loss 0.4443739354610443 Accuracy 99.33975867041056\n",
      "Training:: Epoch 94, Iteration 140, Current loss 0.6925612688064575 Accuracy 99.23197375288942\n",
      "Training:: Epoch 94, Iteration 150, Current loss 0.9388134479522705 Accuracy 98.8969395479615\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 94, Probability Accuracy 67.04727182334176\n",
      "Starting Training\n",
      "Training:: Epoch 95, Iteration 0, Current loss 0.5783601403236389 Accuracy 99.09161190568226\n",
      "Training:: Epoch 95, Iteration 10, Current loss 0.5806329250335693 Accuracy 99.16411544150527\n",
      "Training:: Epoch 95, Iteration 20, Current loss 0.5634894967079163 Accuracy 99.28703835733637\n",
      "Training:: Epoch 95, Iteration 30, Current loss 0.7204105854034424 Accuracy 98.92703862660944\n",
      "Training:: Epoch 95, Iteration 40, Current loss 0.6033923625946045 Accuracy 99.07217341325232\n",
      "Training:: Epoch 95, Iteration 50, Current loss 0.4664950370788574 Accuracy 99.01194199601933\n",
      "Training:: Epoch 95, Iteration 60, Current loss 0.5273107886314392 Accuracy 99.13311500674244\n",
      "Training:: Epoch 95, Iteration 70, Current loss 0.5160425305366516 Accuracy 98.97087650518063\n",
      "Training:: Epoch 95, Iteration 80, Current loss 0.6134604811668396 Accuracy 99.37115333154937\n",
      "Training:: Epoch 95, Iteration 90, Current loss 0.4356042146682739 Accuracy 99.21135646687698\n",
      "Training:: Epoch 95, Iteration 100, Current loss 0.4061743915081024 Accuracy 99.05556633676176\n",
      "Training:: Epoch 95, Iteration 110, Current loss 0.6574957966804504 Accuracy 99.14379634584512\n",
      "Training:: Epoch 95, Iteration 120, Current loss 0.55629563331604 Accuracy 99.00217914898498\n",
      "Training:: Epoch 95, Iteration 130, Current loss 0.5451326966285706 Accuracy 99.05524063576748\n",
      "Training:: Epoch 95, Iteration 140, Current loss 0.4074253737926483 Accuracy 99.28565882444289\n",
      "Training:: Epoch 95, Iteration 150, Current loss 0.5917291641235352 Accuracy 99.2923683267646\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 95, Probability Accuracy 66.70930521605834\n",
      "Starting Training\n",
      "Training:: Epoch 96, Iteration 0, Current loss 0.5511950254440308 Accuracy 99.1208018287322\n",
      "Training:: Epoch 96, Iteration 10, Current loss 0.5492033958435059 Accuracy 99.33492950252727\n",
      "Training:: Epoch 96, Iteration 20, Current loss 0.6376474499702454 Accuracy 98.99303515985567\n",
      "Training:: Epoch 96, Iteration 30, Current loss 0.6009678840637207 Accuracy 99.31646141678907\n",
      "Training:: Epoch 96, Iteration 40, Current loss 0.6720072031021118 Accuracy 99.23230974632844\n",
      "Training:: Epoch 96, Iteration 50, Current loss 0.60393887758255 Accuracy 99.26930460090408\n",
      "Training:: Epoch 96, Iteration 60, Current loss 0.5810577273368835 Accuracy 98.9586855095003\n",
      "Training:: Epoch 96, Iteration 70, Current loss 0.4580424427986145 Accuracy 99.28079571537873\n",
      "Training:: Epoch 96, Iteration 80, Current loss 0.4771927297115326 Accuracy 99.08407102287264\n",
      "Training:: Epoch 96, Iteration 90, Current loss 0.49034255743026733 Accuracy 99.2979460177411\n",
      "Training:: Epoch 96, Iteration 100, Current loss 0.4922071397304535 Accuracy 98.92492384877262\n",
      "Training:: Epoch 96, Iteration 110, Current loss 0.5177019834518433 Accuracy 99.1651832300043\n",
      "Training:: Epoch 96, Iteration 120, Current loss 0.7223673462867737 Accuracy 98.8015671813782\n",
      "Training:: Epoch 96, Iteration 130, Current loss 0.48909008502960205 Accuracy 99.35854990895547\n",
      "Training:: Epoch 96, Iteration 140, Current loss 0.5311110615730286 Accuracy 98.79845839945591\n",
      "Training:: Epoch 96, Iteration 150, Current loss 0.6267324686050415 Accuracy 99.28053053053053\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 96, Probability Accuracy 66.44591291378381\n",
      "Starting Training\n",
      "Training:: Epoch 97, Iteration 0, Current loss 0.5334045886993408 Accuracy 99.33929524826482\n",
      "Training:: Epoch 97, Iteration 10, Current loss 0.5656286478042603 Accuracy 99.21813917122752\n",
      "Training:: Epoch 97, Iteration 20, Current loss 0.4679173529148102 Accuracy 99.18043043043043\n",
      "Training:: Epoch 97, Iteration 30, Current loss 0.5337944030761719 Accuracy 99.16637960730279\n",
      "Training:: Epoch 97, Iteration 40, Current loss 0.5047645568847656 Accuracy 99.37137982129227\n",
      "Training:: Epoch 97, Iteration 50, Current loss 0.4673319458961487 Accuracy 99.2449517120281\n",
      "Training:: Epoch 97, Iteration 60, Current loss 0.34916311502456665 Accuracy 99.4204057159988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 97, Iteration 70, Current loss 0.5622276067733765 Accuracy 99.36561903972137\n",
      "Training:: Epoch 97, Iteration 80, Current loss 0.46981486678123474 Accuracy 99.06326110035596\n",
      "Training:: Epoch 97, Iteration 90, Current loss 0.461620569229126 Accuracy 99.38850387280881\n",
      "Training:: Epoch 97, Iteration 100, Current loss 0.5289860963821411 Accuracy 99.29103154909606\n",
      "Training:: Epoch 97, Iteration 110, Current loss 0.7433874607086182 Accuracy 98.95713463751439\n",
      "Training:: Epoch 97, Iteration 120, Current loss 0.7469585537910461 Accuracy 98.96481687741597\n",
      "Training:: Epoch 97, Iteration 130, Current loss 0.6087750196456909 Accuracy 99.14387137189392\n",
      "Training:: Epoch 97, Iteration 140, Current loss 0.5836606621742249 Accuracy 98.8243938280676\n",
      "Training:: Epoch 97, Iteration 150, Current loss 0.4740819036960602 Accuracy 98.80854300019013\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 97, Probability Accuracy 66.36460620623939\n",
      "Starting Training\n",
      "Training:: Epoch 98, Iteration 0, Current loss 0.5699779987335205 Accuracy 98.90273174077038\n",
      "Training:: Epoch 98, Iteration 10, Current loss 0.5870727300643921 Accuracy 99.3791705380522\n",
      "Training:: Epoch 98, Iteration 20, Current loss 0.5534502863883972 Accuracy 99.15271581187835\n",
      "Training:: Epoch 98, Iteration 30, Current loss 0.6531341075897217 Accuracy 98.91917572302393\n",
      "Training:: Epoch 98, Iteration 40, Current loss 0.5181848406791687 Accuracy 99.14080438486037\n",
      "Training:: Epoch 98, Iteration 50, Current loss 0.3510877788066864 Accuracy 99.47757330446652\n",
      "Training:: Epoch 98, Iteration 60, Current loss 0.46469491720199585 Accuracy 99.14124698094642\n",
      "Training:: Epoch 98, Iteration 70, Current loss 0.5423271656036377 Accuracy 99.29384072185171\n",
      "Training:: Epoch 98, Iteration 80, Current loss 0.5573912262916565 Accuracy 99.34896551724138\n",
      "Training:: Epoch 98, Iteration 90, Current loss 0.5832293033599854 Accuracy 99.25389042848006\n",
      "Training:: Epoch 98, Iteration 100, Current loss 0.4255862236022949 Accuracy 99.42737583966523\n",
      "Training:: Epoch 98, Iteration 110, Current loss 0.5434215068817139 Accuracy 98.97540983606558\n",
      "Training:: Epoch 98, Iteration 120, Current loss 0.6645889282226562 Accuracy 98.77769289533995\n",
      "Training:: Epoch 98, Iteration 130, Current loss 0.6688432693481445 Accuracy 98.98376113148245\n",
      "Training:: Epoch 98, Iteration 140, Current loss 0.47419142723083496 Accuracy 99.35987017085155\n",
      "Training:: Epoch 98, Iteration 150, Current loss 0.6799378991127014 Accuracy 98.6823191183517\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 98, Probability Accuracy 66.11364295479969\n",
      "Starting Training\n",
      "Training:: Epoch 99, Iteration 0, Current loss 0.5984236001968384 Accuracy 99.22436625047295\n",
      "Training:: Epoch 99, Iteration 10, Current loss 0.536869466304779 Accuracy 99.0487514863258\n",
      "Training:: Epoch 99, Iteration 20, Current loss 0.6228512525558472 Accuracy 99.19137466307278\n",
      "Training:: Epoch 99, Iteration 30, Current loss 0.4811674952507019 Accuracy 99.4499545237992\n",
      "Training:: Epoch 99, Iteration 40, Current loss 0.40970659255981445 Accuracy 99.34391705815649\n",
      "Training:: Epoch 99, Iteration 50, Current loss 0.5658890604972839 Accuracy 99.28418365020745\n",
      "Training:: Epoch 99, Iteration 60, Current loss 0.5312153100967407 Accuracy 99.18828451882845\n",
      "Training:: Epoch 99, Iteration 70, Current loss 0.5766907334327698 Accuracy 99.012142416135\n",
      "Training:: Epoch 99, Iteration 80, Current loss 0.5000899434089661 Accuracy 99.01691186580503\n",
      "Training:: Epoch 99, Iteration 90, Current loss 0.5611819624900818 Accuracy 99.46294307196563\n",
      "Training:: Epoch 99, Iteration 100, Current loss 0.5297861695289612 Accuracy 98.98062626717729\n",
      "Training:: Epoch 99, Iteration 110, Current loss 0.39258915185928345 Accuracy 99.24333151878062\n",
      "Training:: Epoch 99, Iteration 120, Current loss 0.4129714071750641 Accuracy 99.4131455399061\n",
      "Training:: Epoch 99, Iteration 130, Current loss 0.6184525489807129 Accuracy 98.83180220029489\n",
      "Training:: Epoch 99, Iteration 140, Current loss 0.5139778256416321 Accuracy 99.28916898724901\n",
      "Training:: Epoch 99, Iteration 150, Current loss 0.5474464297294617 Accuracy 99.15396683101748\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 99, Probability Accuracy 66.59734018312135\n",
      "Starting Training\n",
      "Training:: Epoch 100, Iteration 0, Current loss 0.4625891149044037 Accuracy 99.4874715261959\n",
      "Training:: Epoch 100, Iteration 10, Current loss 0.49376818537712097 Accuracy 98.99605003291639\n",
      "Training:: Epoch 100, Iteration 20, Current loss 0.5624096393585205 Accuracy 98.89450410612761\n",
      "Training:: Epoch 100, Iteration 30, Current loss 0.48885998129844666 Accuracy 99.32554309465924\n",
      "Training:: Epoch 100, Iteration 40, Current loss 0.66764235496521 Accuracy 98.74860335195531\n",
      "Training:: Epoch 100, Iteration 50, Current loss 0.49713340401649475 Accuracy 99.12352221769262\n",
      "Training:: Epoch 100, Iteration 60, Current loss 0.5087944865226746 Accuracy 99.40983606557377\n",
      "Training:: Epoch 100, Iteration 70, Current loss 0.6191637516021729 Accuracy 99.31506849315069\n",
      "Training:: Epoch 100, Iteration 80, Current loss 0.5328962206840515 Accuracy 99.08509205208802\n",
      "Training:: Epoch 100, Iteration 90, Current loss 0.3967665731906891 Accuracy 99.25560141509433\n",
      "Training:: Epoch 100, Iteration 100, Current loss 0.3396247625350952 Accuracy 99.30268733572923\n",
      "Training:: Epoch 100, Iteration 110, Current loss 0.4977874159812927 Accuracy 99.2002285061411\n",
      "Training:: Epoch 100, Iteration 120, Current loss 0.5624988675117493 Accuracy 99.45126086688452\n",
      "Training:: Epoch 100, Iteration 130, Current loss 0.563546895980835 Accuracy 99.19144276930851\n",
      "Training:: Epoch 100, Iteration 140, Current loss 0.6033781170845032 Accuracy 99.16724098322996\n",
      "Training:: Epoch 100, Iteration 150, Current loss 0.4290004074573517 Accuracy 99.43927010074131\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 100, Probability Accuracy 66.19805692505282\n",
      "Starting Training\n",
      "Training:: Epoch 101, Iteration 0, Current loss 0.3290397822856903 Accuracy 99.4148787963221\n",
      "Training:: Epoch 101, Iteration 10, Current loss 0.4397642910480499 Accuracy 99.10933199899925\n",
      "Training:: Epoch 101, Iteration 20, Current loss 0.4650121331214905 Accuracy 99.35211602472658\n",
      "Training:: Epoch 101, Iteration 30, Current loss 0.4618387222290039 Accuracy 99.41890629864065\n",
      "Training:: Epoch 101, Iteration 40, Current loss 0.641788899898529 Accuracy 99.35291486506635\n",
      "Training:: Epoch 101, Iteration 50, Current loss 0.3286084234714508 Accuracy 99.34876989869754\n",
      "Training:: Epoch 101, Iteration 60, Current loss 0.5150331258773804 Accuracy 99.10619894281595\n",
      "Training:: Epoch 101, Iteration 70, Current loss 0.5090753436088562 Accuracy 99.36518013013807\n",
      "Training:: Epoch 101, Iteration 80, Current loss 0.4797941744327545 Accuracy 98.65350776528192\n",
      "Training:: Epoch 101, Iteration 90, Current loss 0.4259873330593109 Accuracy 99.1641078130331\n",
      "Training:: Epoch 101, Iteration 100, Current loss 0.32146671414375305 Accuracy 99.40309353264819\n",
      "Training:: Epoch 101, Iteration 110, Current loss 0.6620296239852905 Accuracy 99.0815989428477\n",
      "Training:: Epoch 101, Iteration 120, Current loss 0.30981141328811646 Accuracy 99.48112774359446\n",
      "Training:: Epoch 101, Iteration 130, Current loss 0.6660634875297546 Accuracy 99.24663219051267\n",
      "Training:: Epoch 101, Iteration 140, Current loss 0.5395666360855103 Accuracy 99.50741295214179\n",
      "Training:: Epoch 101, Iteration 150, Current loss 0.5188791155815125 Accuracy 98.96223205171827\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 101, Probability Accuracy 65.33496291999835\n",
      "Starting Training\n",
      "Training:: Epoch 102, Iteration 0, Current loss 0.4360653758049011 Accuracy 99.14116485686081\n",
      "Training:: Epoch 102, Iteration 10, Current loss 0.40883001685142517 Accuracy 99.24436246801928\n",
      "Training:: Epoch 102, Iteration 20, Current loss 0.38026461005210876 Accuracy 99.2527502940566\n",
      "Training:: Epoch 102, Iteration 30, Current loss 0.4578588604927063 Accuracy 99.30092151255164\n",
      "Training:: Epoch 102, Iteration 40, Current loss 0.38598233461380005 Accuracy 99.3877671160299\n",
      "Training:: Epoch 102, Iteration 50, Current loss 0.5703397393226624 Accuracy 98.28721017202693\n",
      "Training:: Epoch 102, Iteration 60, Current loss 0.36029183864593506 Accuracy 99.42618675013041\n",
      "Training:: Epoch 102, Iteration 70, Current loss 0.45139217376708984 Accuracy 99.06937394247039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 102, Iteration 80, Current loss 0.33888980746269226 Accuracy 99.19265474117461\n",
      "Training:: Epoch 102, Iteration 90, Current loss 0.5572052001953125 Accuracy 98.9804115429772\n",
      "Training:: Epoch 102, Iteration 100, Current loss 0.6777585744857788 Accuracy 99.11072637129142\n",
      "Training:: Epoch 102, Iteration 110, Current loss 0.43020886182785034 Accuracy 99.20812789481548\n",
      "Training:: Epoch 102, Iteration 120, Current loss 0.4680668115615845 Accuracy 99.07246723714918\n",
      "Training:: Epoch 102, Iteration 130, Current loss 0.5630782246589661 Accuracy 98.63089019629406\n",
      "Training:: Epoch 102, Iteration 140, Current loss 0.4916982054710388 Accuracy 98.96025490524903\n",
      "Training:: Epoch 102, Iteration 150, Current loss 0.3722306489944458 Accuracy 99.34320987654321\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 102, Probability Accuracy 65.649210755272\n",
      "Starting Training\n",
      "Training:: Epoch 103, Iteration 0, Current loss 0.47529053688049316 Accuracy 99.35051857818951\n",
      "Training:: Epoch 103, Iteration 10, Current loss 0.4212567210197449 Accuracy 99.39929328621908\n",
      "Training:: Epoch 103, Iteration 20, Current loss 0.4459655284881592 Accuracy 99.24196054428175\n",
      "Training:: Epoch 103, Iteration 30, Current loss 0.5154727101325989 Accuracy 99.17647058823529\n",
      "Training:: Epoch 103, Iteration 40, Current loss 0.4987010657787323 Accuracy 99.01739892351983\n",
      "Training:: Epoch 103, Iteration 50, Current loss 1.0051113367080688 Accuracy 98.56517142425615\n",
      "Training:: Epoch 103, Iteration 60, Current loss 0.5342856049537659 Accuracy 99.27823691460055\n",
      "Training:: Epoch 103, Iteration 70, Current loss 0.6581702828407288 Accuracy 98.66224099778717\n",
      "Training:: Epoch 103, Iteration 80, Current loss 0.36264604330062866 Accuracy 99.27782497875955\n",
      "Training:: Epoch 103, Iteration 90, Current loss 0.41956818103790283 Accuracy 99.37369519832986\n",
      "Training:: Epoch 103, Iteration 100, Current loss 0.4503370523452759 Accuracy 99.2\n",
      "Training:: Epoch 103, Iteration 110, Current loss 0.3001452386379242 Accuracy 99.34622865787767\n",
      "Training:: Epoch 103, Iteration 120, Current loss 0.43501248955726624 Accuracy 99.38309267168327\n",
      "Training:: Epoch 103, Iteration 130, Current loss 0.6758312582969666 Accuracy 98.87149917627677\n",
      "Training:: Epoch 103, Iteration 140, Current loss 0.3845755457878113 Accuracy 99.15785805552329\n",
      "Training:: Epoch 103, Iteration 150, Current loss 0.4332789182662964 Accuracy 98.99644241490999\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 103, Probability Accuracy 65.55868583502506\n",
      "Starting Training\n",
      "Training:: Epoch 104, Iteration 0, Current loss 0.4699196517467499 Accuracy 99.3172119487909\n",
      "Training:: Epoch 104, Iteration 10, Current loss 0.9445883631706238 Accuracy 96.53769022059691\n",
      "Training:: Epoch 104, Iteration 20, Current loss 3.008249521255493 Accuracy 81.66053838122475\n",
      "Training:: Epoch 104, Iteration 30, Current loss 1.6034501791000366 Accuracy 93.85434434279216\n",
      "Training:: Epoch 104, Iteration 40, Current loss 1.0800130367279053 Accuracy 94.12333179935573\n",
      "Training:: Epoch 104, Iteration 50, Current loss 7.451648235321045 Accuracy 62.42911153119093\n",
      "Training:: Epoch 104, Iteration 60, Current loss 2.861150026321411 Accuracy 85.29322023555257\n",
      "Training:: Epoch 104, Iteration 70, Current loss 2.4153807163238525 Accuracy 85.96833658642377\n",
      "Training:: Epoch 104, Iteration 80, Current loss 1.577515959739685 Accuracy 94.70086987395703\n",
      "Training:: Epoch 104, Iteration 90, Current loss 2.493488311767578 Accuracy 87.99141285388434\n",
      "Training:: Epoch 104, Iteration 100, Current loss 7.5788469314575195 Accuracy 65.1375086665126\n",
      "Training:: Epoch 104, Iteration 110, Current loss 9.236587524414062 Accuracy 49.3304095857154\n",
      "Training:: Epoch 104, Iteration 120, Current loss 2.1198315620422363 Accuracy 77.39591658795321\n",
      "Training:: Epoch 104, Iteration 130, Current loss 3.40390682220459 Accuracy 80.18966685140711\n",
      "Training:: Epoch 104, Iteration 140, Current loss 2.8656630516052246 Accuracy 85.35973597359735\n",
      "Training:: Epoch 104, Iteration 150, Current loss 1.8600995540618896 Accuracy 89.96483192933671\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 104, Probability Accuracy 54.824750383229066\n",
      "Starting Training\n",
      "Training:: Epoch 105, Iteration 0, Current loss 2.035158395767212 Accuracy 92.0703784044348\n",
      "Training:: Epoch 105, Iteration 10, Current loss 3.872283697128296 Accuracy 87.98053527980535\n",
      "Training:: Epoch 105, Iteration 20, Current loss 2.398435354232788 Accuracy 90.18739105171412\n",
      "Training:: Epoch 105, Iteration 30, Current loss 1.5407823324203491 Accuracy 93.53964780660623\n",
      "Training:: Epoch 105, Iteration 40, Current loss 1.4694855213165283 Accuracy 95.21120397560425\n",
      "Training:: Epoch 105, Iteration 50, Current loss 1.769866943359375 Accuracy 94.45351473922902\n",
      "Training:: Epoch 105, Iteration 60, Current loss 1.3616713285446167 Accuracy 95.0123337945972\n",
      "Training:: Epoch 105, Iteration 70, Current loss 1.3470088243484497 Accuracy 95.40824083928842\n",
      "Training:: Epoch 105, Iteration 80, Current loss 1.3739807605743408 Accuracy 96.52553101702068\n",
      "Training:: Epoch 105, Iteration 90, Current loss 0.9087813496589661 Accuracy 97.22343205574913\n",
      "Training:: Epoch 105, Iteration 100, Current loss 1.3503061532974243 Accuracy 93.85951699553145\n",
      "Training:: Epoch 105, Iteration 110, Current loss 1.6110724210739136 Accuracy 92.801841389412\n",
      "Training:: Epoch 105, Iteration 120, Current loss 1.0279574394226074 Accuracy 97.7416578054585\n",
      "Training:: Epoch 105, Iteration 130, Current loss 1.176989197731018 Accuracy 96.27387342404194\n",
      "Training:: Epoch 105, Iteration 140, Current loss 0.8851287961006165 Accuracy 98.22207170977437\n",
      "Training:: Epoch 105, Iteration 150, Current loss 1.4865633249282837 Accuracy 94.95406360424029\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 105, Probability Accuracy 68.51244976591954\n",
      "Starting Training\n",
      "Training:: Epoch 106, Iteration 0, Current loss 1.0102587938308716 Accuracy 97.70366841353025\n",
      "Training:: Epoch 106, Iteration 10, Current loss 0.6527479887008667 Accuracy 98.90956072351422\n",
      "Training:: Epoch 106, Iteration 20, Current loss 1.1547776460647583 Accuracy 97.06581258394462\n",
      "Training:: Epoch 106, Iteration 30, Current loss 0.8238930106163025 Accuracy 98.70640365923384\n",
      "Training:: Epoch 106, Iteration 40, Current loss 0.5158541202545166 Accuracy 99.17808219178082\n",
      "Training:: Epoch 106, Iteration 50, Current loss 1.174214482307434 Accuracy 97.55250677506776\n",
      "Training:: Epoch 106, Iteration 60, Current loss 0.8296574354171753 Accuracy 98.52201057526916\n",
      "Training:: Epoch 106, Iteration 70, Current loss 0.548503041267395 Accuracy 99.09008189262967\n",
      "Training:: Epoch 106, Iteration 80, Current loss 0.9323536157608032 Accuracy 98.33487511563368\n",
      "Training:: Epoch 106, Iteration 90, Current loss 0.6259559392929077 Accuracy 99.27914255904392\n",
      "Training:: Epoch 106, Iteration 100, Current loss 0.6007264852523804 Accuracy 98.92539689559104\n",
      "Training:: Epoch 106, Iteration 110, Current loss 0.742247462272644 Accuracy 98.98684468878854\n",
      "Training:: Epoch 106, Iteration 120, Current loss 0.7159422039985657 Accuracy 98.80107423748322\n",
      "Training:: Epoch 106, Iteration 130, Current loss 0.9405609965324402 Accuracy 97.99340690841336\n",
      "Training:: Epoch 106, Iteration 140, Current loss 0.7785213589668274 Accuracy 98.14368370298939\n",
      "Training:: Epoch 106, Iteration 150, Current loss 0.7888174653053284 Accuracy 98.48548946611389\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 106, Probability Accuracy 68.44771512615486\n",
      "Starting Training\n",
      "Training:: Epoch 107, Iteration 0, Current loss 0.7076593041419983 Accuracy 98.78950326401798\n",
      "Training:: Epoch 107, Iteration 10, Current loss 0.5568621158599854 Accuracy 98.90126096895509\n",
      "Training:: Epoch 107, Iteration 20, Current loss 0.6585172414779663 Accuracy 98.56345654456047\n",
      "Training:: Epoch 107, Iteration 30, Current loss 0.5899922251701355 Accuracy 98.85750710993429\n",
      "Training:: Epoch 107, Iteration 40, Current loss 0.7564128041267395 Accuracy 99.00798830470423\n",
      "Training:: Epoch 107, Iteration 50, Current loss 0.7746378779411316 Accuracy 98.80418535127055\n",
      "Training:: Epoch 107, Iteration 60, Current loss 0.9519396424293518 Accuracy 98.62321656581358\n",
      "Training:: Epoch 107, Iteration 70, Current loss 0.6012918949127197 Accuracy 98.86891081970168\n",
      "Training:: Epoch 107, Iteration 80, Current loss 0.7062394618988037 Accuracy 98.80945757997219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 107, Iteration 90, Current loss 0.49082306027412415 Accuracy 98.9268947015426\n",
      "Training:: Epoch 107, Iteration 100, Current loss 0.7926774024963379 Accuracy 98.80503144654088\n",
      "Training:: Epoch 107, Iteration 110, Current loss 0.530031144618988 Accuracy 99.10882052933913\n",
      "Training:: Epoch 107, Iteration 120, Current loss 0.6282165050506592 Accuracy 99.04390489369732\n",
      "Training:: Epoch 107, Iteration 130, Current loss 0.760640561580658 Accuracy 98.56491416309012\n",
      "Training:: Epoch 107, Iteration 140, Current loss 0.9608777165412903 Accuracy 98.47729416520684\n",
      "Training:: Epoch 107, Iteration 150, Current loss 0.5967128276824951 Accuracy 98.48665312127794\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 107, Probability Accuracy 67.58254961262791\n",
      "Starting Training\n",
      "Training:: Epoch 108, Iteration 0, Current loss 0.5469392538070679 Accuracy 99.26753121998078\n",
      "Training:: Epoch 108, Iteration 10, Current loss 0.5175838470458984 Accuracy 99.2053949500758\n",
      "Training:: Epoch 108, Iteration 20, Current loss 0.6373659372329712 Accuracy 99.21462230472655\n",
      "Training:: Epoch 108, Iteration 30, Current loss 0.6320862770080566 Accuracy 98.97522723222242\n",
      "Training:: Epoch 108, Iteration 40, Current loss 0.8504019379615784 Accuracy 98.42576396748636\n",
      "Training:: Epoch 108, Iteration 50, Current loss 0.6372050046920776 Accuracy 99.13597089586176\n",
      "Training:: Epoch 108, Iteration 60, Current loss 0.5372394919395447 Accuracy 99.15181065569607\n",
      "Training:: Epoch 108, Iteration 70, Current loss 0.7536954879760742 Accuracy 98.86933112445641\n",
      "Training:: Epoch 108, Iteration 80, Current loss 0.4500221610069275 Accuracy 99.32020219626983\n",
      "Training:: Epoch 108, Iteration 90, Current loss 0.4960625171661377 Accuracy 99.09543467702768\n",
      "Training:: Epoch 108, Iteration 100, Current loss 0.5881328582763672 Accuracy 99.08862201547444\n",
      "Training:: Epoch 108, Iteration 110, Current loss 0.4710690379142761 Accuracy 98.67814152323096\n",
      "Training:: Epoch 108, Iteration 120, Current loss 0.5804061889648438 Accuracy 99.27217698349787\n",
      "Training:: Epoch 108, Iteration 130, Current loss 0.648539125919342 Accuracy 98.7518954858276\n",
      "Training:: Epoch 108, Iteration 140, Current loss 0.7791919708251953 Accuracy 99.02057687123731\n",
      "Training:: Epoch 108, Iteration 150, Current loss 0.48635753989219666 Accuracy 98.83833494675702\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 108, Probability Accuracy 67.51926502879397\n",
      "Starting Training\n",
      "Training:: Epoch 109, Iteration 0, Current loss 0.6585550308227539 Accuracy 99.06443215099252\n",
      "Training:: Epoch 109, Iteration 10, Current loss 0.38350710272789 Accuracy 99.3702845600903\n",
      "Training:: Epoch 109, Iteration 20, Current loss 0.4607565701007843 Accuracy 99.40135642026294\n",
      "Training:: Epoch 109, Iteration 30, Current loss 0.6576531529426575 Accuracy 99.3003014961033\n",
      "Training:: Epoch 109, Iteration 40, Current loss 0.48436155915260315 Accuracy 99.2198694475402\n",
      "Training:: Epoch 109, Iteration 50, Current loss 0.40282902121543884 Accuracy 99.1017403780176\n",
      "Training:: Epoch 109, Iteration 60, Current loss 0.5348597168922424 Accuracy 99.09218530650445\n",
      "Training:: Epoch 109, Iteration 70, Current loss 0.5604190230369568 Accuracy 98.99363057324841\n",
      "Training:: Epoch 109, Iteration 80, Current loss 0.43631917238235474 Accuracy 99.56911249573763\n",
      "Training:: Epoch 109, Iteration 90, Current loss 0.5582472085952759 Accuracy 98.77915849138871\n",
      "Training:: Epoch 109, Iteration 100, Current loss 0.4979894757270813 Accuracy 99.22886133032695\n",
      "Training:: Epoch 109, Iteration 110, Current loss 0.6180254817008972 Accuracy 99.01763093945505\n",
      "Training:: Epoch 109, Iteration 120, Current loss 0.5574977993965149 Accuracy 99.38295917027702\n",
      "Training:: Epoch 109, Iteration 130, Current loss 0.564590573310852 Accuracy 99.25594163114931\n",
      "Training:: Epoch 109, Iteration 140, Current loss 0.5335891246795654 Accuracy 99.45042363178383\n",
      "Training:: Epoch 109, Iteration 150, Current loss 0.5649725794792175 Accuracy 99.20004538749575\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 109, Probability Accuracy 66.51178688320836\n",
      "Starting Training\n",
      "Training:: Epoch 110, Iteration 0, Current loss 0.4688801169395447 Accuracy 98.92815016154674\n",
      "Training:: Epoch 110, Iteration 10, Current loss 0.47813931107521057 Accuracy 99.43817777971343\n",
      "Training:: Epoch 110, Iteration 20, Current loss 0.5653936266899109 Accuracy 99.06611878968995\n",
      "Training:: Epoch 110, Iteration 30, Current loss 0.5295034050941467 Accuracy 99.34797350164311\n",
      "Training:: Epoch 110, Iteration 40, Current loss 0.5942462086677551 Accuracy 99.29263100061809\n",
      "Training:: Epoch 110, Iteration 50, Current loss 0.7337056994438171 Accuracy 99.09697457724606\n",
      "Training:: Epoch 110, Iteration 60, Current loss 0.5100620985031128 Accuracy 99.33981197845146\n",
      "Training:: Epoch 110, Iteration 70, Current loss 0.4718920886516571 Accuracy 99.33809214795588\n",
      "Training:: Epoch 110, Iteration 80, Current loss 0.6964524984359741 Accuracy 99.24395761400167\n",
      "Training:: Epoch 110, Iteration 90, Current loss 0.6095398664474487 Accuracy 99.27353595255745\n",
      "Training:: Epoch 110, Iteration 100, Current loss 0.46040377020835876 Accuracy 99.39201131141746\n",
      "Training:: Epoch 110, Iteration 110, Current loss 0.46782034635543823 Accuracy 99.24275972013108\n",
      "Training:: Epoch 110, Iteration 120, Current loss 0.6113981604576111 Accuracy 99.11346250089369\n",
      "Training:: Epoch 110, Iteration 130, Current loss 0.5018497705459595 Accuracy 99.18613625201712\n",
      "Training:: Epoch 110, Iteration 140, Current loss 0.6417301893234253 Accuracy 99.31277168288139\n",
      "Training:: Epoch 110, Iteration 150, Current loss 0.6070542335510254 Accuracy 99.07550930026572\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 110, Probability Accuracy 66.997141318308\n",
      "Starting Training\n",
      "Training:: Epoch 111, Iteration 0, Current loss 0.6018428802490234 Accuracy 99.24658498802985\n",
      "Training:: Epoch 111, Iteration 10, Current loss 0.49289974570274353 Accuracy 99.36705936705937\n",
      "Training:: Epoch 111, Iteration 20, Current loss 0.8528538942337036 Accuracy 99.17758148035334\n",
      "Training:: Epoch 111, Iteration 30, Current loss 0.6663639545440674 Accuracy 99.03328572997913\n",
      "Training:: Epoch 111, Iteration 40, Current loss 0.475857675075531 Accuracy 99.14408866995073\n",
      "Training:: Epoch 111, Iteration 50, Current loss 0.45024818181991577 Accuracy 99.27112650645883\n",
      "Training:: Epoch 111, Iteration 60, Current loss 0.477248877286911 Accuracy 99.25671490511853\n",
      "Training:: Epoch 111, Iteration 70, Current loss 0.49845433235168457 Accuracy 99.28031002029894\n",
      "Training:: Epoch 111, Iteration 80, Current loss 0.4768299460411072 Accuracy 99.00857249347744\n",
      "Training:: Epoch 111, Iteration 90, Current loss 0.3539769649505615 Accuracy 99.4913481650504\n",
      "Training:: Epoch 111, Iteration 100, Current loss 0.4366341829299927 Accuracy 99.42240280433397\n",
      "Training:: Epoch 111, Iteration 110, Current loss 0.5264179706573486 Accuracy 98.93780002042692\n",
      "Training:: Epoch 111, Iteration 120, Current loss 0.5139945149421692 Accuracy 99.1825613079019\n",
      "Training:: Epoch 111, Iteration 130, Current loss 0.4604485034942627 Accuracy 99.3510966104767\n",
      "Training:: Epoch 111, Iteration 140, Current loss 0.45363521575927734 Accuracy 99.40605730866885\n",
      "Training:: Epoch 111, Iteration 150, Current loss 0.7126870155334473 Accuracy 99.12127716662195\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 111, Probability Accuracy 66.06548038281477\n",
      "Starting Training\n",
      "Training:: Epoch 112, Iteration 0, Current loss 0.5191103219985962 Accuracy 99.31913757426074\n",
      "Training:: Epoch 112, Iteration 10, Current loss 0.46197837591171265 Accuracy 99.17700112739571\n",
      "Training:: Epoch 112, Iteration 20, Current loss 0.7356778383255005 Accuracy 99.08754908754909\n",
      "Training:: Epoch 112, Iteration 30, Current loss 0.5566511750221252 Accuracy 99.29221435793731\n",
      "Training:: Epoch 112, Iteration 40, Current loss 0.5264042615890503 Accuracy 99.28006155198945\n",
      "Training:: Epoch 112, Iteration 50, Current loss 0.5931735038757324 Accuracy 99.3278243709916\n",
      "Training:: Epoch 112, Iteration 60, Current loss 0.38866254687309265 Accuracy 99.32575281412491\n",
      "Training:: Epoch 112, Iteration 70, Current loss 0.5302879810333252 Accuracy 99.26960257787326\n",
      "Training:: Epoch 112, Iteration 80, Current loss 0.4667564630508423 Accuracy 99.34665854603843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 112, Iteration 90, Current loss 0.49854665994644165 Accuracy 99.27664179529532\n",
      "Training:: Epoch 112, Iteration 100, Current loss 0.5484259724617004 Accuracy 99.09658899020602\n",
      "Training:: Epoch 112, Iteration 110, Current loss 0.47807133197784424 Accuracy 99.40641247833622\n",
      "Training:: Epoch 112, Iteration 120, Current loss 0.513404130935669 Accuracy 99.33840351202622\n",
      "Training:: Epoch 112, Iteration 130, Current loss 0.4928802251815796 Accuracy 99.30730126808558\n",
      "Training:: Epoch 112, Iteration 140, Current loss 0.5919713377952576 Accuracy 99.00403768506057\n",
      "Training:: Epoch 112, Iteration 150, Current loss 0.5642988681793213 Accuracy 99.19612951246744\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 112, Probability Accuracy 66.46341716037618\n",
      "Starting Training\n",
      "Training:: Epoch 113, Iteration 0, Current loss 0.363701730966568 Accuracy 99.63764874295428\n",
      "Training:: Epoch 113, Iteration 10, Current loss 0.5160214304924011 Accuracy 99.53486440466797\n",
      "Training:: Epoch 113, Iteration 20, Current loss 0.4234257638454437 Accuracy 99.31954847557759\n",
      "Training:: Epoch 113, Iteration 30, Current loss 0.4834185838699341 Accuracy 99.23298178331736\n",
      "Training:: Epoch 113, Iteration 40, Current loss 0.5656479001045227 Accuracy 99.41228851291184\n",
      "Training:: Epoch 113, Iteration 50, Current loss 0.5697262287139893 Accuracy 98.99371918687108\n",
      "Training:: Epoch 113, Iteration 60, Current loss 0.5391913056373596 Accuracy 99.46062905704161\n",
      "Training:: Epoch 113, Iteration 70, Current loss 0.4748746454715729 Accuracy 99.36797000535618\n",
      "Training:: Epoch 113, Iteration 80, Current loss 0.6621271371841431 Accuracy 99.21766897285791\n",
      "Training:: Epoch 113, Iteration 90, Current loss 0.4873116910457611 Accuracy 99.18501028258055\n",
      "Training:: Epoch 113, Iteration 100, Current loss 0.7205679416656494 Accuracy 98.9282322615656\n",
      "Training:: Epoch 113, Iteration 110, Current loss 0.5034886002540588 Accuracy 99.39309949345312\n",
      "Training:: Epoch 113, Iteration 120, Current loss 0.418533056974411 Accuracy 99.33834048640915\n",
      "Training:: Epoch 113, Iteration 130, Current loss 0.6428574323654175 Accuracy 98.8165111132493\n",
      "Training:: Epoch 113, Iteration 140, Current loss 0.4252805709838867 Accuracy 99.10581703144167\n",
      "Training:: Epoch 113, Iteration 150, Current loss 0.5299548506736755 Accuracy 98.92688250272826\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 113, Probability Accuracy 65.97133032274103\n",
      "Starting Training\n",
      "Training:: Epoch 114, Iteration 0, Current loss 0.4499756395816803 Accuracy 99.21482747569704\n",
      "Training:: Epoch 114, Iteration 10, Current loss 0.39061105251312256 Accuracy 99.17680074836295\n",
      "Training:: Epoch 114, Iteration 20, Current loss 0.5875301361083984 Accuracy 98.64852889046438\n",
      "Training:: Epoch 114, Iteration 30, Current loss 0.5479477643966675 Accuracy 99.038974906567\n",
      "Training:: Epoch 114, Iteration 40, Current loss 0.47401291131973267 Accuracy 99.15301563663441\n",
      "Training:: Epoch 114, Iteration 50, Current loss 0.6377645134925842 Accuracy 99.21965317919076\n",
      "Training:: Epoch 114, Iteration 60, Current loss 0.4418185353279114 Accuracy 99.37315984423972\n",
      "Training:: Epoch 114, Iteration 70, Current loss 0.404049277305603 Accuracy 99.16287969385314\n",
      "Training:: Epoch 114, Iteration 80, Current loss 0.5741995573043823 Accuracy 98.83674288006418\n",
      "Training:: Epoch 114, Iteration 90, Current loss 0.45223742723464966 Accuracy 99.28426450165581\n",
      "Training:: Epoch 114, Iteration 100, Current loss 0.4056070148944855 Accuracy 99.31814402128721\n",
      "Training:: Epoch 114, Iteration 110, Current loss 0.9299520254135132 Accuracy 98.86621315192744\n",
      "Training:: Epoch 114, Iteration 120, Current loss 0.39463090896606445 Accuracy 99.24608702778005\n",
      "Training:: Epoch 114, Iteration 130, Current loss 0.42789480090141296 Accuracy 99.32658147303674\n",
      "Training:: Epoch 114, Iteration 140, Current loss 0.5564271211624146 Accuracy 99.40685564521691\n",
      "Training:: Epoch 114, Iteration 150, Current loss 0.5853439569473267 Accuracy 99.20197496842347\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 114, Probability Accuracy 65.63729958155528\n",
      "Starting Training\n",
      "Training:: Epoch 115, Iteration 0, Current loss 0.5921486616134644 Accuracy 99.04329107868931\n",
      "Training:: Epoch 115, Iteration 10, Current loss 0.5618805289268494 Accuracy 99.17025949727487\n",
      "Training:: Epoch 115, Iteration 20, Current loss 0.5975479483604431 Accuracy 99.2114342040414\n",
      "Training:: Epoch 115, Iteration 30, Current loss 0.4902400076389313 Accuracy 99.39444663253249\n",
      "Training:: Epoch 115, Iteration 40, Current loss 0.5251560807228088 Accuracy 99.24401245155963\n",
      "Training:: Epoch 115, Iteration 50, Current loss 0.5046265125274658 Accuracy 99.27482921702575\n",
      "Training:: Epoch 115, Iteration 60, Current loss 0.374548077583313 Accuracy 99.46368959667268\n",
      "Training:: Epoch 115, Iteration 70, Current loss 0.502568244934082 Accuracy 99.1699604743083\n",
      "Training:: Epoch 115, Iteration 80, Current loss 0.506963849067688 Accuracy 99.14057704112953\n",
      "Training:: Epoch 115, Iteration 90, Current loss 0.6148946285247803 Accuracy 99.04289864980345\n",
      "Training:: Epoch 115, Iteration 100, Current loss 0.5211290717124939 Accuracy 99.249597998928\n",
      "Training:: Epoch 115, Iteration 110, Current loss 0.772085428237915 Accuracy 98.81575323602313\n",
      "Training:: Epoch 115, Iteration 120, Current loss 0.4173063039779663 Accuracy 99.35651127262626\n",
      "Training:: Epoch 115, Iteration 130, Current loss 0.5829260945320129 Accuracy 99.25793596262196\n",
      "Training:: Epoch 115, Iteration 140, Current loss 0.4636608362197876 Accuracy 99.4175100023535\n",
      "Training:: Epoch 115, Iteration 150, Current loss 0.4053625464439392 Accuracy 99.4339535099522\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 115, Probability Accuracy 65.39410448688736\n",
      "Starting Training\n",
      "Training:: Epoch 116, Iteration 0, Current loss 0.4355350136756897 Accuracy 99.39166219359456\n",
      "Training:: Epoch 116, Iteration 10, Current loss 0.5129541158676147 Accuracy 99.29639959432049\n",
      "Training:: Epoch 116, Iteration 20, Current loss 0.5678828954696655 Accuracy 99.51101321585904\n",
      "Training:: Epoch 116, Iteration 30, Current loss 0.35888034105300903 Accuracy 99.60852544584603\n",
      "Training:: Epoch 116, Iteration 40, Current loss 0.3986101746559143 Accuracy 99.56115731999796\n",
      "Training:: Epoch 116, Iteration 50, Current loss 0.49140042066574097 Accuracy 99.05257874262769\n",
      "Training:: Epoch 116, Iteration 60, Current loss 0.4900675415992737 Accuracy 99.36685830725565\n",
      "Training:: Epoch 116, Iteration 70, Current loss 0.35929200053215027 Accuracy 99.48061146196069\n",
      "Training:: Epoch 116, Iteration 80, Current loss 0.38102665543556213 Accuracy 99.27560710130241\n",
      "Training:: Epoch 116, Iteration 90, Current loss 0.34597304463386536 Accuracy 99.50946608829611\n",
      "Training:: Epoch 116, Iteration 100, Current loss 0.48405569791793823 Accuracy 99.43150759811961\n",
      "Training:: Epoch 116, Iteration 110, Current loss 0.33644816279411316 Accuracy 99.35665706758164\n",
      "Training:: Epoch 116, Iteration 120, Current loss 0.62101811170578 Accuracy 98.92278446637096\n",
      "Training:: Epoch 116, Iteration 130, Current loss 0.6043184995651245 Accuracy 99.11830166067455\n",
      "Training:: Epoch 116, Iteration 140, Current loss 0.6455836892127991 Accuracy 99.23397392823546\n",
      "Training:: Epoch 116, Iteration 150, Current loss 0.6338264346122742 Accuracy 98.5355435777484\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 116, Probability Accuracy 65.57183991382524\n",
      "Starting Training\n",
      "Training:: Epoch 117, Iteration 0, Current loss 0.39802417159080505 Accuracy 99.31772575250837\n",
      "Training:: Epoch 117, Iteration 10, Current loss 0.6637997031211853 Accuracy 99.12714097496706\n",
      "Training:: Epoch 117, Iteration 20, Current loss 0.5053744912147522 Accuracy 99.45252251721905\n",
      "Training:: Epoch 117, Iteration 30, Current loss 0.6141195297241211 Accuracy 99.11849249441073\n",
      "Training:: Epoch 117, Iteration 40, Current loss 0.4749370813369751 Accuracy 99.19256344144388\n",
      "Training:: Epoch 117, Iteration 50, Current loss 0.4695046544075012 Accuracy 99.51063829787235\n",
      "Training:: Epoch 117, Iteration 60, Current loss 0.45871174335479736 Accuracy 99.37899219304471\n",
      "Training:: Epoch 117, Iteration 70, Current loss 0.577509343624115 Accuracy 99.16472416472416\n",
      "Training:: Epoch 117, Iteration 80, Current loss 0.4891900420188904 Accuracy 99.09154884524342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 117, Iteration 90, Current loss 0.49652644991874695 Accuracy 99.4641281049048\n",
      "Training:: Epoch 117, Iteration 100, Current loss 0.5415360927581787 Accuracy 99.22375665673798\n",
      "Training:: Epoch 117, Iteration 110, Current loss 0.53877854347229 Accuracy 99.02578796561605\n",
      "Training:: Epoch 117, Iteration 120, Current loss 0.4753129780292511 Accuracy 99.43289224952741\n",
      "Training:: Epoch 117, Iteration 130, Current loss 0.3725583851337433 Accuracy 99.27511218501898\n",
      "Training:: Epoch 117, Iteration 140, Current loss 0.39621466398239136 Accuracy 99.2619926199262\n",
      "Training:: Epoch 117, Iteration 150, Current loss 0.45047861337661743 Accuracy 99.2351684696479\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 117, Probability Accuracy 65.29135766665286\n",
      "Starting Training\n",
      "Training:: Epoch 118, Iteration 0, Current loss 0.40967175364494324 Accuracy 99.50756090120713\n",
      "Training:: Epoch 118, Iteration 10, Current loss 0.47227850556373596 Accuracy 99.42497046081134\n",
      "Training:: Epoch 118, Iteration 20, Current loss 0.5840655565261841 Accuracy 99.0881199931822\n",
      "Training:: Epoch 118, Iteration 30, Current loss 0.36191368103027344 Accuracy 99.40309744030975\n",
      "Training:: Epoch 118, Iteration 40, Current loss 0.43914273381233215 Accuracy 99.43237525156097\n",
      "Training:: Epoch 118, Iteration 50, Current loss 0.48516330122947693 Accuracy 99.46347769877181\n",
      "Training:: Epoch 118, Iteration 60, Current loss 0.3950504958629608 Accuracy 99.33933089682317\n",
      "Training:: Epoch 118, Iteration 70, Current loss 0.5189154148101807 Accuracy 99.2716770264331\n",
      "Training:: Epoch 118, Iteration 80, Current loss 0.38578978180885315 Accuracy 99.11395520551316\n",
      "Training:: Epoch 118, Iteration 90, Current loss 0.5103801488876343 Accuracy 99.36643583321055\n",
      "Training:: Epoch 118, Iteration 100, Current loss 0.42965805530548096 Accuracy 99.50447636893608\n",
      "Training:: Epoch 118, Iteration 110, Current loss 0.51767498254776 Accuracy 99.30414868316744\n",
      "Training:: Epoch 118, Iteration 120, Current loss 0.5752089619636536 Accuracy 98.9785872507999\n",
      "Training:: Epoch 118, Iteration 130, Current loss 0.400664746761322 Accuracy 99.3197836420259\n",
      "Training:: Epoch 118, Iteration 140, Current loss 0.6071329116821289 Accuracy 99.2533223729278\n",
      "Training:: Epoch 118, Iteration 150, Current loss 0.34176644682884216 Accuracy 99.3925149159284\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 118, Probability Accuracy 64.48875170899449\n",
      "Starting Training\n",
      "Training:: Epoch 119, Iteration 0, Current loss 0.4902419149875641 Accuracy 99.29174611822391\n",
      "Training:: Epoch 119, Iteration 10, Current loss 0.49747997522354126 Accuracy 99.31336742147553\n",
      "Training:: Epoch 119, Iteration 20, Current loss 0.41556188464164734 Accuracy 99.4980234674029\n",
      "Training:: Epoch 119, Iteration 30, Current loss 0.4145604074001312 Accuracy 99.14390312588003\n",
      "Training:: Epoch 119, Iteration 40, Current loss 0.5578067302703857 Accuracy 99.00922446190638\n",
      "Training:: Epoch 119, Iteration 50, Current loss 0.49150529503822327 Accuracy 99.1754068716094\n",
      "Training:: Epoch 119, Iteration 60, Current loss 0.4978697597980499 Accuracy 99.05650087260035\n",
      "Training:: Epoch 119, Iteration 70, Current loss 0.36056581139564514 Accuracy 99.31542495696385\n",
      "Training:: Epoch 119, Iteration 80, Current loss 0.561773419380188 Accuracy 99.19180136643892\n",
      "Training:: Epoch 119, Iteration 90, Current loss 0.5088054537773132 Accuracy 98.94456762749445\n",
      "Training:: Epoch 119, Iteration 100, Current loss 0.4849351942539215 Accuracy 99.26971159797004\n",
      "Training:: Epoch 119, Iteration 110, Current loss 0.4196079969406128 Accuracy 99.39316360878017\n",
      "Training:: Epoch 119, Iteration 120, Current loss 0.32982203364372253 Accuracy 99.4121202131751\n",
      "Training:: Epoch 119, Iteration 130, Current loss 0.44007793068885803 Accuracy 99.07240634005764\n",
      "Training:: Epoch 119, Iteration 140, Current loss 0.6618877053260803 Accuracy 98.99449982811963\n",
      "Training:: Epoch 119, Iteration 150, Current loss 0.4790317118167877 Accuracy 99.26584956137621\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 119, Probability Accuracy 64.45726478021295\n",
      "Starting Training\n",
      "Training:: Epoch 120, Iteration 0, Current loss 0.40605422854423523 Accuracy 99.30604233842703\n",
      "Training:: Epoch 120, Iteration 10, Current loss 0.4333254396915436 Accuracy 99.47805065655733\n",
      "Training:: Epoch 120, Iteration 20, Current loss 0.39067721366882324 Accuracy 99.31371336713244\n",
      "Training:: Epoch 120, Iteration 30, Current loss 0.5329375863075256 Accuracy 99.13116378324823\n",
      "Training:: Epoch 120, Iteration 40, Current loss 0.4925262928009033 Accuracy 99.2949081961007\n",
      "Training:: Epoch 120, Iteration 50, Current loss 0.5501621961593628 Accuracy 99.24287343215508\n",
      "Training:: Epoch 120, Iteration 60, Current loss 0.4763667583465576 Accuracy 99.13711209626346\n",
      "Training:: Epoch 120, Iteration 70, Current loss 0.40811336040496826 Accuracy 99.30559939397766\n",
      "Training:: Epoch 120, Iteration 80, Current loss 0.5476452112197876 Accuracy 99.08993192404157\n",
      "Training:: Epoch 120, Iteration 90, Current loss 0.39603638648986816 Accuracy 99.42230148883375\n",
      "Training:: Epoch 120, Iteration 100, Current loss 0.7088167071342468 Accuracy 99.0409764603313\n",
      "Training:: Epoch 120, Iteration 110, Current loss 0.41843658685684204 Accuracy 99.22901375439463\n",
      "Training:: Epoch 120, Iteration 120, Current loss 0.33992621302604675 Accuracy 99.27890806077775\n",
      "Training:: Epoch 120, Iteration 130, Current loss 0.327644407749176 Accuracy 99.43636266036825\n",
      "Training:: Epoch 120, Iteration 140, Current loss 0.39246469736099243 Accuracy 99.25177061914553\n",
      "Training:: Epoch 120, Iteration 150, Current loss 1.0890917778015137 Accuracy 96.30840781919919\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 120, Probability Accuracy 65.5182914198119\n",
      "Starting Training\n",
      "Training:: Epoch 121, Iteration 0, Current loss 0.45159077644348145 Accuracy 98.93875615106421\n",
      "Training:: Epoch 121, Iteration 10, Current loss 0.5561254620552063 Accuracy 98.94171145101282\n",
      "Training:: Epoch 121, Iteration 20, Current loss 0.35896408557891846 Accuracy 99.04601571268238\n",
      "Training:: Epoch 121, Iteration 30, Current loss 0.3549876809120178 Accuracy 99.11014911014911\n",
      "Training:: Epoch 121, Iteration 40, Current loss 0.39787766337394714 Accuracy 99.46611909650925\n",
      "Training:: Epoch 121, Iteration 50, Current loss 0.642126202583313 Accuracy 98.9591322449874\n",
      "Training:: Epoch 121, Iteration 60, Current loss 0.6350219249725342 Accuracy 98.95899970921779\n",
      "Training:: Epoch 121, Iteration 70, Current loss 0.6208896636962891 Accuracy 98.34749763928234\n",
      "Training:: Epoch 121, Iteration 80, Current loss 0.4012572169303894 Accuracy 99.26853139648689\n",
      "Training:: Epoch 121, Iteration 90, Current loss 0.7005577087402344 Accuracy 98.43717656799835\n",
      "Training:: Epoch 121, Iteration 100, Current loss 0.5757496356964111 Accuracy 99.04423812124521\n",
      "Training:: Epoch 121, Iteration 110, Current loss 0.4194003641605377 Accuracy 99.41421412943751\n",
      "Training:: Epoch 121, Iteration 120, Current loss 0.40453311800956726 Accuracy 99.03894427311195\n",
      "Training:: Epoch 121, Iteration 130, Current loss 0.503551185131073 Accuracy 99.36176385262547\n",
      "Training:: Epoch 121, Iteration 140, Current loss 0.4011911153793335 Accuracy 99.33726067746686\n",
      "Training:: Epoch 121, Iteration 150, Current loss 0.36339324712753296 Accuracy 99.43640073850938\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 121, Probability Accuracy 64.4006090234909\n",
      "Starting Training\n",
      "Training:: Epoch 122, Iteration 0, Current loss 0.43343180418014526 Accuracy 99.4032350939643\n",
      "Training:: Epoch 122, Iteration 10, Current loss 0.5467333197593689 Accuracy 99.45448534273748\n",
      "Training:: Epoch 122, Iteration 20, Current loss 0.36548641324043274 Accuracy 98.91352063213346\n",
      "Training:: Epoch 122, Iteration 30, Current loss 0.43907320499420166 Accuracy 99.12573926459244\n",
      "Training:: Epoch 122, Iteration 40, Current loss 0.36392807960510254 Accuracy 99.20699832865118\n",
      "Training:: Epoch 122, Iteration 50, Current loss 0.38047054409980774 Accuracy 99.40343330221988\n",
      "Training:: Epoch 122, Iteration 60, Current loss 0.35483938455581665 Accuracy 99.31712646817809\n",
      "Training:: Epoch 122, Iteration 70, Current loss 0.3904266357421875 Accuracy 99.04268049461508\n",
      "Training:: Epoch 122, Iteration 80, Current loss 0.571427583694458 Accuracy 98.98029726927065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 122, Iteration 90, Current loss 0.43030619621276855 Accuracy 99.18167687665722\n",
      "Training:: Epoch 122, Iteration 100, Current loss 0.4299054741859436 Accuracy 99.15764139590854\n",
      "Training:: Epoch 122, Iteration 110, Current loss 0.7292227745056152 Accuracy 97.87614233930668\n",
      "Training:: Epoch 122, Iteration 120, Current loss 0.5788427591323853 Accuracy 98.87634007911483\n",
      "Training:: Epoch 122, Iteration 130, Current loss 0.9646243453025818 Accuracy 98.039430449069\n",
      "Training:: Epoch 122, Iteration 140, Current loss 0.4624815285205841 Accuracy 98.71950503191448\n",
      "Training:: Epoch 122, Iteration 150, Current loss 0.6987612247467041 Accuracy 98.11814486943933\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 122, Probability Accuracy 64.60123461904959\n",
      "Starting Training\n",
      "Training:: Epoch 123, Iteration 0, Current loss 0.5945984721183777 Accuracy 97.99763932214822\n",
      "Training:: Epoch 123, Iteration 10, Current loss 0.5123807787895203 Accuracy 99.16515931543064\n",
      "Training:: Epoch 123, Iteration 20, Current loss 0.43646690249443054 Accuracy 98.8171985721136\n",
      "Training:: Epoch 123, Iteration 30, Current loss 0.700977623462677 Accuracy 98.4533183352081\n",
      "Training:: Epoch 123, Iteration 40, Current loss 0.7696844935417175 Accuracy 96.99854971940223\n",
      "Training:: Epoch 123, Iteration 50, Current loss 1.1993669271469116 Accuracy 95.69320106460198\n",
      "Training:: Epoch 123, Iteration 60, Current loss 1.0390818119049072 Accuracy 96.66194618652641\n",
      "Training:: Epoch 123, Iteration 70, Current loss 1.1657567024230957 Accuracy 94.8608895977317\n",
      "Training:: Epoch 123, Iteration 80, Current loss 2.5829851627349854 Accuracy 86.89140923616768\n",
      "Training:: Epoch 123, Iteration 90, Current loss 2.8356213569641113 Accuracy 85.52388935456831\n",
      "Training:: Epoch 123, Iteration 100, Current loss 2.005100965499878 Accuracy 88.58355437665783\n",
      "Training:: Epoch 123, Iteration 110, Current loss 1.8107348680496216 Accuracy 90.88723892954437\n",
      "Training:: Epoch 123, Iteration 120, Current loss 3.952200412750244 Accuracy 78.48044549702362\n",
      "Training:: Epoch 123, Iteration 130, Current loss 5.2912821769714355 Accuracy 72.32931418477844\n",
      "Training:: Epoch 123, Iteration 140, Current loss 3.540131092071533 Accuracy 80.38742089754118\n",
      "Training:: Epoch 123, Iteration 150, Current loss 3.319312810897827 Accuracy 75.82346955171121\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 123, Probability Accuracy 57.66199196254713\n",
      "Starting Training\n",
      "Training:: Epoch 124, Iteration 0, Current loss 3.812253713607788 Accuracy 85.8787050662182\n",
      "Training:: Epoch 124, Iteration 10, Current loss 1.520952820777893 Accuracy 96.03601930932047\n",
      "Training:: Epoch 124, Iteration 20, Current loss 1.7770469188690186 Accuracy 94.08472012102874\n",
      "Training:: Epoch 124, Iteration 30, Current loss 1.5071909427642822 Accuracy 94.31877615062761\n",
      "Training:: Epoch 124, Iteration 40, Current loss 1.1043930053710938 Accuracy 95.98471741637832\n",
      "Training:: Epoch 124, Iteration 50, Current loss 2.610933542251587 Accuracy 85.39231641761602\n",
      "Training:: Epoch 124, Iteration 60, Current loss 1.5502090454101562 Accuracy 94.10096426545661\n",
      "Training:: Epoch 124, Iteration 70, Current loss 1.7322496175765991 Accuracy 95.5340699815838\n",
      "Training:: Epoch 124, Iteration 80, Current loss 0.8039611577987671 Accuracy 97.11013908684328\n",
      "Training:: Epoch 124, Iteration 90, Current loss 1.3354827165603638 Accuracy 93.82463323097919\n",
      "Training:: Epoch 124, Iteration 100, Current loss 1.1771950721740723 Accuracy 95.79613095238095\n",
      "Training:: Epoch 124, Iteration 110, Current loss 1.313141107559204 Accuracy 95.9638593945789\n",
      "Training:: Epoch 124, Iteration 120, Current loss 1.5781792402267456 Accuracy 94.86191920019667\n",
      "Training:: Epoch 124, Iteration 130, Current loss 0.9233957529067993 Accuracy 97.44239047860218\n",
      "Training:: Epoch 124, Iteration 140, Current loss 1.6161081790924072 Accuracy 95.60348200312988\n",
      "Training:: Epoch 124, Iteration 150, Current loss 1.4108049869537354 Accuracy 95.89087046458322\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 124, Probability Accuracy 63.9709781663007\n",
      "Starting Training\n",
      "Training:: Epoch 125, Iteration 0, Current loss 1.0090432167053223 Accuracy 97.30112249877989\n",
      "Training:: Epoch 125, Iteration 10, Current loss 1.3529654741287231 Accuracy 93.1228861330327\n",
      "Training:: Epoch 125, Iteration 20, Current loss 1.6253854036331177 Accuracy 93.51578469225528\n",
      "Training:: Epoch 125, Iteration 30, Current loss 0.763587474822998 Accuracy 98.17634953300617\n",
      "Training:: Epoch 125, Iteration 40, Current loss 2.9541335105895996 Accuracy 84.37159603713886\n",
      "Training:: Epoch 125, Iteration 50, Current loss 2.416425943374634 Accuracy 84.25714546616747\n",
      "Training:: Epoch 125, Iteration 60, Current loss 1.5874347686767578 Accuracy 93.56855739169272\n",
      "Training:: Epoch 125, Iteration 70, Current loss 1.004532814025879 Accuracy 96.53453596012342\n",
      "Training:: Epoch 125, Iteration 80, Current loss 1.2500615119934082 Accuracy 95.10325213305305\n",
      "Training:: Epoch 125, Iteration 90, Current loss 0.8038821220397949 Accuracy 97.54088985474094\n",
      "Training:: Epoch 125, Iteration 100, Current loss 1.2882944345474243 Accuracy 95.27374009922535\n",
      "Training:: Epoch 125, Iteration 110, Current loss 0.8796601891517639 Accuracy 96.59449916372421\n",
      "Training:: Epoch 125, Iteration 120, Current loss 0.6175592541694641 Accuracy 98.9639696321463\n",
      "Training:: Epoch 125, Iteration 130, Current loss 0.9134057760238647 Accuracy 97.46376811594203\n",
      "Training:: Epoch 125, Iteration 140, Current loss 0.862266480922699 Accuracy 97.67579196018053\n",
      "Training:: Epoch 125, Iteration 150, Current loss 1.2324750423431396 Accuracy 96.83329055441479\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 125, Probability Accuracy 62.08694121058955\n",
      "Starting Training\n",
      "Training:: Epoch 126, Iteration 0, Current loss 1.030036211013794 Accuracy 97.07977736549165\n",
      "Training:: Epoch 126, Iteration 10, Current loss 0.7509149312973022 Accuracy 97.62480953661378\n",
      "Training:: Epoch 126, Iteration 20, Current loss 0.8172767758369446 Accuracy 98.48443547947446\n",
      "Training:: Epoch 126, Iteration 30, Current loss 2.5416111946105957 Accuracy 90.79506894033761\n",
      "Training:: Epoch 126, Iteration 40, Current loss 1.0437649488449097 Accuracy 92.59720674054161\n",
      "Training:: Epoch 126, Iteration 50, Current loss 0.9178727865219116 Accuracy 95.80820445452918\n",
      "Training:: Epoch 126, Iteration 60, Current loss 0.7419223189353943 Accuracy 98.5363493407524\n",
      "Training:: Epoch 126, Iteration 70, Current loss 0.6198164820671082 Accuracy 98.03092968890488\n",
      "Training:: Epoch 126, Iteration 80, Current loss 0.7169814705848694 Accuracy 98.62080155768295\n",
      "Training:: Epoch 126, Iteration 90, Current loss 0.727031409740448 Accuracy 97.86769088139859\n",
      "Training:: Epoch 126, Iteration 100, Current loss 0.5596257448196411 Accuracy 98.61621011013838\n",
      "Training:: Epoch 126, Iteration 110, Current loss 0.7462848424911499 Accuracy 98.83744137311646\n",
      "Training:: Epoch 126, Iteration 120, Current loss 1.0022019147872925 Accuracy 97.91952894995093\n",
      "Training:: Epoch 126, Iteration 130, Current loss 0.6510862112045288 Accuracy 98.77506078818186\n",
      "Training:: Epoch 126, Iteration 140, Current loss 0.8685289621353149 Accuracy 98.83333333333333\n",
      "Training:: Epoch 126, Iteration 150, Current loss 1.0428500175476074 Accuracy 97.39326231921711\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 126, Probability Accuracy 66.9672080208808\n",
      "Starting Training\n",
      "Training:: Epoch 127, Iteration 0, Current loss 0.7701931595802307 Accuracy 98.87727448703059\n",
      "Training:: Epoch 127, Iteration 10, Current loss 0.7886016964912415 Accuracy 98.6269298911668\n",
      "Training:: Epoch 127, Iteration 20, Current loss 0.7462260127067566 Accuracy 98.83409484795966\n",
      "Training:: Epoch 127, Iteration 30, Current loss 0.5188979506492615 Accuracy 98.96888346552777\n",
      "Training:: Epoch 127, Iteration 40, Current loss 0.5115118622779846 Accuracy 99.25242200015256\n",
      "Training:: Epoch 127, Iteration 50, Current loss 0.7142614126205444 Accuracy 99.0667655139331\n",
      "Training:: Epoch 127, Iteration 60, Current loss 0.5565309524536133 Accuracy 99.17251638696482\n",
      "Training:: Epoch 127, Iteration 70, Current loss 0.6013492941856384 Accuracy 98.79335355400238\n",
      "Training:: Epoch 127, Iteration 80, Current loss 0.5809147953987122 Accuracy 99.1227089852481\n",
      "Training:: Epoch 127, Iteration 90, Current loss 0.5873656868934631 Accuracy 98.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 127, Iteration 100, Current loss 0.6068020462989807 Accuracy 99.07478797224364\n",
      "Training:: Epoch 127, Iteration 110, Current loss 0.5325449705123901 Accuracy 99.20295753629087\n",
      "Training:: Epoch 127, Iteration 120, Current loss 0.6488892436027527 Accuracy 99.09572589191099\n",
      "Training:: Epoch 127, Iteration 130, Current loss 0.5631281733512878 Accuracy 98.77665729872706\n",
      "Training:: Epoch 127, Iteration 140, Current loss 0.6588969826698303 Accuracy 98.77203647416414\n",
      "Training:: Epoch 127, Iteration 150, Current loss 0.7286840677261353 Accuracy 99.03804489123841\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 127, Probability Accuracy 67.00014500559307\n",
      "Starting Training\n",
      "Training:: Epoch 128, Iteration 0, Current loss 0.7402961850166321 Accuracy 98.86429258902791\n",
      "Training:: Epoch 128, Iteration 10, Current loss 0.5920819044113159 Accuracy 98.90163534293386\n",
      "Training:: Epoch 128, Iteration 20, Current loss 0.5563989877700806 Accuracy 99.11824226655102\n",
      "Training:: Epoch 128, Iteration 30, Current loss 0.4392954409122467 Accuracy 99.1176664252321\n",
      "Training:: Epoch 128, Iteration 40, Current loss 0.567061722278595 Accuracy 98.83430474604496\n",
      "Training:: Epoch 128, Iteration 50, Current loss 0.5122211575508118 Accuracy 99.20845885758166\n",
      "Training:: Epoch 128, Iteration 60, Current loss 0.5534143447875977 Accuracy 99.33053557154277\n",
      "Training:: Epoch 128, Iteration 70, Current loss 0.5518401861190796 Accuracy 98.5372049226996\n",
      "Training:: Epoch 128, Iteration 80, Current loss 0.44225025177001953 Accuracy 99.26606209652628\n",
      "Training:: Epoch 128, Iteration 90, Current loss 0.6400383710861206 Accuracy 99.0011580775912\n",
      "Training:: Epoch 128, Iteration 100, Current loss 0.5069029927253723 Accuracy 99.31041325594086\n",
      "Training:: Epoch 128, Iteration 110, Current loss 0.5803881883621216 Accuracy 98.93999504091248\n",
      "Training:: Epoch 128, Iteration 120, Current loss 0.5375975966453552 Accuracy 99.0588127266884\n",
      "Training:: Epoch 128, Iteration 130, Current loss 0.3711664080619812 Accuracy 99.30476237771266\n",
      "Training:: Epoch 128, Iteration 140, Current loss 0.6217414736747742 Accuracy 99.11801487527151\n",
      "Training:: Epoch 128, Iteration 150, Current loss 0.8346384763717651 Accuracy 99.28232434309527\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 128, Probability Accuracy 66.36294899946141\n",
      "Starting Training\n",
      "Training:: Epoch 129, Iteration 0, Current loss 0.5847998261451721 Accuracy 99.10220994475138\n",
      "Training:: Epoch 129, Iteration 10, Current loss 0.5956753492355347 Accuracy 99.01249100778236\n",
      "Training:: Epoch 129, Iteration 20, Current loss 0.4141222834587097 Accuracy 99.27664704630877\n",
      "Training:: Epoch 129, Iteration 30, Current loss 0.5477553606033325 Accuracy 98.98989898989899\n",
      "Training:: Epoch 129, Iteration 40, Current loss 0.4219999313354492 Accuracy 99.25922640170333\n",
      "Training:: Epoch 129, Iteration 50, Current loss 0.5228272080421448 Accuracy 99.2854515840908\n",
      "Training:: Epoch 129, Iteration 60, Current loss 0.5753805637359619 Accuracy 99.24514292785547\n",
      "Training:: Epoch 129, Iteration 70, Current loss 0.4950540065765381 Accuracy 99.0590248075278\n",
      "Training:: Epoch 129, Iteration 80, Current loss 0.4347616732120514 Accuracy 99.29782765430551\n",
      "Training:: Epoch 129, Iteration 90, Current loss 0.5015456676483154 Accuracy 98.95434227330779\n",
      "Training:: Epoch 129, Iteration 100, Current loss 0.5051220059394836 Accuracy 98.79973537472829\n",
      "Training:: Epoch 129, Iteration 110, Current loss 0.4985860288143158 Accuracy 99.23230974632844\n",
      "Training:: Epoch 129, Iteration 120, Current loss 0.6144852638244629 Accuracy 99.15470807941813\n",
      "Training:: Epoch 129, Iteration 130, Current loss 0.45688891410827637 Accuracy 99.49464012251148\n",
      "Training:: Epoch 129, Iteration 140, Current loss 0.44833827018737793 Accuracy 99.42974452554745\n",
      "Training:: Epoch 129, Iteration 150, Current loss 0.5429205298423767 Accuracy 99.07657252503448\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 129, Probability Accuracy 66.40904006297386\n",
      "Starting Training\n",
      "Training:: Epoch 130, Iteration 0, Current loss 0.48151224851608276 Accuracy 99.4765222182863\n",
      "Training:: Epoch 130, Iteration 10, Current loss 0.5871812701225281 Accuracy 99.33998169292288\n",
      "Training:: Epoch 130, Iteration 20, Current loss 0.514607310295105 Accuracy 99.3843646733964\n",
      "Training:: Epoch 130, Iteration 30, Current loss 0.522449791431427 Accuracy 99.1334808259587\n",
      "Training:: Epoch 130, Iteration 40, Current loss 0.5312013030052185 Accuracy 99.118843199277\n",
      "Training:: Epoch 130, Iteration 50, Current loss 0.5180441737174988 Accuracy 99.35006857074713\n",
      "Training:: Epoch 130, Iteration 60, Current loss 0.3819727897644043 Accuracy 99.2751677852349\n",
      "Training:: Epoch 130, Iteration 70, Current loss 0.4433434307575226 Accuracy 99.28231594226273\n",
      "Training:: Epoch 130, Iteration 80, Current loss 0.6835510730743408 Accuracy 99.26277288833009\n",
      "Training:: Epoch 130, Iteration 90, Current loss 0.5404438972473145 Accuracy 99.32095066906331\n",
      "Training:: Epoch 130, Iteration 100, Current loss 0.3261219263076782 Accuracy 99.19604573606479\n",
      "Training:: Epoch 130, Iteration 110, Current loss 0.718677818775177 Accuracy 98.8086529417912\n",
      "Training:: Epoch 130, Iteration 120, Current loss 0.5781861543655396 Accuracy 99.29303992830827\n",
      "Training:: Epoch 130, Iteration 130, Current loss 0.7355054616928101 Accuracy 99.03880899038809\n",
      "Training:: Epoch 130, Iteration 140, Current loss 0.46116411685943604 Accuracy 99.43550663279706\n",
      "Training:: Epoch 130, Iteration 150, Current loss 0.5165799856185913 Accuracy 99.35069493191071\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 130, Probability Accuracy 66.53074118573144\n",
      "Starting Training\n",
      "Training:: Epoch 131, Iteration 0, Current loss 0.5765758752822876 Accuracy 99.44411186179765\n",
      "Training:: Epoch 131, Iteration 10, Current loss 0.6627160906791687 Accuracy 99.24613146409204\n",
      "Training:: Epoch 131, Iteration 20, Current loss 0.5135245323181152 Accuracy 99.20287810383748\n",
      "Training:: Epoch 131, Iteration 30, Current loss 0.5677829384803772 Accuracy 99.01732882502114\n",
      "Training:: Epoch 131, Iteration 40, Current loss 0.5447182655334473 Accuracy 99.23237380058406\n",
      "Training:: Epoch 131, Iteration 50, Current loss 0.43435534834861755 Accuracy 99.40431868950111\n",
      "Training:: Epoch 131, Iteration 60, Current loss 0.4658748209476471 Accuracy 99.3351653222232\n",
      "Training:: Epoch 131, Iteration 70, Current loss 0.5744320154190063 Accuracy 99.317660194777\n",
      "Training:: Epoch 131, Iteration 80, Current loss 0.598246693611145 Accuracy 98.75386090105442\n",
      "Training:: Epoch 131, Iteration 90, Current loss 0.7725890874862671 Accuracy 98.57937729371375\n",
      "Training:: Epoch 131, Iteration 100, Current loss 0.5012383460998535 Accuracy 99.27565392354124\n",
      "Training:: Epoch 131, Iteration 110, Current loss 0.6382092833518982 Accuracy 99.29276172181962\n",
      "Training:: Epoch 131, Iteration 120, Current loss 0.4523169994354248 Accuracy 99.19758539458186\n",
      "Training:: Epoch 131, Iteration 130, Current loss 0.7199759483337402 Accuracy 99.17699760391707\n",
      "Training:: Epoch 131, Iteration 140, Current loss 0.3829942047595978 Accuracy 99.38800489596083\n",
      "Training:: Epoch 131, Iteration 150, Current loss 0.48123282194137573 Accuracy 99.31273062730628\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 131, Probability Accuracy 65.96832663545594\n",
      "Starting Training\n",
      "Training:: Epoch 132, Iteration 0, Current loss 0.6381735801696777 Accuracy 99.06625831030104\n",
      "Training:: Epoch 132, Iteration 10, Current loss 0.6066044569015503 Accuracy 99.32507653771222\n",
      "Training:: Epoch 132, Iteration 20, Current loss 0.4104553759098053 Accuracy 99.39103958242714\n",
      "Training:: Epoch 132, Iteration 30, Current loss 0.5314600467681885 Accuracy 99.45475910693303\n",
      "Training:: Epoch 132, Iteration 40, Current loss 0.511532723903656 Accuracy 99.41323481473432\n",
      "Training:: Epoch 132, Iteration 50, Current loss 0.4768500328063965 Accuracy 98.86525665399239\n",
      "Training:: Epoch 132, Iteration 60, Current loss 0.3948081433773041 Accuracy 99.36779471922648\n",
      "Training:: Epoch 132, Iteration 70, Current loss 0.35437676310539246 Accuracy 99.52792375523292\n",
      "Training:: Epoch 132, Iteration 80, Current loss 0.41373658180236816 Accuracy 99.28895612708018\n",
      "Training:: Epoch 132, Iteration 90, Current loss 0.9284826517105103 Accuracy 99.34275682002341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 132, Iteration 100, Current loss 0.6347201466560364 Accuracy 99.14746349286739\n",
      "Training:: Epoch 132, Iteration 110, Current loss 0.5422479510307312 Accuracy 99.22022490355413\n",
      "Training:: Epoch 132, Iteration 120, Current loss 0.5481919050216675 Accuracy 99.07058528008038\n",
      "Training:: Epoch 132, Iteration 130, Current loss 0.5514870882034302 Accuracy 99.19187937321375\n",
      "Training:: Epoch 132, Iteration 140, Current loss 0.5991522073745728 Accuracy 99.35698287695976\n",
      "Training:: Epoch 132, Iteration 150, Current loss 0.5236381888389587 Accuracy 99.00595077089532\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 132, Probability Accuracy 65.5641753324771\n",
      "Starting Training\n",
      "Training:: Epoch 133, Iteration 0, Current loss 0.3503459095954895 Accuracy 99.55506589262257\n",
      "Training:: Epoch 133, Iteration 10, Current loss 0.5657762885093689 Accuracy 99.35752309857432\n",
      "Training:: Epoch 133, Iteration 20, Current loss 0.4640781581401825 Accuracy 99.52741951363592\n",
      "Training:: Epoch 133, Iteration 30, Current loss 0.48948419094085693 Accuracy 99.24725469358837\n",
      "Training:: Epoch 133, Iteration 40, Current loss 0.5937251448631287 Accuracy 99.34345778897804\n",
      "Training:: Epoch 133, Iteration 50, Current loss 0.43287134170532227 Accuracy 99.1394148020654\n",
      "Training:: Epoch 133, Iteration 60, Current loss 0.479997456073761 Accuracy 99.22937989163155\n",
      "Training:: Epoch 133, Iteration 70, Current loss 0.5194473266601562 Accuracy 99.3891557995882\n",
      "Training:: Epoch 133, Iteration 80, Current loss 0.4208567440509796 Accuracy 99.28872315447828\n",
      "Training:: Epoch 133, Iteration 90, Current loss 0.9593243598937988 Accuracy 98.73058292968098\n",
      "Training:: Epoch 133, Iteration 100, Current loss 0.5074719190597534 Accuracy 99.33884297520662\n",
      "Training:: Epoch 133, Iteration 110, Current loss 0.3404989242553711 Accuracy 99.42160964804332\n",
      "Training:: Epoch 133, Iteration 120, Current loss 0.5042426586151123 Accuracy 99.32070075080443\n",
      "Training:: Epoch 133, Iteration 130, Current loss 0.4062781035900116 Accuracy 99.21198902413283\n",
      "Training:: Epoch 133, Iteration 140, Current loss 0.5983216166496277 Accuracy 99.1617620831104\n",
      "Training:: Epoch 133, Iteration 150, Current loss 0.33772993087768555 Accuracy 99.31822369633315\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 133, Probability Accuracy 65.74356796619298\n",
      "Starting Training\n",
      "Training:: Epoch 134, Iteration 0, Current loss 0.3877902626991272 Accuracy 99.36454536959162\n",
      "Training:: Epoch 134, Iteration 10, Current loss 0.5498220324516296 Accuracy 99.27540758323443\n",
      "Training:: Epoch 134, Iteration 20, Current loss 0.5596415400505066 Accuracy 99.32486738466484\n",
      "Training:: Epoch 134, Iteration 30, Current loss 0.3509819805622101 Accuracy 99.36280590955228\n",
      "Training:: Epoch 134, Iteration 40, Current loss 0.36953940987586975 Accuracy 99.40722891566266\n",
      "Training:: Epoch 134, Iteration 50, Current loss 0.4523623585700989 Accuracy 99.36186186186187\n",
      "Training:: Epoch 134, Iteration 60, Current loss 0.5324996709823608 Accuracy 99.10489055252665\n",
      "Training:: Epoch 134, Iteration 70, Current loss 0.4686344265937805 Accuracy 99.36627337832964\n",
      "Training:: Epoch 134, Iteration 80, Current loss 0.4748798906803131 Accuracy 99.3423980222497\n",
      "Training:: Epoch 134, Iteration 90, Current loss 0.5546582937240601 Accuracy 99.18632427590326\n",
      "Training:: Epoch 134, Iteration 100, Current loss 0.3770811855792999 Accuracy 99.23177703668414\n",
      "Training:: Epoch 134, Iteration 110, Current loss 0.5882840156555176 Accuracy 98.25046040515653\n",
      "Training:: Epoch 134, Iteration 120, Current loss 0.3960912227630615 Accuracy 99.48031274872419\n",
      "Training:: Epoch 134, Iteration 130, Current loss 0.423763245344162 Accuracy 99.49328045825072\n",
      "Training:: Epoch 134, Iteration 140, Current loss 0.34233230352401733 Accuracy 99.48011222974088\n",
      "Training:: Epoch 134, Iteration 150, Current loss 0.3377496898174286 Accuracy 99.37546168826808\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 134, Probability Accuracy 64.9148610017815\n",
      "Starting Training\n",
      "Training:: Epoch 135, Iteration 0, Current loss 0.5447843670845032 Accuracy 99.11232502560601\n",
      "Training:: Epoch 135, Iteration 10, Current loss 0.37961098551750183 Accuracy 99.338241738978\n",
      "Training:: Epoch 135, Iteration 20, Current loss 0.3833862841129303 Accuracy 99.43542343242568\n",
      "Training:: Epoch 135, Iteration 30, Current loss 0.5356053113937378 Accuracy 99.40484062954636\n",
      "Training:: Epoch 135, Iteration 40, Current loss 0.4836181104183197 Accuracy 99.4872553369817\n",
      "Training:: Epoch 135, Iteration 50, Current loss 0.5481348037719727 Accuracy 99.35866983372921\n",
      "Training:: Epoch 135, Iteration 60, Current loss 0.3860783278942108 Accuracy 99.06611188164587\n",
      "Training:: Epoch 135, Iteration 70, Current loss 0.7476403713226318 Accuracy 99.21493537577788\n",
      "Training:: Epoch 135, Iteration 80, Current loss 0.4072744846343994 Accuracy 99.30051236355536\n",
      "Training:: Epoch 135, Iteration 90, Current loss 0.4718394875526428 Accuracy 99.25840474620962\n",
      "Training:: Epoch 135, Iteration 100, Current loss 0.3999400734901428 Accuracy 99.32408395588759\n",
      "Training:: Epoch 135, Iteration 110, Current loss 0.4886179268360138 Accuracy 99.3629149435565\n",
      "Training:: Epoch 135, Iteration 120, Current loss 0.638204038143158 Accuracy 99.29462546717903\n",
      "Training:: Epoch 135, Iteration 130, Current loss 0.4250265955924988 Accuracy 99.39170922198859\n",
      "Training:: Epoch 135, Iteration 140, Current loss 0.5028010010719299 Accuracy 99.28390533577854\n",
      "Training:: Epoch 135, Iteration 150, Current loss 0.4570735692977905 Accuracy 99.50966657326983\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 135, Probability Accuracy 64.86959854165804\n",
      "Starting Training\n",
      "Training:: Epoch 136, Iteration 0, Current loss 0.5516108274459839 Accuracy 99.33431380137782\n",
      "Training:: Epoch 136, Iteration 10, Current loss 0.4503241181373596 Accuracy 99.3841153073008\n",
      "Training:: Epoch 136, Iteration 20, Current loss 0.4200461506843567 Accuracy 99.53432760682148\n",
      "Training:: Epoch 136, Iteration 30, Current loss 0.3722304403781891 Accuracy 99.3771663778163\n",
      "Training:: Epoch 136, Iteration 40, Current loss 0.41342276334762573 Accuracy 99.42930289615447\n",
      "Training:: Epoch 136, Iteration 50, Current loss 0.5853621959686279 Accuracy 99.2504258943782\n",
      "Training:: Epoch 136, Iteration 60, Current loss 0.38672029972076416 Accuracy 99.47161066048668\n",
      "Training:: Epoch 136, Iteration 70, Current loss 0.37535375356674194 Accuracy 99.36240554156171\n",
      "Training:: Epoch 136, Iteration 80, Current loss 0.33071431517601013 Accuracy 99.49197747766817\n",
      "Training:: Epoch 136, Iteration 90, Current loss 0.5982179045677185 Accuracy 99.22061396532527\n",
      "Training:: Epoch 136, Iteration 100, Current loss 0.4507421851158142 Accuracy 99.48280009622324\n",
      "Training:: Epoch 136, Iteration 110, Current loss 0.4250921905040741 Accuracy 99.37589523224882\n",
      "Training:: Epoch 136, Iteration 120, Current loss 0.4865057170391083 Accuracy 99.52241657214495\n",
      "Training:: Epoch 136, Iteration 130, Current loss 0.5228617191314697 Accuracy 99.27524429967427\n",
      "Training:: Epoch 136, Iteration 140, Current loss 0.5459772348403931 Accuracy 99.0795193045257\n",
      "Training:: Epoch 136, Iteration 150, Current loss 0.5829653143882751 Accuracy 99.28885878767355\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 136, Probability Accuracy 65.35671375895927\n",
      "Starting Training\n",
      "Training:: Epoch 137, Iteration 0, Current loss 0.31616419553756714 Accuracy 99.46169253880821\n",
      "Training:: Epoch 137, Iteration 10, Current loss 0.6186141967773438 Accuracy 99.33596181780452\n",
      "Training:: Epoch 137, Iteration 20, Current loss 0.382235050201416 Accuracy 99.28019194881365\n",
      "Training:: Epoch 137, Iteration 30, Current loss 0.370637446641922 Accuracy 99.2721488479707\n",
      "Training:: Epoch 137, Iteration 40, Current loss 0.2947545051574707 Accuracy 99.40884221118169\n",
      "Training:: Epoch 137, Iteration 50, Current loss 0.3889920711517334 Accuracy 99.44757485360734\n",
      "Training:: Epoch 137, Iteration 60, Current loss 0.4654264748096466 Accuracy 99.32558473238628\n",
      "Training:: Epoch 137, Iteration 70, Current loss 0.34526777267456055 Accuracy 99.53570527378248\n",
      "Training:: Epoch 137, Iteration 80, Current loss 0.4265832304954529 Accuracy 99.51996799786653\n",
      "Training:: Epoch 137, Iteration 90, Current loss 0.3762521743774414 Accuracy 99.19234652180184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 137, Iteration 100, Current loss 0.4217488765716553 Accuracy 99.30570315267445\n",
      "Training:: Epoch 137, Iteration 110, Current loss 0.40304988622665405 Accuracy 99.18101798698814\n",
      "Training:: Epoch 137, Iteration 120, Current loss 0.40034669637680054 Accuracy 99.44141335411796\n",
      "Training:: Epoch 137, Iteration 130, Current loss 0.4667029082775116 Accuracy 99.26405887528998\n",
      "Training:: Epoch 137, Iteration 140, Current loss 0.4890896677970886 Accuracy 99.4433910330907\n",
      "Training:: Epoch 137, Iteration 150, Current loss 0.43057483434677124 Accuracy 99.3165550681136\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 137, Probability Accuracy 64.42971371752911\n",
      "Starting Training\n",
      "Training:: Epoch 138, Iteration 0, Current loss 0.46432554721832275 Accuracy 99.26417803302225\n",
      "Training:: Epoch 138, Iteration 10, Current loss 0.4544091820716858 Accuracy 99.07928029238121\n",
      "Training:: Epoch 138, Iteration 20, Current loss 0.426565945148468 Accuracy 99.49689022699363\n",
      "Training:: Epoch 138, Iteration 30, Current loss 0.35797497630119324 Accuracy 99.44182885987478\n",
      "Training:: Epoch 138, Iteration 40, Current loss 0.29540398716926575 Accuracy 99.39641744548287\n",
      "Training:: Epoch 138, Iteration 50, Current loss 0.3984631597995758 Accuracy 99.4135199094557\n",
      "Training:: Epoch 138, Iteration 60, Current loss 0.2837337255477905 Accuracy 99.33967659751468\n",
      "Training:: Epoch 138, Iteration 70, Current loss 0.38827183842658997 Accuracy 99.1373869135283\n",
      "Training:: Epoch 138, Iteration 80, Current loss 0.42442941665649414 Accuracy 99.39512288054867\n",
      "Training:: Epoch 138, Iteration 90, Current loss 0.319143146276474 Accuracy 99.60926973861493\n",
      "Training:: Epoch 138, Iteration 100, Current loss 0.37959128618240356 Accuracy 99.4503862150921\n",
      "Training:: Epoch 138, Iteration 110, Current loss 0.5856653451919556 Accuracy 99.25468483816013\n",
      "Training:: Epoch 138, Iteration 120, Current loss 0.6516321897506714 Accuracy 99.02025435704192\n",
      "Training:: Epoch 138, Iteration 130, Current loss 0.5560972094535828 Accuracy 98.71666904320547\n",
      "Training:: Epoch 138, Iteration 140, Current loss 0.4722617268562317 Accuracy 99.05329057871431\n",
      "Training:: Epoch 138, Iteration 150, Current loss 0.37801995873451233 Accuracy 99.3575624082232\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 138, Probability Accuracy 64.10842275344906\n",
      "Starting Training\n",
      "Training:: Epoch 139, Iteration 0, Current loss 0.5906004905700684 Accuracy 99.32473378928232\n",
      "Training:: Epoch 139, Iteration 10, Current loss 0.34031471610069275 Accuracy 99.22403150902358\n",
      "Training:: Epoch 139, Iteration 20, Current loss 0.3173048794269562 Accuracy 99.26245319414501\n",
      "Training:: Epoch 139, Iteration 30, Current loss 0.457234263420105 Accuracy 99.26900584795321\n",
      "Training:: Epoch 139, Iteration 40, Current loss 0.48302382230758667 Accuracy 99.35097353969046\n",
      "Training:: Epoch 139, Iteration 50, Current loss 0.409373015165329 Accuracy 99.25830574966287\n",
      "Training:: Epoch 139, Iteration 60, Current loss 0.35965460538864136 Accuracy 99.45731053196188\n",
      "Training:: Epoch 139, Iteration 70, Current loss 0.47585466504096985 Accuracy 99.1506228765572\n",
      "Training:: Epoch 139, Iteration 80, Current loss 0.5566636919975281 Accuracy 98.45414270902064\n",
      "Training:: Epoch 139, Iteration 90, Current loss 0.42519524693489075 Accuracy 99.12423748263575\n",
      "Training:: Epoch 139, Iteration 100, Current loss 0.4338601231575012 Accuracy 99.28224614735065\n",
      "Training:: Epoch 139, Iteration 110, Current loss 0.4201871156692505 Accuracy 99.14377452729218\n",
      "Training:: Epoch 139, Iteration 120, Current loss 0.3775014281272888 Accuracy 99.16166999777926\n",
      "Training:: Epoch 139, Iteration 130, Current loss 0.37011998891830444 Accuracy 99.2852888624181\n",
      "Training:: Epoch 139, Iteration 140, Current loss 0.5534331202507019 Accuracy 99.16731431109137\n",
      "Training:: Epoch 139, Iteration 150, Current loss 0.4594782888889313 Accuracy 98.9504\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 139, Probability Accuracy 64.65188300120147\n",
      "Starting Training\n",
      "Training:: Epoch 140, Iteration 0, Current loss 0.506022572517395 Accuracy 98.82052636211601\n",
      "Training:: Epoch 140, Iteration 10, Current loss 0.32063332200050354 Accuracy 99.4244288224956\n",
      "Training:: Epoch 140, Iteration 20, Current loss 0.8575429916381836 Accuracy 95.7145484642266\n",
      "Training:: Epoch 140, Iteration 30, Current loss 0.8509053587913513 Accuracy 98.2614040479431\n",
      "Training:: Epoch 140, Iteration 40, Current loss 0.590646505355835 Accuracy 97.93802880035861\n",
      "Training:: Epoch 140, Iteration 50, Current loss 0.5161402225494385 Accuracy 98.85072068067814\n",
      "Training:: Epoch 140, Iteration 60, Current loss 0.5367892384529114 Accuracy 97.91979531675844\n",
      "Training:: Epoch 140, Iteration 70, Current loss 0.49462890625 Accuracy 98.78428644872531\n",
      "Training:: Epoch 140, Iteration 80, Current loss 0.9463987350463867 Accuracy 95.78941271863779\n",
      "Training:: Epoch 140, Iteration 90, Current loss 0.7141683101654053 Accuracy 98.62480376766091\n",
      "Training:: Epoch 140, Iteration 100, Current loss 0.7658804655075073 Accuracy 97.9203187250996\n",
      "Training:: Epoch 140, Iteration 110, Current loss 0.7388821244239807 Accuracy 97.08153735632185\n",
      "Training:: Epoch 140, Iteration 120, Current loss 0.6507143974304199 Accuracy 97.92862853405045\n",
      "Training:: Epoch 140, Iteration 130, Current loss 0.8786750435829163 Accuracy 98.05792919764733\n",
      "Training:: Epoch 140, Iteration 140, Current loss 0.35605233907699585 Accuracy 98.9797002869593\n",
      "Training:: Epoch 140, Iteration 150, Current loss 0.6491864919662476 Accuracy 98.05076310550763\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 140, Probability Accuracy 64.05756722044993\n",
      "Starting Training\n",
      "Training:: Epoch 141, Iteration 0, Current loss 0.5771965980529785 Accuracy 98.72916237909035\n",
      "Training:: Epoch 141, Iteration 10, Current loss 0.5764983296394348 Accuracy 98.32675826418175\n",
      "Training:: Epoch 141, Iteration 20, Current loss 0.5355297923088074 Accuracy 98.53002357509361\n",
      "Training:: Epoch 141, Iteration 30, Current loss 0.6486753225326538 Accuracy 98.79631371073914\n",
      "Training:: Epoch 141, Iteration 40, Current loss 0.7696393132209778 Accuracy 96.8107725017718\n",
      "Training:: Epoch 141, Iteration 50, Current loss 0.4097641408443451 Accuracy 99.11095305832148\n",
      "Training:: Epoch 141, Iteration 60, Current loss 0.4904719293117523 Accuracy 99.12620742247077\n",
      "Training:: Epoch 141, Iteration 70, Current loss 0.8030048608779907 Accuracy 98.19948556730495\n",
      "Training:: Epoch 141, Iteration 80, Current loss 0.3910770118236542 Accuracy 98.88409462877549\n",
      "Training:: Epoch 141, Iteration 90, Current loss 0.5919134020805359 Accuracy 98.45742492801317\n",
      "Training:: Epoch 141, Iteration 100, Current loss 0.6043761968612671 Accuracy 99.29508993680116\n",
      "Training:: Epoch 141, Iteration 110, Current loss 0.39875417947769165 Accuracy 99.08666211747415\n",
      "Training:: Epoch 141, Iteration 120, Current loss 0.5053552985191345 Accuracy 99.08675799086758\n",
      "Training:: Epoch 141, Iteration 130, Current loss 0.4225301146507263 Accuracy 99.0966845344451\n",
      "Training:: Epoch 141, Iteration 140, Current loss 0.7229199409484863 Accuracy 98.65276367998524\n",
      "Training:: Epoch 141, Iteration 150, Current loss 0.4524960517883301 Accuracy 99.25263494091345\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 141, Probability Accuracy 64.48119070306997\n",
      "Starting Training\n",
      "Training:: Epoch 142, Iteration 0, Current loss 0.326577752828598 Accuracy 99.30712379388216\n",
      "Training:: Epoch 142, Iteration 10, Current loss 0.48080766201019287 Accuracy 99.17249730893434\n",
      "Training:: Epoch 142, Iteration 20, Current loss 0.5075801014900208 Accuracy 98.72127181613962\n",
      "Training:: Epoch 142, Iteration 30, Current loss 0.5090878009796143 Accuracy 98.80075381188966\n",
      "Training:: Epoch 142, Iteration 40, Current loss 0.548023521900177 Accuracy 98.93423210056477\n",
      "Training:: Epoch 142, Iteration 50, Current loss 0.5179562568664551 Accuracy 99.14202961910487\n",
      "Training:: Epoch 142, Iteration 60, Current loss 0.3109753727912903 Accuracy 99.45331668890707\n",
      "Training:: Epoch 142, Iteration 70, Current loss 0.33162540197372437 Accuracy 99.31399557068006\n",
      "Training:: Epoch 142, Iteration 80, Current loss 0.3979438245296478 Accuracy 99.0498160048333\n",
      "Training:: Epoch 142, Iteration 90, Current loss 0.3947834372520447 Accuracy 99.12511759172155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 142, Iteration 100, Current loss 0.61323481798172 Accuracy 99.11111111111111\n",
      "Training:: Epoch 142, Iteration 110, Current loss 0.4542406499385834 Accuracy 99.02505446623094\n",
      "Training:: Epoch 142, Iteration 120, Current loss 0.4792109727859497 Accuracy 99.25845644857681\n",
      "Training:: Epoch 142, Iteration 130, Current loss 0.46103161573410034 Accuracy 99.2115416876363\n",
      "Training:: Epoch 142, Iteration 140, Current loss 0.45345836877822876 Accuracy 99.09489464007919\n",
      "Training:: Epoch 142, Iteration 150, Current loss 0.3851708471775055 Accuracy 99.37161540819454\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 142, Probability Accuracy 64.9854994406927\n",
      "Starting Training\n",
      "Training:: Epoch 143, Iteration 0, Current loss 0.41437870264053345 Accuracy 99.47076175602984\n",
      "Training:: Epoch 143, Iteration 10, Current loss 0.5280050039291382 Accuracy 99.37158630410525\n",
      "Training:: Epoch 143, Iteration 20, Current loss 0.42659705877304077 Accuracy 99.21033740129218\n",
      "Training:: Epoch 143, Iteration 30, Current loss 0.3063085377216339 Accuracy 99.34468635481785\n",
      "Training:: Epoch 143, Iteration 40, Current loss 0.39654746651649475 Accuracy 99.30870598401383\n",
      "Training:: Epoch 143, Iteration 50, Current loss 0.2645847201347351 Accuracy 99.48335552596538\n",
      "Training:: Epoch 143, Iteration 60, Current loss 0.3174438774585724 Accuracy 99.50785955633906\n",
      "Training:: Epoch 143, Iteration 70, Current loss 0.3346758186817169 Accuracy 99.29698007091332\n",
      "Training:: Epoch 143, Iteration 80, Current loss 0.39889413118362427 Accuracy 99.34674367655846\n",
      "Training:: Epoch 143, Iteration 90, Current loss 0.2949727177619934 Accuracy 99.24729947138589\n",
      "Training:: Epoch 143, Iteration 100, Current loss 0.4037495255470276 Accuracy 99.1850704914025\n",
      "Training:: Epoch 143, Iteration 110, Current loss 0.5001197457313538 Accuracy 99.3084051823505\n",
      "Training:: Epoch 143, Iteration 120, Current loss 0.4339470863342285 Accuracy 99.3114672008012\n",
      "Training:: Epoch 143, Iteration 130, Current loss 0.5049340724945068 Accuracy 99.12781577461254\n",
      "Training:: Epoch 143, Iteration 140, Current loss 0.40456706285476685 Accuracy 99.26232821341956\n",
      "Training:: Epoch 143, Iteration 150, Current loss 0.32369792461395264 Accuracy 99.46509268652586\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 143, Probability Accuracy 63.87941749181754\n",
      "Starting Training\n",
      "Training:: Epoch 144, Iteration 0, Current loss 0.4797484278678894 Accuracy 99.10671861636415\n",
      "Training:: Epoch 144, Iteration 10, Current loss 0.47993558645248413 Accuracy 99.12215758857747\n",
      "Training:: Epoch 144, Iteration 20, Current loss 0.3210463225841522 Accuracy 99.46100724271517\n",
      "Training:: Epoch 144, Iteration 30, Current loss 0.38925033807754517 Accuracy 99.30857509444722\n",
      "Training:: Epoch 144, Iteration 40, Current loss 0.35965222120285034 Accuracy 99.447391688771\n",
      "Training:: Epoch 144, Iteration 50, Current loss 0.3229353427886963 Accuracy 99.51694467506982\n",
      "Training:: Epoch 144, Iteration 60, Current loss 0.5768679976463318 Accuracy 99.09362549800797\n",
      "Training:: Epoch 144, Iteration 70, Current loss 0.3467154800891876 Accuracy 99.37370827331371\n",
      "Training:: Epoch 144, Iteration 80, Current loss 0.3097872734069824 Accuracy 99.48999010428561\n",
      "Training:: Epoch 144, Iteration 90, Current loss 0.49404260516166687 Accuracy 99.33951479740887\n",
      "Training:: Epoch 144, Iteration 100, Current loss 0.4593638777732849 Accuracy 99.4332439214329\n",
      "Training:: Epoch 144, Iteration 110, Current loss 0.5296649932861328 Accuracy 99.13600325269364\n",
      "Training:: Epoch 144, Iteration 120, Current loss 0.4313250482082367 Accuracy 99.44042132982226\n",
      "Training:: Epoch 144, Iteration 130, Current loss 0.3695111572742462 Accuracy 99.13139792468229\n",
      "Training:: Epoch 144, Iteration 140, Current loss 0.5008002519607544 Accuracy 99.08231873523533\n",
      "Training:: Epoch 144, Iteration 150, Current loss 0.45406433939933777 Accuracy 99.271324474925\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 144, Probability Accuracy 64.00215436881138\n",
      "Starting Training\n",
      "Training:: Epoch 145, Iteration 0, Current loss 0.37930724024772644 Accuracy 99.22331791604852\n",
      "Training:: Epoch 145, Iteration 10, Current loss 0.4470919966697693 Accuracy 99.45221277209168\n",
      "Training:: Epoch 145, Iteration 20, Current loss 0.3123002052307129 Accuracy 99.5258309248555\n",
      "Training:: Epoch 145, Iteration 30, Current loss 0.37610533833503723 Accuracy 99.50029982010794\n",
      "Training:: Epoch 145, Iteration 40, Current loss 0.3881452679634094 Accuracy 99.42996742671009\n",
      "Training:: Epoch 145, Iteration 50, Current loss 0.5433667302131653 Accuracy 99.20570448596443\n",
      "Training:: Epoch 145, Iteration 60, Current loss 0.5071502923965454 Accuracy 99.36645548516172\n",
      "Training:: Epoch 145, Iteration 70, Current loss 0.38820594549179077 Accuracy 99.37614678899082\n",
      "Training:: Epoch 145, Iteration 80, Current loss 0.4018843173980713 Accuracy 99.4005994005994\n",
      "Training:: Epoch 145, Iteration 90, Current loss 0.45349088311195374 Accuracy 99.22106510879688\n",
      "Training:: Epoch 145, Iteration 100, Current loss 0.4609379470348358 Accuracy 99.19235946413691\n",
      "Training:: Epoch 145, Iteration 110, Current loss 0.3898957073688507 Accuracy 99.1165611814346\n",
      "Training:: Epoch 145, Iteration 120, Current loss 0.42501354217529297 Accuracy 99.36110401226681\n",
      "Training:: Epoch 145, Iteration 130, Current loss 0.5066800117492676 Accuracy 99.41780995291855\n",
      "Training:: Epoch 145, Iteration 140, Current loss 0.35471782088279724 Accuracy 99.1771269177127\n",
      "Training:: Epoch 145, Iteration 150, Current loss 0.3737047612667084 Accuracy 99.48471315699072\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 145, Probability Accuracy 64.155756722045\n",
      "Starting Training\n",
      "Training:: Epoch 146, Iteration 0, Current loss 0.3007866144180298 Accuracy 99.23169950914135\n",
      "Training:: Epoch 146, Iteration 10, Current loss 0.53895103931427 Accuracy 99.34737466627114\n",
      "Training:: Epoch 146, Iteration 20, Current loss 0.5089156031608582 Accuracy 99.25411522633745\n",
      "Training:: Epoch 146, Iteration 30, Current loss 0.4005890190601349 Accuracy 99.40546967895362\n",
      "Training:: Epoch 146, Iteration 40, Current loss 0.5237956643104553 Accuracy 99.5388123121567\n",
      "Training:: Epoch 146, Iteration 50, Current loss 0.3732457458972931 Accuracy 99.29432519847104\n",
      "Training:: Epoch 146, Iteration 60, Current loss 0.36776843667030334 Accuracy 99.41757266474373\n",
      "Training:: Epoch 146, Iteration 70, Current loss 0.46930617094039917 Accuracy 99.20612009237875\n",
      "Training:: Epoch 146, Iteration 80, Current loss 0.3777175545692444 Accuracy 99.48288503839771\n",
      "Training:: Epoch 146, Iteration 90, Current loss 0.40177199244499207 Accuracy 99.26743159752868\n",
      "Training:: Epoch 146, Iteration 100, Current loss 0.5846914649009705 Accuracy 99.19113241461953\n",
      "Training:: Epoch 146, Iteration 110, Current loss 0.481025367975235 Accuracy 99.17335129508297\n",
      "Training:: Epoch 146, Iteration 120, Current loss 0.3808230757713318 Accuracy 99.32115726523355\n",
      "Training:: Epoch 146, Iteration 130, Current loss 0.33005064725875854 Accuracy 99.52365126841697\n",
      "Training:: Epoch 146, Iteration 140, Current loss 0.2539913058280945 Accuracy 99.4424105885666\n",
      "Training:: Epoch 146, Iteration 150, Current loss 0.3127937912940979 Accuracy 99.47124557100027\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 146, Probability Accuracy 64.10314040684426\n",
      "Starting Training\n",
      "Training:: Epoch 147, Iteration 0, Current loss 0.3634995222091675 Accuracy 98.93940711118839\n",
      "Training:: Epoch 147, Iteration 10, Current loss 0.3608623147010803 Accuracy 99.45207245536002\n",
      "Training:: Epoch 147, Iteration 20, Current loss 0.3275705575942993 Accuracy 99.27827219448665\n",
      "Training:: Epoch 147, Iteration 30, Current loss 0.472633421421051 Accuracy 99.30633072875139\n",
      "Training:: Epoch 147, Iteration 40, Current loss 0.46983253955841064 Accuracy 99.35972060535507\n",
      "Training:: Epoch 147, Iteration 50, Current loss 0.3327031135559082 Accuracy 99.52863961813843\n",
      "Training:: Epoch 147, Iteration 60, Current loss 0.2965032458305359 Accuracy 99.42475980662138\n",
      "Training:: Epoch 147, Iteration 70, Current loss 0.3723236322402954 Accuracy 99.44857078550528\n",
      "Training:: Epoch 147, Iteration 80, Current loss 0.43410104513168335 Accuracy 99.19663351185922\n",
      "Training:: Epoch 147, Iteration 90, Current loss 0.3520525097846985 Accuracy 99.56604285326824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 147, Iteration 100, Current loss 0.41471147537231445 Accuracy 99.30847532639964\n",
      "Training:: Epoch 147, Iteration 110, Current loss 0.44263148307800293 Accuracy 99.48898678414096\n",
      "Training:: Epoch 147, Iteration 120, Current loss 0.5226681232452393 Accuracy 99.1747146619842\n",
      "Training:: Epoch 147, Iteration 130, Current loss 0.3176780343055725 Accuracy 99.41182142512775\n",
      "Training:: Epoch 147, Iteration 140, Current loss 0.3065427839756012 Accuracy 99.53883620913001\n",
      "Training:: Epoch 147, Iteration 150, Current loss 0.4315129518508911 Accuracy 99.30080445079318\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 147, Probability Accuracy 63.17945477897005\n",
      "Starting Training\n",
      "Training:: Epoch 148, Iteration 0, Current loss 0.3783794045448303 Accuracy 99.30635235214353\n",
      "Training:: Epoch 148, Iteration 10, Current loss 0.4240259528160095 Accuracy 99.082823790995\n",
      "Training:: Epoch 148, Iteration 20, Current loss 0.323102742433548 Accuracy 99.56375838926175\n",
      "Training:: Epoch 148, Iteration 30, Current loss 0.5752971172332764 Accuracy 98.95823557015486\n",
      "Training:: Epoch 148, Iteration 40, Current loss 0.4244205951690674 Accuracy 99.268637936458\n",
      "Training:: Epoch 148, Iteration 50, Current loss 0.4715981185436249 Accuracy 98.88479025478253\n",
      "Training:: Epoch 148, Iteration 60, Current loss 0.41006797552108765 Accuracy 99.41906823100581\n",
      "Training:: Epoch 148, Iteration 70, Current loss 0.32683873176574707 Accuracy 99.4295360614704\n",
      "Training:: Epoch 148, Iteration 80, Current loss 0.3999403715133667 Accuracy 99.20004538749575\n",
      "Training:: Epoch 148, Iteration 90, Current loss 0.4555765390396118 Accuracy 99.38947824204374\n",
      "Training:: Epoch 148, Iteration 100, Current loss 0.3647622764110565 Accuracy 98.96423057128152\n",
      "Training:: Epoch 148, Iteration 110, Current loss 0.3500674068927765 Accuracy 99.1681781224592\n",
      "Training:: Epoch 148, Iteration 120, Current loss 0.44223296642303467 Accuracy 99.17699553792761\n",
      "Training:: Epoch 148, Iteration 130, Current loss 0.36310654878616333 Accuracy 99.51465286123694\n",
      "Training:: Epoch 148, Iteration 140, Current loss 0.38951656222343445 Accuracy 99.36082788007914\n",
      "Training:: Epoch 148, Iteration 150, Current loss 0.31329697370529175 Accuracy 99.56109134045077\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 148, Probability Accuracy 62.91492314703567\n",
      "Starting Training\n",
      "Training:: Epoch 149, Iteration 0, Current loss 0.5272954702377319 Accuracy 98.57876036320569\n",
      "Training:: Epoch 149, Iteration 10, Current loss 0.3048473596572876 Accuracy 99.19154228855722\n",
      "Training:: Epoch 149, Iteration 20, Current loss 0.3557024300098419 Accuracy 99.375\n",
      "Training:: Epoch 149, Iteration 30, Current loss 0.3765588402748108 Accuracy 99.16858659721527\n",
      "Training:: Epoch 149, Iteration 40, Current loss 0.5635481476783752 Accuracy 99.00275660775094\n",
      "Training:: Epoch 149, Iteration 50, Current loss 0.6740921139717102 Accuracy 98.96611858495267\n",
      "Training:: Epoch 149, Iteration 60, Current loss 0.616468071937561 Accuracy 98.84073672806068\n",
      "Training:: Epoch 149, Iteration 70, Current loss 0.3883388340473175 Accuracy 99.32832587028744\n",
      "Training:: Epoch 149, Iteration 80, Current loss 0.5053551197052002 Accuracy 98.7535953978907\n",
      "Training:: Epoch 149, Iteration 90, Current loss 0.4952300190925598 Accuracy 98.80231552332158\n",
      "Training:: Epoch 149, Iteration 100, Current loss 0.4693771302700043 Accuracy 98.754066588219\n",
      "Training:: Epoch 149, Iteration 110, Current loss 0.5098415613174438 Accuracy 99.08720777515104\n",
      "Training:: Epoch 149, Iteration 120, Current loss 0.5120853781700134 Accuracy 98.61914915219819\n",
      "Training:: Epoch 149, Iteration 130, Current loss 0.4298953711986542 Accuracy 99.03143751630577\n",
      "Training:: Epoch 149, Iteration 140, Current loss 0.3780635595321655 Accuracy 99.11552592189413\n",
      "Training:: Epoch 149, Iteration 150, Current loss 0.4854384660720825 Accuracy 98.84914814396932\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 149, Probability Accuracy 64.31091270663298\n",
      "Starting Training\n",
      "Training:: Epoch 150, Iteration 0, Current loss 0.5163995027542114 Accuracy 98.92581403155421\n",
      "Training:: Epoch 150, Iteration 10, Current loss 0.3971513509750366 Accuracy 99.23534087897944\n",
      "Training:: Epoch 150, Iteration 20, Current loss 0.4787656366825104 Accuracy 98.82371270546078\n",
      "Training:: Epoch 150, Iteration 30, Current loss 0.3856068551540375 Accuracy 99.4049292474932\n",
      "Training:: Epoch 150, Iteration 40, Current loss 0.36137905716896057 Accuracy 99.24751410911045\n",
      "Training:: Epoch 150, Iteration 50, Current loss 0.4591343104839325 Accuracy 98.78986099754701\n",
      "Training:: Epoch 150, Iteration 60, Current loss 0.4188615083694458 Accuracy 98.90542435026421\n",
      "Training:: Epoch 150, Iteration 70, Current loss 0.3910829424858093 Accuracy 98.9156405072597\n",
      "Training:: Epoch 150, Iteration 80, Current loss 0.6652955412864685 Accuracy 96.59843765450411\n",
      "Training:: Epoch 150, Iteration 90, Current loss 6.284425735473633 Accuracy 67.14100905562742\n",
      "Training:: Epoch 150, Iteration 100, Current loss 8.61634635925293 Accuracy 45.32930741649368\n",
      "Training:: Epoch 150, Iteration 110, Current loss 7.741338729858398 Accuracy 57.019438444924404\n",
      "Training:: Epoch 150, Iteration 120, Current loss 8.249642372131348 Accuracy 57.765063054647364\n",
      "Training:: Epoch 150, Iteration 130, Current loss 4.973278045654297 Accuracy 69.92094365666959\n",
      "Training:: Epoch 150, Iteration 140, Current loss 4.637069225311279 Accuracy 76.24274182127178\n",
      "Training:: Epoch 150, Iteration 150, Current loss 3.4401822090148926 Accuracy 82.83512687657765\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 150, Probability Accuracy 51.55912085180428\n",
      "Starting Training\n",
      "Training:: Epoch 151, Iteration 0, Current loss 5.047912120819092 Accuracy 70.23303799004734\n",
      "Training:: Epoch 151, Iteration 10, Current loss 2.5213301181793213 Accuracy 87.8866784709926\n",
      "Training:: Epoch 151, Iteration 20, Current loss 3.383373498916626 Accuracy 86.37079831932773\n",
      "Training:: Epoch 151, Iteration 30, Current loss 1.9027575254440308 Accuracy 92.65995028130315\n",
      "Training:: Epoch 151, Iteration 40, Current loss 4.075745105743408 Accuracy 78.79018847006652\n",
      "Training:: Epoch 151, Iteration 50, Current loss 2.2382664680480957 Accuracy 91.72983075485412\n",
      "Training:: Epoch 151, Iteration 60, Current loss 4.24195671081543 Accuracy 73.86123998312948\n",
      "Training:: Epoch 151, Iteration 70, Current loss 1.8984098434448242 Accuracy 91.71042539382854\n",
      "Training:: Epoch 151, Iteration 80, Current loss 1.807533621788025 Accuracy 92.35283963404109\n",
      "Training:: Epoch 151, Iteration 90, Current loss 1.3495126962661743 Accuracy 97.17869868697223\n",
      "Training:: Epoch 151, Iteration 100, Current loss 2.0982775688171387 Accuracy 91.58824019322063\n",
      "Training:: Epoch 151, Iteration 110, Current loss 1.4610645771026611 Accuracy 95.23907384573229\n",
      "Training:: Epoch 151, Iteration 120, Current loss 1.1367841958999634 Accuracy 97.05148983917218\n",
      "Training:: Epoch 151, Iteration 130, Current loss 0.9249279499053955 Accuracy 97.4140606144192\n",
      "Training:: Epoch 151, Iteration 140, Current loss 1.5010613203048706 Accuracy 93.64820015317845\n",
      "Training:: Epoch 151, Iteration 150, Current loss 1.1016401052474976 Accuracy 95.928581741181\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 151, Probability Accuracy 65.34014169117951\n",
      "Starting Training\n",
      "Training:: Epoch 152, Iteration 0, Current loss 1.3567993640899658 Accuracy 95.89760638297872\n",
      "Training:: Epoch 152, Iteration 10, Current loss 1.789425015449524 Accuracy 93.0406099605256\n",
      "Training:: Epoch 152, Iteration 20, Current loss 1.7151777744293213 Accuracy 94.06322795341099\n",
      "Training:: Epoch 152, Iteration 30, Current loss 1.1736934185028076 Accuracy 97.01153657339225\n",
      "Training:: Epoch 152, Iteration 40, Current loss 0.7003206610679626 Accuracy 98.21746125836226\n",
      "Training:: Epoch 152, Iteration 50, Current loss 1.0662401914596558 Accuracy 97.32637312755334\n",
      "Training:: Epoch 152, Iteration 60, Current loss 0.8415801525115967 Accuracy 97.53521126760563\n",
      "Training:: Epoch 152, Iteration 70, Current loss 0.5993738174438477 Accuracy 98.7088713036235\n",
      "Training:: Epoch 152, Iteration 80, Current loss 0.795577883720398 Accuracy 98.60032804811372\n",
      "Training:: Epoch 152, Iteration 90, Current loss 0.8842189908027649 Accuracy 97.56227948251897\n",
      "Training:: Epoch 152, Iteration 100, Current loss 0.7616947889328003 Accuracy 98.44794512585014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 152, Iteration 110, Current loss 0.8944223523139954 Accuracy 98.4399788471708\n",
      "Training:: Epoch 152, Iteration 120, Current loss 0.6911454200744629 Accuracy 98.84265453572384\n",
      "Training:: Epoch 152, Iteration 130, Current loss 0.6959322690963745 Accuracy 98.7802983219391\n",
      "Training:: Epoch 152, Iteration 140, Current loss 0.8556119203567505 Accuracy 98.47135291285508\n",
      "Training:: Epoch 152, Iteration 150, Current loss 0.6876572370529175 Accuracy 98.67549668874172\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 152, Probability Accuracy 67.78866470563865\n",
      "Starting Training\n",
      "Training:: Epoch 153, Iteration 0, Current loss 0.6505434513092041 Accuracy 99.06719640697875\n",
      "Training:: Epoch 153, Iteration 10, Current loss 0.635615885257721 Accuracy 99.16402675114396\n",
      "Training:: Epoch 153, Iteration 20, Current loss 0.5708250403404236 Accuracy 98.79505337702146\n",
      "Training:: Epoch 153, Iteration 30, Current loss 0.6255656480789185 Accuracy 99.02308601984677\n",
      "Training:: Epoch 153, Iteration 40, Current loss 0.5013249516487122 Accuracy 99.00132244972411\n",
      "Training:: Epoch 153, Iteration 50, Current loss 0.7100318074226379 Accuracy 99.24593695845628\n",
      "Training:: Epoch 153, Iteration 60, Current loss 0.5118169188499451 Accuracy 99.16127998290507\n",
      "Training:: Epoch 153, Iteration 70, Current loss 0.6027303338050842 Accuracy 98.46635207221995\n",
      "Training:: Epoch 153, Iteration 80, Current loss 0.3945101201534271 Accuracy 99.27541379555304\n",
      "Training:: Epoch 153, Iteration 90, Current loss 0.7213232517242432 Accuracy 99.16495681633822\n",
      "Training:: Epoch 153, Iteration 100, Current loss 0.6438458561897278 Accuracy 98.10251897481025\n",
      "Training:: Epoch 153, Iteration 110, Current loss 0.4832039475440979 Accuracy 98.95464601769912\n",
      "Training:: Epoch 153, Iteration 120, Current loss 0.6770156621932983 Accuracy 98.90064129257932\n",
      "Training:: Epoch 153, Iteration 130, Current loss 0.5761277675628662 Accuracy 98.64275037369208\n",
      "Training:: Epoch 153, Iteration 140, Current loss 0.6132944822311401 Accuracy 99.36855161526752\n",
      "Training:: Epoch 153, Iteration 150, Current loss 0.5734617114067078 Accuracy 98.81161377447671\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 153, Probability Accuracy 67.6314372125782\n",
      "Starting Training\n",
      "Training:: Epoch 154, Iteration 0, Current loss 0.4440414309501648 Accuracy 99.3164637182563\n",
      "Training:: Epoch 154, Iteration 10, Current loss 0.5420645475387573 Accuracy 98.95287958115183\n",
      "Training:: Epoch 154, Iteration 20, Current loss 0.6321595907211304 Accuracy 98.78180416345413\n",
      "Training:: Epoch 154, Iteration 30, Current loss 0.4195002019405365 Accuracy 99.15066187160456\n",
      "Training:: Epoch 154, Iteration 40, Current loss 0.5073419213294983 Accuracy 98.94873038977842\n",
      "Training:: Epoch 154, Iteration 50, Current loss 0.5522868633270264 Accuracy 99.11906214000136\n",
      "Training:: Epoch 154, Iteration 60, Current loss 0.7015930414199829 Accuracy 98.87991927346116\n",
      "Training:: Epoch 154, Iteration 70, Current loss 0.4950789511203766 Accuracy 99.34869135206851\n",
      "Training:: Epoch 154, Iteration 80, Current loss 0.3996303975582123 Accuracy 99.36419357789772\n",
      "Training:: Epoch 154, Iteration 90, Current loss 0.44606897234916687 Accuracy 99.37367638444555\n",
      "Training:: Epoch 154, Iteration 100, Current loss 0.4873269200325012 Accuracy 99.19631048865679\n",
      "Training:: Epoch 154, Iteration 110, Current loss 0.7070027589797974 Accuracy 99.19252367094315\n",
      "Training:: Epoch 154, Iteration 120, Current loss 0.5152457356452942 Accuracy 99.01568121648225\n",
      "Training:: Epoch 154, Iteration 130, Current loss 0.411909282207489 Accuracy 99.15971223021583\n",
      "Training:: Epoch 154, Iteration 140, Current loss 0.655405580997467 Accuracy 99.18026994558689\n",
      "Training:: Epoch 154, Iteration 150, Current loss 0.5343080163002014 Accuracy 99.40695835529785\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 154, Probability Accuracy 66.24124787670381\n",
      "Starting Training\n",
      "Training:: Epoch 155, Iteration 0, Current loss 0.6356908679008484 Accuracy 99.32168014609967\n",
      "Training:: Epoch 155, Iteration 10, Current loss 0.4682162404060364 Accuracy 99.20500844678526\n",
      "Training:: Epoch 155, Iteration 20, Current loss 0.5494921207427979 Accuracy 99.46142649199417\n",
      "Training:: Epoch 155, Iteration 30, Current loss 0.41208699345588684 Accuracy 99.41977964665233\n",
      "Training:: Epoch 155, Iteration 40, Current loss 0.43288716673851013 Accuracy 99.3827859569649\n",
      "Training:: Epoch 155, Iteration 50, Current loss 0.6118305921554565 Accuracy 99.16531604538088\n",
      "Training:: Epoch 155, Iteration 60, Current loss 0.5305241346359253 Accuracy 99.39195748812018\n",
      "Training:: Epoch 155, Iteration 70, Current loss 0.3254726827144623 Accuracy 99.54014482795074\n",
      "Training:: Epoch 155, Iteration 80, Current loss 0.4857926070690155 Accuracy 99.37601559961001\n",
      "Training:: Epoch 155, Iteration 90, Current loss 0.4352013170719147 Accuracy 99.23796716534991\n",
      "Training:: Epoch 155, Iteration 100, Current loss 0.47531858086586 Accuracy 99.40698792606048\n",
      "Training:: Epoch 155, Iteration 110, Current loss 0.4913633465766907 Accuracy 99.14839529710835\n",
      "Training:: Epoch 155, Iteration 120, Current loss 0.6236303448677063 Accuracy 99.38525849880125\n",
      "Training:: Epoch 155, Iteration 130, Current loss 0.4936428368091583 Accuracy 99.45573294629898\n",
      "Training:: Epoch 155, Iteration 140, Current loss 0.5467879772186279 Accuracy 99.00114733076872\n",
      "Training:: Epoch 155, Iteration 150, Current loss 0.49833786487579346 Accuracy 99.21229086648005\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 155, Probability Accuracy 66.25460910635124\n",
      "Starting Training\n",
      "Training:: Epoch 156, Iteration 0, Current loss 0.4780576229095459 Accuracy 99.00414937759336\n",
      "Training:: Epoch 156, Iteration 10, Current loss 0.5770529508590698 Accuracy 99.32054139261837\n",
      "Training:: Epoch 156, Iteration 20, Current loss 0.43280041217803955 Accuracy 99.47446519104736\n",
      "Training:: Epoch 156, Iteration 30, Current loss 0.6410273313522339 Accuracy 99.3282498184459\n",
      "Training:: Epoch 156, Iteration 40, Current loss 0.3601856827735901 Accuracy 99.34309457855164\n",
      "Training:: Epoch 156, Iteration 50, Current loss 0.44219088554382324 Accuracy 99.41096948628814\n",
      "Training:: Epoch 156, Iteration 60, Current loss 0.573019802570343 Accuracy 99.24707025320299\n",
      "Training:: Epoch 156, Iteration 70, Current loss 0.5855214595794678 Accuracy 99.31750121874782\n",
      "Training:: Epoch 156, Iteration 80, Current loss 0.491288959980011 Accuracy 99.45251311687325\n",
      "Training:: Epoch 156, Iteration 90, Current loss 0.40087413787841797 Accuracy 99.2808769112616\n",
      "Training:: Epoch 156, Iteration 100, Current loss 0.4666403532028198 Accuracy 99.14163090128756\n",
      "Training:: Epoch 156, Iteration 110, Current loss 0.5746471881866455 Accuracy 99.2077325305023\n",
      "Training:: Epoch 156, Iteration 120, Current loss 0.34234070777893066 Accuracy 99.50053514092045\n",
      "Training:: Epoch 156, Iteration 130, Current loss 0.5260151624679565 Accuracy 99.49655537890833\n",
      "Training:: Epoch 156, Iteration 140, Current loss 0.46611249446868896 Accuracy 99.45470875032076\n",
      "Training:: Epoch 156, Iteration 150, Current loss 0.5371357202529907 Accuracy 99.19720049403047\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 156, Probability Accuracy 65.78178729751005\n",
      "Starting Training\n",
      "Training:: Epoch 157, Iteration 0, Current loss 0.5262970328330994 Accuracy 99.258625605931\n",
      "Training:: Epoch 157, Iteration 10, Current loss 0.47683173418045044 Accuracy 99.45714155376123\n",
      "Training:: Epoch 157, Iteration 20, Current loss 0.47292351722717285 Accuracy 99.40841367221735\n",
      "Training:: Epoch 157, Iteration 30, Current loss 0.4980623126029968 Accuracy 99.48042532624456\n",
      "Training:: Epoch 157, Iteration 40, Current loss 0.3969353437423706 Accuracy 99.61613266455113\n",
      "Training:: Epoch 157, Iteration 50, Current loss 0.5035318732261658 Accuracy 99.22813036020584\n",
      "Training:: Epoch 157, Iteration 60, Current loss 0.43771207332611084 Accuracy 99.4003907565856\n",
      "Training:: Epoch 157, Iteration 70, Current loss 0.9357107877731323 Accuracy 98.95147197203926\n",
      "Training:: Epoch 157, Iteration 80, Current loss 0.45848575234413147 Accuracy 99.24319604133314\n",
      "Training:: Epoch 157, Iteration 90, Current loss 0.49701470136642456 Accuracy 99.4460467884448\n",
      "Training:: Epoch 157, Iteration 100, Current loss 0.3455037474632263 Accuracy 99.49605941194301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 157, Iteration 110, Current loss 0.5407188534736633 Accuracy 99.2234685073339\n",
      "Training:: Epoch 157, Iteration 120, Current loss 0.8344746232032776 Accuracy 99.13244650086756\n",
      "Training:: Epoch 157, Iteration 130, Current loss 0.4422830641269684 Accuracy 99.30245937005608\n",
      "Training:: Epoch 157, Iteration 140, Current loss 0.48658815026283264 Accuracy 99.44647125424582\n",
      "Training:: Epoch 157, Iteration 150, Current loss 0.42359039187431335 Accuracy 99.37514464244389\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 157, Probability Accuracy 65.7216099763848\n",
      "Starting Training\n",
      "Training:: Epoch 158, Iteration 0, Current loss 0.47358494997024536 Accuracy 99.45407397297666\n",
      "Training:: Epoch 158, Iteration 10, Current loss 0.46647655963897705 Accuracy 99.46252612720215\n",
      "Training:: Epoch 158, Iteration 20, Current loss 0.4323607385158539 Accuracy 99.50128765891345\n",
      "Training:: Epoch 158, Iteration 30, Current loss 0.49557268619537354 Accuracy 99.39591078066914\n",
      "Training:: Epoch 158, Iteration 40, Current loss 0.4463440179824829 Accuracy 99.45567283570786\n",
      "Training:: Epoch 158, Iteration 50, Current loss 0.3612140417098999 Accuracy 99.38890915724188\n",
      "Training:: Epoch 158, Iteration 60, Current loss 0.39180251955986023 Accuracy 99.33061039255729\n",
      "Training:: Epoch 158, Iteration 70, Current loss 0.5083923935890198 Accuracy 99.17305668320553\n",
      "Training:: Epoch 158, Iteration 80, Current loss 0.48585283756256104 Accuracy 99.31222996208447\n",
      "Training:: Epoch 158, Iteration 90, Current loss 0.5044330954551697 Accuracy 99.33801675222912\n",
      "Training:: Epoch 158, Iteration 100, Current loss 0.7480132579803467 Accuracy 99.14643119941134\n",
      "Training:: Epoch 158, Iteration 110, Current loss 0.43094977736473083 Accuracy 99.4474761255116\n",
      "Training:: Epoch 158, Iteration 120, Current loss 0.5479623675346375 Accuracy 99.21889529477404\n",
      "Training:: Epoch 158, Iteration 130, Current loss 0.4878295660018921 Accuracy 99.18361699183617\n",
      "Training:: Epoch 158, Iteration 140, Current loss 0.5494916439056396 Accuracy 99.34865207164827\n",
      "Training:: Epoch 158, Iteration 150, Current loss 0.45143523812294006 Accuracy 99.4316236389783\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 158, Probability Accuracy 66.01151758710694\n",
      "Starting Training\n",
      "Training:: Epoch 159, Iteration 0, Current loss 0.5208116769790649 Accuracy 99.40096724554847\n",
      "Training:: Epoch 159, Iteration 10, Current loss 0.511759340763092 Accuracy 99.56780923994039\n",
      "Training:: Epoch 159, Iteration 20, Current loss 0.36428821086883545 Accuracy 99.61669134942068\n",
      "Training:: Epoch 159, Iteration 30, Current loss 0.4460683763027191 Accuracy 99.29538546559823\n",
      "Training:: Epoch 159, Iteration 40, Current loss 0.39149782061576843 Accuracy 99.5634533408659\n",
      "Training:: Epoch 159, Iteration 50, Current loss 0.3327699899673462 Accuracy 99.38281719372986\n",
      "Training:: Epoch 159, Iteration 60, Current loss 0.395221084356308 Accuracy 99.51792708647183\n",
      "Training:: Epoch 159, Iteration 70, Current loss 0.5113771557807922 Accuracy 99.53638265392661\n",
      "Training:: Epoch 159, Iteration 80, Current loss 0.491201788187027 Accuracy 99.36415191613679\n",
      "Training:: Epoch 159, Iteration 90, Current loss 0.4073870778083801 Accuracy 99.54679480179098\n",
      "Training:: Epoch 159, Iteration 100, Current loss 0.6144527792930603 Accuracy 99.30839096741938\n",
      "Training:: Epoch 159, Iteration 110, Current loss 0.5895211696624756 Accuracy 99.47723312372149\n",
      "Training:: Epoch 159, Iteration 120, Current loss 0.4291733503341675 Accuracy 99.53492101675889\n",
      "Training:: Epoch 159, Iteration 130, Current loss 0.5646166801452637 Accuracy 99.23302872062663\n",
      "Training:: Epoch 159, Iteration 140, Current loss 0.5538150668144226 Accuracy 99.27079377136346\n",
      "Training:: Epoch 159, Iteration 150, Current loss 0.40265876054763794 Accuracy 99.47867596256516\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 159, Probability Accuracy 65.0904213448233\n",
      "Starting Training\n",
      "Training:: Epoch 160, Iteration 0, Current loss 0.3425461947917938 Accuracy 99.54383622607922\n",
      "Training:: Epoch 160, Iteration 10, Current loss 0.3429754972457886 Accuracy 99.52949084187532\n",
      "Training:: Epoch 160, Iteration 20, Current loss 0.5150462985038757 Accuracy 99.50262503453993\n",
      "Training:: Epoch 160, Iteration 30, Current loss 0.5553299784660339 Accuracy 99.35475106975481\n",
      "Training:: Epoch 160, Iteration 40, Current loss 0.5117172598838806 Accuracy 99.16928996152501\n",
      "Training:: Epoch 160, Iteration 50, Current loss 0.5510803461074829 Accuracy 99.27265380341449\n",
      "Training:: Epoch 160, Iteration 60, Current loss 0.33611860871315 Accuracy 99.37983462256602\n",
      "Training:: Epoch 160, Iteration 70, Current loss 0.4067649245262146 Accuracy 99.37112821475502\n",
      "Training:: Epoch 160, Iteration 80, Current loss 0.44327521324157715 Accuracy 99.42773651528066\n",
      "Training:: Epoch 160, Iteration 90, Current loss 0.4501858055591583 Accuracy 99.43775100401606\n",
      "Training:: Epoch 160, Iteration 100, Current loss 0.5779102444648743 Accuracy 99.26322163907953\n",
      "Training:: Epoch 160, Iteration 110, Current loss 0.5612855553627014 Accuracy 99.30720318876341\n",
      "Training:: Epoch 160, Iteration 120, Current loss 0.49967890977859497 Accuracy 99.31147748089192\n",
      "Training:: Epoch 160, Iteration 130, Current loss 0.366488516330719 Accuracy 99.51847704367302\n",
      "Training:: Epoch 160, Iteration 140, Current loss 0.4235475957393646 Accuracy 99.5427129735483\n",
      "Training:: Epoch 160, Iteration 150, Current loss 0.36931055784225464 Accuracy 99.53295777360489\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 160, Probability Accuracy 64.07703940009115\n",
      "Starting Training\n",
      "Training:: Epoch 161, Iteration 0, Current loss 0.30289772152900696 Accuracy 99.52978955215322\n",
      "Training:: Epoch 161, Iteration 10, Current loss 0.4560270607471466 Accuracy 99.57270174581858\n",
      "Training:: Epoch 161, Iteration 20, Current loss 0.41339826583862305 Accuracy 99.62848297213623\n",
      "Training:: Epoch 161, Iteration 30, Current loss 0.44227737188339233 Accuracy 99.4711849773365\n",
      "Training:: Epoch 161, Iteration 40, Current loss 0.41890138387680054 Accuracy 99.48634227920682\n",
      "Training:: Epoch 161, Iteration 50, Current loss 0.43676838278770447 Accuracy 99.38244650988709\n",
      "Training:: Epoch 161, Iteration 60, Current loss 0.6547690033912659 Accuracy 99.16519694297472\n",
      "Training:: Epoch 161, Iteration 70, Current loss 0.3254474401473999 Accuracy 99.56567737624589\n",
      "Training:: Epoch 161, Iteration 80, Current loss 0.4593001902103424 Accuracy 99.53890153504932\n",
      "Training:: Epoch 161, Iteration 90, Current loss 0.4291013479232788 Accuracy 99.36984476663764\n",
      "Training:: Epoch 161, Iteration 100, Current loss 0.3098282814025879 Accuracy 99.5418164946062\n",
      "Training:: Epoch 161, Iteration 110, Current loss 0.36446061730384827 Accuracy 99.33670797695933\n",
      "Training:: Epoch 161, Iteration 120, Current loss 0.6059497594833374 Accuracy 98.95598771750255\n",
      "Training:: Epoch 161, Iteration 130, Current loss 0.3907907009124756 Accuracy 99.53177257525084\n",
      "Training:: Epoch 161, Iteration 140, Current loss 0.4194956123828888 Accuracy 99.53502324883756\n",
      "Training:: Epoch 161, Iteration 150, Current loss 0.4118274450302124 Accuracy 99.43494639235004\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 161, Probability Accuracy 64.67321953846792\n",
      "Starting Training\n",
      "Training:: Epoch 162, Iteration 0, Current loss 0.34389254450798035 Accuracy 99.66388727296231\n",
      "Training:: Epoch 162, Iteration 10, Current loss 0.48242369294166565 Accuracy 99.55476402493322\n",
      "Training:: Epoch 162, Iteration 20, Current loss 0.4503336250782013 Accuracy 99.45025831235925\n",
      "Training:: Epoch 162, Iteration 30, Current loss 0.4252612292766571 Accuracy 99.60740945534974\n",
      "Training:: Epoch 162, Iteration 40, Current loss 0.3308047950267792 Accuracy 99.3833171048361\n",
      "Training:: Epoch 162, Iteration 50, Current loss 0.4883740544319153 Accuracy 99.28214447978192\n",
      "Training:: Epoch 162, Iteration 60, Current loss 0.3955075442790985 Accuracy 99.62628290941544\n",
      "Training:: Epoch 162, Iteration 70, Current loss 0.4010201394557953 Accuracy 99.48096885813149\n",
      "Training:: Epoch 162, Iteration 80, Current loss 0.43860435485839844 Accuracy 99.36991760460984\n",
      "Training:: Epoch 162, Iteration 90, Current loss 0.45854082703590393 Accuracy 99.23035618399857\n",
      "Training:: Epoch 162, Iteration 100, Current loss 0.36629199981689453 Accuracy 99.54829584340922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 162, Iteration 110, Current loss 0.40960225462913513 Accuracy 99.4337134434877\n",
      "Training:: Epoch 162, Iteration 120, Current loss 0.7133123278617859 Accuracy 99.14548884696482\n",
      "Training:: Epoch 162, Iteration 130, Current loss 0.3563498258590698 Accuracy 99.51499118165785\n",
      "Training:: Epoch 162, Iteration 140, Current loss 0.39966773986816406 Accuracy 99.5109088589291\n",
      "Training:: Epoch 162, Iteration 150, Current loss 0.33612874150276184 Accuracy 99.51578539608755\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 162, Probability Accuracy 63.7645523470191\n",
      "Starting Training\n",
      "Training:: Epoch 163, Iteration 0, Current loss 0.4383767545223236 Accuracy 99.44598337950139\n",
      "Training:: Epoch 163, Iteration 10, Current loss 0.3900819420814514 Accuracy 99.46137910624445\n",
      "Training:: Epoch 163, Iteration 20, Current loss 0.3733285963535309 Accuracy 99.42769095311468\n",
      "Training:: Epoch 163, Iteration 30, Current loss 0.44914621114730835 Accuracy 99.62633205701896\n",
      "Training:: Epoch 163, Iteration 40, Current loss 0.35311490297317505 Accuracy 99.53608845246839\n",
      "Training:: Epoch 163, Iteration 50, Current loss 0.385111004114151 Accuracy 99.48560279075268\n",
      "Training:: Epoch 163, Iteration 60, Current loss 0.2892674207687378 Accuracy 99.56651314007044\n",
      "Training:: Epoch 163, Iteration 70, Current loss 0.34957796335220337 Accuracy 99.61967002356974\n",
      "Training:: Epoch 163, Iteration 80, Current loss 0.4527861773967743 Accuracy 99.51910466031302\n",
      "Training:: Epoch 163, Iteration 90, Current loss 0.36142316460609436 Accuracy 99.45131018922106\n",
      "Training:: Epoch 163, Iteration 100, Current loss 0.43988335132598877 Accuracy 99.42643552233035\n",
      "Training:: Epoch 163, Iteration 110, Current loss 0.3400294780731201 Accuracy 99.37704124833677\n",
      "Training:: Epoch 163, Iteration 120, Current loss 0.3310709297657013 Accuracy 99.56536707117652\n",
      "Training:: Epoch 163, Iteration 130, Current loss 0.43942201137542725 Accuracy 99.59438904850431\n",
      "Training:: Epoch 163, Iteration 140, Current loss 0.3402831256389618 Accuracy 99.47484123106986\n",
      "Training:: Epoch 163, Iteration 150, Current loss 0.33312729001045227 Accuracy 99.40889611349195\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 163, Probability Accuracy 63.976156937481875\n",
      "Starting Training\n",
      "Training:: Epoch 164, Iteration 0, Current loss 0.49627453088760376 Accuracy 99.5951179439033\n",
      "Training:: Epoch 164, Iteration 10, Current loss 0.30407702922821045 Accuracy 99.54091992584091\n",
      "Training:: Epoch 164, Iteration 20, Current loss 0.4332084655761719 Accuracy 99.63566399214017\n",
      "Training:: Epoch 164, Iteration 30, Current loss 0.2851903736591339 Accuracy 99.6447444380301\n",
      "Training:: Epoch 164, Iteration 40, Current loss 0.3555295169353485 Accuracy 99.4295900178253\n",
      "Training:: Epoch 164, Iteration 50, Current loss 0.41697511076927185 Accuracy 99.42817932296431\n",
      "Training:: Epoch 164, Iteration 60, Current loss 0.41076987981796265 Accuracy 99.54737103174604\n",
      "Training:: Epoch 164, Iteration 70, Current loss 0.3507117033004761 Accuracy 99.49953528276257\n",
      "Training:: Epoch 164, Iteration 80, Current loss 0.3881682753562927 Accuracy 99.44360727947142\n",
      "Training:: Epoch 164, Iteration 90, Current loss 0.4721110165119171 Accuracy 99.37651973315045\n",
      "Training:: Epoch 164, Iteration 100, Current loss 0.3444097638130188 Accuracy 99.54032404878937\n",
      "Training:: Epoch 164, Iteration 110, Current loss 0.45313963294029236 Accuracy 99.47669896046956\n",
      "Training:: Epoch 164, Iteration 120, Current loss 0.42504453659057617 Accuracy 99.4137794756949\n",
      "Training:: Epoch 164, Iteration 130, Current loss 0.31873002648353577 Accuracy 99.4902687673772\n",
      "Training:: Epoch 164, Iteration 140, Current loss 0.5010628700256348 Accuracy 98.76990664470071\n",
      "Training:: Epoch 164, Iteration 150, Current loss 0.4587869942188263 Accuracy 99.42790739250917\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 164, Probability Accuracy 64.75670132990844\n",
      "Starting Training\n",
      "Training:: Epoch 165, Iteration 0, Current loss 0.34803715348243713 Accuracy 99.46043165467626\n",
      "Training:: Epoch 165, Iteration 10, Current loss 0.30214056372642517 Accuracy 99.54876937101184\n",
      "Training:: Epoch 165, Iteration 20, Current loss 0.3670627772808075 Accuracy 99.34896337919008\n",
      "Training:: Epoch 165, Iteration 30, Current loss 0.4128383994102478 Accuracy 99.42041691018898\n",
      "Training:: Epoch 165, Iteration 40, Current loss 0.5489286184310913 Accuracy 99.32574430823118\n",
      "Training:: Epoch 165, Iteration 50, Current loss 0.5148442387580872 Accuracy 99.32013006207508\n",
      "Training:: Epoch 165, Iteration 60, Current loss 0.4043799042701721 Accuracy 99.61886093869106\n",
      "Training:: Epoch 165, Iteration 70, Current loss 0.4463576674461365 Accuracy 99.5210969400615\n",
      "Training:: Epoch 165, Iteration 80, Current loss 0.3229820132255554 Accuracy 99.54413999313759\n",
      "Training:: Epoch 165, Iteration 90, Current loss 0.5286960601806641 Accuracy 98.9506586291583\n",
      "Training:: Epoch 165, Iteration 100, Current loss 0.3004339337348938 Accuracy 99.3562097631102\n",
      "Training:: Epoch 165, Iteration 110, Current loss 0.6808056831359863 Accuracy 99.22350200595315\n",
      "Training:: Epoch 165, Iteration 120, Current loss 0.3818226456642151 Accuracy 99.38729623383924\n",
      "Training:: Epoch 165, Iteration 130, Current loss 0.5120739936828613 Accuracy 99.20541987286718\n",
      "Training:: Epoch 165, Iteration 140, Current loss 0.3736023008823395 Accuracy 99.47625492849997\n",
      "Training:: Epoch 165, Iteration 150, Current loss 0.3475210666656494 Accuracy 99.51877631692017\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 165, Probability Accuracy 63.9776069934126\n",
      "Starting Training\n",
      "Training:: Epoch 166, Iteration 0, Current loss 0.4097425639629364 Accuracy 99.5423748959943\n",
      "Training:: Epoch 166, Iteration 10, Current loss 0.5360161662101746 Accuracy 99.42078123896306\n",
      "Training:: Epoch 166, Iteration 20, Current loss 0.388521283864975 Accuracy 99.47521865889213\n",
      "Training:: Epoch 166, Iteration 30, Current loss 0.311927855014801 Accuracy 99.52406653255483\n",
      "Training:: Epoch 166, Iteration 40, Current loss 0.31634262204170227 Accuracy 99.53417892019306\n",
      "Training:: Epoch 166, Iteration 50, Current loss 0.43439462780952454 Accuracy 99.37716262975779\n",
      "Training:: Epoch 166, Iteration 60, Current loss 0.5105995535850525 Accuracy 99.0034257240735\n",
      "Training:: Epoch 166, Iteration 70, Current loss 0.3727399706840515 Accuracy 99.4041095890411\n",
      "Training:: Epoch 166, Iteration 80, Current loss 0.48259294033050537 Accuracy 99.33392844880862\n",
      "Training:: Epoch 166, Iteration 90, Current loss 0.5083757042884827 Accuracy 98.97884219806053\n",
      "Training:: Epoch 166, Iteration 100, Current loss 0.5879374742507935 Accuracy 99.4065837966144\n",
      "Training:: Epoch 166, Iteration 110, Current loss 0.2817143201828003 Accuracy 99.47515065120197\n",
      "Training:: Epoch 166, Iteration 120, Current loss 0.40653082728385925 Accuracy 99.35923107729275\n",
      "Training:: Epoch 166, Iteration 130, Current loss 0.3545196056365967 Accuracy 99.348324262389\n",
      "Training:: Epoch 166, Iteration 140, Current loss 0.30080604553222656 Accuracy 99.52722360764754\n",
      "Training:: Epoch 166, Iteration 150, Current loss 0.42203450202941895 Accuracy 99.6118044830321\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 166, Probability Accuracy 63.800700169863696\n",
      "Starting Training\n",
      "Training:: Epoch 167, Iteration 0, Current loss 0.3447670042514801 Accuracy 99.28115015974441\n",
      "Training:: Epoch 167, Iteration 10, Current loss 0.39664313197135925 Accuracy 99.23236846313769\n",
      "Training:: Epoch 167, Iteration 20, Current loss 0.3928057849407196 Accuracy 99.48313564376119\n",
      "Training:: Epoch 167, Iteration 30, Current loss 0.2807883620262146 Accuracy 99.60477496370382\n",
      "Training:: Epoch 167, Iteration 40, Current loss 0.2576296031475067 Accuracy 99.465128474043\n",
      "Training:: Epoch 167, Iteration 50, Current loss 0.27768465876579285 Accuracy 99.48552109363516\n",
      "Training:: Epoch 167, Iteration 60, Current loss 0.37784337997436523 Accuracy 99.46778070275839\n",
      "Training:: Epoch 167, Iteration 70, Current loss 0.4700743556022644 Accuracy 99.52224544640191\n",
      "Training:: Epoch 167, Iteration 80, Current loss 0.4423341453075409 Accuracy 99.47957169348567\n",
      "Training:: Epoch 167, Iteration 90, Current loss 0.42286697030067444 Accuracy 99.50983874405058\n",
      "Training:: Epoch 167, Iteration 100, Current loss 0.44887876510620117 Accuracy 99.37852645351214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 167, Iteration 110, Current loss 0.33140236139297485 Accuracy 99.57061474364424\n",
      "Training:: Epoch 167, Iteration 120, Current loss 0.4572756588459015 Accuracy 99.36162495466087\n",
      "Training:: Epoch 167, Iteration 130, Current loss 0.3327023684978485 Accuracy 99.50965761309854\n",
      "Training:: Epoch 167, Iteration 140, Current loss 0.37308868765830994 Accuracy 99.53949532446784\n",
      "Training:: Epoch 167, Iteration 150, Current loss 0.30612149834632874 Accuracy 99.48261106356493\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 167, Probability Accuracy 63.65041223018602\n",
      "Starting Training\n",
      "Training:: Epoch 168, Iteration 0, Current loss 0.32720980048179626 Accuracy 99.50011258725512\n",
      "Training:: Epoch 168, Iteration 10, Current loss 0.33834245800971985 Accuracy 99.38885730528709\n",
      "Training:: Epoch 168, Iteration 20, Current loss 0.4123702049255371 Accuracy 99.02409769926943\n",
      "Training:: Epoch 168, Iteration 30, Current loss 0.3957119882106781 Accuracy 99.24395646730233\n",
      "Training:: Epoch 168, Iteration 40, Current loss 0.7413065433502197 Accuracy 97.21455254182231\n",
      "Training:: Epoch 168, Iteration 50, Current loss 0.3249084949493408 Accuracy 99.39471591411152\n",
      "Training:: Epoch 168, Iteration 60, Current loss 0.4948936104774475 Accuracy 98.6478085172521\n",
      "Training:: Epoch 168, Iteration 70, Current loss 0.5127663612365723 Accuracy 98.86752136752136\n",
      "Training:: Epoch 168, Iteration 80, Current loss 0.588158369064331 Accuracy 97.77001164919288\n",
      "Training:: Epoch 168, Iteration 90, Current loss 0.782465398311615 Accuracy 98.80366874916922\n",
      "Training:: Epoch 168, Iteration 100, Current loss 0.35329076647758484 Accuracy 99.0981616371835\n",
      "Training:: Epoch 168, Iteration 110, Current loss 0.5227675437927246 Accuracy 98.51140790205899\n",
      "Training:: Epoch 168, Iteration 120, Current loss 0.7602604627609253 Accuracy 97.69872151195108\n",
      "Training:: Epoch 168, Iteration 130, Current loss 0.6782740950584412 Accuracy 97.99495737783647\n",
      "Training:: Epoch 168, Iteration 140, Current loss 0.4411922097206116 Accuracy 98.88996138996139\n",
      "Training:: Epoch 168, Iteration 150, Current loss 0.5191271305084229 Accuracy 98.61888463861486\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 168, Probability Accuracy 61.97476902680532\n",
      "Starting Training\n",
      "Training:: Epoch 169, Iteration 0, Current loss 0.832560658454895 Accuracy 97.23394701392007\n",
      "Training:: Epoch 169, Iteration 10, Current loss 2.896873950958252 Accuracy 84.7857228124627\n",
      "Training:: Epoch 169, Iteration 20, Current loss 1.1291488409042358 Accuracy 95.88610358983317\n",
      "Training:: Epoch 169, Iteration 30, Current loss 1.8131712675094604 Accuracy 92.66845329249617\n",
      "Training:: Epoch 169, Iteration 40, Current loss 1.956308364868164 Accuracy 92.05808713069604\n",
      "Training:: Epoch 169, Iteration 50, Current loss 1.510148525238037 Accuracy 93.52804058686412\n",
      "Training:: Epoch 169, Iteration 60, Current loss 1.9105054140090942 Accuracy 94.08978583196046\n",
      "Training:: Epoch 169, Iteration 70, Current loss 1.999786376953125 Accuracy 90.9903791737408\n",
      "Training:: Epoch 169, Iteration 80, Current loss 1.308349847793579 Accuracy 94.23190111830489\n",
      "Training:: Epoch 169, Iteration 90, Current loss 2.0823099613189697 Accuracy 89.32418474251143\n",
      "Training:: Epoch 169, Iteration 100, Current loss 0.83840012550354 Accuracy 97.30149972413102\n",
      "Training:: Epoch 169, Iteration 110, Current loss 0.8720172047615051 Accuracy 97.14216517316541\n",
      "Training:: Epoch 169, Iteration 120, Current loss 0.8917137980461121 Accuracy 97.1890277023357\n",
      "Training:: Epoch 169, Iteration 130, Current loss 1.3374005556106567 Accuracy 95.21200191519924\n",
      "Training:: Epoch 169, Iteration 140, Current loss 1.5242046117782593 Accuracy 95.80288551620761\n",
      "Training:: Epoch 169, Iteration 150, Current loss 1.53680419921875 Accuracy 93.9568345323741\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 169, Probability Accuracy 60.80529891867258\n",
      "Starting Training\n",
      "Training:: Epoch 170, Iteration 0, Current loss 4.663742542266846 Accuracy 73.15014922301121\n",
      "Training:: Epoch 170, Iteration 10, Current loss 1.1054209470748901 Accuracy 95.42367166048462\n",
      "Training:: Epoch 170, Iteration 20, Current loss 1.0473594665527344 Accuracy 97.1090893631494\n",
      "Training:: Epoch 170, Iteration 30, Current loss 1.0870096683502197 Accuracy 97.21951219512195\n",
      "Training:: Epoch 170, Iteration 40, Current loss 1.444879412651062 Accuracy 96.81437125748504\n",
      "Training:: Epoch 170, Iteration 50, Current loss 1.0573941469192505 Accuracy 96.75955414012739\n",
      "Training:: Epoch 170, Iteration 60, Current loss 1.0839167833328247 Accuracy 96.95713612812058\n",
      "Training:: Epoch 170, Iteration 70, Current loss 0.8170071244239807 Accuracy 98.06427711401315\n",
      "Training:: Epoch 170, Iteration 80, Current loss 0.6356599926948547 Accuracy 98.60035735556879\n",
      "Training:: Epoch 170, Iteration 90, Current loss 0.6566986441612244 Accuracy 97.87461135087474\n",
      "Training:: Epoch 170, Iteration 100, Current loss 0.7861756086349487 Accuracy 98.35005825534634\n",
      "Training:: Epoch 170, Iteration 110, Current loss 0.4842480719089508 Accuracy 98.45312077816806\n",
      "Training:: Epoch 170, Iteration 120, Current loss 0.6606361865997314 Accuracy 98.74438428752448\n",
      "Training:: Epoch 170, Iteration 130, Current loss 0.6136806607246399 Accuracy 98.64288059538141\n",
      "Training:: Epoch 170, Iteration 140, Current loss 0.6082442998886108 Accuracy 98.72655707339662\n",
      "Training:: Epoch 170, Iteration 150, Current loss 0.48300525546073914 Accuracy 98.88537322019272\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 170, Probability Accuracy 68.11036997141318\n",
      "Starting Training\n",
      "Training:: Epoch 171, Iteration 0, Current loss 0.5240672826766968 Accuracy 99.17338607434453\n",
      "Training:: Epoch 171, Iteration 10, Current loss 0.5620779991149902 Accuracy 99.25720349359236\n",
      "Training:: Epoch 171, Iteration 20, Current loss 0.3749280869960785 Accuracy 99.31154853098032\n",
      "Training:: Epoch 171, Iteration 30, Current loss 0.5032012462615967 Accuracy 99.27048972342045\n",
      "Training:: Epoch 171, Iteration 40, Current loss 0.5399559140205383 Accuracy 98.97815362931642\n",
      "Training:: Epoch 171, Iteration 50, Current loss 0.5506944060325623 Accuracy 99.10983001992285\n",
      "Training:: Epoch 171, Iteration 60, Current loss 0.619209349155426 Accuracy 99.18582030405649\n",
      "Training:: Epoch 171, Iteration 70, Current loss 0.5518239736557007 Accuracy 99.10260951706222\n",
      "Training:: Epoch 171, Iteration 80, Current loss 0.5874765515327454 Accuracy 98.80053053457125\n",
      "Training:: Epoch 171, Iteration 90, Current loss 0.4125683307647705 Accuracy 99.34526854219949\n",
      "Training:: Epoch 171, Iteration 100, Current loss 0.5281407833099365 Accuracy 99.26308032424465\n",
      "Training:: Epoch 171, Iteration 110, Current loss 0.5023014545440674 Accuracy 98.63867475807774\n",
      "Training:: Epoch 171, Iteration 120, Current loss 0.6113647222518921 Accuracy 98.86241682764542\n",
      "Training:: Epoch 171, Iteration 130, Current loss 0.6065801978111267 Accuracy 98.99807921922857\n",
      "Training:: Epoch 171, Iteration 140, Current loss 0.45016372203826904 Accuracy 99.27545954649135\n",
      "Training:: Epoch 171, Iteration 150, Current loss 0.5168073773384094 Accuracy 99.15014164305948\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 171, Probability Accuracy 67.45649832207813\n",
      "Starting Training\n",
      "Training:: Epoch 172, Iteration 0, Current loss 0.6710000038146973 Accuracy 99.15704027474243\n",
      "Training:: Epoch 172, Iteration 10, Current loss 0.4085526168346405 Accuracy 99.34737923946557\n",
      "Training:: Epoch 172, Iteration 20, Current loss 0.44038864970207214 Accuracy 99.35601379041177\n",
      "Training:: Epoch 172, Iteration 30, Current loss 0.6308514475822449 Accuracy 98.96352663342955\n",
      "Training:: Epoch 172, Iteration 40, Current loss 0.4809889495372772 Accuracy 99.36185909258576\n",
      "Training:: Epoch 172, Iteration 50, Current loss 0.5629755258560181 Accuracy 99.14230019493178\n",
      "Training:: Epoch 172, Iteration 60, Current loss 0.38040396571159363 Accuracy 99.4624767417821\n",
      "Training:: Epoch 172, Iteration 70, Current loss 0.46118831634521484 Accuracy 99.41774944839422\n",
      "Training:: Epoch 172, Iteration 80, Current loss 0.5366402268409729 Accuracy 99.28195061473292\n",
      "Training:: Epoch 172, Iteration 90, Current loss 0.31839367747306824 Accuracy 99.39888955169091\n",
      "Training:: Epoch 172, Iteration 100, Current loss 0.47927749156951904 Accuracy 99.33561937419631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 172, Iteration 110, Current loss 0.6053410172462463 Accuracy 99.22582865007455\n",
      "Training:: Epoch 172, Iteration 120, Current loss 0.43524593114852905 Accuracy 99.32675458821359\n",
      "Training:: Epoch 172, Iteration 130, Current loss 0.37437039613723755 Accuracy 99.49587534372135\n",
      "Training:: Epoch 172, Iteration 140, Current loss 0.40497300028800964 Accuracy 99.52952773546016\n",
      "Training:: Epoch 172, Iteration 150, Current loss 0.47572287917137146 Accuracy 99.61082688355415\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 172, Probability Accuracy 66.6623855491569\n",
      "Starting Training\n",
      "Training:: Epoch 173, Iteration 0, Current loss 0.37074100971221924 Accuracy 99.47348193697157\n",
      "Training:: Epoch 173, Iteration 10, Current loss 0.612930178642273 Accuracy 99.30215392873917\n",
      "Training:: Epoch 173, Iteration 20, Current loss 0.4609525799751282 Accuracy 99.33322664959726\n",
      "Training:: Epoch 173, Iteration 30, Current loss 0.3815232813358307 Accuracy 99.52016097825246\n",
      "Training:: Epoch 173, Iteration 40, Current loss 0.48121920228004456 Accuracy 99.40125606065537\n",
      "Training:: Epoch 173, Iteration 50, Current loss 0.5695396065711975 Accuracy 99.25112331502746\n",
      "Training:: Epoch 173, Iteration 60, Current loss 0.4535469710826874 Accuracy 99.35997107213885\n",
      "Training:: Epoch 173, Iteration 70, Current loss 0.417236864566803 Accuracy 99.39924176144649\n",
      "Training:: Epoch 173, Iteration 80, Current loss 0.5145733952522278 Accuracy 99.52981004325747\n",
      "Training:: Epoch 173, Iteration 90, Current loss 0.42820319533348083 Accuracy 99.5370580492897\n",
      "Training:: Epoch 173, Iteration 100, Current loss 0.5300489068031311 Accuracy 99.21143035602748\n",
      "Training:: Epoch 173, Iteration 110, Current loss 0.2974958121776581 Accuracy 99.41913359927217\n",
      "Training:: Epoch 173, Iteration 120, Current loss 0.6259243488311768 Accuracy 99.33686231167471\n",
      "Training:: Epoch 173, Iteration 130, Current loss 0.3993639051914215 Accuracy 99.37738940469688\n",
      "Training:: Epoch 173, Iteration 140, Current loss 0.3060719966888428 Accuracy 99.54224221008673\n",
      "Training:: Epoch 173, Iteration 150, Current loss 0.3626539707183838 Accuracy 99.26124295872195\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 173, Probability Accuracy 66.53985582301031\n",
      "Starting Training\n",
      "Training:: Epoch 174, Iteration 0, Current loss 0.4292743504047394 Accuracy 99.35827651406635\n",
      "Training:: Epoch 174, Iteration 10, Current loss 0.7404852509498596 Accuracy 98.84949518666353\n",
      "Training:: Epoch 174, Iteration 20, Current loss 0.517493724822998 Accuracy 99.30968360498562\n",
      "Training:: Epoch 174, Iteration 30, Current loss 0.36139172315597534 Accuracy 99.45119560956488\n",
      "Training:: Epoch 174, Iteration 40, Current loss 0.3683490753173828 Accuracy 99.41028943265776\n",
      "Training:: Epoch 174, Iteration 50, Current loss 0.41506534814834595 Accuracy 99.33720644791752\n",
      "Training:: Epoch 174, Iteration 60, Current loss 0.46990489959716797 Accuracy 99.47112580206105\n",
      "Training:: Epoch 174, Iteration 70, Current loss 0.4676814675331116 Accuracy 99.21189077082613\n",
      "Training:: Epoch 174, Iteration 80, Current loss 0.38985225558280945 Accuracy 99.39615159298815\n",
      "Training:: Epoch 174, Iteration 90, Current loss 0.4881373345851898 Accuracy 99.37764208548614\n",
      "Training:: Epoch 174, Iteration 100, Current loss 0.32880064845085144 Accuracy 99.5111664909422\n",
      "Training:: Epoch 174, Iteration 110, Current loss 0.5007363557815552 Accuracy 99.32832587028744\n",
      "Training:: Epoch 174, Iteration 120, Current loss 0.45753541588783264 Accuracy 99.3971520631951\n",
      "Training:: Epoch 174, Iteration 130, Current loss 0.39849328994750977 Accuracy 99.38531999517897\n",
      "Training:: Epoch 174, Iteration 140, Current loss 0.45491501688957214 Accuracy 99.16696207018788\n",
      "Training:: Epoch 174, Iteration 150, Current loss 0.36225712299346924 Accuracy 99.31124033195958\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 174, Probability Accuracy 66.11737167005013\n",
      "Starting Training\n",
      "Training:: Epoch 175, Iteration 0, Current loss 0.4045926630496979 Accuracy 99.58757504567998\n",
      "Training:: Epoch 175, Iteration 10, Current loss 0.43581289052963257 Accuracy 99.36063495563027\n",
      "Training:: Epoch 175, Iteration 20, Current loss 0.4264609217643738 Accuracy 99.50105754108141\n",
      "Training:: Epoch 175, Iteration 30, Current loss 0.37169989943504333 Accuracy 99.5525019210776\n",
      "Training:: Epoch 175, Iteration 40, Current loss 0.3197557330131531 Accuracy 99.41930316379656\n",
      "Training:: Epoch 175, Iteration 50, Current loss 0.490853488445282 Accuracy 99.42441442161643\n",
      "Training:: Epoch 175, Iteration 60, Current loss 0.3688231110572815 Accuracy 99.59935451560848\n",
      "Training:: Epoch 175, Iteration 70, Current loss 0.5254465937614441 Accuracy 99.5069704182251\n",
      "Training:: Epoch 175, Iteration 80, Current loss 0.48893123865127563 Accuracy 99.61107003953586\n",
      "Training:: Epoch 175, Iteration 90, Current loss 0.6477459669113159 Accuracy 99.25548589341693\n",
      "Training:: Epoch 175, Iteration 100, Current loss 0.3569289743900299 Accuracy 99.28769855402807\n",
      "Training:: Epoch 175, Iteration 110, Current loss 0.527623176574707 Accuracy 99.51210587302555\n",
      "Training:: Epoch 175, Iteration 120, Current loss 0.3752131760120392 Accuracy 99.3222201868474\n",
      "Training:: Epoch 175, Iteration 130, Current loss 0.440130352973938 Accuracy 99.5163719971458\n",
      "Training:: Epoch 175, Iteration 140, Current loss 0.5004534721374512 Accuracy 99.51906260930396\n",
      "Training:: Epoch 175, Iteration 150, Current loss 0.44035929441452026 Accuracy 99.46821769837973\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 175, Probability Accuracy 65.64558561544517\n",
      "Starting Training\n",
      "Training:: Epoch 176, Iteration 0, Current loss 0.35999536514282227 Accuracy 99.49940269639912\n",
      "Training:: Epoch 176, Iteration 10, Current loss 0.41124793887138367 Accuracy 99.34136050221262\n",
      "Training:: Epoch 176, Iteration 20, Current loss 0.4281172752380371 Accuracy 99.54797952292779\n",
      "Training:: Epoch 176, Iteration 30, Current loss 0.34441667795181274 Accuracy 99.4894753377317\n",
      "Training:: Epoch 176, Iteration 40, Current loss 0.38132303953170776 Accuracy 99.47845182541862\n",
      "Training:: Epoch 176, Iteration 50, Current loss 0.34232422709465027 Accuracy 99.4572591587517\n",
      "Training:: Epoch 176, Iteration 60, Current loss 0.455085813999176 Accuracy 99.32376587271771\n",
      "Training:: Epoch 176, Iteration 70, Current loss 0.33994874358177185 Accuracy 99.49223190602501\n",
      "Training:: Epoch 176, Iteration 80, Current loss 0.4205707311630249 Accuracy 99.58098307816277\n",
      "Training:: Epoch 176, Iteration 90, Current loss 0.3936067819595337 Accuracy 99.48797139141743\n",
      "Training:: Epoch 176, Iteration 100, Current loss 0.30750855803489685 Accuracy 99.57052345112047\n",
      "Training:: Epoch 176, Iteration 110, Current loss 0.46357524394989014 Accuracy 99.43163364728808\n",
      "Training:: Epoch 176, Iteration 120, Current loss 0.4953920841217041 Accuracy 99.0724335591454\n",
      "Training:: Epoch 176, Iteration 130, Current loss 0.3230378031730652 Accuracy 99.53301476976542\n",
      "Training:: Epoch 176, Iteration 140, Current loss 0.41272804141044617 Accuracy 99.16085012940161\n",
      "Training:: Epoch 176, Iteration 150, Current loss 0.34259292483329773 Accuracy 99.58718301552979\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 176, Probability Accuracy 65.1843642540498\n",
      "Starting Training\n",
      "Training:: Epoch 177, Iteration 0, Current loss 0.36728373169898987 Accuracy 99.52362434376823\n",
      "Training:: Epoch 177, Iteration 10, Current loss 0.4654316008090973 Accuracy 99.53119574024771\n",
      "Training:: Epoch 177, Iteration 20, Current loss 0.24396631121635437 Accuracy 99.60921060542296\n",
      "Training:: Epoch 177, Iteration 30, Current loss 0.5950415134429932 Accuracy 99.35862195345427\n",
      "Training:: Epoch 177, Iteration 40, Current loss 0.3448285460472107 Accuracy 99.54686530105525\n",
      "Training:: Epoch 177, Iteration 50, Current loss 0.3990282714366913 Accuracy 99.6583299166325\n",
      "Training:: Epoch 177, Iteration 60, Current loss 0.40415993332862854 Accuracy 99.25449871465295\n",
      "Training:: Epoch 177, Iteration 70, Current loss 0.3036068379878998 Accuracy 99.55440029706646\n",
      "Training:: Epoch 177, Iteration 80, Current loss 0.2854959964752197 Accuracy 99.64410087635376\n",
      "Training:: Epoch 177, Iteration 90, Current loss 0.4101743698120117 Accuracy 99.2827868852459\n",
      "Training:: Epoch 177, Iteration 100, Current loss 0.43703174591064453 Accuracy 99.48649252065194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 177, Iteration 110, Current loss 0.3735576272010803 Accuracy 99.43668319062641\n",
      "Training:: Epoch 177, Iteration 120, Current loss 0.2737676203250885 Accuracy 99.50777803575575\n",
      "Training:: Epoch 177, Iteration 130, Current loss 0.4317043125629425 Accuracy 99.48620251703036\n",
      "Training:: Epoch 177, Iteration 140, Current loss 0.38714709877967834 Accuracy 99.60011936735303\n",
      "Training:: Epoch 177, Iteration 150, Current loss 0.2414124310016632 Accuracy 99.52761627906976\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 177, Probability Accuracy 65.59473008244603\n",
      "Starting Training\n",
      "Training:: Epoch 178, Iteration 0, Current loss 0.3671926259994507 Accuracy 99.37986541760127\n",
      "Training:: Epoch 178, Iteration 10, Current loss 0.36065787076950073 Accuracy 99.35506986743103\n",
      "Training:: Epoch 178, Iteration 20, Current loss 0.3822423815727234 Accuracy 99.73392841816153\n",
      "Training:: Epoch 178, Iteration 30, Current loss 0.43707501888275146 Accuracy 99.52448271430407\n",
      "Training:: Epoch 178, Iteration 40, Current loss 0.4197095036506653 Accuracy 99.4051307473045\n",
      "Training:: Epoch 178, Iteration 50, Current loss 0.4401111602783203 Accuracy 99.51534733441034\n",
      "Training:: Epoch 178, Iteration 60, Current loss 0.34572574496269226 Accuracy 99.56045316659444\n",
      "Training:: Epoch 178, Iteration 70, Current loss 0.3042154014110565 Accuracy 99.39759036144578\n",
      "Training:: Epoch 178, Iteration 80, Current loss 0.3485899567604065 Accuracy 99.69125876374864\n",
      "Training:: Epoch 178, Iteration 90, Current loss 0.29451310634613037 Accuracy 99.48567229977958\n",
      "Training:: Epoch 178, Iteration 100, Current loss 0.32696568965911865 Accuracy 99.60183560534485\n",
      "Training:: Epoch 178, Iteration 110, Current loss 0.41189324855804443 Accuracy 99.50955943474646\n",
      "Training:: Epoch 178, Iteration 120, Current loss 0.4203271269798279 Accuracy 99.60352652720539\n",
      "Training:: Epoch 178, Iteration 130, Current loss 0.3372524082660675 Accuracy 99.26294630037332\n",
      "Training:: Epoch 178, Iteration 140, Current loss 0.4581131935119629 Accuracy 99.44087934430395\n",
      "Training:: Epoch 178, Iteration 150, Current loss 0.25026774406433105 Accuracy 99.56399359788068\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 178, Probability Accuracy 65.75257902804823\n",
      "Starting Training\n",
      "Training:: Epoch 179, Iteration 0, Current loss 0.3944138288497925 Accuracy 99.60845732184808\n",
      "Training:: Epoch 179, Iteration 10, Current loss 0.4464448094367981 Accuracy 99.36940602115541\n",
      "Training:: Epoch 179, Iteration 20, Current loss 0.392734557390213 Accuracy 99.46789212722216\n",
      "Training:: Epoch 179, Iteration 30, Current loss 0.39403027296066284 Accuracy 99.6263646447545\n",
      "Training:: Epoch 179, Iteration 40, Current loss 0.3191089630126953 Accuracy 99.52071208490243\n",
      "Training:: Epoch 179, Iteration 50, Current loss 0.34073981642723083 Accuracy 99.49033657970429\n",
      "Training:: Epoch 179, Iteration 60, Current loss 0.27679288387298584 Accuracy 99.67466978983668\n",
      "Training:: Epoch 179, Iteration 70, Current loss 0.35034263134002686 Accuracy 99.64676792652773\n",
      "Training:: Epoch 179, Iteration 80, Current loss 0.5065272450447083 Accuracy 99.20265780730897\n",
      "Training:: Epoch 179, Iteration 90, Current loss 0.3435164988040924 Accuracy 99.60121231456372\n",
      "Training:: Epoch 179, Iteration 100, Current loss 0.4761446714401245 Accuracy 99.58865991853854\n",
      "Training:: Epoch 179, Iteration 110, Current loss 0.6104943156242371 Accuracy 99.01320361362058\n",
      "Training:: Epoch 179, Iteration 120, Current loss 0.8022275567054749 Accuracy 99.0520694259012\n",
      "Training:: Epoch 179, Iteration 130, Current loss 0.3477732539176941 Accuracy 99.55099966113183\n",
      "Training:: Epoch 179, Iteration 140, Current loss 0.41427549719810486 Accuracy 99.49060634277264\n",
      "Training:: Epoch 179, Iteration 150, Current loss 0.5305739045143127 Accuracy 99.24963924963924\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 179, Probability Accuracy 65.50783030202594\n",
      "Starting Training\n",
      "Training:: Epoch 180, Iteration 0, Current loss 0.3511476218700409 Accuracy 99.4875478927203\n",
      "Training:: Epoch 180, Iteration 10, Current loss 0.3340184986591339 Accuracy 99.56983696821095\n",
      "Training:: Epoch 180, Iteration 20, Current loss 0.30451926589012146 Accuracy 99.60589694935047\n",
      "Training:: Epoch 180, Iteration 30, Current loss 0.3297669291496277 Accuracy 99.64217083029621\n",
      "Training:: Epoch 180, Iteration 40, Current loss 0.31177687644958496 Accuracy 99.52184450121176\n",
      "Training:: Epoch 180, Iteration 50, Current loss 0.35069677233695984 Accuracy 99.5253164556962\n",
      "Training:: Epoch 180, Iteration 60, Current loss 0.43333953619003296 Accuracy 99.30359495576887\n",
      "Training:: Epoch 180, Iteration 70, Current loss 0.5127220153808594 Accuracy 99.54420537569543\n",
      "Training:: Epoch 180, Iteration 80, Current loss 0.4709285795688629 Accuracy 99.26124295872195\n",
      "Training:: Epoch 180, Iteration 90, Current loss 0.49907127022743225 Accuracy 99.29982668977469\n",
      "Training:: Epoch 180, Iteration 100, Current loss 0.24768927693367004 Accuracy 99.49993670084821\n",
      "Training:: Epoch 180, Iteration 110, Current loss 0.3635425567626953 Accuracy 99.58853712847323\n",
      "Training:: Epoch 180, Iteration 120, Current loss 0.3419661223888397 Accuracy 99.68804336708602\n",
      "Training:: Epoch 180, Iteration 130, Current loss 0.2762477993965149 Accuracy 99.520005999925\n",
      "Training:: Epoch 180, Iteration 140, Current loss 0.2949256896972656 Accuracy 99.46002211957583\n",
      "Training:: Epoch 180, Iteration 150, Current loss 0.5079686641693115 Accuracy 99.39639490656523\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 180, Probability Accuracy 65.45397108174173\n",
      "Starting Training\n",
      "Training:: Epoch 181, Iteration 0, Current loss 0.4930516481399536 Accuracy 99.38608458390178\n",
      "Training:: Epoch 181, Iteration 10, Current loss 0.40785589814186096 Accuracy 99.65824357912179\n",
      "Training:: Epoch 181, Iteration 20, Current loss 0.3084612488746643 Accuracy 99.5945411392405\n",
      "Training:: Epoch 181, Iteration 30, Current loss 0.4447225034236908 Accuracy 99.41198088937891\n",
      "Training:: Epoch 181, Iteration 40, Current loss 0.39920175075531006 Accuracy 99.52956270715279\n",
      "Training:: Epoch 181, Iteration 50, Current loss 0.3636794686317444 Accuracy 99.50468540829986\n",
      "Training:: Epoch 181, Iteration 60, Current loss 0.41713380813598633 Accuracy 99.65951651344909\n",
      "Training:: Epoch 181, Iteration 70, Current loss 0.3428724408149719 Accuracy 99.4931842013282\n",
      "Training:: Epoch 181, Iteration 80, Current loss 0.3472004234790802 Accuracy 99.4711412901541\n",
      "Training:: Epoch 181, Iteration 90, Current loss 0.44662797451019287 Accuracy 99.29770657302754\n",
      "Training:: Epoch 181, Iteration 100, Current loss 0.3004116117954254 Accuracy 99.43044394164001\n",
      "Training:: Epoch 181, Iteration 110, Current loss 0.34704893827438354 Accuracy 99.2626957170077\n",
      "Training:: Epoch 181, Iteration 120, Current loss 0.35914117097854614 Accuracy 99.49936701576706\n",
      "Training:: Epoch 181, Iteration 130, Current loss 0.30373263359069824 Accuracy 99.6423271565009\n",
      "Training:: Epoch 181, Iteration 140, Current loss 0.31169241666793823 Accuracy 99.64566929133858\n",
      "Training:: Epoch 181, Iteration 150, Current loss 0.3515700399875641 Accuracy 99.54200560530454\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 181, Probability Accuracy 65.23304470315284\n",
      "Starting Training\n",
      "Training:: Epoch 182, Iteration 0, Current loss 0.2842159867286682 Accuracy 99.58322670273827\n",
      "Training:: Epoch 182, Iteration 10, Current loss 0.27278172969818115 Accuracy 99.37439421975505\n",
      "Training:: Epoch 182, Iteration 20, Current loss 0.31575337052345276 Accuracy 99.55583424779223\n",
      "Training:: Epoch 182, Iteration 30, Current loss 0.5030034184455872 Accuracy 99.42111699959234\n",
      "Training:: Epoch 182, Iteration 40, Current loss 0.3764550983905792 Accuracy 99.44868995633188\n",
      "Training:: Epoch 182, Iteration 50, Current loss 0.3562581539154053 Accuracy 99.60706150341686\n",
      "Training:: Epoch 182, Iteration 60, Current loss 0.41450726985931396 Accuracy 99.38077545965514\n",
      "Training:: Epoch 182, Iteration 70, Current loss 0.31252521276474 Accuracy 99.36374922408442\n",
      "Training:: Epoch 182, Iteration 80, Current loss 0.3594508171081543 Accuracy 99.30642600484666\n",
      "Training:: Epoch 182, Iteration 90, Current loss 0.3579378128051758 Accuracy 99.48082173387638\n",
      "Training:: Epoch 182, Iteration 100, Current loss 0.3245927691459656 Accuracy 99.43492093641673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 182, Iteration 110, Current loss 0.44168826937675476 Accuracy 99.50368586234582\n",
      "Training:: Epoch 182, Iteration 120, Current loss 0.32629433274269104 Accuracy 99.52845528455285\n",
      "Training:: Epoch 182, Iteration 130, Current loss 0.38630548119544983 Accuracy 99.10142786804529\n",
      "Training:: Epoch 182, Iteration 140, Current loss 0.3573949337005615 Accuracy 99.45156747943378\n",
      "Training:: Epoch 182, Iteration 150, Current loss 0.5275856256484985 Accuracy 99.25980212532063\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 182, Probability Accuracy 64.57730869619257\n",
      "Starting Training\n",
      "Training:: Epoch 183, Iteration 0, Current loss 0.44952720403671265 Accuracy 99.20537144265123\n",
      "Training:: Epoch 183, Iteration 10, Current loss 0.47972404956817627 Accuracy 99.44744816113125\n",
      "Training:: Epoch 183, Iteration 20, Current loss 0.3036709427833557 Accuracy 99.64680768484378\n",
      "Training:: Epoch 183, Iteration 30, Current loss 0.2599327266216278 Accuracy 99.52861952861953\n",
      "Training:: Epoch 183, Iteration 40, Current loss 0.2951217293739319 Accuracy 99.6447973255328\n",
      "Training:: Epoch 183, Iteration 50, Current loss 0.43092212080955505 Accuracy 99.56515066250576\n",
      "Training:: Epoch 183, Iteration 60, Current loss 0.3658800721168518 Accuracy 99.40810208317319\n",
      "Training:: Epoch 183, Iteration 70, Current loss 0.3727368712425232 Accuracy 99.1338453995751\n",
      "Training:: Epoch 183, Iteration 80, Current loss 0.27492138743400574 Accuracy 99.54357987352213\n",
      "Training:: Epoch 183, Iteration 90, Current loss 0.41441524028778076 Accuracy 99.29043308049914\n",
      "Training:: Epoch 183, Iteration 100, Current loss 0.38034528493881226 Accuracy 99.47259496628614\n",
      "Training:: Epoch 183, Iteration 110, Current loss 0.5338706970214844 Accuracy 98.68255080588648\n",
      "Training:: Epoch 183, Iteration 120, Current loss 0.403715044260025 Accuracy 99.49061126648023\n",
      "Training:: Epoch 183, Iteration 130, Current loss 0.3827364146709442 Accuracy 99.37632028970928\n",
      "Training:: Epoch 183, Iteration 140, Current loss 0.3558558225631714 Accuracy 99.59017948620651\n",
      "Training:: Epoch 183, Iteration 150, Current loss 0.37695029377937317 Accuracy 99.55368212248946\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 183, Probability Accuracy 64.47321539545096\n",
      "Starting Training\n",
      "Training:: Epoch 184, Iteration 0, Current loss 0.4371801018714905 Accuracy 99.54914337240757\n",
      "Training:: Epoch 184, Iteration 10, Current loss 0.40371155738830566 Accuracy 99.44915561353918\n",
      "Training:: Epoch 184, Iteration 20, Current loss 0.38525858521461487 Accuracy 99.52748072618752\n",
      "Training:: Epoch 184, Iteration 30, Current loss 0.251298725605011 Accuracy 99.592595239797\n",
      "Training:: Epoch 184, Iteration 40, Current loss 0.264628529548645 Accuracy 99.58198153231844\n",
      "Training:: Epoch 184, Iteration 50, Current loss 0.5950541496276855 Accuracy 98.28982898289829\n",
      "Training:: Epoch 184, Iteration 60, Current loss 0.30492183566093445 Accuracy 99.59132348318138\n",
      "Training:: Epoch 184, Iteration 70, Current loss 0.40760350227355957 Accuracy 99.4165631246286\n",
      "Training:: Epoch 184, Iteration 80, Current loss 0.4142765998840332 Accuracy 99.27858340013117\n",
      "Training:: Epoch 184, Iteration 90, Current loss 0.30097365379333496 Accuracy 99.62839974699557\n",
      "Training:: Epoch 184, Iteration 100, Current loss 0.2798800766468048 Accuracy 99.6373328672551\n",
      "Training:: Epoch 184, Iteration 110, Current loss 0.37027430534362793 Accuracy 99.45252099286058\n",
      "Training:: Epoch 184, Iteration 120, Current loss 0.38010430335998535 Accuracy 99.61123873475879\n",
      "Training:: Epoch 184, Iteration 130, Current loss 0.4658471345901489 Accuracy 99.25953627524308\n",
      "Training:: Epoch 184, Iteration 140, Current loss 0.46041321754455566 Accuracy 99.17003385388227\n",
      "Training:: Epoch 184, Iteration 150, Current loss 0.4226910471916199 Accuracy 99.3234100135318\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 184, Probability Accuracy 64.7918133985168\n",
      "Starting Training\n",
      "Training:: Epoch 185, Iteration 0, Current loss 0.2813102602958679 Accuracy 99.25664608794254\n",
      "Training:: Epoch 185, Iteration 10, Current loss 0.3584948778152466 Accuracy 99.56286963229623\n",
      "Training:: Epoch 185, Iteration 20, Current loss 0.4029751420021057 Accuracy 99.37576046130243\n",
      "Training:: Epoch 185, Iteration 30, Current loss 0.35767701268196106 Accuracy 99.4476383722241\n",
      "Training:: Epoch 185, Iteration 40, Current loss 0.35706350207328796 Accuracy 99.273681640625\n",
      "Training:: Epoch 185, Iteration 50, Current loss 0.4178481101989746 Accuracy 99.17317708333333\n",
      "Training:: Epoch 185, Iteration 60, Current loss 0.6745634078979492 Accuracy 98.74634350188049\n",
      "Training:: Epoch 185, Iteration 70, Current loss 0.36153531074523926 Accuracy 99.53219592735277\n",
      "Training:: Epoch 185, Iteration 80, Current loss 0.3765700161457062 Accuracy 99.25388892799324\n",
      "Training:: Epoch 185, Iteration 90, Current loss 0.3529146611690521 Accuracy 99.28648648648648\n",
      "Training:: Epoch 185, Iteration 100, Current loss 0.4393884241580963 Accuracy 99.42705506302394\n",
      "Training:: Epoch 185, Iteration 110, Current loss 0.4301292896270752 Accuracy 99.24641271807577\n",
      "Training:: Epoch 185, Iteration 120, Current loss 0.4436909556388855 Accuracy 99.3965192090091\n",
      "Training:: Epoch 185, Iteration 130, Current loss 0.33671140670776367 Accuracy 99.37049614843038\n",
      "Training:: Epoch 185, Iteration 140, Current loss 0.35145291686058044 Accuracy 99.4179661761476\n",
      "Training:: Epoch 185, Iteration 150, Current loss 0.4800432324409485 Accuracy 99.13468562117211\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 185, Probability Accuracy 63.2868624932676\n",
      "Starting Training\n",
      "Training:: Epoch 186, Iteration 0, Current loss 0.3833233118057251 Accuracy 99.50934450631236\n",
      "Training:: Epoch 186, Iteration 10, Current loss 0.3961181938648224 Accuracy 99.18116683725691\n",
      "Training:: Epoch 186, Iteration 20, Current loss 2.635801076889038 Accuracy 78.20627802690584\n",
      "Training:: Epoch 186, Iteration 30, Current loss 2.1586623191833496 Accuracy 88.57165267864146\n",
      "Training:: Epoch 186, Iteration 40, Current loss 5.422898292541504 Accuracy 65.79079919409\n",
      "Training:: Epoch 186, Iteration 50, Current loss 2.186082363128662 Accuracy 90.62876139164327\n",
      "Training:: Epoch 186, Iteration 60, Current loss 2.8342883586883545 Accuracy 85.20833333333333\n",
      "Training:: Epoch 186, Iteration 70, Current loss 2.5510354042053223 Accuracy 85.62929768987097\n",
      "Training:: Epoch 186, Iteration 80, Current loss 2.4094300270080566 Accuracy 88.0485254130935\n",
      "Training:: Epoch 186, Iteration 90, Current loss 1.4127118587493896 Accuracy 95.51830120145291\n",
      "Training:: Epoch 186, Iteration 100, Current loss 1.4208749532699585 Accuracy 95.88538274820911\n",
      "Training:: Epoch 186, Iteration 110, Current loss 1.7901829481124878 Accuracy 92.32488822652758\n",
      "Training:: Epoch 186, Iteration 120, Current loss 1.387068748474121 Accuracy 95.3267589049161\n",
      "Training:: Epoch 186, Iteration 130, Current loss 1.0925482511520386 Accuracy 94.60261875761267\n",
      "Training:: Epoch 186, Iteration 140, Current loss 0.810667097568512 Accuracy 97.03188241141802\n",
      "Training:: Epoch 186, Iteration 150, Current loss 1.801547646522522 Accuracy 93.2634164777022\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 186, Probability Accuracy 68.18856941624891\n",
      "Starting Training\n",
      "Training:: Epoch 187, Iteration 0, Current loss 1.0779814720153809 Accuracy 96.87099725526075\n",
      "Training:: Epoch 187, Iteration 10, Current loss 0.8257220983505249 Accuracy 97.3796567601507\n",
      "Training:: Epoch 187, Iteration 20, Current loss 0.7204241156578064 Accuracy 97.84500912536852\n",
      "Training:: Epoch 187, Iteration 30, Current loss 1.009761929512024 Accuracy 98.36443468715697\n",
      "Training:: Epoch 187, Iteration 40, Current loss 0.6453840136528015 Accuracy 98.89813809466361\n",
      "Training:: Epoch 187, Iteration 50, Current loss 0.6172642111778259 Accuracy 98.29371250833148\n",
      "Training:: Epoch 187, Iteration 60, Current loss 0.7798171043395996 Accuracy 98.37762669962918\n",
      "Training:: Epoch 187, Iteration 70, Current loss 0.5290353894233704 Accuracy 98.33872936680643\n",
      "Training:: Epoch 187, Iteration 80, Current loss 0.4287474751472473 Accuracy 99.08833312800296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:: Epoch 192, Probability Accuracy 65.682769192526\n",
      "Starting Training\n",
      "Training:: Epoch 193, Iteration 0, Current loss 0.34367337822914124 Accuracy 99.48711610949762\n",
      "Training:: Epoch 193, Iteration 10, Current loss 0.4601042568683624 Accuracy 99.658689834387\n",
      "Training:: Epoch 193, Iteration 20, Current loss 0.3832114338874817 Accuracy 99.7263454681151\n",
      "Training:: Epoch 193, Iteration 30, Current loss 0.50337815284729 Accuracy 99.39959258067975\n",
      "Training:: Epoch 193, Iteration 40, Current loss 0.3128560185432434 Accuracy 99.45202592338902\n",
      "Training:: Epoch 193, Iteration 50, Current loss 0.7672080993652344 Accuracy 99.34844577168454\n",
      "Training:: Epoch 193, Iteration 60, Current loss 0.4079411029815674 Accuracy 99.49175923909098\n",
      "Training:: Epoch 193, Iteration 70, Current loss 0.5011250972747803 Accuracy 99.4717700034638\n",
      "Training:: Epoch 193, Iteration 80, Current loss 0.43806707859039307 Accuracy 99.63551538125091\n",
      "Training:: Epoch 193, Iteration 90, Current loss 0.4326714277267456 Accuracy 99.54599502756459\n",
      "Training:: Epoch 193, Iteration 100, Current loss 0.3342600464820862 Accuracy 99.52306285116947\n",
      "Training:: Epoch 193, Iteration 110, Current loss 0.2935525178909302 Accuracy 99.54977805960685\n",
      "Training:: Epoch 193, Iteration 120, Current loss 0.4213319718837738 Accuracy 99.70411377302663\n",
      "Training:: Epoch 193, Iteration 130, Current loss 0.4470946192741394 Accuracy 99.54106568124925\n",
      "Training:: Epoch 193, Iteration 140, Current loss 0.4051840007305145 Accuracy 99.28847326692417\n",
      "Training:: Epoch 193, Iteration 150, Current loss 0.3524336516857147 Accuracy 99.56331877729258\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 193, Probability Accuracy 66.07262708704478\n",
      "Starting Training\n",
      "Training:: Epoch 194, Iteration 0, Current loss 0.43798717856407166 Accuracy 99.51200963305659\n",
      "Training:: Epoch 194, Iteration 10, Current loss 0.4324389398097992 Accuracy 99.53630868593194\n",
      "Training:: Epoch 194, Iteration 20, Current loss 0.44248339533805847 Accuracy 99.54383724112763\n",
      "Training:: Epoch 194, Iteration 30, Current loss 0.33045151829719543 Accuracy 99.553208773355\n",
      "Training:: Epoch 194, Iteration 40, Current loss 0.30374014377593994 Accuracy 99.6800551289624\n",
      "Training:: Epoch 194, Iteration 50, Current loss 0.2944123148918152 Accuracy 99.47938578039546\n",
      "Training:: Epoch 194, Iteration 60, Current loss 0.34583139419555664 Accuracy 99.43552180633021\n",
      "Training:: Epoch 194, Iteration 70, Current loss 0.4135127067565918 Accuracy 99.37678250765818\n",
      "Training:: Epoch 194, Iteration 80, Current loss 0.266467422246933 Accuracy 99.62860281424909\n",
      "Training:: Epoch 194, Iteration 90, Current loss 0.3457068204879761 Accuracy 99.46332737030411\n",
      "Training:: Epoch 194, Iteration 100, Current loss 0.35495853424072266 Accuracy 99.50597257041734\n",
      "Training:: Epoch 194, Iteration 110, Current loss 0.41142538189888 Accuracy 99.42267319804058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-76d53e4fb509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmiddle_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_stages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mfinal_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_dilated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 259\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initialize_epoch = 15\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0\n",
    "for epoch in range(1000):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        \n",
    "        loss = 0\n",
    "        for p in predictions:\n",
    "            loss += ce_criterion(p, item_2)\n",
    "            loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(p[:, :, 1:], dim=1), \n",
    "                                                                F.log_softmax(p.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                        max=16) * src_mask_mse[:, :, 1:])\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(predictions[-1], dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "    # Calculating Expectation Step\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Calculating Validation Data Accuracy\")\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, item in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            item_0 = item[0].to(device)\n",
    "            item_1 = item[1].to(device)\n",
    "            item_2 = item[2].to(device)\n",
    "            src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "            src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "            middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "            pred = torch.argmax(predictions[-1], dim=1)\n",
    "            correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "            total += float(torch.sum(src_mask).item())\n",
    "    val_acc = correct * 100.0 / total\n",
    "    if val_acc > best_val_acc:\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-best-model.wt\")\n",
    "    torch.save(model.state_dict(), config.output_dir + \"ms-tcn-last-model.wt\")\n",
    "    print(f\"Validation:: Epoch {epoch}, Probability Accuracy {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\n",
    "\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-speed/final-em-maximized.wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 57.770101729343025\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "\n",
    "print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_labels(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            select_item = np.random.randint(start, i, 1)[0]\n",
    "            unique_ids.append(select_item)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    select_item = np.random.randint(start, len(labels_arr), 1)[0]\n",
    "    unique_ids.append(select_item)\n",
    "    return unique_ids\n",
    "# get_selected_labels(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            unique_ids.append(i - 1)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    unique_ids.append(len(labels_arr) - 1)\n",
    "    return unique_ids\n",
    "# get_boundary(np.array([2, 2, 2, 2, 3, 3, 4, 4, 4, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "boundary_dict = {}\n",
    "for file in glob.glob(\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/*txt\"):\n",
    "    video_id = file.split(\"/\")[-1].split(\".txt\")[0]\n",
    "    data = open(file).read().split(\"\\n\")[0:-1]\n",
    "    data = np.array(data)\n",
    "    boundary = get_boundary(data)\n",
    "    boundary_dict[video_id] = boundary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[5]\n",
    "    for i, count in enumerate(count_all):\n",
    "        video_id = video_ids[i]\n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_selected_labels(labels)\n",
    "\n",
    "        loaded_vidid_selected_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_boundary_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[4]\n",
    "    for i, count in enumerate(count_all):\n",
    "        \n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_boundary(labels)\n",
    "        video_id = video_ids[i]\n",
    "        video_id_boundary_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in video_id_boundary_frames.keys():\n",
    "    if len(video_id_boundary_frames[ele]) != len(loaded_vidid_selected_frames[ele + \".txt\"]):\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(video_id_boundary_frames, open(\"dump_dir/video_id_boundary_frames_dict.pkl\", \"wb\"))\n",
    "# pickle.dump(loaded_vidid_selected_frames, open(\"dump_dir/loaded_vidid_selected_frames_dict.pkl\", \"wb\"))\n",
    "pickle.dump(boundary_dict, open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "    \n",
    "    weights = [1, 1, 1, 1, 0, 0]\n",
    "    ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "    for i, outp_ele in enumerate(outp[1]):\n",
    "        upped_logit = F.upsample(outp_ele, size=outp[0].shape[-1], mode='linear', align_corners=True)\n",
    "        ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i + 1] / sum(weights)\n",
    "    \n",
    "    return ensemble_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/results/c2f-tcn-model/split2_c2ftcn_model.wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration done\n",
      "11 iteration done\n",
      "21 iteration done\n",
      "31 iteration done\n",
      "41 iteration done\n",
      "51 iteration done\n",
      "61 iteration done\n",
      "71 iteration done\n",
      "81 iteration done\n",
      "91 iteration done\n",
      "101 iteration done\n",
      "111 iteration done\n",
      "121 iteration done\n",
      "131 iteration done\n",
      "141 iteration done\n",
      "151 iteration done\n",
      "Train Boundary avergage error = 107.269\n",
      "Train From boundary avergage accuracy = 87.407\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        calculate_element_probb(prob, item_1, item[4], item_2)\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f'{i+1} iteration done')\n",
    "get_boundary_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    idx = 4\n",
    "\n",
    "    cur_vid_feat = torch.softmax(predictions[-1], dim=1).permute(0, 2, 1)[idx]\n",
    "    cur_vidid = item[4][idx]\n",
    "\n",
    "    selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "    \n",
    "    bound_list = video_id_boundary_frames[cur_vidid]\n",
    "    \n",
    "    cumsum_feat = torch.cumsum(cur_vid_feat, dim=0)\n",
    "    prob_each_segment = prob_vals_per_segment(selected_frames, cur_vid_feat, item_2[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min prob 0 = 7.953912266787591e-36\n",
      "Min prob 1 = 2.7495868628582206e-249\n",
      "Min prob 2 = 8.185175464823537e-201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 442)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBEAAAEzCAYAAABnp5vxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXxU5fU/8M9zZ8lkmewbIYFs7JuyKrhg0Yr7VqFWrcVvtd/uLv221W8r6M8itrVKrVatitIvVbEiUkEUF1DZAwKyBQIhZCELZM8ksz6/PyYzgGzZ5j5zk8/79brNZHIzz0HoLOee5xwhpQQRERERERER0bloqgMgIiIiIiIiImNgEoGIiIiIiIiIOoRJBCIiIiIiIiLqECYRiIiIiIiIiKhDmEQgIiIiIiIiog5hEoGIiIiIiIiIOuScSQQhxKtCiGohxM4z/FwIIf4qhCgSQuwQQozt+TCJiIiIiIiISLWOVCK8BmD6WX5+FYBB7ce9AP7e/bCIiIiIiIiIKNycM4kgpfwcQO1ZTrkBwELptwFAvBCiX08FSEREREREREThoSd6IvQHUHrC92Xt9xERERERERFRL2LugccQp7lPnvZEIe6Ff8sDoqOjxw1NNvl/kDwIdRXlAICEDOYfOupQ4yEAQHZstm5rempaAQDmlEjd1iQiIv24iosBANacHMWREIWfo0ePAgCSk5MVR0LUNRUVFThy5Mgp9/fr1w8ZGRkKIqJw4nD43wNEReVgy5YtR6WUKac7ryeSCGUAsk74PhNAxelOlFK+BOAlABg/frwsmHet/weXz8Fbj/4WADBz9rweCKlveGbLMwCA+8bdp9ua1S/uAACk/mi0bmsSEZF+qp/6CwAg9cEHFEdCFH4WLFgAAJg1a5biSIi6Z/27BzD55nxIedprv9RHbdn6PQDAuLH/ghCi5Ezn9UQSYRmAnwkh3gQwCUCDlPLU9NbpXD6nB5bvu/RMHhARUd/A5AERUe934U15qkMgAztnEkEI8QaAqQCShRBlAGYDsACAlPIFACsAXA2gCIADAFOzREREREREYWz27NmqQyCDOmcSQUp52zl+LgH8tEurv3WH/+vM/+vSr/d19392PwDg6cueVhwJERH1FmU//wUAIPPZvyqOhIiIQuWDF7/GpH63qA6DDKontjN0naNO6fJGV++sVx0CERH1Mt56vrYQEfV2bc1u1SGQgfXEiEciIiIiIiIi6gOUJxEc8GHPsT0oiqyGB17V4RARERERERHRGSjdzvAJHHhQHIX3/RlAGnBz9fkqwyEiIiIiIiKis1CaRHjG6kK2iMN3x/4Mf9j4B7i0vlmJ0OZpQ01rDWocNWh2N2N82nhEWaLO+XuT+k3SIToiIupLoi68QHUIREQUYplDE1SHQCHg9XlRVF8Ei2ZBlCUKaVFpEEL0+DrKkgiNzkY4PRH4y9S/YFTyKPxh4x8gIVWF02McbgeK6otQVF+EKkcVGp2NcHgc8Pq8cPvcqHfWo66tDm3eNri8LjS6GtHkajrpMeIi4jBj8AyMSB4BAQGX14Xatlo4PA5EmiNht9oRbYnG2NSxSI5KhtPrRIQpQtGfmIhCSUoJt88Np9cJr88LTdNgEiYICJg0EzShQYMGTWgheZGgviflJz9RHQIREYXYhGtyVIdAPczldeGB1Q9gTdma4H12qx2jkkfhksxLcF3edYi1xvbIWsqSCDWtNbgk7hJMGzANNY4aADB0CqG0sRSv7HwFyw4sg9t3vNtptCUa0eZomDQTTMKEBFsCkiKTEGWOgtVkhd1qR2pUKpIjk5ESmQIA+Pe+f+Plr1/uVFIlNSoVWfYsZNmzMCxxGEYkj4Ddavf/LDIVMdaYnv0DE1G3NTgbUNpUitKmUuyv249dx3ahpLEEbR5/ktHpdcLlc3X48TShBZMKwQSD0DDAPgAjk0ciIyYDUsrgc0vg9omJijZPmz9hIb2INEciyhIFszj9S4UQAjaTDVGWKESaI/3nm6OCv2e32v2HxQ6LydIj/82IiIiI+iopJTzSA7fXDZfXBbfPjWhLNCyaBQ+ueRBrytbgp+f9FNmx2Wh0NWJP7R5sq96GeZvm4Zktz+CO4Xfgl2N/2e04lCUR2rxtuKfRAW3RrTDd8iIAQApjpRE8Pg/Wlq/FO/vfwedln8MkTLgp/yZM6T8Fg+IHIT0mHRat82+cp/SfgmpHNWrbauGTPlg1KxJsCYi2RKPV04pmVzOa3c14bMNjcHqduGLgFShrKkNZUxk+L/scS4uWnvR4ZmHG+Wnn49LMS3Fl9pVIj07vqf8ERH2WlBJe6a8w8vg8cPvcwSf0BlcD6p31aHD6vzY6G/3ft99f11aHsqYyNLoag49nEibkx+djTMoYRFmiEGGKgNVkRYQpwn9bs8KkmeCTPvikD17pPel2IJ7AfYH73V43ihuK8f7B99Hibjnrn8lmssFqssJmssGkmdDqaUWLuwVSnv652Su9HU52RpojYbf4kwqJkYkYYB+ATHsmEm2JiI+I9x+2eCREJCDWGguTZur4Xwb1qMP33AsAGPCPlxRHQkREofKfZ7cBAK77+XmKIyEAWFWyCm8VvgWX1+U/fK7jt9u/d3v9F3xO997LrJnh8Xnw8KSHcdvQ2075+e5ju/HK16/g5a9fxrDEYfh29re7Fa+yJILVZMV0rwXwtkHAX4JrlO0M5c3lWLJ/CZYWLUW1oxpJtiTcNeIu3DHsDqREpfTIGqlRqUiNSj3lfpvZhgSbfw+TzWSDzWTDj8f8OPhzKSUqWyqxp3aP/x+ZlCisK8QX5V/gzwV/xlMFT2Fc2jjcMvgWfHvgt2E1WXskXiIj8kkfjrYexdHWozjWegzH2o6d9muzu/mkREHgdmees+wWO+Ii4hAXEYcEWwJGJY9Clj0LmfZMZNmzMMA+ADazLaR/1jZPG4QQEBDHv0IAwp9s7Ox2CCklXD4XWt2taPW0wuFxoNXTGkw+NLmaTjqa3c1odDWixlGDz0o/Q21b7WkfV0AgNiL2eHLhxMMWj5TIFAyIHYAsexaSbEncxtHDZFub6hCIiCjEPC6f6hConU/68FTBU3B6nciLy0O0JTp4AclissBqssKq+S8sWUwWWDWr/z6TFRbNgmZ3M461HsOYlDFnTA4MTxqOeZfMQ9mKMvxh4x8wPn08Em2JXY5ZWRIhKyYL5vbkgSb8kybDPYVQ0VyBF3e8iPeK3oOExJSMKXh44sO4JOuSLlUchIIQAv1i+qFfTL/gfVfjatw/7n6UNJbgg+IP8P7B9/HQFw/hT5v/hJvyb8KMITOQEZOhMGoi/Xl9Xvzwox+ioKrglJ9FmiORZEtCUmQSBtgHIMYaA6vJCrMww2KywKJZYNbMsGiW4GHWzMEn9riIOMRHxAc/CMdaY2HWlPaxhSa0DjVs7QwhRLBSIh7xnf59h9vhr8xw1qGhrQF1zjrUO+v9R1t98HaVowqFdYWob6tHm/fkD7iR5khk2bOQHZuNnLick45Ic2RP/VGJiIiIQqKgsgDlzeWYd/E8XJN7TcjWsWgWPD7lccx8fyYe3/A4nrr0qS5fiFH2rvbEK27Hkwjhm0Z4/+D7eGTtIwCA24behrtG3GW4bQEDYwfiv8f8N+4dfS82HNmAt/a+hQW7FmDBrgW4pP8lmDl0JiZnTA7+fRD1Zu8WvYuCqgLcPfJujE4ZHUwaJNmSevzDNp1elCUKUZaoTiUxWz2tqGqpCvaSKG0qxeGmwyisK8THhz+GT/qvrGhCQ25cLkYkjcDEfhMxMX2i4Z6ziYjCmdPrRIOz4eTD1RDcytfgbECjqxENzga0edr82/+kBx6fx98ouL0hsEmYjn+FgFkzBxP13/xqM9sQa41FrDUWcRFxiLXGItIcCZvZFuzNYzP7K3UjTBHQNC1YdRd4fyuEgFn412AlG4WDJUVLYLfYMW3AtJCvNShhEH5y3k8wf+t8/H7t7/HIhY90qTJd7aWxdsEkQpj2RHh91+v4c8GfMSF9AuZeNNfwb0Q1oWFyxmRMzpiMypZKLC5cjHf2v4PVH6/GAPsA3D3yblyfdz0boRnIifvz3T7/vnyPz990xe1zw+VzwevzwiP9L9wenyd42yu9/ukh0n3S9xLypP31Puk7432a0IIv0MGv4viLtiY0/xVrcwQitPavpoiT9uAH7gv0AghlMqvB2YC/bv0rxqWNw31j7+ObCAOJNEciOy4b2XHZp/zM5XWhtKkUBxsOYl/dPuw+thtrytbgvQPvAfAnUiemT8SkfpMwIX1Ct8r4iIh6s0ZXI4rqilDeXI4jLUeCR42jJpgcaPW0nvH3zZoZ8RHxiLPGITYiFjHWGH9yoP3Du0kzAfJ4bx2vzwsf2vv5tL9PcfvcweRDoP+Qw+NAk6vprGt3lCY0/9bg9qSDzXzy7cBEtMARSF7YrfZgxWGkORJlTWUobSr1V+Wd0N8n3hbPijg6p0ZXIz4u+Rg35t8Y0m2tJ7p75N1wep14YfsLKGkswd+m/Q1xEXGdegy1SYTBVwII7+0MgQTCFQOvwLyL54VVD4FLMy/t9mOkR6fjF2N/gR+P+TFWlazC67tfx5z1c/D37X/H3SPvxs2DbtbtHzT5y7vLmstQ0VyB8uZyVDmq4HA7/IfHgRZ3Cxwex0n3tXpa4fK6wrqSpysCe78izBHBF/ITX8BjI/y3k2xJSI9OR3Jksn+vmGYJbjkI3Ib0N3N1ep1o9bTi1Z2vosHVgIcmPsQEQi9iNVmRF5+HvPg8XDHwCgD+fYb76/ZjU+UmbDyyESuKV+DtfW8D8O8PvD7velyTcw3ibZ3fjtFbxUydqjoEItKRx+fB5srNKKgqwL7afSisK8SRliMnnZNoS0R6dDoyYjIwPGk44qxxiLfFBysCAh+q46z+25HmyJC+vgbGpDe6GtHmaQserd7W49972yClDF7wCEwkCjQeDpwTmErU6mkN3m52NaPaUR3s6ePwOLoUp81kO55U+EYT4cDXuIg42K121DhqUNZcBq/PG6zUizJHIdoSffzrCfd3d5tk9qjkbv0+9YwPDn4Ap9eJmwbdpNuamtDw0/N+ivz4fPzPmv/BG3vfwH+P+e9OPYY4U9ftUBs/frwsKPDvRXa4HZj0r0mYVjsUz9z/tpJ4TmdHzQ7c9cFdmJo1FX++9M99olu4lBLrKtbhxR0v4qvqr5BkS8I1uddgfNp4jE8fj9YFxQCA1B+NVhypMbm9blS0VKC8qRxlzWUoby73H03+r3XOupPOt2gWxFhiTnrRiDL7b0dbooMj9QIfmgMNVgJH4PtAGWBg1KhZMx//qplgFuaT7g+MBzyxkiBwCCGg4fj3AE55cfZJ3ykv1k6vEy6vC21e//jCwBjD4Pdnud/h9l95CLxhaHT6v544TrWzZg6Zid9d8Ltu/X2S8Xh8Huw+thsbj2zEqpJV2FO7B1bNintH34u7R97NCiwiOqsFCxYAAGbNmqU4ku7bX7cfbxW+hY8OfYQ6Zx1MwoTs2GwMThyMIQlDMChhEAbYByA9Or3PX1By+9xodjUHkwqBnj0OjwP9o/sjy54Ft3Sjvs3f5+fEr4FzT7y/ydXU7ZgClQ/ZcdnIj8/HhPQJmJQ+qVtj3d1e/5aTQNUIL7R0jZQSrZ5WNLmacKztGGocNahtqz1pi0/g697avUiNSsXb172t5L/3be/fBovJgoVXLQQAbNn6PQDAuLH/ghBii5Ry/Ol+L7y2MyiO40RNrib8+vNfIzUqFY9OebRPJBAA/z6xKf2nYHLGZBRUFeDVna/ijb1vYOHuhbBb7bhfm4VLfZNUhxn2jrUew+5ju1FYV4jSptJgqVtlS+VJFQNmzYyM6Az0j+mPaQOnoX9Mf2TGZCIjxn9foi2RT+BnIKVEm7cNx1qP4UjLERxrPRYseQxs4wgcUspgaWKEKQJ2qx0XZ16s+o9ACpg1M0anjMbolNG4Z/Q9KKwtxEs7XsLftv0NKw+txJ3D78SEtAnItGfy/3tE1CttrtyMF7a/gE2Vm2DVrJg2YBq+nf1tTOk/heX3Z2DRLEiwJQQnpJ1RByvC3T63v3dEe5KhydWE5MhkZNozYTVZ/ZWnbgdaPC3B6tNANeqJVanHWo+huKEYS/YvwaI9i2AWZpyXeh4u6n8RhiQOCV5IOnEryUm9INrfklY6KvFe0Xv4uORjuHyuYJwmYUKEKSJYbRKoPEmLSkP/mP7+w+5/72qkflLVjmrM3zofFc0V/h4d0r+Fxid9wSpWq2aF2WT2T0j45oU6kwUmYYLD40CjszF4oevEiVQe6Tnt2iZhCv63jLXGYnjycNw57E5l7zkm95+Ml79+GQ3Ohk5taVCbRFjg7z5p+v5SAIAvjHoi/Lngz6hsqcRr019DrDVWdTinNWulPwu+YPqCHn9sIQQmpE/AhPQJcHqd2FGzA09veRqPuf6Kaz3fwlz5DN9gf0NBZQE+PPQh1h9Zj5LGkuD9SbYkZNmzMC5tHDLtmciMyfQnC+yZSIlM6TMJqp4mhECkOdL/39SeqTocMqghiUPw1NSnsLp0NeZtmofZ62YD8I/ZnZA+AePTxmNC+gQMsA/oM895JXd+HwAw8J8LFUdCRD2ppLEET2x6AmvL1yI1MhX3j7sfN+ffzO1cClg0C5Ijk5EcefotBdGW6E49ntvrxraabVhbvhZrK9bima3PdDomu9WOmwbdhH7R/YK9szw+D9o8bcEq0AZXA/bX7ceX5V+e0pciISIBmfZMDE4YjOFJw5Edm43UqFREW6Lh8DjQ5mk7qWlmYLKdV3qDvTC80hv83m61Iyky6aQJeB6fB42uRnh8nmDF64mVrxISkIAPPnh8Hri8rmCvMAmJKHMUihuLMW/TPDg9ToxIHoEIc8RJlRcn9hRrcbcEe4sF+42d0KMj0hzpTwZExCLeFo8B9gGIjYg9qZdGki0JqVGpSLQlIi4iDlHmqLB6P3FR/4vw0o6XsPHIxjOOhzydsKhECPyHDJc93Q63A8sPLsctg27BeannqQ5HuQhTBCakT8DrV72O3y94EO+bP8U9jcXIjctVHVpY8Pq8eG7bc/jH1/9ApDkSE9In4NbBt2JE0ggMTRzarbIyItLH1KypuDTzUhQ3FKOgqgCbKzdj45GNWH5wOQAgNTIV49LHYUL6BIxIGoGcuBzYTDY0OBvQ5G5CelQ6t0IQUViSUuK9A+9h7sa5MGtmPDDuAdw29LY+v0WhN7GYLMGLf/eNuw81jhqUN5cHP+wGjsAVdwmJze8XAxCYeF0OosxRmJg+scP/JqSUqG2rPb4tt7k8WHW7qmQV3tn/To/8uQQEYiwxsJgskFKi3lnfI58XRyaNxNyL5yInLqcHojS2UcmjYLfYsa5infGSCOG2neHz8s/h9DoxPWe66lDCikWzYIJvDJbjM7i9Xd+L3ptUtlTi92t/jw1HNuCWQbfgtxN/yxdlIoMSQiA3Phe58bmYMWQGpJQ41Hgo2HCsoLIAHxR/4D8XAhbNEiz71ISGftH9kGXPwgD7AGTHZfubPMblITUqNayuOhBR3yGlxOMbHsfifYsxIX0CnrjoCaRFp6kOi0IsJSoFKVEpZz2nzbUVADA9e2ynH18I4R+LHZmE0Skn90mTUqKipQJlTWWodlTD4XYgyhKFSHMkPNJ/lT9wRR/wv34G+nGZhCk48rPB2YBqRzWa3c3Bzx2JkYlIiEiAWTMHe3cFpoGdOBUMwElNtgON8VvdrcGt2ydWOPRlZs2MCzIuwJflX6IzvRLDIokg0F6JECbbGVYdWoVEWyLGpnb+/1S9ndb+dxWYxd5XeX1evLH3DTz71bPwSR8enfwobh50s+qwiKgHCSGQE5eDnLicYFLhcNNh7Kvbh6L6IjjcDqRGpSLGEoOKlgocbjyMw42H8cGhD05qmmW32JEbn4v8+PxgYiEvnskFIgq9+VvnY/G+xZg1YhZ+OfaX3EJJISeECPZLIGOYnDEZq0pW4UD9gQ7/TngkEYSAfwuL+iRCq6cVX5R/getyr+MT7WkI+LN7fTWJ4JM+fHjoQzy/7XkcajyEKf2n4HeTfsc9+UR9gBACA2MHYmDswOAIydMJlHkeqD+AAw0H/F/rD+DTw5+eVOIZSC4EEgv58fnIjc9FWlQakwtE1G0Ldy3EKztfwYzBM3D/uPv5vEJEpzUlYwoAYG3FWozs4O+oTSKMuDF4U4MIiyTC2vK1aPW0dmpPiCpXZl+p+5qmPppEkFLik8Of4Lltz6Govgj58fl4Zuoz+NaAb/FFmYhOcmKZ58R+E0/6WTC5UH8ARfVFONhwEKtLV2PJ/iXBc2IsMRgYOzDYiDXTnoksexYy7ZlIi0rr9mzwc7Ffxa18REa36+guPLXlKVw+4HI8POlhvlehU+SPS1UdAoWJfjH9kB2bjYLKAow8xwCSALVJhIn3BG8KiDBIIQAflXyEhIgEjEsbpzqUc/ru0O/qvmZg64kPfSOJIKXEF+Vf4G9f/Q17avcgOzYbf7zkj7gy+8rgnisioo5KtCUiMT0RE9InnHR/ILlwsP4giuqLUNpUir21e/HJ4U/g8R0fE2UWZvSL8fdeCCQYTkw22K327sf4ve91+zGISB2Pz4M56+cgyZaEx6Y8xspaOq1RU1lFS8cNTRyKr49+DSQkduh8tUkEl8P/1RoFIdX3RHD73FhTugZX5VwV8is9PSEwWkXPmb5aH6pE2Fy5Gc9sfQY7anYgMyYTj095HNfkXmOIfxtEZCxnSi54fV5UO6pR2lSKsuayYPfrsqYyfHTsI9Q76086P9oSjbSoNKRHpyMtKg1p0WmnfG+32M96VdLX6n9t0SI5L57IiBbtWYS9tXvx1KVP9UhikXont8sLALBYmWQiIC8+DysPrYTTF48I7dwXStV+Glp0q//rrOXtlQhqkwiljaVweByGqEIAgJ98/BMAwILpC3Rbsy80Vtxftx/PbH0Gn5d9jrSoNMy+cDZuyL+BXVyJSHcmzYR+Mf3QL6YfJmLiKT9vcjWhrKkMZc1lKG8qR6WjElUtVahyVKGorgg1rTWnvLZGmiOPJxWi0oKVDYGj+d77ISAw8J8L9fpjElEPqWypxHPbnsPUzKln7d1C9P6z2wEANz3IRvIE5MfnAwAq2tqQExV1zvPD5pJqOGxnONhwEACQG5erOJLwJXpxEqGqpQrPbXsO7x14D9HmaNw/7n58b+j3OLKRiMKW3WrHsKRhGJY07LQ/d/vcOOo4iipHVTDBUNlSiSqHP9Gw4cgGVDuqT0o0RE7VkN0cibGb5mFE0ggMTxqO7NhslkQTGcDLX78Mt8+N3076LfsgEFGH5cXnAQDKnU6jJRHUb2cobigGAOTE5SiNI5yZZO/bzuCTPry5903M3zofbp8bdwy7A/eMugfxtnjVoRERdYtFswQrGc7E5XWhvLkcpU2lKG0qxc53XsZBeyuW7F+CRXsWAQDiIuJwWdZluGLgFbiw34WwmFiZRRRuqlqqsGT/EtyYfyPH6xFRp2TZs2DRLChva+vQ+eGTRJDqKxGKG4qRFpWGKMu5sy99VWDEo1d6FUfSMxpdjfj5Jz/H1uqtmJwxGb+/4Pcc10hEfYrVZEVOXE4wgV5S+CEAIPP1BShuKMauY7uw8chGfFzyMZYWLYXdYsfULH+p9OT+kxFhilAZPhG1W7BrAaSU+OGoH6oOhYgMxqyZkROXg3LnkY6dH+J4OkwA8ClOIxQ3FLMK4RwCjRWlVJ3y6T6n14lffvpL7Di6A/9vyv/DDXk3sPSPiKidSTMhPyEf+Qn5uCH/Bri8Lmw4sgGrSlbh08Of4j8H/4MocxQuH3g5ZgyZgdHJo/kcSqTI0daj+Pe+f+PavGtZhUBEXZIXn4dNZcUdOldtEuG842OkNMWNFaWUKG4sxvV51yuLobNuyL9B9zUDjRWNXong9Xnx0BcPoaCqAPMunodrcq9RHRIRUViIu+mm095vNVlxSeYluCTzEjxy4SPYfGQzPiz5ECuLV2LZgWUYljgMPxrzI3wr61tMJhDp7F97/gW3z417Rt1z7pOJAAy98Mzb3Khvyo/PxwfFH6DNe+7PeWqTCOffHrwppIBU+J6j2lGNFneLoZoq3ph/o+5r9oZKBCklntj0BFaVrMKvxv+KCQQiohPE33z6JMKJLJoFk/tPxuT+k/HrCb/G8oPLsXD3Qtz32X0YljgMtw+7Hd/O/rauI4iJ+iqf9GHZgWWYnDEZA2IHqA6HDGLYZCYR6GSB5ooVTuc5zz33EMhQajnmP9DeWFFhJUJxo/GaKta11aGurU7XNXvDdIYXtr+AtwrfwqwRs3DXiLtUh0NEFFY8dXXw1HX8tSXaEo0ZQ2Zg6Q1L8fiUx9HqacXv1v4O0xZPw9yNc7Gvbl8IoyWizZWbUeWowg15+leoknG1NrvQ2uxSHQaFkby49gkNHWiuqLYSYfH3/V9nLQcUb2cw4mSGB1Y/AABYMH2BbmtqBk8ivLn3TTy//XncmH8j7h93v+pwiIjCTvkvfgkAGPjPhZ36PbNmxg35N+D6vOtRUFWAf+/7N/697994Y+8bGJ0yGt8Z9B1cmX0lmxcT9bBlB5YhxhKDqVlTVYdCBrLyxZ0AgJseHKs4EgoXWfYsmIVAedhXIpxAg9rtDAfrDyLaEo2UyBR1QRhAYDuDD8ZLIqw8tBJzN87F1KypmH3hbO7ZJSIKASEEJqRPwJOXPIlPbv0E/zP+f9DkasIj6x7BtLen4fENj6OwtlB1mES9gsPtwKqSVbgy+0rYzDbV4RCRgZk0E/pFRBigEuEEQqrfzpATm8MPluegGXTE4/qK9Xjoi4dwfur5+NMlf4JZC5t/+kREvVaCLQHfH/F93Dn8Tmyt3op/7/s33t3/Lt4qfAsT0yfintH3YFL6JL72EnXRJ4c/Qaun1VCNwYkofPWPiMA+h+Oc54VNJYIIg+0MufHGaaqoihEbK1Y0V+DB1Q8iNy4Xz057lpl6IiKdCSEwLm0cnrj4CXw641M8OO5BFDcU456P7sEdK+7AmtI1hnpdIQoX7x98H/1j+uP81PNVh0JEvcDQmBgMjIyEx+c563lhlURQVSDf4m5BtaPaUP0QVBEGG/EYGOXogw/zLzMaUycAACAASURBVJuPWGus6pCIiPq0uIg4/GDkD/DBLR/g9xf8Hkdbj+Jnn/4MM96fgXXl61SHR2QYTq8TW6q24LKsy1jNQ0Q9YmpiIn45cOA5q7bV1nRPuDt4U8jA/+gv2FQx1lhJhJlDZuq+pslglQiv7HwFW6u3Yu5Fc5Fpz1QdDhFR2Eu47bu6rBNhisCMITNw06CbsOLgCvx9+9/xo49/hAv6XYCHJj7E6kCic9hRswNOrxMT0yeqDoUMaOSl/VWHQAamNokw8pbgTZXbGY60HAEA9Lcb6/9M03Om676mkUY8FtYW4vltz+OqnKtwbe61qsMhIjKE2Kuv1nU9i2bBDfk34Kqcq7C4cDH+vv3vuOU/t+CHo36IH476ISJMEbrGQ2QUmyo3QRMaxqWPUx0KGdCg8WmqQyADU7udoaHMfyCQRFDD6fWPsTDaG5XKlkpUtlTquqYm26czhHkSQUqJuRvnwm61438n/S/L/IiIOsh95AjcR47ovq7VZMUdw+/AshuX4crsK/HC9hdw2/LbsK9un+6xEJ3LO/vewSclnyiNYdORTRiWOIxbNalLmmrb0FR77i78RKejNomw5Ef+A4CAuukMbq8bgP8NjJE89MVDeOiLh3RdUzNIJcLy4uXYWr0V9429D3ERcarDISIyjIpf/wYVv/6NsvWTIpMw7+J5eG7ac6htrcVt79+GRXsWGWYbHfUN//j6H7hv9X2Yv3W+kvdErZ5W7Di6g1sZqMs+XrAbHy/YrToMMqjwaawoBXyKeiK4vC4AxqtEUEEYYMRjs6sZTxU8hZFJI3HToJtUh0NERF1wSeYlWHLDEkzOmIx5m+bhoS8fQqunVXVYRAD874OiLdF4+euX8as1v9I9kbCtehs8Pg8mpE/QdV0iIiCMkgiawu0MLp8/iWDRLIoiMA4jjHhcuHshjrYexcOTHoYmwuafOBERdVKiLRHzvzUfPz//51hxcAW+/8H3UdVSpTosIvh8PlyZfSXuG3sfVpWswmu7XtN1/c2Vm2ESJoxNG6vrukREQBglEVRuZwhUIhhtO4MKWpiPeHS4HXhj7xuYmjUVo1JGqQ6HiIi6SRMa7h19L56b9hxKm0px+4rb2SeBlPNKLzSh4e6Rd+OKgVfg2a3PYnvNdt3W31S5CSOTRyLaEq3bmkREAWGURBCQinrfBZMIGpMI5xKsRFBWN3J27xa9i3pnPe4eefe5TyYiIsO4OPNivD79dUhI3PXBXVhfsV51SNSHSUiYhAlCCMyZPAepUan4zee/gcPtCPnaDrcDu47uYj8EIlJGbRJh8s/8BwAhFVYi+FwwCRNMmknJ+l1114i7cNeIu3RdM1iJ4Au/SgSPz4OFuxbi/NTzcX7q+arDISIypMRZs5A4a5bqME5rSOIQLLp6EfrF9MNPPv4J3it6T3VI1Ed5pTc49jrWGovHL3oc5c3l+M+B/4R87V3HdsEjPXyvQ91y3hUDcN4VA1SHQQZlVrr6kKuCN/0jHtVtZzDiVoapWVN1XzOcKxE+OvQRKloq8NuJv1UdChGRYdm/dZnqEM4qPTodr09/HQ+sfgC/W/s71DvrdU+oE/mk76SLT+PTxmNY4jC8te8tzBgyI6SjpQPbJkanjA7ZGtT75YxOVh0CGViHKhGEENOFEIVCiCIhxCmf0IQQA4QQnwkhvhJC7BBCXN2h1Y/u9x9gEqErihuKUdxQrOuaIoxHPC7auwjZsdm4NOtS1aEQERmW82AxnAf1fW3pLLvVjuenPY8rs6/Enwv+rMvVX6IT+aQv+J4IAIQQmDlkJvbX7cdX1V+FdO3t1duRHZvNEdbULXWVLairbFEdBhnUOZMIQggTgOcAXAVgOIDbhBDDv3Ha7wAsllKeD+C7AJ7v0Or/uc9/oH07g6KeCG6f25D9EB5b/xgeW/+YrmtqYTrisbihGDtqduCWQbdwIgMRUTdUzp6NytmzVYdxThaTBXMvmouJ6RPxyNpHsK5ineqQqA/xSR9M4uRtsFflXAW7xY63Ct8K2bpSSmyv2Y4xKWNCtgb1DasXFWL1okLVYZBBdeTT1kQARVLKg1JKF4A3AdzwjXMkgNj223EAKjobiICAj5UIYS9cRzwuO7AMmtBwTe41qkMhIiKdWE1WPHPZM8iNz8WDqx9ESWOJ6pCoj/BJHzTt5LfRUZYoXJ9/PT4q+QjHWo+FZN3SplLUOeswJpVJBCJSpyNJhP4ASk/4vqz9vhPNAXCHEKIMwAoAPz/dAwkh7hVCFAghCmpqak7+WfuQRxVcPhcsmkXJ2kYTjiMevT4v/nPgP5icMRkpUSmqwyEiIh3ZrXY8+61nYdJMuH/1/bp0xyfySm/wwsqJZgyeAY/Pg+UHl4dk3UA/BFYiEJFKHUkinG6TwTc/7d8G4DUpZSaAqwH8U4hTa8qllC9JKcdLKcenpJz8YU/liEen18lKhA4Kx0qETZWbUOWowg153yyQISKiviAjJgN/vPiPKKorwmMbHgur1yjqnaSUp90+mRufi9y4XHxZ/mVI1t1esx3RlmjkxeWF5PGJiDqiI0mEMgBZJ3yfiVO3K/wXgMUAIKVcD8AGoFMtP1WOeHR7jdkTQQURhpUIyw4sg91ix2UDwrujOBERhc7k/pPx0/N+iuUHl+O9Axz9SKHlld4z9mCanDEZW6q2oM3T1uPr7qjZgZHJIw03lpyIepeOjHjcDGCQECIHQDn8jRO/941zDgOYBuA1IcQw+JMINTiXS34VvKm0J4LPmD0R7h19r5J1NamFzXQGh9uBTw5/gmtyr0GEKUJ1OEREhpf84/9WHUKX/XDUD7HhyAY8sfEJjEsbhyx71rl/iaiTAhe9vtlYMWByxmT8357/w9aqrZjcf3KPretwO7Cvbh/+a9R/9dhjUt81/ups1SGQgZ2zEkFK6QHwMwAfAtgD/xSGXUKIx4QQ17ef9iCAe4QQ2wG8AeAHsiO1hHmX+Q/499qrKj40amPFCzMuxIUZF+q+rqZwHOc3fVb6GVo9rbg291rVoRAR9QrRkycjenLPffDRk0kz4Q8X/QGa0PC/X/4vvL7wqZqj3iPwHuhMlQjj08fDoll6fGLIrmO74JVe9kOgHpE1LBFZwxJVh0EG1aFZeFLKFVLKwVLKPCnlH9rve0RKuaz99m4p5RQp5Rgp5XlSyo86tPqRHf4DgRGPnM7QGXtr92Jv7V7d19Wghc12hhXFK5AenY7zU89XHQoRUa/QtmcP2vbsUR1Gl2XEZODhSQ/jq+qvsHD3QtXhUC/kg78a80xJhEhzJMamjcXairU9uu6Wqi0QEEwiUI+oKW1CTWmT6jDIoDqURAiZlQ/5D7Q3VlQUhttnzJ4IT256Ek9uelL3dTVoYdG0qq6tDuvK1+GqnKvO+EJORESdUzX3CVTNfUJ1GN1ybe61+FbWt/DctudwuPGw6nColzlXJQLg39JQVF+Eakd1j61bUFWAwQmDERcR12OPSX3Xl4v348vF+1WHQQYVNp+8/AMe1XwwdXqdsJg44rGjBERYVCKsKlkFj/TgmpxrVIdCRERhRAiBhyc9DItmwWPrOa2BelZHkghTMqYAANZXrO+RNd1eN7ZXb8f49PE98nhERN0RRkkEoXY7gwErEVQJl0qE5QeXIy8uD4MTBqsOhYiIwkxadBruH3c/NlZuxNKiparDoV6kI0mEQQmDkGRL6rG+CLuO7UKbtw3j05hEICL1wieJIBVvZzBgTwRVtDCoRNhStQVbq7fi6tyrIYRQGgsREYWn7wz+Ds5LOQ/zt86Hw+1QHQ71EueazgD4Ewxj08bi66Nf98iaBVUFAIBxaeN65PGIiLojfJIIULedweV1cTxgJ2hQN+LR6XXiT5v/hFkrZyEjOgM35t+oJA4iIgp/mtDw4PgHcaztGJssUo8JvF8910WMYYnDUNpUikZXY7fXLKgsQH58PhJsCd1+LCKi7jIrXX3aI8GbQuHYQJfXZcieCL8c+0sl6woIZUmE13a+hoW7F2LG4Bl4YPwDiLZEK4mDiKi3Srn/ftUh9KjzUs/D5QMux4KdC3Dr4FuRFJmkOiQyuI5UIgDA8KThAIDC2kJMSJ/Q5fU8Pg++qv4K1+Vd1+XHIPqmC27MUx0CGZjaSoQBk/wH2j+YKqhKl1LC5TNmT4TzUs/Deann6b6uSWElwvaa7RiUMAi/v/D3TCAQEYVA1NjzETW2d43N/cXYX8DpdeLFHS+qDoV6gXONeAwYmjgUALD72O5urbfn2B44PA42VaQe1S8vDv3yOOmDukZtEuHwRv8BQEg12xk8Pg8AGLInwrbqbdhWvU33dVVWjRTWFWJIwhAlaxMR9QWOrV/BsfUr1WH0qJy4HNw86Ga8Xfg2ShtLVYdDBteRxooAkBSZhLSotG4nEQL9ENhUkXrSkQMNOHKgQXUYZFBqkwifPOY/4P9gCgUfTJ1eJwAYshJh/tb5mL91vu7ratDg9enfWLG+rR7VjmpOYyAiCqGap59GzdNPqw6jx/14zI9hMVnw16/+qjoUMriObmcAgGFJw7Cndk+31ltXsQ55cXlIjkzu1uMQnWjD0gPYsPSA6jDIoMKosaKa6QwunwsADNkTQRVNakoqEQrrCgGAlQhERNRpKVEpuHP4nVh5aCV2Ht2pOhwysI42VgT8fREONRzq8nSQRlcjCioLcGnWpV36fSKiUAifJIIEfEL/D6Yurz+JYMTtDKoIRSMe99XtAwAMTmQlAhERdd6sEbOQEJGAp7c8DSlVDZYmo+tMJcLwxOGQkMELIZ21tnwtPNKDy7Iu69LvExGFQtgkETRF++zdXjcAcMRjJ6ga8VhYW4gkWxLL+YiIqEtirDH40ZgfYVPlJmyq3KQ6HDKozlQiDEsaBqDrzRU/K/0MibZEjEoe1aXfJyIKhbBJIqjezmDEngiqaIpGPO6r24chidzKQEREXfedwd9Bki0Jr+58VXUofd6cOXNUh9AlgekMHalESIlMQZItqUtJBLfPjS/LvsSlmZfCpJ17LSIivZiVrj79ieBNAUAq3M5gxJ4Iv5n4GyXrqqhEcPvcKKovwh3D7tB1XSKivibt4YdUhxBSEaYI3DH8DszfOh97ju0JXikm/T366KOGTCR0dDoD4K9WGJ40vEvNFbdWbUWTuwlTs6Z2+neJzuWiGYNUh0AGprYSod9o/wFASDXbGYxciTA0cWhwBrGeNAjd95IeajgEt8/NfghERCFmGzYMtmG9+4P1jCEzEG2JxoKdC1SHQgbUmSQC4H+/drD+YPDCVUetLl2NCFMELuh3QadjJDqXlCw7UrLsqsMgg1KbRDjwmf+Awu0MBm6suL5iPdZXrNd9XQ2a7o0VOZmBiEgfLevWoWXdOtVhhFSsNRYzBs/AhyUforSpVHU4fcqcOXMghAj2EwjcNlJFQmeTCHnxefBKLw43Hu7wGj7pw6eHP8WkfpMQZYnqUpxEZ1O6pxale2pVh0EGpTaJ8Pmf/QfatzOoqEQwcBLhpR0v4aUdL+m+roCmeyXCvtp9sGgWZMdl67ouEVFfc/TvL+Do319QHUbI3T7sdmhCw+u7XlcdSp8yZ84cSCmD7yMCtw2ZROjg2+jcuFwAwMGGgx1eY1v1NlS0VGB69vTOB0jUAQUrDqFgxSHVYZBBhU9jRSkgz93ktscZOYmgiqZgxOO+un3Ij8+HRTNe7woiIgo/adFpuC73OiwtWopjrcdUh0MGEmys2MFmh4ELIJ1JIiw/uByR5khMGzCt0/EREYVa+CQRAPjYE8EQNGjBF1C9HGo8hJy4HF3XJCKi3u0HI38Al9eFN/a+oTqUPmn27NmqQ+iSzm5niDRHIiM6A8UNxR063+1148OSDzE1ayq3MhBRWAqjJIKixoqsROg0AQGfT78kgsfnQWVLJfrH9NdtTSIi6v1y43JxWdZleGPvG3C4HarD6XOMtIXhRJ3dzgAAOfE5HU4ifFn+JRqcDbg299ouxUdEFGphlUSAgO577d0+NwBWInSGSedKhMqWSnilF1n2LN3WJCKivuHuUXej0dWId/a/ozoUMohgEkHr+Nvo3LhcFDcUd2hE9vsH30dCRAIuzLiwyzESEYWSWenq1z0TvCnaGyJISH9CQSdOrxMAYDEZb6/9Ixc+omRdAdGhF8GeUtZcBgCsRCAi0kH6o4+qDkFXY1LGYGzqWCzaswjfG/q9Du9zp76rS5UIcTlo87ahsqUSGTEZZzyvxd2CNWVrcPOgm9kHikJq6u2ceEZdp7YSIXmQ/wCCaQO9G/YZeTtDTlyOkj4BmtR0TSKUN5UDADLtmbqtSUTUV0Xk5iAit2/1oLlt6G0oby7HuorePdqSekZneyIAHZ/QUN5cDqfXibFpY7seIFEHJKRHIyE9WnUYZFBqkwiFH/gP+Dv+A9zO0BmrS1djdelq3dfVdB7xWNZcBrMwIy0qTbc1iYj6qqZPP0PTp5+pDkNX0wZMQ5ItCYsLF6sOhQwgkEToTNVKMIlQf/YkQrOrGQAQa4ntYnREHVO84yiKdxxVHQYZlNrtDOv+5v865KrgdgY9r3ADxq5ECMy2npo1Vdd1hc4jHsubytEvph9LTImIdFC7YAEAwP6tyxRHoh+LyYKbB92MV3a+giPNR9Avpp/qkCiMBfpCdWb7bYItAfER8ShuPHtzxWa3P4kQY43peoBEHbBt1WEAQM7oZMWRkBGFUWNFPxVJBLNm7lRJWl9nUlCJwH4IREQUSt8Z/B1IKfH2vrdVh0JhLliJIDp3cSM3LveclQhNriYATCIQUXgLm0/OgWyu3mMeXT6XIbcyqKR7JUJzOfshEBFRSGXEZOCSzEuwZP8SuL1u1eFQGOtKTwTA38vqXGMeW9wtAAC7xd614IiIdBBGSQQ/FY0VjbiVQSUN+jVWdLgdqG2rZSUCERGF3IwhM3Cs7Rg+Kf1EdSgUxrqTRKhz1qGure6M57ASgYiMIHySCFJNY0WXl5UInaXpOOIxMN4xM4aVCEREFFpTMqagf0x/Nliks+pqEiHQXPFs1QjN7maYhAk2k63rARIRhZjaxoo3vxi8GdjOoHtPBJ8LFpMx5/A+cfETStbVoAWbCoVaWVN7EoHbGYiIdJHxxydVh6CMSTPhO4O/g/lb5+Ng/UHkxueqDonCUFeTCNlx2QCAksaSM45wbHI1IcYaAyE63rSRqCsunzVcdQhkYGorEeIy/QfUNlaMMEXoumZPSY9OR3p0uu7rCgjdKkbKm8sBAP1j+mPOnDm6rElE1JdZ+vWDpV/fnU5wU/5NMGtmLN7HagQ6va42VsyIzoBFs5x1QkOzuxkxFm5loNCzJ9pgT2TFC3WN2iTCznf8B9RVIri9bsP2RFhZvBIri1fqvq4Jmm69K8qayhBtiUZ8RDweffRRXdYkIurLGlesQOOKFarDUCYpMglXDLwCy4qWweF2qA6HwlBwxGMnqwVMmgkD7ANQ0lByxnNaXC2wW9lUkUJvf0EV9hdUqQ6DDEptEmHzq/4Dx3siqNjOYNSeCG8VvoW3Ct/SfV2h44jH8uZy9I/pz7I+IiKd1L3xJureeFN1GErNHDITTe4mrDykf6Kewl9XKxEAYGDsQJQ0njmJ0ORuQrQlusuxEXXUzjXl2LmmXHUYZFDh01ix/avuIx69xu2JoIqm44jHj//xMZbcsCSYRBBCQAjBrQ1ERBQyY1PHIj8+X0minsJfV3siAMDAuIE43HQYXt/p30c1u5o53pGIwl4YJREUVSJwOkOn6TXiUUoJ+7V2PLnpyWDlg5QSUkomEYiIKGSEELh18K3YfWw3dh7dqTocCjPdSSLkxObA7XOjoqXitD9vdjdzvCMRhb3wSSK0b2fQ6wp3gMvnMmxPBFWETiMe65x1aPO2ISM6I+RrERERnei6vOsQaY5kNQKdoluVCLEDAeCMWxqaXE1srEhEYS9skgiBQPTaax/g8jKJ0Fma1KcSocZRAwBIiUoBAMyePTvkaxIREQGA3WrH1TlXY2XxSjQ4G1SHQ2GkJ5IIhxoOnfq4UqLFzcaKRBT+zEpXn7EweFPpdgaDJhH+MvUvStbVdKpEqGltTyJE+pMI3MJARBR6/f86X3UIYWPmkJl4Z/87WHZgGe4cfqfqcChMBKYzdCWJkGhLhN1qx6HGQ6f8rNXTCq/0srEi6WL6j0aqDoEMTG0lQnSS/8AJSQRwOkNHJdgSkGBL0H1dvXoiBCsR2pMIREQUeuaEBJgT9H9tCUfDkoZhdPJoLC5crHulJIWv7kxnEEIgOzb7tNsZmt3NAMBKBNJFZIwVkTHG/AxE6qlNIny1yH8AEO2vzT6fvkkEt9dt2EqEpUVLsbRoqe7ratB0maJxtPUoACA5KjnkaxERkV/9kndRv+Rd1WGEjRlDZuBQ4yFsrtysOhQKE93ZzgD4tzScrhKh2eVPIrAnAulhz7oj2LPuiOowyKDUJhG2/ct/QG0lgkUz5ojH94rew3tF7+m+rtBpxGNNaw3sFjsizZEhX4uIiPwa3n0XDe8yiRBwZfaViLXGssEiBfVEEqGypRKtntaT7m9yNwEApzOQLvauP4K965lEoK4Jm8aKgSSC3uWCTq/TsJUIqui1neFo61FWIRARkVI2sw035t+ITw9/GtxmR31bd5MI2XHZAIDDjYdPur/F1QKAlQhEFP7CJ4kQ2M6gY2NFn/TB4/MwidBJejVWPNp6lP0QiIhIuVsH3wqP9GDJ/iWqQ+kVjN4ouTuNFQEgOzYbAE7Z0sBKBCIyivBJIiiYzuD2uQEAEaYI3dbsDfRsrJgcyUoEIiJSKzsuGxf0uwCL9y2G2+tWHY7hPfroo6pD6BFdaawIAAPsAwCcOuYx0BPBbmFjRSIKb2GURPDTM4ng8roAwLA9EVQROlQiSClZiUBERGHjzuF3otpRjZWHVqoOhRQLVCIIIc5x5ulFWaLQP6Y/iuqLTro/MJ2BlQhEFO46lEQQQkwXQhQKIYqEEL89wzkzhBC7hRC7hBD/6tDqt7/tP+AvkQf0bawYSCIYdTvD85c/j+cvf173dU06VCI0u5vR5m1DShSTCEREesp66UVkvfSi6jDCzkX9L0JeXB5e3/U6xz12wZw5cyCECH7wDtw24taG7ox4DBiSMAR7a/eedF+Ty7+dIdoS3fXgiDro2p+PwbU/H6M6DDKocyYRhBAmAM8BuArAcAC3CSGGf+OcQQAeAjBFSjkCwH0dWt0a5T8AQOrfWDGwncGqGTOJEGmOVDK5QEAL+d9TTau/eRW3MxAR6UuLjIQWyak436QJDXeNuAuFdYXYcGSD6nAMZ86cOZBSBt8/BG4bMonQ3shLoGuVCAAwNHEoShpL4HA7gve1uFsQbYnucq8Fos6wWE2wWLueCKO+rSPPUhMBFEkpD0opXQDeBHDDN865B8BzUso6AJBSVndo9U3/8B84/kSsx+jAAKNXIry59028ufdN3dfVoIX87+mo4ygAcDsDEZHOav/1L9T+q2MFhX3NNbnXIDkyGa/vel11KKSQhIQmtC5vZwD8SQQJiX11+4L3NbmaOJmBdPP16jJ8vbpMdRhkUB1JIvQHUHrC92Xt951oMIDBQoi1QogNQojpHVp911L/ATU9EZxeJwDAYjJmT4QPD32IDw99qPu6GgQkZEirEYKVCFHJhrxKQURkVE0frETTB9z3fzpWkxW3D7sdayvWnlKKTh03e/Zs1SF0SyCJ0B1DE4cCAAprC4P3NbubYbeyqSLpo2hLNYq2dOy6L9E3deQZ8HRp1m9+ejQDGARgKoDbALwshIg/5YGEuFcIUSCEKKipOXnWsqZgO4PL569EiNA4naEzhAz9JI2jrccrEXpLF2ciIjK+GUNmIMYSg3/s+IfqUAzL6BcHfPBB62Zv8vTodMRaY7G37ngyqtnVzEoEIjKEjjwDlgHIOuH7TAAVpznnPSmlW0pZDKAQ/qTCSaSUL0kpx0spx6eknFymLhQ0VgyMaTLqdgZVTO3/bEL5d1XjqIHNZOOLKRERhZVYayxuG3obVpWswsGGg6rDIQUkJExa9/aSCyEwLHEY9h47nkRocjdxMgMRGUJHkgibAQwSQuQIIawAvgtg2TfOWQrgMgAQQiTDv72hU6+sKkc8MonQOcGETwj/rt59/l0U3FkATfP/EzVyF2ciIupd7hh+B2xmG175+hXVoZACErJbTRUDhiQOwf76/fD4PAD8jRV58YSIjOCcSQQppQfAzwB8CGAPgMVSyl1CiMeEENe3n/YhgGNCiN0APgPwP1LKY50JRI8Ppt8U2M5g0YzZE0GVQAlfKP+uhn53KO5ccWev6OJMRES9S6ItEbcMugXLDy5HWRMbk3WX0V7bJWS3xjsGDE0cCqfXiZLGEgDtjRVZiUBEBmDuyElSyhUAVnzjvkdOuC0BPNB+dNys5cGbeuyz/yajVyIsmL5Aybp6JBFqWmuQH58fsscnIqLTG/jPhapDMIQfjPhBcErSryb8SnU4hvboo48aKpEgIYOVkt0RaK64p3YP8uLz0Oxqht3Cxoqkj5seHKs6BDKwsBlEGygK07WxYiCJoBkziaCKHkmEo46jwfGORu/iTEREvU9adBouG3AZlh5YGpz2RH2DhOx2Y0UAyI7LhlWzorC2EC6vCy6fi5UIRGQIapMIa//qP6B2O4NRKxFe2/kaXtv5mu7rhvrvqs3ThiZ3E1Ki/EkEI12dICIyumOvvIpjr7yqOgxDmDlkJhqcDfjo0EeqQzGcOXPmBPsdAcbqfdQTIx4B/3baQQmDsOfYHjS5mgAA0Zbobj8uUUd89dFhfPXRYdVhkEGpTSLs+9B/gI0Vu2JN2RqsKVuj+7paiJMINa3+8Z/JkckheXwiIjqz5tWr0bx6teowDGFi+kRkx2ZjceFi1aEYzpw5c4L9jgBj9T7ywdcjSQQAmNRvEgqqCoKTPuxWbmcgfRz6+igOfX1UdRhkUGGznUGTZaXiIwAAIABJREFU+o94NHoSQZVQb2c42up/QgtsZyAiIgpHQgjcOvhWbKvZhsLaQtXhkE56qhIBAGYMmQEJiVd2+id9cDoDERlB2CQRVGxncPvcANgTobNCXYkQSCKwEoGIiMLdDfk3wKpZ8fa+t1WHYlhG633UU9MZAKB/TH9cmnkp1pavBcBKBCIyhjBKIvgpaazISoROCXUlQm1rLQD/CC0iIqJwFhcRh+k50/GfA/9Bi7tFdTiGZIQtDCeSkMFeDj3he8O+F7zNSgQiMgK1SQSLzX/g+IhHr/TqtnygsaJFs+i2Zk+KMEcgwhyh+7oixEmEOmcdACDeFh+SxyciojMTNhuEzaY6DEOZMWQGHB4Hlh9cfu6TyfB6shIBACalT0JeXB4AJhFIP2arBrM1bK4nk8GYla5+xzvBmyq2Mzi9Tlg0S49mk/X0wuUvKFk3uJ0hRP0r6trqYLfaDZvcISIysgH/eEl1CIYzOnk0hiQMweLCxbh18K2GfV9BHdOTPREAf2+N/xr1X/jj5j8iKTKpxx6X6Gyu+/l5qkMgAwub9FMgiSCh33YGt9eNCJP+V/KNTpMhrkRoq0NCREJIHpuIiKinCSEwY8gMFNYV4uujX6sOh0KsJ6czBFyXdx3WzFyDKEtUjz4uEVEoqE0irPmj/4C6EY9G7ofwwvYX8MJ2/asRQt0Toc5ZhwQbkwhERCrUPP88ap5/XnUYhnNN7jWIMkfhrcK3VIdCIdbTlQgBoXhMojPZvLwYm5cXqw6DDErts9XBNf4Dx3si6JlEaPO2wWYy7r7PjUc2YuORjbqvG+rpDHVtTCIQEaniWL8BjvUbVIdhONGWaFyXdx0+PPQhGpwNqsOhEApVEoFIT2V761C2t051GGRQYfMMqKISodXTCpvZuEkEVULdv4LbGYiIyIhuHXwrnF4nlh1YpjoUCqGebqxIRGQ0YZREUFCJ4GljEqELQrmdQUqJWmctKxGIiMhwhiQOwZiUMVhcuFjXkdWkr54e8UhEZDThk0Ro386g54tuq6cVkeZI3dbrLUKZRGhxt8Dj8yDRlnjGc4w2T5qIiPqOmUNm4lDjIWyu3Kw6FAoRH3ysRCCiPk1tEiEqwX/g+HYGr/TqtrzRKxHiI+IRHxGv+7qh7IlQ1+bfm3W2P9ejjz7a4+sSEZGfKT4epnj9X1t6i29nfxtxEXFssNjLsScCGZ0txgJbDMepU9eYla4+8/+CN1WMeGzztiHdlK7bej3t6cueVrKuCGElQq2zFgC4nYGISJHMZ/+qOgRDizBF4Ma8G7FozyJUO6qRGpWqOiTqYaEY8Uikt6t+NEp1CGRgYfMMGOqO/6fD7QxdYwphEqG+rR4ATtnOMGfOHAghgnsQA7e5tYGIiMLNzKEzISGxYOcC1aFQCHA6AxH1dWqfAT+e4z8AiPYCBE5n6LhntjyDZ7Y8o/u6wSaYCEElQpu/EuGb2xnmzJkDKWWwZ0bgNpMIREQ9q/qpv6D6qb+oDsPQsuxZuC7vOiwuXIxqR7XqcKiHMYlAvcH6dw9g/bsHVIdBBqX2GbB0s//ACdsZdGys2OZpM3Qlwvaa7dhes133dUPaE8Hp74lwtsaKREQUOq3btqF12zbVYRjevaPvhU/68OrOV1WHQj2MSQTqDSoPNqDyYIPqMMigwuYZMJBE0KuxopTS8JUIqoRyOkN9Wz0iTBFnTe7Mnj27x9clIiLqSVn2LFyffz3eLnwbVS1VZz33oS8ewsrilTpFRt0lITmdgYj6tPBJIrQXIOjVWNHlc0FCGroSQRURwkqE2rZaxEfEn3X+MrcwEBGREdwz6h74pA8Ldy884zkt7ha8f/B9TnMwEB98Z32fQkTU24VPEkHnxoptnjYAYBKhC0wydJUIdc46bmUgIqJeIdOeicsHXo53978Lh9tx2nNKGksAANtqtp3xHAo/rEQgor5MbRIhNsN/QP8kQqunFQBgMxl3O0NadBrSotN0Xzcw4jEUW0/q2+o53pGISCFzejrM6cYdfxxubh92O5rcTVh2YNlpf36o4RAAwOPzoKCqQMfIqKt88AW3dhIZVUxCBGISIlSHQQZlVrr6Lf8I3gwUhemeRDBwT4R5F89Tsm7ghTMUTTBr22qRac/s8cclIqKO6f+nP6oOoVcZkzIGI5JGYNGeRZgxZMYpDflKGksgIGDRLNhwZAMuybxEUaTUUWysSL3BFXePUB0CGVjYPAMKCEByO4MRhHo6A7czEBFRbyGEwO3DbsehxkNYV7HulJ8XNxYjIyYD56edj/UV6xVESJ0lIWHSuJ2BiPoutUmED37rP9oJCFYidMKTm57Ek5ue1H3dUE1ncHldaHG3cDsDEZFClXPnonLuXNVh9CrTs6cjOTIZr+96/ZSflTSWYGDsQFzY70IU1RfhaOv/b+/Ow6Oszr+Bf88s2cjGFpKwBWXf94CggEsVtEKtIIqWgmtbW8S2gEvNRPBXsa9aqdVqVURxbauVKqB1o6jsENkRlLBlJyH7TDKZ8/7xzAxJyDIzmZkzz+T7ua65ZsmTOTce58nMPefcd5GCCMkbEtK9DZdIrza/+x02v/ud6jBIp9QmEfL2aRcngeB1ZwiHlQiHiw/jcPHhoI8bqPoVJdYSAEBiZKJfn5eIiDxnO3QYtkPB/9sSzsxGM342+GfYmrsVu/J3uR+XUiK7NBtp8WmYkDoBALgaQQfY4pHCQdGpChSdqlAdBulUyGxnALQPp4Eo1teU6jptJYKekwiqBGo7Q4lNSyJwOwMREYWbuQPnonNUZ/xlz1/cNYWKqotQZa9C7/jeGNRpEBIiE7A1d6viSKk1bPFIRO1dSCURDFIEpFhfU8KhO4Mq7u0M8G8SodhaDADczkBERGEn2hSNO4ffiV35u9yJguyybABAWkIaDMKA9OR0bM3ZGrT3QuQbrkQgovYupJIIAsEvrKjnmgiqGALU4vGc9RwAoGMkkwhERBR+ZvefjeQOyXh2z7PaVgZXEiE+DQAwMXUiCqoLcLz0uLogqVXszkBE7Z3aM2Dni7WLUzALK4ZDTYTe8b3RO7530McNVItH13YGrkQgIlInIi0NEWlpqsMISxHGCNw57E7sLdqLbXnbcKL0BCKNkUjukAwAmJDirIuQy7oIoYxJBAoHid1ikNgtRnUYpFMmpaNfv6rBXRHEFo+u7Qx6TiJYLrEoGddVWNHfKxGKrcUwCAPiI+L9+rxEROS5lOWPqg4hrM3sOxPPf/s8Xtn3CiKMEegV38v9gbRHXA/0jOuJLTlbMG/QPMWRUnOYRKBwMO3WgapDIB0LqTNgsFs8GoURZoM5KOOFE2OAViKcs55DQkQCey8TEVHYijRG4tZBt2JL7hbszN/p3srgMiFlAnbk7UCto1ZNgNQqJhGIqL1TewZc9xvt4iQggtbisdpejShTlK6r61q+scDyjSXo4waqxeM52zkkRCb49TmJiMg7uX94BLl/eER1GGFtzoA5iDXHorK28oIkwsTUiaiyV2Ff4b6mf5mUc8DBJALp3hdrD+OLtWznS75RewY8+712cQpqTYQ6q+47M5woO4ETZSeCPq5BBiaJUFpTisTIRL8+JxEReacmOxs12dmqwwhrcRFxmDNgDgCtM0N945PHQ0Cw1WMIY3cGCgfn8qtwLr9KdRikUyGVRg1mTQSr3arreggquVs8+nmuymxlXIlARETtwvwh8zGjzwxcknpJg8cTIhMwpPMQbMlhccVQxe0MRNTehdQZMNg1Edje0TciQC0eS22lTCIQEVG70CmqE1ZethJdortc8LMJqROwr2gfKmoqFERGrWESgYjau5A6Awa7xSNXIvgmUC0eS2tK2ZmBiIjavfHJ41En65BVmKU6FGoCkwhE1N6pbfGYPKzBXSER1MKKek8iDOykpjWLIQAtHmvralFZW8mVCEREikUOYtsv1UZ0HQGjMGJX/i5M7j5ZdTjUCJMIFA669IxVHQLpmNokwvTHG9wVEH5fIt+cans1usZ0DcpYgbJ0/FIl47pXIvgx4VNaUwoATCIQESmW/OCDqkNo92LMMRjceTB25+9WHQo1gYUVKRxcOqe/6hBIx0IqjSoQxMKKYdCdQRVDAFo8ltnKAIDdGYiIiACMThqNfUX7YKuzqQ6FGnHAoesW4UREbaU2ifCvO7WLkwHC7/vsmxMONRGWbV6GZZuXBX3cQHRncK9EiOBKBCIilc78fgnO/H6J6jDavTHdxqDWUYt9hftUh0L1SEhAgCsRSPf++8oB/PeVA6rDIJ1Su52hLKfBXSHZncEb+ZX5SsYVAViJUGrjdgYiolBgz8tTHQIBGN1tNABgV/4ujE0eqzgacnFt5eRKBNK7ihKuciLfhdx2hmAVVrTarYgxxQRlrHBjCECLR1cSIT6S3RmIiIgSIhPQN7EvdhewLkIocb1P5UoEImrPQiyJIFDnCHxhRYd0aDURdL4SQZVAtHg8ZzsHgCsRiIiIXMZ0G4OsgizYHXbVoZCTK4nA7gxE1J55dAYUQlwjhDgihDgmhGh2E74Q4kYhhBRC+LTuTkgBBwK/ncFqtwIAkwg+CkSLx1JbKYzCiDhznN+ek4iISM/GdBuDKnsVjhQfUR0KOTGJQETkQU0EIYQRwF8BXAXgNIAdQoh1UsqDjY6LA/AbANs8Hr3nuIZjwb/fbjfHWqclEfReWHFE1xFKxg3ESoSymjLER8RzjyERkWLRI0eqDoGcxnQbAwDYnrcdQ7oMURwNAXB/2cXtDKR3yRdx9S/5zpPCiuMBHJNS/gAAQoi3AcwEcLDRccsBPAHgdx6PfqWlwV2B4BRWdK9E0HmLx/vG3KdkXBGglQjcykBEpF7Sb+9XHQI5JcUkoU9CH2zL24YFQxeoDodQr7Ai+KUH6dvEn1ysOgTSMU/WYnUHcKre/dPOx9yEEKMA9JRSftiWYIKVRKi2VwPQ/0oElYzC6PfuDCyqSERE1FB6cjp25+9GbV2t6lAI9QorGrgSgYjaL0+SCE2lWt3r2IUQBgBPA/htq08kxF1CiJ1CiJ2FhYXAO7dqF1cw0r9tA5vjWolgk2bM2n0UBTZ9/mFe/MViLP5isZKxhRB+7aRxznYOCRFciUBEpNrpX/8Gp3/9G9VhkNOElAmotldjb9Fe1aEQWBOBwseGF/Zhwwv7VIdBOuXJGfA0gJ717vcAkFPvfhyAoQC+FEJkA5gAYF1TxRWllC9KKcdKKcd27doVqCrRLm7BKazoWomwrqgK20or8WS2Pntin7Odc3c1CDajMPp1O0NZTRm3MxARhYC6c+dQd07N3xa60NjksTAIA7blel5yigLHnUQIrQZnRF6zVtTCWqHPL1JJPU/OgDsA9BNC9BFCRACYC2Cd64dSylIpZRcpZZqUMg3AVgDXSyl3ehuMgAhKYcWb9mjlHD4utkECWJNzFslfZKH3pm8DPna4MAiDX+eq1FaKxMhEvz0fERFROEiITMCgToOYRAgRri+7DAYmEYio/Wr1DCiltAO4F8DHAA4BeFdKeUAI8agQ4np/BiPg32J9zXm8XxIAINJZWDHaIHBDUiJ2TBgc8LHDhYDw21zVOmpRUVvBmghERERNSE9Jx97CvaiqrVIdCjlxJQIRtWcenQGllOullP2llBdLKR9zPvaIlHJdE8dO9WUVAgAIGZyVCFFCW7pjlWZEGgSsDok4kxFJkeaAjx0ujMLot7kqrykHANZEICIiakJ6Sjrs0o7dBbtVh9LuuVcisCYCEbVjas+AF03RLk6GIHdnmJ2SjPVj+mN+amcU1NgDPq6/paekIz0lXcnYQvhvJUKprRQAWBOBiCgExEycgJiJE1SHQfWMShoFs8GMrTlbVYfSJhaLRXUIbcbuDBQuegzsiB4DO6oOg3TKpHT0KUsa3BVAUAsrrhhwMWIjovH4gJ6t/EZoumfEPcrG9meLx7YkESwWS1i8KSEiChVdf/lL1SFQI9GmaIxOGo2vc77G7/A71eH4LDMzU/d/s11JBNFk8zIi/Rh3bR/VIZCOhdRaLCEFHI7gtXiMMkUFfKxwJYT/tp64kwg+bGfIzMz0SwxERESh7NIel+LYuWPIq9RnR6lw4V6JILgSgYjaL7VJhLU/1S5OwVyJYDaYYTKoXYjRVvd8eg/u+VTNagR/tngsrdGSCOzOQESk3sk778LJO+9SHQY1Mrn7ZADA5jObFUfiHYvFAiEEhNC+uXfd1uuKBHeLR9ZEIJ37z1+y8J+/ZKkOg3RK7Rmw1qpdnILV4tFaZw2LVQg2uw02u03J2EII9x/StnKtRKjfnaGlNxfh9oaEiCiUSKsV0mpt/UAKqosSLkJqh1RsPq2/JIKU0v3+znVbr3+zWViRwoW9xgF7TeC/vKXwFFJnQBHEworRxuiAjxPODMKAOof/CisKCMRFxLkfa2mbQri9ISEiImqNEAKX9rgU23K3oaauRnU47R63MxBRexZaSQQJvy2Rb0m1vRrRZiYR2sIojH5diRAfGc+sPhERUQsmd5+MKnuVbls9ZmRkqA6hzVwrEVyrIYmI2qOQ+tQWtO0MdiuijOe3M+TbajFr91EU2GoDPna4EPBvi8eEiASftimEwxsSIiIiT4xPHg+zway7LQ0u4bBikIUViYhUt3jsf3WDuwaIoBVWrF8T4ansPGwrrcST2XlYqaN2j1N6TFE2ttHgxxaPNaVIjExs0K7R0+4P4fCGhIgolMROnao6BGpGjDkG45LH4X+n/4ffj/u96nDaJXeLR65EIJ1LG9ZFdQikY2qTCJN+0/C+DN5KhGhTNHpv+hY2x/nx1uScxZqcs4g0CJyYMiLgcbTVz4f+XNnY/qxfUWorRceojn55LiIiapvOty9UHQK1YFrPaXhs22M4WnIU/Tr2Ux1Ou8OVCBQuRv2ol+oQSMdCbDsDglJY0dWdYfuEwbghKRHRBi2bHG0QuCEpETsmDA54DHpnFH5ciWArRXxEfIPHuE2BiIjoQlf1vgpGYcSG4xtUh9IuscUjEZHqJMLqa7WLkz/32bfE1Z2hW6QZsSYjrA6JSIOA1SERZzIiKdIc8Bj8YcHGBViwcYGSsT3dbuCJYmsxOkV1avAYtykQEalx4raf4cRtP1MdBjWjc3RnpKekY8PxDUFZvUkNscUjhYv3n9yN95/UZ5FWUi+kzoCGIBVWrKipQIeIDgCAwho75qd2xvox/TE/tTMKauwBHz8cGIXRLwmfans1quxV6Bzd2Q9RERERhb9r0q7B6YrTOHD2gOpQ2h2uRCAiUl0ToREhEZTCihW1FYiLiAMArB7Wx/344zoqqqiaQRj8kvApsZYAwAUrEYiIiKhpV/S+Asu3LseG4xswtMtQ1eG0K0wiEBGF2EoEfxbra05NXQ1sdTbEmeMCOk64E8I/W0+KrcUAmEQgIiLyVHxEPCZ3n4yN2RuDUkuKzmNhRSKikEsiBL6wYnlNOQC4VyKQb4zC6JdVI0wiEBEReW96n+koqCrA7nzuaQ4mrkQgIlK9nWHIrAZ3hQz8SgRXEiE2Ijag4wTD1WlXKxtbCAGHo+1zdbb6LAAmEYiIQkXc9GtUh0AemNJjCqJN0diYvRFjk8eqDqfdYBKBwkXfMUmqQyAdU5tEGH9ng7siCIUVXUmExi0F9WjuwLnKxuZKBCKi8NTplltUh0AeiDHHYEqPKfgk+xMsHb8UZoM+OkvpHbszULgYNrWH6hBIx9SeAWuqtIuTQOALK5bXOlcimPW/EqHaXo1qe7WSsf2V8Cm2FiPaFI0Yc4wfoiIiorZyVFfDUa3mbwt5Z3qf6SixlWB77nbVobQbXIlA4aK2pg61NW2vb0btk9oz4BuztYuTgH+WyLcknGoi/PLTX+KXn/5SydhGg39aPBZbi7kKgYgohJy6626cuutu1WGQByZ3n4w4cxw2HN+gOpR2g0kEChcf/uVbfPiXb1WHQToVUmdAIUXAVyJU1FQACI8kgkoG+KfFY7G1GJ2jOvshIiIiovYlwhiBy3tdjs9OfgZbnU11OO0CuzMQEYVYEsEAdmfQC3+1eDxbfZYrEYiIiHw0o88MVNRWYPPpzapDaRdcSQQhhOJIiIjUCakkQlAKK9aWwyAMiDFxD35bGIXRbysROkUziUBEROSL8SnjkdohFX/79m+oc+h/f7PFYlEdQotcK2a5EoGI2rOQSyL449vtlpTXlCPWHMsMchv5YyWCQzpQYi3hSgQiIiIfmQwmLB6zGEdKjmDd9+tUh9NmmZmZqkPwiCG03kITEQWV2haPIxu1kZII+EqEipqKsNnKMLPvTGVjG4WxzVtPymvKYZd2JhGIiEJIwk9+ojoE8tLVaVdj7aG1WLVnFX6U9iN0MHdQHVLYcrd4NDCJQPo2cGKK6hBIx9SeAUfN0y5OBgS+sGJ5TXnYJBFm9Z2FWX1nKRlbCNHmJMJZ61kAYBKBiCiEJN7wEyTewESCngghsGTcEhRVF+GV/a+oDsdrFosFQgj3KlHX7VDc2sDCihQuBl2SgkGXMJFAvlGbRKg8q12cBNr+wbQ1ZTVlYZNEKLGWoMRaomRsozC2OeFTXF0MAOgcze4MREShwl5SAnuJmr8t5LvhXYfjqt5X4Z0j7+iuU4PFYoGU0r0a1XU7lJMIAtwWS/pWXVGD6ooa1WGQTqlNIrz7M+3iJJw7GQKZSKiorUCsOTZgzx9M9395P+7/8n4lYxtE21s8Flu1JAJXIhARhY4zv1mEM79ZpDoM8sFNA25Cqa0Un2R/ojqUsOVeiWDgSgTSt40v7MfGF/arDoN0KqQ2dLmyuoFMIoTTdgaVDMLQ5sKKTCIQERH5z/jk8egV1wv//O6fqkPxWUZGhuoQWsSVCEREIZpECGRxxXAqrKiSAf5ZiSAgkBiZ6KeoiIiI2i8hBG7sfyN2F+zGsZJjqsPxSShuYaiPLR6JiEIuiaAJVHFFh3SgopZJBH/wR4vHs9VnkRiZCJNBbZMQIiKicDGz70yYDWb886h+VyPoAbszEFF7FlJnQCG1NEKdo20fTptTWVsJCRk2NRFU8keLx2JrMbcyEBER+VGnqE64steVWPf9OlTVVqkOJ+y4WzyG1ltoIqKgUvsV8LiFDe66ViK49pv5W3lNOQAgPiI+IM8fbDcNuEnZ2P5o8VhsLUanaCYRiIhCSceb56oOgdrolkG3YEP2Brx39D3cOvhW1eGEFdd7VINgEoH0beiU7qpDIB1Tm0QY+tMGdw0BLqzoSiLERoTHSoRr+lyjbGx/rUQY0GmAnyIiIiJ/iJ8xQ3UI1EYjk0ZidNJovHbwNdw08CaYDWbVIYUNJhEoXPQb2011CKRjas+Apae1i5NrO0OgkwjhUhMhrzIPeZV5Ssb2R4vHs9az6BzV2U8RERGRP9Tm5qI2N1d1GNRGC4cuRG5lLjYe36g6lLDibvHIwoqkc+XFVpQXW1WHQTqlNonw3t3axcm9nSFA3RnCLYnwwOYH8MDmB5SM3dYWj7V1tSivKWdNBCKiEJOzZClylixVHQa10aU9LkXfxL5YfWB1QLte+aw8D1g9HSjPVx2JV7gSgcLFp6sP4tPVB1WHQToVUmdAV4vHtlb9b05FbQUAIM4cHkkElQzC0KbaFcXWYgBgTQQiIqIAMAgDFgxdgKMlR/HlqS9Vh3OhTU8AJ7cCm1aqjsQrDuEsrMgkAhG1YyF1BnQlEQJVWLGspgxA+KxEUEmgbS0eC6sLAQBdorr4KyQiIiKqZ3qf6egd3xur9qwKWOcrr61IAiwJwM6XAenQri0J2uM6ICEBqRWYJiJqr0IriRDgmggVNdpKhHAprKiS0dC2woquWg7JHZL9FRIRERHVYzaY8etRv8axc8fw4Q8fqg5Hs2gvMHQ2YIrW7puigWGzgUX71MblIQnp/tKLiKi9Cq0kgvM6kIUVo03RDaoUWyyWgIwV7gTa1uKRSQQiIqLA+1HvH2FI5yF4NutZ2OpsqsMB4pKByDigzgaYorTryHggTh+V4iUkDKH19pmIKOjUngUvuVe7OIlAt3isLUesueEqhMzMzICMFQzzh8zH/CHzlYzd1haPeZV5iDRGIjEy0Y9RERFRW3VasACdFixQHQb5iRACi8csRl5lHl478JrqcDSVBcCYBcAdn2rXFfoprsiVCBQuRl7VCyOv6qU6DNIpk9LRB0xvcDcYKxHCqR7C1J5TlY1tEIY2zVN+VT66xXTzeE9hQZkV9761B8/eMgpJcVE+j0tERC2Lu3ya6hDIz9JT0nFV76vwXNZzGJ8yHiO6jlAb0Nw3zt++7il1cfiASQQKF32Gsy4Z+U7tSoSio9rFyVUTIZAtHmMjYmGxWCCEcH+Add3W29aG46XHcbz0uJKx25pEyKvM82orw6rPjmJHdjFWfXq09YOJiMhnth+Ow/aDmr8tFDiWSyzo1qEbfr/p9yi1laoOR7cccDCJQGGhJK8SJXmVqsMgnVKbRPjPfdrFyb2dAYErrBgXEQeLxQIppTtZ4bqttyTCo1sexaNbHlUytkEY2pTsyavyLIkw4OENSFv2EdZuOwkpgbXbTiJt2UcY8PAGn8cmIqLm5WVkIC8jQ3UY5GfxEfH402V/QmF1IZZtXoZaR63qkHSJKxEoXHz5xhF8+cYR1WGQTqndztCI65TcltaBLSmvLUePuB4Bee72xiAMPs9TnaMOhVWF6BbTehGlzUumYcX6Q/jkQB6stQ5EmQ24ekgyHrp2kE9jExERtVfDug7DA+MfwPKty/Hg5gfx+KWPw2gwXnCcQzrwXcl32Fe0Dz+c+wESEkZhxMBOAzGp+yR0iuqkIPrQwCQCEVGoJRGCsJ2hcU2EDOe3LRaLRXcrEVQyCAMktBUc3vZKLqwuRJ2s82glQlJ8FOIiTbDZHYg0GWCzOxAXabqgLgLnj4iIqHVzBsxBRW0Fnt71NMwGM5aOX4qEyAT3z7NLs/HgVw9iX5HWcjHaFA2TMKHGUQNbnQ1O6FEVAAAgAElEQVQCAqOSRmFW31m4Ou1qxJhjvAugPA/45wLgxld105GhPnZnICLyMIkghLgGwDMAjABeklI+3ujn9wO4A4AdQCGAhVLKE94GYwhgdwYpZZNJBNcHz8zMTH4I9YIrceCQDhjFhd9itCS/SqvC7GlNhKIKG+al98Yt43vhze0nUVhuveAYzh8REZFnFg5dCJvdhue+fQ6fnPgEV6ddjeQOyaiqrcI/v/snIowR+MOEP2BCygT0jOsJIbS2zgfPHsTm05ux/vh6PPLNI1i5YyXm9J+DeYPmoVsHDxMCm54ATm4FNq3UXVFFgCsRiIgAD5IIQggjgL8CuArAaQA7hBDrpJQH6x22B8BYKWWVEOIXAJ4AcJO3wQSyO4OtzoZaR21YdWdQyZU4cMABI7xLIuRV5gGAR9sZCsqsKKmqxfJZQ5EUF4UVs4Z6HywRERE18IuRv8DlvS7HO0fewUc/fIRqezUMwoCJqRNhmWi5IClgEAYM7TIUQ7sMxT0j7kFWYRbeOvQW1hxcg9cPvY7rLroOPx/yc1yceHHTA65IAuy28/d3vqxdTJHAwwUB/Jf6FwsrEhF5VlhxPIBjUsofpJQ1AN4GMLP+AVLKL6SUVc67WwF4Vnjgst9pFycRwJUIFbUVAIA48/kkgt67NNw1/C7cNfwuJWMbhPa/ji9z5UoieLISoaWuDHqfPyKiUNTlF/egyy/uUR0GBcGATgPwyMRHsG3eNuydvxdZP8vC81c+3+qqAiG0LQ1PTHkCH/3kI8zpPwcbj2/ErA9m4Q9f/wHlNeUX/tKivcDQ2YApWrtvigaGzQZu/wxYPR0ozw/AvzAwmESgcDB2RhrGzkhTHQbplCfbGboDOFXv/mkA6S0cfzuAJkvnCyHuAnAXAPTq1Qu4uGEvaldNhEB0Zyi2FgNAg31/9ffRCyECVoshUCamTlQ2dluTCNGmaMRHxDd7zICHN8BmP//ca7edxNptJxFpMuDIiukA9D9/REShqMMll6gOgXSkR1wPPJD+AO4ZcQ9ePfAqXj3wKrblbsNjkx/DuORx5w+MSwYi44A6G2CK0q4j44Fdq3W1vYErEShc9BzUfgukUtt5shKhqTNlk5/WhBC3AhgL4E9N/VxK+aKUcqyUcmzXrl2B3L3apdFAgfgw6M2333pxuPgwDhcfVjK2q6iQL3OVX5WP5A7JLRZk3LxkGq4fmYooszZOlNmAmSNTsXnptGZ/h4iI2s566BCshw6pDoN0pmNURywesxivTX8NEcYI3PHJHXj+2+dR56jXyamyABizALjjUwDi/JYG6dCuLQnatocQxpoIFC4KT5Wj8FQTq4aIPOBJEuE0gJ717vcAkNP4ICHElQAeAnC9lNLW+OdN2viAdnE9RwC3M+RW5AIAUmNTm/x5hg57Yq/cvhIrt69UMrZrJYIvbR7zKvNarYfgaVcGFz3OHxFRKMr/vz8i///+qDoM0qkRXUfg3evexYw+M/Bc1nMYN38cbHXOt4Vz39BWGyQPA+4/2PT2hkX71AXvASYRKFx89e5RfPXuhduFiTzhSRJhB4B+Qog+QogIAHMBrKt/gBBiFIAXoCUQfK6O497OEIAkQk5lDkwGE7pEd2ny59xH7522bGfIr8z3aEWIqyvD+7+chHnpvVFY0XxuivNHREQUGmLMMfi/yf+HRyY+gj1r92DV7lUXHtTc9oYQb/vIFo9ERB7URJBS2oUQ9wL4GFqLx1eklAeEEI8C2CmlXAdt+0IsgH84l6iflFJe720wgezOkFuZi+SYZPeHX2qb+i0evVHrqEVhdaFHSYQXbhvrvs2uDERERPohhMDs/rMBAK8dfA2X9bgM6SmNSmq5tjeMXQDsXA1UhH5xRa5EICLybCUCpJTrpZT9pZQXSykfcz72iDOBACnllVLKblLKkc6L1wkEIPDbGVJiU/z+vO2Vu8Wjl3NVWFUICYnkGN9qUxSUWTHnhS0oKLf69PtEREQUWI27J+3/+X5MSJ2AZQ8va3hg/e0N1z2l3Q9xLKxIRORhEiFYXCdl2XTdxjbJrcxFSgcmEfzF1+0MbS1w2VLLRyIiIlLPYrFASukuvryvcB9GrBmB2OtiFUfmH0wiEFF750mLx8C54pEGd4Uzd+DvlQiuJfThlkRYNHqRsrGDnUTwpOUjERG1XdfFi1WHQGFmaJehuGnATXj7yNv4ab+fYlDnQapD8hlXIlC4mDDrYtUhkI6pXYnQK127OLlOyr5U/G9JQVUBHNIRdkmEkUkjMTJppJKxfU0inCo/BQBezwVbPhIRBUfM6FGIGT1KdRgUJlzdk3416ldIjEzEH7f/MSCtvIOFNREoXKRcnICUixNUh0E6pTaJcHKbdnEyuLYz+PmPS06F1pEy3GoiZBVkIasgS8nY7iQCvEsiHD13FD1ieyDGHOPV73nb8pGIiHxTtXsPqnbvUR0GhQlX96T4iHgsGr0Iewr24MMfPlQbVBswiUDhIvf7UuR+X6o6DNIptUmEzx7VLk6B2s7gWkIfbisRntn9DJ7Z/YySsX1difBdyXfo37G/T2N60/KRiIh8U/j00yh8+mnVYVAYmtV3FgZ3Hoxn9zyL2rpa1eH4hC0eKVxs/ff32Prv71WHQToVUmfBQBVWzK3MBeBZEiHfVotZu4+iwKbPP27B4ksSwWq34kTZCfTv5FsS4YXbxmLFrKEYnBqPFbOGNmgBSURERKHNIAz41chfIacyBx98/4HqcHzClQhERCGaRKhz+LcmQk5FDjpFdUKUqfWl709l52FbaSWezM7zawzhxpWF9yaJ8H3p93BIh88rEYiIiEjfLu1+KYZ3GY4X976oy9UILKxIRBRySQSNt/vsW5NXmdfqKoTem75F8hdZWJNzFhLAmpyzSP4iC703fevXWMKFLysRjpZobRn7JfYLSExEREQU2oQQ+MXIXyC3MhfvH3tfdThe40oEIqJQSyLIABVWrMxpNYmwfcJg3JCUiGiDFkO0QeCGpETsmDDYr7GEC1+SCN+VfIcoYxR6xvUMVFhEREQU4ialTsLwrtpqBKvdqjocrzCJQEQEmJSOfs0fG9x1nZT9WVhRSom8yjxMSp3U4nHdIs2INRlhdUhEGgSsDok4kxFJkWa/xeJvS8cvVTa2r0mEvol9YTQYmz3GYrG4KzkTEVHwdXvwAdUhUJgTQuC+0fdh4ccL8frB13Hn8DtVh+QxJhEoXEyew5XB5Du1KxFShmsXJ/d2Bj8mEc7ZzqHaXo3U2NRWjy2ssWN+amesH9Mf81M7o6DG7rc4AmFgp4EY2GmgkrG9TSJIKfFd8XetFlXMzMxsc2xEROS7qEGDEDVokOowKMyNSx6Hy3tejpf2vYSi6iLV4XiM3RkoXHTtGYeuPeNUh0E6pfYs+P0X2sUpECsRvOnMsHpYHzw+oCeGxEbj8QE9sXpYH7/FEQhbcrZgS84WJWO7kwge1q84az2LElsJiyoSEYW4ym++QeU336gOg9qB+8fej5q6Gvw166+qQ/EYVyJQuDh1qBinDhWrDoN0Sm0S4X//T7s4GZw1EfxZWDG3wplEiG09iaA3L+59ES/ufVHJ2O4kgsOzufqu+DsAaDKJYLFYIISAENr8u25zWwMRUfAVPf83FD3/N9VhULgpzwNWTwfK890P9Y7vjbkD5+K9o+/hWMkxhcF5jt0ZKFzsXJ+NneuzVYdBOhWS67H8WVjxRPkJAED3Dt399pzk/UqEo+ea78xgsVggpXTPu+t2a0mEgjIr5rywBQXl+irKRERE1O5segI4uRXYtLLBw3cPvxsxphg8m/WsosC8w5UIREQhlkRwnZTrZJ3fnnN/0X50j+2OxKhEvz0nwb0f0NOtJ0eKjyApOsmjefA0ObDqs6PYkV2MVZ8e9SgGIiIiCrIVSYAlAdj5MiAd2rUlQXscQGJUIuYPmY/PTn6G/UX7FQfbOiYRiIhCLYngXIDgz5UIB4oOYGiXoX57PtIYDN4lEXYX7PZoHjIyMlpNDgx4eAPSln2EtdtOQkpg7baTSFv2EQY8vMHzfwAREREF3qK9wNDZgClau2+KBobNBhbtcx9y2+Db0CmqE57Z/YyiID3HwopERCGWRDD4ubBiUXURcipzMKzLML88H53nzUqEU+WncKbiDNJT0ls8bsDDG/CqdVyryYHNS6bh+pGpiDJrMUSZDZg5MhWbl07z8V9DREREARGXDETGAXU2wBSlXUfGA3Hd3Id0MHfAHcPuwNbcrdiWu01hsK2T8N8XXUREemVSOvqP/9zgrrs7g58KKx4oOgAAGNJ5iF+eL9Q8MvERZWN70+LR9YZgQuqEFo/bvGQaVqw/hE8O5MFa60CU2YCrhyTjoWsbthpLio9CXKQJNrsDkSYDbHYH4iJNSIqL8vFfQ0RELslstUv+VlkAjFkAjF0A7FwNVORfcMicAXPw6oFX8VzWcxifPN5dbDnUcCUChYup8waoDoF0TG0SoUvDInuuPxeeVvxvzf6z+2EQBgzuPNgvzxdq+iSoa0HpSiJ4Ur9ia+5WJEUnoU98y/F6kxwoqrBhXnpv3DK+F97cfhKFLK5IROQXkReFdntj0qG5b5y/fd1TTR4SaYzEwqEL8fj2x7EzfyfGJY8LUnDeYU0EChcdkzuoDoF0TG0S4YhzmfqA6QAA4ecWj/uK9uHixIsRY47xy/OFmi9PfQkAmNpzatDHdiURWqtf4ZAObM/djsndJ3v0rYKnyYEXbhvrvr1iFmteEBH5S/nnXwAA4i7nFjEKrhtTLsXL8gk8v+sZjLt2repwmsQWjxQuju8tAgD0Gd5FcSSkR2qTCN842/m4kgh+rIkgpcSBogOY1jN83wStObAGgNokQmtzdbTkKEpsJa3WQ3BhcoCISK3i1asBMIlAwRf51Z+xoKQYT4hvsTNvJ8Ymj239l4KMKxEoXGT99yQAJhHINyG1qct1UvZHd4bTFadxznbOp84MFoulzeOHO6MwAmg9ibA1dysAeJxEaCvOHRERkc7UawN5Y1k5Otvr8OK/57rbQIYSJhGIiEItieDMHfhjJYKrqKIvSYRMFpVqlWtrgidJhLT4NCR3SA5GWJw7IiIivanXBjJaSsyrsOLfG8tw5OcfqI7sAiysSEQUakkEP25n2Fe0DxGGCPTr2K/1g8lrnmxnqKytxK78XUFbhUBEREQ61KgN5Jyycyj8oBCvZ3+kOrILsMUjEVGIJREMfmrx6JAOfH7yc4xMGgmzwezR71gsFggh3N+wu25zeXzT3EmEFubqox8+QrW9Gj+++McBjYVzR0REpHOuNpB3fIqEUT8HAHx0/CMUVhWqjasRrkQgIlJdWPGGFxrcdbd4bONKhK/OfIXTFaexaPQij3/HYrG4P3QKIfxSlyHQ/njpH5WN7foD2lyLRykl3j7yNgZ1GoThXYYHNBY9zh0RUahKfWKl6hCoPZr7BiwWCzJ/fP49Q9bPspD0syRkZGSEzBcDrIlA4eLKBYNVh0A6pjaVmtBDuzi5Wjy29UPgW4ffQtforrii1xVtep5Ql9whOWi1BhozGFpu8ZhVmIWjJUcxZ8Acj1o7EhFRaDCnpMCckqI6DNI5Xz70WywWSCnd7y0Wfb4Il7x5CZY8tMTP0fmOLR4pXMR1ikJcpyjVYZBOqU0i7P+XdnFynZKb+3bbEyfLTuLrM19jdv/ZMBs928rQWEZGhs/jB9PG4xux8fhGJWO7ViI0t2rk7cNvI9Ycixl9ZgQ8loIyK+a8sAUF5VbdzB0RUagqW78eZevXqw6DdM4fhY4XDl2Ispoy/OO7f/ghIv/gSgQKF0d35uPoznzVYZBOqU0i7HhFuzj5o8XjO0fegVEYcWP/G31+jlBZMtead468g3eOvKNk7JZaPBZVF+G/J/6LmX1nIsYcE/BYVn12FDuyi7Hq06O6mTsiolBV8tbbKHnrbdVhUDuXkZGB4V2HIz05HWsOrEFNXY3qkACwJgKFj/2bzmD/pjOqwyCdCqmzoGhjYcX8yny8d/Q9XNn7SnSN6dr8cbZazNp9FAW2Wp/GoZZbPD6962lISNw88OaAxjDg4Q1IW/YR1m47CSmBtdtOIm3ZRxjw8IaAjktEREQXarbQ8QO/8+m5AOCO4XegsLoQ/z72b3+G6jOuRCAiCrEkAqBV/felsKKUEplbMmF32HHvqHtbPPap7DxsK63Ek9l5vobZ7jXX4nF77nas+34dFgxZgN7xvQMaw+Yl03D9yFREmbVYoswGzByZis1LpwV0XCIiIrpQ45oG8j+LIS2JsEzyvWB2enI6hncZjlf2vwK7w+6vUH0mBZMIREShl0SAb0mED77/AJvPbMZ9Y+5r9sNr703fIvmLLKzJOQsJYE3OWSR/kYXem75tY9TtT1NJhJq6Gizfuhw943riruF3BTyGpPgoxEWaYLM7EGkywGZ3IC7ShKQ4FokhIiJSZkWSdr3zZUA6tGtLwvnHvSCEwB3D7sCZijN498i7fg7UO673PK5C4ERE7VXIJRGEEF4nEU6UncAT25/AmG5jWlxCv33CYNyQlIhog3byjzYI3JCUiB0T2OLEW64kgqsIppQST+x4Atll2Xg4/WFEmYLzQb6owoZ56b3x/i8nYV56bxRW2IIyLhERETVj0V5kzB4FmKK1+6ZoYNhsYNE+n55uas+pmJQ6Cc/sfgY5FTl+DNQ7rvc8XIlARO2dSenoc1674CGjMHpVWPFE2Qks3LgQJoMJyy9Z7v5w25RukWbEmoywOiQiDQJWh0ScyYikSN+6OKj21NSnlI3t+u/sWrb49O6n8c6Rd/DzIT/HJd0vCVocL9w21n17xayhQRuXiChcdV/1jOoQSO/ikmG5bQqw+1XAFAXU2YDIeCCum09PJ4TAIxMfwawPZuHRrY/i+SueV9I+2vX+lIUVKRxcczffN5Pv1J4FO3TWLvV4sxLhVPkpLNy4ELWOWrx09UvoGd+z1d8prLFjfmpnrB/TH/NTO6OgRv3+Ol91jOqIjlEdlYztSiLYpR1/2fMXrN6/GjcNuAn3j7nf5+es36qRiIjUMHXsCFNHNX9bKIxUFgBjFgB3fKpdV7StlVxqbCoWjV6Er898jX8d/VfrvxAAbWlBThRqomMjEB0boToM0im1KxH2vKFdj5rnfsggDB6fpF/d/yrKa8uxdsZa9O/Y36PfWT2sj/v24wOaTzrk22px94FsvDgkLWRXKrgqFc/qOyvoY7taPL607yUUVRfhhn434MH0B9v0zUD9Vo0rfjLMX6ESEZEXzr33PgAg8YafKI6EdG3uG+dvX+eflZM3D7wZm05twvKtyxEXEYer0672y/N6iisRKJwc+iYXADDokhTFkZAeqT0LZr2pXeoxwAAJz7YzfJPzDdKT0z1OINTnah3UHD10cPjg2Af44NgHSsZ2JQvOVp/F4jGLYZloaXErSUvYqpGIKHSUvv8+St9/X3UYRBcwCAP+PO3PGNF1BJb9bxk+O/lZUMdnTQQKJ4e35OLwllzVYZBOhVwq1WDwrDvDybKTOF1x2uf995mZmU0+zg4Onokzx2F2/9l49opnsXDowjatQAhEq8bWkkRERESkPzHmGDx3xXMY2GkgFn+xGKt2rwpa60d3dwYmEYionQu9JIKHLR6/yfkGAHBJqn+L+LGDg2dcRY4u63FZm58rKT4Ke95/sc2tGuvXVGguSUREREQhrjwPWD0dKG+6jkJsRCxevvplzOo7C3/f93fM3zgfO/J2eFWY2xdMIhARaUIuieBpYcVvcr5B99ju6BXXy+PntlgsEEK4vzV33a7/rXX9Dg7Va/6m+w4OevHVu39rc6vG+jUViIiIKPgsFkurSYBWbXoCOLkV2LSy2UNizDF4dNKjePzSx3G6/DQWfrwQt224Deu+X4fK2krfxm0FtzMQEWnUFlZsgkG0vhKh1lGL7XnbMaPPDK+W0VssFnfCQAjRbMba1cFh5ZoX8IsHHkK+jjs46ImrRaO3rRoHPLwBNrsD5756A6Vfv4XHnI+7/t/IyMjg9gYiIqIgyMzMhGVs2fkkgDdFFVckAfZ6XyLsfFm7mCKBhwua/JVrL7oWV/S6Av8+9m+8dvA1PPTVQ1huXI7J3Se7L906+NZasjEWViQi0ihLIuTk5ADz/nfB4wbRemHFvYV7UVlbiUmpkwISm6uDw0q03MFBteeufE51CG1isVgabDvw9UP/5iXTsGL9IXxivg2Jk+chymzAkRUzkF9W7fWWCCKi9q7niy+oDoH0akWSdr3z5fPXrSQBGli0F/j4YeDwh4C9GjBFA4OuA370WIu/FmWKwtyBc3HTgJvwbeG3+PCHD/HFqS/w6clPAQD9OvbD5NTJGJE0AsO7DEfXmK4+/fPY4pHCyXW/HqE6BNIxZUmE3NxcICLmgsc9WYnw1ZmvYBRGjEsZ5/P4GRkZTT7urw+2wRBtilYdQpt4ujKkNUnxUYiLNDWoqQCACQQiIh8YovX9t4WC74L3TpllAICMaR1g+dXcVpMAbnHJQGQcUGcDTFFAnQ2Wfx2A5aeerSQQQmBk0kiMTBqJh9IfwrFzx/DVma/w9Zmv8fqh17H6wGoAQEqHFAzrMgz9O/ZHamwqUjqkIDU2FUkxSTAZmn9r7Hp/ypUIFA7MEUbVIZCOqd3OsP3v2vX4O90PGWCAzW7Dlpwt+KH0BzikA3WOOpTWlOJs9VnsKdiD7LJsjEseh/iIeJ+Hbi4h4K8PtsHw9uG3AQBzB85VHEnbFJRZtetyq88f/IsqbJiX3hu3jO+Fl776AW9OvdX9fPXnlIiIWlb8ptZ6udMttyiOhPTC/Xe2PA8iPgUyI0FbfVBXA0TGA3FebCeoLADGLADGLgB2rkam5WlY3mz91xoTQqBfx37o17EfFgxdAFudDYfOHsK+on3YW7gX+4r24ZMTnzT4HaMwoltMN6TEpiC1QypSYlPQNborok3RiDZFo8pepT03ayJQGNj35WkAwLCpPRRHQnqkNIkg0u8CAGRknMEg5/lYCIEN2RuwIXtDg2ONwoiEyAQM6TwEs/vPxrUXXRuUGPNttZjym/vxv1VPhVxxxY+zPwag/yTCqs+OImHSzVj16VGs+Mkwn57jhdvGum/HmI2InjDX/XyZmZlMIhAReah8w0YATCKQdywWi1YLAQC6DgB++hKwczVQ4WVxxblvnL89ZQmAp7UCjd4kIpoQaYx0r1JwsdqtyKvMQ05FDnIqc5BTkYPcylzkVORgR/4OFBwvaHJ1bAQi2hQLUSg4tkvbYsQkAvnCoySCEOIaAM8AMAJ4SUr5eKOfRwJ4DcAYAGcB3CSlzG7teeUrM7QbCyx4J3MZAODmgTfjRNkJXNbjMgzrMgwmgwlGYUQHcweviig2xduEQEZGBp7KzsORF5/Fk/cvwf1pybj7QDZeHJIWcgkFPer/0AYUbHodiZPnIXHyPKzddhJrt51EpMmAIyume/18rgKLLq7nA7RVDs89+TiTCURERH7iXoGwPAmZmYWwZMQjY0oEUHgY+Ntkz2shNPG8DbZHxCcDAKZMmYIvv/zST9FrtRTSEtKQlpDW5M9rHbUotZWi2l6NqtoqVNur8eFHHyIFKX6LgYhIj1rd1CWEMAL4K4DpAAYDuFkIMbjRYbcDKJFS9gXwNLSahK3KN8Zh4KexKLDVojQiCr87eAZje83Gm68WomvieEy6z4LTNWaM/fUDKKyxI99Wi4F3/xoHyqsaXBfYapv9Wf1j3AmB7LxWY+u96Vv8bcosrMk5CwBYk3MWI745gE+feRLLv89pUxxt/Xe4jim3dcT6P5XgYE4phlx3u/u6oNyKgjJrg8c8+Zknx3xnq8Flb/4/v4xx3fAUlH79FowGLTkUZTZg5shUbF46zZP/fS6weck0XD8yFVFmA8599QZOrLwOJ1ZeBwDoFh+NzMxM/Oq3y4L23ypUxgiVODgGx+AY+hjDaLTi7z/8B8jdB8vMvhdel+cD5Xne/8xfx3CMkBkjMzNT+9mQG7Q/xMIIy9QorSDisNnAon0+/T23mJ6DzIiHzNC2rbpub9q0Kaj/rcz5h/Hs3AnoWXEOby24FiOjU9DXHotDax8OyfngGO10DB/jiHQU+/T6JAKgtatp6QJgIoCP691/AMADjY75GMBE520TgCIAoqXnTUlJkUv+/bwEIJccPilvWPuuBCAv23qwyeslh0/KJYdPev0z13W3z/fIbp/vaXC715dZsjm/fehhCaDJS/3n8TaOtv476h8zYNU/JAB55ZNfNrh+6L298qH39nr9M0+OmfrwRr+M0Xvph7L30g8b3O699EP50Ht7m50TTzz43l6ZtuzDBs/pGqP+dTD+W4XKGKESB8fgGBxDH2OU3jdcApDy2fFNX/9nsZT/Wez9z/x1DMcIqTGaumRMidB+x1dluVL+43Ypl3fTxlreTcpnRobEf6uDf5oe0vPBMdrhGD7G8f2KW+V7/29Xm953U/jZuetmuXPXzVJKKQHslM18ljehdd0BnKp3/zSA9OaOkVLahRClADpDSyY0qTAuEWviJwDQvuVHaj8AwJEqW5PXrhUB3v7MdZ1/+Sj3Ma7bA368EDcdvqPpAHvOwMj3ZiI3wYj8K0aj22e7td+9YvQFh3oTR1v/HWtyziLyv2cAANYzWgXtowUVDa5dS/i9/ZknxxyvtftlDNcKgfq3k6feio9ibnMf44vv8svRNTYSHWPMyD5bhfwvX28whut6x3sv+OXf0dZjgjFGqMTBMTgGxwj9MVZ8OxlIcB5UeLjpa1f7Pm9/5q9jOIbSMSz/yAIAiHu3o7HajETsjRiDUkMCtu07hKfObLngGE/dXlqFK+1WXNbbAPGHfAD5DcZ1xRHs/1aDgjBGMP4dHCOMxvAxjotq1+Gi2nWosUTgtpR1IAKA67qXAgCe2NHy+VvIVroPCCFmA7haSnmH8/5tAMZLKX9d75gDzmNOOy3x1bkAAAdPSURBVO9/7zzmbKPnugvAXc67Y7z49/iVqf8gOMpKCx15OSdbOs7YvdfFss5e68jL8a2hMHkkoltfOKzlhfbS/Bbnw1umhG69DFFxXesqzgIQqKvksi0iIiJ/SokVSI0T2JXrwJgUA4qrZeHxc7LNf8/7dTJcXOuQtSXVsqRXgqF3UZWMzK1o+T0rEXnPIJDvkDitOo4WdEELX0xTQPWWUjb5OdiTlQinAfSsd78HgJxmjjkthDBB+x7jgk9sUsoXAbwIAEKInVLKsY2PIX3hPOof5zA8cB7DA+dR/ziH4YHzqH+cw/DAeQxNrRZWBLADQD8hRB8hRASAuQAar3lZB2C+8/aNAD6XrS1xICIiIiIiIiJdaXUlgrPGwb3QiicaAbwipTwghHgUWrGFdQBeBvC6EOIYtBUIcwMZNBEREREREREFnyfbGSClXA9gfaPHHql32wpgtpdjv+jl8RSaOI/6xzkMD5zH8MB51D/OYXjgPOof5zA8cB5DUKuFFYmIiIiIiIiIAM9qIhARERERERERqUkiCCGuEUIcEUIcE0IsUxEDeU8IkS2E2CeEyBJC7HQ+1kkI8V8hxFHndUfVcVJDQohXhBAFQoj99R5rct6EZpXztblXCDFaXeRUXzPzaBFCnHG+JrOEEDPq/ewB5zweEUJcrSZqqk8I0VMI8YUQ4pAQ4oAQYpHzcb4edaKFOeRrUUeEEFFCiO1CiG+d85jpfLyPEGKb87X4jrOgOIQQkc77x5w/T1MZP2lamMdXhRDH670eRzof5zk1RAkhjEKIPUKID533+VoMcUFPIgghjAD+CmA6gMEAbhZCDA52HOSzaVLKkfVarSwD8JmUsh+Az5z3KbS8CuCaRo81N2/TAfRzXu4C8HyQYqTWvYoL5xEAnna+Jkc669fAeU6dC2CI83eec557SS07gN9KKQcBmADgV8654utRP5qbQ4CvRT2xAbhcSjkCwEgA1wghJgBYCW0e+wEoAXC78/jbAZRIKfsCeNp5HKnX3DwCwO/rvR6znI/xnBq6FgE4VO8+X4shTsVKhPEAjkkpf5BS1gB4G8BMBXGQf8wEsMZ5ew2AWQpjoSZIKf8HrWtKfc3N20wAr0nNVgCJQoiU4ERKLWlmHpszE8DbUkqblPI4gGPQzr2kkJQyV0q523m7HNobpu7g61E3WpjD5vC1GIKcr6kK512z8yIBXA7gn87HG78WXa/RfwK4QgghghQuNaOFeWwOz6khSAjRA8C1AF5y3hfgazHkqUgidAdwqt7902j5DzCFDgngEyHELiHEXc7HukkpcwHtzRWAJGXRkTeamze+PvXnXueyzFfE+e1EnMcQ51yCOQrANvD1qEuN5hDga1FXnMunswAUAPgvgO8BnJNS2p2H1J8r9zw6f14KoHNwI6amNJ5HKaXr9fiY8/X4tBAi0vkYX4+h6c8AlgBwOO93Bl+LIU9FEqGpbBFbROjDJCnlaGjLwX4lhLhMdUDkd3x96svzAC6GtowzF8CTzsc5jyFMCBEL4F8A7pNSlrV0aBOPcR5DQBNzyNeizkgp66SUIwH0gLY6ZFBThzmvOY8hqvE8CiGGAngAwEAA4wB0ArDUeTjnMcQIIa4DUCCl3FX/4SYO5WsxxKhIIpwG0LPe/R4AchTEQV6SUuY4rwsAvA/tj26+aymY87pAXYTkhebmja9PHZFS5jvfQDkA/B3nl0lzHkOUEMIM7cPnG1LK95wP8/WoI03NIV+L+iWlPAfgS2g1LhKFECbnj+rPlXsenT9PgOfbyygI6s3jNc5tR1JKaQOwGnw9hrJJAK4XQmRD2+J+ObSVCXwthjgVSYQdAPo5q25GQCs4tE5BHOQFIUQHIUSc6zaAHwHYD23u5jsPmw/gAzURkpeam7d1AH7mrGA8AUCpa5k1hZ5Gezl/Au01CWjzONdZxbgPtCJS24MdHzXk3Lf5MoBDUsqn6v2Ir0edaG4O+VrUFyFEVyFEovN2NIArodW3+ALAjc7DGr8WXa/RGwF8LqXkt5+KNTOPh+slZQW0vfT1X488p4YQKeUDUsoeUso0aJ8JP5dSzgNfiyHP1Poh/iWltAsh7gXwMQAjgFeklAeCHQd5rRuA9521S0wA3pRSbhRC7ADwrhDidgAnAcxWGCM1QQjxFoCpALoIIU4DyADwOJqet/UAZkAr/lUFYEHQA6YmNTOPU52tqySAbAB3A4CU8oAQ4l0AB6FVk/+VlLJORdzUwCQAtwHY59zDCwAPgq9HPWluDm/ma1FXUgCscXbKMAB4V0r5oRDiIIC3hRArAOyBljCC8/p1IcQxaN96zlURNF2guXn8XAjRFdrS9ywA9ziP5zlVP5aCr8WQJpi8ISIiIiIiIiJPqNjOQEREREREREQ6xCQCEREREREREXmESQQiIiIiIiIi8giTCERERERERETkESYRiIiIiIiIiMgjTCIQERERERERkUeYRCAiIiIiIiIijzCJQEREREREREQe+f+3jBjHRpm4EAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ymax = 1\n",
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "\n",
    "plt.plot([0, item[1][idx].item()], [0, 0])\n",
    "\n",
    "for pivot in loaded_vidid_selected_frames[cur_vidid + \".txt\"]:\n",
    "    plt.plot([pivot, pivot], [0, ymax], '--')\n",
    "    \n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax])\n",
    "    \n",
    "all_prob = []\n",
    "for i, segment in enumerate(prob_each_segment):\n",
    "    segment = segment.cpu().numpy()\n",
    "    print(f\"Min prob {i} = {np.min(segment).item()}\")\n",
    "    xs = np.arange(loaded_vidid_selected_frames[cur_vidid + \".txt\"][i], \n",
    "                   loaded_vidid_selected_frames[cur_vidid + \".txt\"][i+1])\n",
    "    plt.plot(xs, segment, '*')\n",
    "\n",
    "    \n",
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "\n",
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))\n",
    "loss_arr = -loss_arr[0].detach().cpu().numpy()\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr/np.max(loss_arr), 'k+')\n",
    "\n",
    "plt.ylim([0, ymax])\n",
    "plt.xlim([0, item[1][idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1895, 48])\n"
     ]
    }
   ],
   "source": [
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "print(loss_prob.shape)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1387])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf5b8ecb00>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3987, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([75, 345, 1305, 1415, 1440, 1519],\n",
       " [73, 282, 455, 1312, 1434, 1458],\n",
       " tensor(1520, device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid], loaded_vidid_selected_frames[cur_vidid + \".txt\"], item_1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_features_shortened = cur_vid_feat[:item[1][idx]]\n",
    "cur_features_shortened[:, item[2][idx][:item[1][idx]]].shape\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 47, 47, 47, 47, 47, 47,  4,  4,  4,  4,  4,  4,  4,  4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[2][idx][video_id_boundary_frames[cur_vidid][1]-6:video_id_boundary_frames[cur_vidid][1]+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid][0]-6,video_id_boundary_frames[cur_vidid][0]+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0794, 13.3350],\n",
       "        [ 1.0831, 13.5102],\n",
       "        [ 1.1767, 13.7662],\n",
       "        [ 1.3750, 14.0177],\n",
       "        [ 1.7298, 14.1727],\n",
       "        [ 2.1706, 14.0367],\n",
       "        [ 2.7338, 13.4563],\n",
       "        [ 4.5600, 12.1745],\n",
       "        [ 7.8450, 10.3246],\n",
       "        [11.0151,  8.3563],\n",
       "        [13.4775,  6.6546],\n",
       "        [14.1836,  5.2898],\n",
       "        [11.2142,  4.2791],\n",
       "        [ 5.9829,  3.5694],\n",
       "        [ 2.3440,  2.9389],\n",
       "        [ 0.7406,  2.3515]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_vid_feat[video_id_boundary_frames[cur_vidid][1]-7:video_id_boundary_frames[cur_vidid][1]+9][:,[47,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frames_and_get_classification_value(model_output, count_list, video_ids_list, labels_all, p_criterion):\n",
    "    new_selected_frames = {}\n",
    "#     new_selected_frames_probs_values = {}\n",
    "#     new_selected_frames_labels = {}\n",
    "    \n",
    "    labels_arr = []\n",
    "    probs_arr = []\n",
    "    \n",
    "    for i, video_id in enumerate(video_ids_list):\n",
    "        current_boundary_frames = boundaries_dict[video_id]\n",
    "        current_video_logits = model_output[i].T\n",
    "        current_video_count = count_list[i]\n",
    "        current_selected_frames = loaded_vidid_selected_frames[video_id]\n",
    "        labels = labels_all[i]\n",
    "        cur_seg_start = 0\n",
    "        for seg_num, ele in enumerate(current_boundary_frames):\n",
    "            cur_seg_end = ele + 1\n",
    "            total_seg_len = cur_seg_end - cur_seg_start\n",
    "            per_frames_far_from_boundary = int(0.3 * total_seg_len)\n",
    "            new_frames_start = cur_seg_start + per_frames_far_from_boundary\n",
    "            new_frames_end = cur_seg_end - per_frames_far_from_boundary\n",
    "            \n",
    "            frames_added = list(range(new_frames_start, new_frames_end))\n",
    "            frames_added.append(current_selected_frames[seg_num])\n",
    "            frames_added = list(set(frames_added))\n",
    "            frames_added.sort()\n",
    "            new_selected_frames[video_id] = frames_added\n",
    "\n",
    "            \n",
    "            labels_seg = torch.stack([labels[current_selected_frames[seg_num]]] * len(frames_added))\n",
    "            frames_added = torch.tensor(np.array(frames_added), dtype=torch.long, device=current_video_logits.device)\n",
    "            prob_seg = current_video_logits[frames_added, :]\n",
    "            \n",
    "            labels_arr.append(labels_seg)\n",
    "            probs_arr.append(prob_seg)\n",
    "            \n",
    "            cur_seg_start = cur_seg_end\n",
    "            \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    probs_arr = torch.cat(probs_arr)\n",
    "    loss = p_criterion(probs_arr, labels_arr)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
