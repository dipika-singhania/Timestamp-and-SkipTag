{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mstcn_model import *\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 8, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 4, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split4/', 'project_name': 'breakfast-split-4', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split4.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split4.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split4_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=4,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split4/\",\n",
    "    project_name=\"breakfast-split-4\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1136\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 576\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", allow_pickle=True).item()\n",
    "# loaded_vidid_selected_frames\n",
    "video_id_boundary_frames = pickle.load(open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"rb\"))\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_var_actions = pickle.load(open(\"dump_dir/mean_var_actions.pkl\", \"rb\"))\n",
    "def get_possion_prob(cur_len, cur_class):\n",
    "    mean_class, std_class = loaded_mean_var_actions[cur_class]\n",
    "    mean_class = mean_class * 10\n",
    "    prob = cur_len * torch.log(torch.tensor(mean_class, device=device) + 1e-5)\n",
    "    prob = prob - mean_class\n",
    "    factorials = torch.cumsum(torch.log(torch.arange(1, torch.max(cur_len)+1, 1).type(torch.float).to(device)), \n",
    "                              dim=0)[min(cur_len)-1:]\n",
    "    prob = prob - factorials\n",
    "    return prob\n",
    "\n",
    "# get_possion_prob(torch.arange(10, 21), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_video_each_segment_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vals_per_segment(selected_frames, cur_vid_feat, labels):\n",
    "    prob_each_segment = []\n",
    "    \n",
    "    log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "    cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "    cur_boundary = 0\n",
    "    for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "\n",
    "        next_ele = selected_frames[i + 1]\n",
    "        label_cur_ele = labels[cur_ele]\n",
    "        label_next_ele = labels[next_ele]\n",
    "\n",
    "        indices = torch.arange(cur_ele, next_ele).to(cumsum_feat.device)\n",
    "        strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "        end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "        left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "        right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "\n",
    "        cur_boundary_len = indices - cur_boundary\n",
    "        len_prob = get_possion_prob(cur_boundary_len, label_cur_ele.item())\n",
    "\n",
    "        prob = torch.softmax((left_sum + right_sum + len_prob), dim=0)\n",
    "#         prob = torch.softmax((left_sum + right_sum), dim=0)\n",
    "\n",
    "        cur_boundary = round(np.sum(np.arange(cur_ele, next_ele, 1) * prob.detach().cpu().numpy()).item())\n",
    "        prob_each_segment.append(prob)\n",
    "    \n",
    "    return prob_each_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_element_probb(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_video = prob_vals_per_segment(selected_frames, cur_vid_feat, labels)\n",
    "        prob_video_each_segment_dict[cur_vidid] = prob_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_loss(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    loss_arr = []\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_each_video = prob_video_each_segment_dict[cur_vidid]\n",
    "        \n",
    "        log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "        cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "        \n",
    "        \n",
    "        current_vid_prob_arr = []\n",
    "        # Start segment log probability\n",
    "        start_ele_idx = selected_frames[0]\n",
    "        if start_ele_idx > 0:\n",
    "            start_ele_label_idx = labels[selected_frames[0]]\n",
    "            first_segment_prob = (cumsum_feat[start_ele_idx - 1, :])[start_ele_label_idx]\n",
    "            current_vid_prob_arr.append(first_segment_prob.unsqueeze(0))\n",
    "        \n",
    "        for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "            next_ele = selected_frames[i + 1]\n",
    "            label_cur_ele = labels[cur_ele]\n",
    "            label_next_ele = labels[next_ele]\n",
    "\n",
    "            indices = torch.arange(cur_ele, next_ele)\n",
    "            strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "            end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "            left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "            right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "            current_vid_prob_arr.append((left_sum + right_sum)  * (prob_each_video[i]))\n",
    "        \n",
    "        # End segment log probability\n",
    "        end_index = cur_vid_count.item() - 1\n",
    "        last_ele_label_idx = labels[selected_frames[-1]]\n",
    "        last_segment_sum_prob = (cumsum_feat[end_index, :] - cumsum_feat[selected_frames[-1] - 1, :])[last_ele_label_idx]\n",
    "        current_vid_prob_arr.append(last_segment_sum_prob.unsqueeze(0))\n",
    "            \n",
    "        loss_arr.append(torch.cat(current_vid_prob_arr))\n",
    "    return -torch.mean(torch.cat(loss_arr)), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_estimated_boundaries():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames, video_id_boundary_frames\n",
    "    estimated_boundary_dict = {}\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        selected_ele_list = loaded_vidid_selected_frames[ele + \".txt\"]\n",
    "        boundary_list = []\n",
    "        for i, segment in enumerate(estimated_boundary_probs):\n",
    "            estimated_boundary = np.sum(np.arange(selected_ele_list[i], selected_ele_list[i + 1], 1) \\\n",
    "                                        * segment.detach().cpu().numpy())\n",
    "            estimated_boundary = round(estimated_boundary.item())\n",
    "            \n",
    "            if (estimated_boundary < selected_ele_list[i]) or (estimated_boundary > selected_ele_list[i + 1]):\n",
    "                print(\"Estimated value wrong\")\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            boundary_list.append(estimated_boundary)\n",
    "\n",
    "        estimated_boundary_dict[ele] = boundary_list\n",
    "    return estimated_boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_boundary_err():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    err_list = []\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    estimated_boundar_dict = get_estimated_boundaries()\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        estimated_boundary_list = estimated_boundar_dict[ele]\n",
    "        \n",
    "        estimated = np.array(estimated_boundary_list) #.detach().cpu().numpy()\n",
    "        actual = np.array(video_id_boundary_frames[ele][:-1])\n",
    "        if len(actual) != len(estimated):\n",
    "            print(ele)\n",
    "            continue\n",
    "        \n",
    "        mse_err = (actual - estimated)**2\n",
    "        err_list.append(mse_err)\n",
    "        \n",
    "        estimated_labels = []\n",
    "        actual_labels = []\n",
    "        start_v1 = 0\n",
    "        start_v2 = 0\n",
    "        for i, v1 in enumerate(estimated):\n",
    "            estimated_labels.extend([i] * (v1 - start_v1))\n",
    "            start_v1 = v1\n",
    "            v2 = actual[i]\n",
    "            actual_labels.extend([i] * (v2 - start_v2))\n",
    "            start_v2 = v2\n",
    "            \n",
    "        last_ele = video_id_boundary_frames[ele][-1]\n",
    "        estimated_labels.extend([i+1] * (last_ele - start_v1))\n",
    "        actual_labels.extend([i+1] * (last_ele - start_v2))\n",
    "        \n",
    "        correct += np.sum(np.array(actual_labels) == np.array(estimated_labels))\n",
    "        total += len(actual_labels)\n",
    "        \n",
    "    print(f\"Train Boundary avergage error = {np.sqrt(np.mean(np.concatenate(err_list))):.3f}\")\n",
    "    print(f\"Train From boundary avergage accuracy = {correct * 100.0 / total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = MultiStageModel(num_stages=4, num_layers=10, num_f_maps=64, dim=2048, num_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(labels_all, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((labels_all.shape[0], labels_all.shape[1]), dtype=torch.long, device=labels_all.device) * (-100)\n",
    "    for iter_num, labels in enumerate(labels_all):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(loaded_vidid_selected_frames[cur_vidid + \".txt\"]))\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = labels[frame_idx_tensor]\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split4/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Training:: Epoch 0, Iteration 0, Current loss 15.722561836242676 Accuracy 3.737044714243411\n",
      "Training:: Epoch 0, Iteration 10, Current loss 13.530548095703125 Accuracy 23.141452126120914\n",
      "Training:: Epoch 0, Iteration 20, Current loss 13.128864288330078 Accuracy 0.8774082709534913\n",
      "Training:: Epoch 0, Iteration 30, Current loss 12.450479507446289 Accuracy 19.317244791380354\n",
      "Training:: Epoch 0, Iteration 40, Current loss 14.374771118164062 Accuracy 1.5980873285516548\n",
      "Training:: Epoch 0, Iteration 50, Current loss 11.84317684173584 Accuracy 31.332837854576987\n",
      "Training:: Epoch 0, Iteration 60, Current loss 12.652199745178223 Accuracy 13.702894231406054\n",
      "Training:: Epoch 0, Iteration 70, Current loss 14.351591110229492 Accuracy 8.943959113329768\n",
      "Training:: Epoch 0, Iteration 80, Current loss 11.659465789794922 Accuracy 10.758064516129032\n",
      "Training:: Epoch 0, Iteration 90, Current loss 12.443076133728027 Accuracy 15.255775577557756\n",
      "Training:: Epoch 0, Iteration 100, Current loss 11.19002914428711 Accuracy 29.88451567287297\n",
      "Training:: Epoch 0, Iteration 110, Current loss 12.220780372619629 Accuracy 16.85402545875351\n",
      "Training:: Epoch 0, Iteration 120, Current loss 10.682416915893555 Accuracy 24.55106237148732\n",
      "Training:: Epoch 0, Iteration 130, Current loss 11.446630477905273 Accuracy 7.524063661488611\n",
      "Training:: Epoch 0, Iteration 140, Current loss 13.022395133972168 Accuracy 13.373983739837398\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 0, Probability Accuracy 20.15127549853267\n",
      "Starting Training\n",
      "Training:: Epoch 1, Iteration 0, Current loss 9.654546737670898 Accuracy 19.873817034700316\n",
      "Training:: Epoch 1, Iteration 10, Current loss 10.116249084472656 Accuracy 25.524354307505632\n",
      "Training:: Epoch 1, Iteration 20, Current loss 10.346878051757812 Accuracy 22.485422740524783\n",
      "Training:: Epoch 1, Iteration 30, Current loss 10.753870964050293 Accuracy 12.835357243716464\n",
      "Training:: Epoch 1, Iteration 40, Current loss 10.010920524597168 Accuracy 21.51177199504337\n",
      "Training:: Epoch 1, Iteration 50, Current loss 8.577531814575195 Accuracy 29.093441209629287\n",
      "Training:: Epoch 1, Iteration 60, Current loss 9.33928108215332 Accuracy 18.88315276666909\n",
      "Training:: Epoch 1, Iteration 70, Current loss 9.002187728881836 Accuracy 27.211370493885976\n",
      "Training:: Epoch 1, Iteration 80, Current loss 8.293466567993164 Accuracy 26.667173598965856\n",
      "Training:: Epoch 1, Iteration 90, Current loss 8.666682243347168 Accuracy 42.98886764871247\n",
      "Training:: Epoch 1, Iteration 100, Current loss 9.488439559936523 Accuracy 28.47374386097469\n",
      "Training:: Epoch 1, Iteration 110, Current loss 8.893525123596191 Accuracy 24.55397431447414\n",
      "Training:: Epoch 1, Iteration 120, Current loss 9.137090682983398 Accuracy 28.075964217627217\n",
      "Training:: Epoch 1, Iteration 130, Current loss 12.25930118560791 Accuracy 15.659012268008809\n",
      "Training:: Epoch 1, Iteration 140, Current loss 10.939274787902832 Accuracy 25.790692777426827\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 1, Probability Accuracy 30.620053072242303\n",
      "Starting Training\n",
      "Training:: Epoch 2, Iteration 0, Current loss 7.6982269287109375 Accuracy 26.1569991897431\n",
      "Training:: Epoch 2, Iteration 10, Current loss 7.9055681228637695 Accuracy 23.940870426955158\n",
      "Training:: Epoch 2, Iteration 20, Current loss 8.288490295410156 Accuracy 14.912189545879663\n",
      "Training:: Epoch 2, Iteration 30, Current loss 8.707863807678223 Accuracy 30.359992906543713\n",
      "Training:: Epoch 2, Iteration 40, Current loss 9.073894500732422 Accuracy 12.394396904126015\n",
      "Training:: Epoch 2, Iteration 50, Current loss 10.121249198913574 Accuracy 25.513333679887715\n",
      "Training:: Epoch 2, Iteration 60, Current loss 8.900223731994629 Accuracy 25.697865353037766\n",
      "Training:: Epoch 2, Iteration 70, Current loss 7.668145179748535 Accuracy 34.37991953821935\n",
      "Training:: Epoch 2, Iteration 80, Current loss 7.786390781402588 Accuracy 31.26249741432807\n",
      "Training:: Epoch 2, Iteration 90, Current loss 6.80257511138916 Accuracy 51.309882366312436\n",
      "Training:: Epoch 2, Iteration 100, Current loss 6.6711039543151855 Accuracy 41.09721402114982\n",
      "Training:: Epoch 2, Iteration 110, Current loss 6.868269920349121 Accuracy 34.50256567358505\n",
      "Training:: Epoch 2, Iteration 120, Current loss 7.528464317321777 Accuracy 45.80044205873066\n",
      "Training:: Epoch 2, Iteration 130, Current loss 7.718919277191162 Accuracy 30.32817804602037\n",
      "Training:: Epoch 2, Iteration 140, Current loss 11.545702934265137 Accuracy 25.909442724458206\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 2, Probability Accuracy 32.37431290400591\n",
      "Starting Training\n",
      "Training:: Epoch 3, Iteration 0, Current loss 8.20356559753418 Accuracy 35.5519110746663\n",
      "Training:: Epoch 3, Iteration 10, Current loss 9.173042297363281 Accuracy 28.93923049262855\n",
      "Training:: Epoch 3, Iteration 20, Current loss 8.180692672729492 Accuracy 36.64344449520329\n",
      "Training:: Epoch 3, Iteration 30, Current loss 7.842562198638916 Accuracy 32.793638628089674\n",
      "Training:: Epoch 3, Iteration 40, Current loss 7.649465560913086 Accuracy 36.40786539631056\n",
      "Training:: Epoch 3, Iteration 50, Current loss 6.174551963806152 Accuracy 52.60394792104158\n",
      "Training:: Epoch 3, Iteration 60, Current loss 7.081543445587158 Accuracy 33.917771736114965\n",
      "Training:: Epoch 3, Iteration 70, Current loss 6.282346248626709 Accuracy 41.94539249146758\n",
      "Training:: Epoch 3, Iteration 80, Current loss 7.598437309265137 Accuracy 38.045443824200014\n",
      "Training:: Epoch 3, Iteration 90, Current loss 5.594529151916504 Accuracy 43.648763853367434\n",
      "Training:: Epoch 3, Iteration 100, Current loss 6.7731781005859375 Accuracy 37.51731175741672\n",
      "Training:: Epoch 3, Iteration 110, Current loss 6.415505886077881 Accuracy 40.614420696276085\n",
      "Training:: Epoch 3, Iteration 120, Current loss 6.930757522583008 Accuracy 40.19239578561613\n",
      "Training:: Epoch 3, Iteration 130, Current loss 6.447843074798584 Accuracy 44.586680053547525\n",
      "Training:: Epoch 3, Iteration 140, Current loss 6.435457229614258 Accuracy 39.70933173895047\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 3, Probability Accuracy 41.76236445989843\n",
      "Starting Training\n",
      "Training:: Epoch 4, Iteration 0, Current loss 8.193450927734375 Accuracy 30.051911278905145\n",
      "Training:: Epoch 4, Iteration 10, Current loss 5.691405296325684 Accuracy 52.4639932222536\n",
      "Training:: Epoch 4, Iteration 20, Current loss 7.978551864624023 Accuracy 44.91789202533004\n",
      "Training:: Epoch 4, Iteration 30, Current loss 4.6690287590026855 Accuracy 55.160450997398094\n",
      "Training:: Epoch 4, Iteration 40, Current loss 5.798801898956299 Accuracy 44.940549607775424\n",
      "Training:: Epoch 4, Iteration 50, Current loss 6.354409694671631 Accuracy 52.44373611513571\n",
      "Training:: Epoch 4, Iteration 60, Current loss 9.021916389465332 Accuracy 39.586919104991395\n",
      "Training:: Epoch 4, Iteration 70, Current loss 5.97468900680542 Accuracy 56.732804232804234\n",
      "Training:: Epoch 4, Iteration 80, Current loss 6.680028915405273 Accuracy 44.39199722847739\n",
      "Training:: Epoch 4, Iteration 90, Current loss 7.953474998474121 Accuracy 37.07865168539326\n",
      "Training:: Epoch 4, Iteration 100, Current loss 5.372946739196777 Accuracy 65.78148011782032\n",
      "Training:: Epoch 4, Iteration 110, Current loss 5.209530353546143 Accuracy 40.93436441490382\n",
      "Training:: Epoch 4, Iteration 120, Current loss 6.068652629852295 Accuracy 48.77394396955138\n",
      "Training:: Epoch 4, Iteration 130, Current loss 5.719677925109863 Accuracy 52.898645411385516\n",
      "Training:: Epoch 4, Iteration 140, Current loss 4.899818420410156 Accuracy 54.359643400739294\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 4, Probability Accuracy 46.546382656095794\n",
      "Starting Training\n",
      "Training:: Epoch 5, Iteration 0, Current loss 6.362322807312012 Accuracy 47.992840705701866\n",
      "Training:: Epoch 5, Iteration 10, Current loss 4.346924304962158 Accuracy 55.31350482315113\n",
      "Training:: Epoch 5, Iteration 20, Current loss 4.652973651885986 Accuracy 58.514553362328535\n",
      "Training:: Epoch 5, Iteration 30, Current loss 5.90736198425293 Accuracy 52.90081445944438\n",
      "Training:: Epoch 5, Iteration 40, Current loss 5.153216361999512 Accuracy 58.867189876498145\n",
      "Training:: Epoch 5, Iteration 50, Current loss 5.1854472160339355 Accuracy 53.957612456747405\n",
      "Training:: Epoch 5, Iteration 60, Current loss 6.181012153625488 Accuracy 45.85317786171785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 5, Iteration 70, Current loss 4.773314476013184 Accuracy 50.60524150899048\n",
      "Training:: Epoch 5, Iteration 80, Current loss 6.668547630310059 Accuracy 39.68072976054732\n",
      "Training:: Epoch 5, Iteration 90, Current loss 4.948241710662842 Accuracy 61.53430942581472\n",
      "Training:: Epoch 5, Iteration 100, Current loss 7.499157905578613 Accuracy 36.47749022218524\n",
      "Training:: Epoch 5, Iteration 110, Current loss 5.376407623291016 Accuracy 50.15013211626231\n",
      "Training:: Epoch 5, Iteration 120, Current loss 7.042083740234375 Accuracy 40.49944564700913\n",
      "Training:: Epoch 5, Iteration 130, Current loss 4.917335033416748 Accuracy 63.2504548211037\n",
      "Training:: Epoch 5, Iteration 140, Current loss 5.702054500579834 Accuracy 54.11630258010637\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 5, Probability Accuracy 49.34909705292191\n",
      "Starting Training\n",
      "Training:: Epoch 6, Iteration 0, Current loss 5.21519660949707 Accuracy 68.49813748730105\n",
      "Training:: Epoch 6, Iteration 10, Current loss 4.9889349937438965 Accuracy 57.78231000429369\n",
      "Training:: Epoch 6, Iteration 20, Current loss 6.046958923339844 Accuracy 50.96106456382454\n",
      "Training:: Epoch 6, Iteration 30, Current loss 5.8421149253845215 Accuracy 52.56888203282341\n",
      "Training:: Epoch 6, Iteration 40, Current loss 5.384449481964111 Accuracy 55.583448753462605\n",
      "Training:: Epoch 6, Iteration 50, Current loss 5.616092205047607 Accuracy 56.21877289987343\n",
      "Training:: Epoch 6, Iteration 60, Current loss 6.872646331787109 Accuracy 42.721723518850986\n",
      "Training:: Epoch 6, Iteration 70, Current loss 5.3864898681640625 Accuracy 60.22136734831612\n",
      "Training:: Epoch 6, Iteration 80, Current loss 4.8376970291137695 Accuracy 60.577651515151516\n",
      "Training:: Epoch 6, Iteration 90, Current loss 7.559432506561279 Accuracy 45.77663068981699\n",
      "Training:: Epoch 6, Iteration 100, Current loss 4.878479480743408 Accuracy 57.09615771474552\n",
      "Training:: Epoch 6, Iteration 110, Current loss 7.455664157867432 Accuracy 42.40805308739174\n",
      "Training:: Epoch 6, Iteration 120, Current loss 5.464522361755371 Accuracy 51.62404557023391\n",
      "Training:: Epoch 6, Iteration 130, Current loss 7.699869155883789 Accuracy 42.18715270060013\n",
      "Training:: Epoch 6, Iteration 140, Current loss 4.80727481842041 Accuracy 64.31212189110464\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 6, Probability Accuracy 51.78636788475742\n",
      "Starting Training\n",
      "Training:: Epoch 7, Iteration 0, Current loss 5.361830711364746 Accuracy 51.05792010456662\n",
      "Training:: Epoch 7, Iteration 10, Current loss 6.247742176055908 Accuracy 48.26011651708392\n",
      "Training:: Epoch 7, Iteration 20, Current loss 4.413731575012207 Accuracy 53.86226999453452\n",
      "Training:: Epoch 7, Iteration 30, Current loss 4.311649799346924 Accuracy 63.030131674610224\n",
      "Training:: Epoch 7, Iteration 40, Current loss 5.729333877563477 Accuracy 53.415651734164946\n",
      "Training:: Epoch 7, Iteration 50, Current loss 5.8419189453125 Accuracy 45.65566980166658\n",
      "Training:: Epoch 7, Iteration 60, Current loss 5.480252742767334 Accuracy 49.11718720207451\n",
      "Training:: Epoch 7, Iteration 70, Current loss 5.1191182136535645 Accuracy 61.428445307671936\n",
      "Training:: Epoch 7, Iteration 80, Current loss 4.740818500518799 Accuracy 61.80165289256198\n",
      "Training:: Epoch 7, Iteration 90, Current loss 3.98126220703125 Accuracy 75.62039814562313\n",
      "Training:: Epoch 7, Iteration 100, Current loss 5.111085891723633 Accuracy 52.78916794806082\n",
      "Training:: Epoch 7, Iteration 110, Current loss 4.984472751617432 Accuracy 61.59467149508536\n",
      "Training:: Epoch 7, Iteration 120, Current loss 5.140017032623291 Accuracy 61.756373937677054\n",
      "Training:: Epoch 7, Iteration 130, Current loss 4.5034260749816895 Accuracy 63.4438633016808\n",
      "Training:: Epoch 7, Iteration 140, Current loss 3.770719051361084 Accuracy 76.22818358112475\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 7, Probability Accuracy 52.196828083843684\n",
      "Starting Training\n",
      "Training:: Epoch 8, Iteration 0, Current loss 5.401976108551025 Accuracy 46.55524505588994\n",
      "Training:: Epoch 8, Iteration 10, Current loss 4.368478775024414 Accuracy 60.39796690818644\n",
      "Training:: Epoch 8, Iteration 20, Current loss 5.519047260284424 Accuracy 56.954985226811885\n",
      "Training:: Epoch 8, Iteration 30, Current loss 5.172807693481445 Accuracy 52.65093108765271\n",
      "Training:: Epoch 8, Iteration 40, Current loss 5.105322360992432 Accuracy 57.90005347911344\n",
      "Training:: Epoch 8, Iteration 50, Current loss 3.1658949851989746 Accuracy 74.47227694905713\n",
      "Training:: Epoch 8, Iteration 60, Current loss 4.367843151092529 Accuracy 69.99122977977002\n",
      "Training:: Epoch 8, Iteration 70, Current loss 4.249768257141113 Accuracy 65.97447132787198\n",
      "Training:: Epoch 8, Iteration 80, Current loss 5.933849334716797 Accuracy 58.423375437528534\n",
      "Training:: Epoch 8, Iteration 90, Current loss 4.113250255584717 Accuracy 66.65091367359798\n",
      "Training:: Epoch 8, Iteration 100, Current loss 4.468923568725586 Accuracy 66.09542596473925\n",
      "Training:: Epoch 8, Iteration 110, Current loss 4.75887393951416 Accuracy 65.06727595836507\n",
      "Training:: Epoch 8, Iteration 120, Current loss 4.2390971183776855 Accuracy 68.98242746219861\n",
      "Training:: Epoch 8, Iteration 130, Current loss 4.312014102935791 Accuracy 65.69741433300528\n",
      "Training:: Epoch 8, Iteration 140, Current loss 2.891836166381836 Accuracy 82.4620763548187\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 8, Probability Accuracy 56.74154733037471\n",
      "Starting Training\n",
      "Training:: Epoch 9, Iteration 0, Current loss 5.280501842498779 Accuracy 53.130189305878446\n",
      "Training:: Epoch 9, Iteration 10, Current loss 4.47150993347168 Accuracy 66.17316341829086\n",
      "Training:: Epoch 9, Iteration 20, Current loss 4.796149730682373 Accuracy 65.67358505016466\n",
      "Training:: Epoch 9, Iteration 30, Current loss 5.66731595993042 Accuracy 63.71740733293116\n",
      "Training:: Epoch 9, Iteration 40, Current loss 7.514945030212402 Accuracy 44.58653638910841\n",
      "Training:: Epoch 9, Iteration 50, Current loss 7.6053547859191895 Accuracy 54.64846597309655\n",
      "Training:: Epoch 9, Iteration 60, Current loss 7.937252044677734 Accuracy 38.34437086092715\n",
      "Training:: Epoch 9, Iteration 70, Current loss 3.9566051959991455 Accuracy 68.27009530659953\n",
      "Training:: Epoch 9, Iteration 80, Current loss 3.4716382026672363 Accuracy 80.14461742787574\n",
      "Training:: Epoch 9, Iteration 90, Current loss 5.353593826293945 Accuracy 55.024128686327074\n",
      "Training:: Epoch 9, Iteration 100, Current loss 4.4795823097229 Accuracy 64.10575858250277\n",
      "Training:: Epoch 9, Iteration 110, Current loss 4.251086235046387 Accuracy 73.82815025538243\n",
      "Training:: Epoch 9, Iteration 120, Current loss 4.457841396331787 Accuracy 69.83442957586755\n",
      "Training:: Epoch 9, Iteration 130, Current loss 4.019833564758301 Accuracy 72.33631977946244\n",
      "Training:: Epoch 9, Iteration 140, Current loss 3.925172805786133 Accuracy 69.9299008943679\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 9, Probability Accuracy 54.2437368871692\n",
      "Starting Training\n",
      "Training:: Epoch 10, Iteration 0, Current loss 4.4760518074035645 Accuracy 67.34662018004892\n",
      "Training:: Epoch 10, Iteration 10, Current loss 4.199649333953857 Accuracy 68.23876197494474\n",
      "Training:: Epoch 10, Iteration 20, Current loss 4.203332901000977 Accuracy 68.15795378056988\n",
      "Training:: Epoch 10, Iteration 30, Current loss 4.319386005401611 Accuracy 70.69226245484332\n",
      "Training:: Epoch 10, Iteration 40, Current loss 3.0237677097320557 Accuracy 81.21393239901072\n",
      "Training:: Epoch 10, Iteration 50, Current loss 3.829324722290039 Accuracy 71.50988385508808\n",
      "Training:: Epoch 10, Iteration 60, Current loss 3.94911527633667 Accuracy 74.7081114297419\n",
      "Training:: Epoch 10, Iteration 70, Current loss 3.306670665740967 Accuracy 78.49724019229627\n",
      "Training:: Epoch 10, Iteration 80, Current loss 3.8596842288970947 Accuracy 74.00134498991258\n",
      "Training:: Epoch 10, Iteration 90, Current loss 3.969848394393921 Accuracy 74.68752134417048\n",
      "Training:: Epoch 10, Iteration 100, Current loss 6.113509654998779 Accuracy 60.9362389023406\n",
      "Training:: Epoch 10, Iteration 110, Current loss 5.4957380294799805 Accuracy 60.49225094868248\n",
      "Training:: Epoch 10, Iteration 120, Current loss 4.837258338928223 Accuracy 59.34495346804463\n",
      "Training:: Epoch 10, Iteration 130, Current loss 4.8085551261901855 Accuracy 68.20849964780464\n",
      "Training:: Epoch 10, Iteration 140, Current loss 5.0215325355529785 Accuracy 65.74888463989802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 10, Probability Accuracy 53.132536813966105\n",
      "Starting Training\n",
      "Training:: Epoch 11, Iteration 0, Current loss 4.954178810119629 Accuracy 67.63485477178423\n",
      "Training:: Epoch 11, Iteration 10, Current loss 5.0713210105896 Accuracy 63.31548415885765\n",
      "Training:: Epoch 11, Iteration 20, Current loss 3.100390672683716 Accuracy 79.98162925541075\n",
      "Training:: Epoch 11, Iteration 30, Current loss 4.185771942138672 Accuracy 73.13340227507756\n",
      "Training:: Epoch 11, Iteration 40, Current loss 5.424829483032227 Accuracy 53.67817725356449\n",
      "Training:: Epoch 11, Iteration 50, Current loss 5.473266124725342 Accuracy 62.954476015887565\n",
      "Training:: Epoch 11, Iteration 60, Current loss 3.7783470153808594 Accuracy 78.60595475039604\n",
      "Training:: Epoch 11, Iteration 70, Current loss 3.3842875957489014 Accuracy 78.64332194185666\n",
      "Training:: Epoch 11, Iteration 80, Current loss 3.047518253326416 Accuracy 81.88976377952756\n",
      "Training:: Epoch 11, Iteration 90, Current loss 3.6086313724517822 Accuracy 75.86923311946227\n",
      "Training:: Epoch 11, Iteration 100, Current loss 3.3075003623962402 Accuracy 81.1363425137086\n",
      "Training:: Epoch 11, Iteration 110, Current loss 3.712340831756592 Accuracy 76.92076830732293\n",
      "Training:: Epoch 11, Iteration 120, Current loss 4.583146095275879 Accuracy 63.857430296062084\n",
      "Training:: Epoch 11, Iteration 130, Current loss 3.7268621921539307 Accuracy 70.75766174801362\n",
      "Training:: Epoch 11, Iteration 140, Current loss 4.814743995666504 Accuracy 61.45710928319624\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 11, Probability Accuracy 60.50080392682305\n",
      "Starting Training\n",
      "Training:: Epoch 12, Iteration 0, Current loss 4.1617255210876465 Accuracy 72.74287860093342\n",
      "Training:: Epoch 12, Iteration 10, Current loss 3.5017950534820557 Accuracy 74.33783108445778\n",
      "Training:: Epoch 12, Iteration 20, Current loss 2.9906444549560547 Accuracy 80.70140872820411\n",
      "Training:: Epoch 12, Iteration 30, Current loss 3.5905697345733643 Accuracy 75.70820126782884\n",
      "Training:: Epoch 12, Iteration 40, Current loss 3.780864715576172 Accuracy 73.87453874538745\n",
      "Training:: Epoch 12, Iteration 50, Current loss 3.977693557739258 Accuracy 70.3256760686246\n",
      "Training:: Epoch 12, Iteration 60, Current loss 2.8064136505126953 Accuracy 80.44436052366567\n",
      "Training:: Epoch 12, Iteration 70, Current loss 5.823517322540283 Accuracy 58.93650974581101\n",
      "Training:: Epoch 12, Iteration 80, Current loss 4.183723449707031 Accuracy 63.869179600886916\n",
      "Training:: Epoch 12, Iteration 90, Current loss 3.1977388858795166 Accuracy 76.48126177681624\n",
      "Training:: Epoch 12, Iteration 100, Current loss 4.19877290725708 Accuracy 74.6004936988437\n",
      "Training:: Epoch 12, Iteration 110, Current loss 3.078953504562378 Accuracy 80.13904982618772\n",
      "Training:: Epoch 12, Iteration 120, Current loss 3.767888069152832 Accuracy 73.28046609483735\n",
      "Training:: Epoch 12, Iteration 130, Current loss 3.4915878772735596 Accuracy 73.08554572271386\n",
      "Training:: Epoch 12, Iteration 140, Current loss 4.051002502441406 Accuracy 69.30213380063911\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 12, Probability Accuracy 59.149569605030095\n",
      "Starting Training\n",
      "Training:: Epoch 13, Iteration 0, Current loss 4.525183200836182 Accuracy 70.77450166112956\n",
      "Training:: Epoch 13, Iteration 10, Current loss 4.450504779815674 Accuracy 67.30254167805411\n",
      "Training:: Epoch 13, Iteration 20, Current loss 3.5345301628112793 Accuracy 74.72202674173117\n",
      "Training:: Epoch 13, Iteration 30, Current loss 2.8412129878997803 Accuracy 78.40314764166787\n",
      "Training:: Epoch 13, Iteration 40, Current loss 3.689225196838379 Accuracy 80.52445638652536\n",
      "Training:: Epoch 13, Iteration 50, Current loss 4.0437445640563965 Accuracy 71.40478934562684\n",
      "Training:: Epoch 13, Iteration 60, Current loss 2.896089553833008 Accuracy 84.02085569697898\n",
      "Training:: Epoch 13, Iteration 70, Current loss 3.815103054046631 Accuracy 78.7295382360127\n",
      "Training:: Epoch 13, Iteration 80, Current loss 5.644371509552002 Accuracy 66.8675799086758\n",
      "Training:: Epoch 13, Iteration 90, Current loss 3.5535411834716797 Accuracy 76.98515369925461\n",
      "Training:: Epoch 13, Iteration 100, Current loss 3.70647931098938 Accuracy 74.13590552154825\n",
      "Training:: Epoch 13, Iteration 110, Current loss 2.430391550064087 Accuracy 85.51707580577396\n",
      "Training:: Epoch 13, Iteration 120, Current loss 2.9426512718200684 Accuracy 82.3015873015873\n",
      "Training:: Epoch 13, Iteration 130, Current loss 4.318094730377197 Accuracy 68.99535330876462\n",
      "Training:: Epoch 13, Iteration 140, Current loss 3.0386645793914795 Accuracy 84.32523051131601\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 13, Probability Accuracy 62.16388669206988\n",
      "Starting Training\n",
      "Training:: Epoch 14, Iteration 0, Current loss 2.947679281234741 Accuracy 77.48729565909508\n",
      "Training:: Epoch 14, Iteration 10, Current loss 4.006710529327393 Accuracy 71.93976683937824\n",
      "Training:: Epoch 14, Iteration 20, Current loss 2.6984801292419434 Accuracy 84.70680295397858\n",
      "Training:: Epoch 14, Iteration 30, Current loss 3.515960216522217 Accuracy 75.42215176786821\n",
      "Training:: Epoch 14, Iteration 40, Current loss 2.797264575958252 Accuracy 82.84562485179038\n",
      "Training:: Epoch 14, Iteration 50, Current loss 4.071300029754639 Accuracy 72.64508603091281\n",
      "Training:: Epoch 14, Iteration 60, Current loss 3.363867998123169 Accuracy 79.96574755951362\n",
      "Training:: Epoch 14, Iteration 70, Current loss 3.862638473510742 Accuracy 76.30577128064566\n",
      "Training:: Epoch 14, Iteration 80, Current loss 3.2265453338623047 Accuracy 80.9741110558894\n",
      "Training:: Epoch 14, Iteration 90, Current loss 2.878929615020752 Accuracy 81.0116663764583\n",
      "Training:: Epoch 14, Iteration 100, Current loss 3.136019229888916 Accuracy 81.83834401182848\n",
      "Training:: Epoch 14, Iteration 110, Current loss 3.32340407371521 Accuracy 82.14285714285714\n",
      "Training:: Epoch 14, Iteration 120, Current loss 3.5370395183563232 Accuracy 76.5091082271699\n",
      "Training:: Epoch 14, Iteration 130, Current loss 3.8631229400634766 Accuracy 73.4197622086384\n",
      "Training:: Epoch 14, Iteration 140, Current loss 2.272655963897705 Accuracy 86.82072117826307\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 14, Probability Accuracy 63.689060059216075\n",
      "Starting Training\n",
      "Training:: Epoch 15, Iteration 0, Current loss 3.397399425506592 Accuracy 78.04284323271665\n",
      "Training:: Epoch 15, Iteration 10, Current loss 2.54371976852417 Accuracy 84.68897174254317\n",
      "Training:: Epoch 15, Iteration 20, Current loss 3.5330491065979004 Accuracy 75.47565020073311\n",
      "Training:: Epoch 15, Iteration 30, Current loss 2.366562604904175 Accuracy 87.427841393016\n",
      "Training:: Epoch 15, Iteration 40, Current loss 3.15985107421875 Accuracy 76.71342685370742\n",
      "Training:: Epoch 15, Iteration 50, Current loss 2.464493751525879 Accuracy 89.32016536518144\n",
      "Training:: Epoch 15, Iteration 60, Current loss 3.474365711212158 Accuracy 79.57186544342508\n",
      "Training:: Epoch 15, Iteration 70, Current loss 2.9898221492767334 Accuracy 80.31184547358622\n",
      "Training:: Epoch 15, Iteration 80, Current loss 2.8179805278778076 Accuracy 82.67745113288744\n",
      "Training:: Epoch 15, Iteration 90, Current loss 3.481236219406128 Accuracy 75.37779850746269\n",
      "Training:: Epoch 15, Iteration 100, Current loss 3.9927163124084473 Accuracy 79.1825735459241\n",
      "Training:: Epoch 15, Iteration 110, Current loss 3.4068822860717773 Accuracy 79.76059740830222\n",
      "Training:: Epoch 15, Iteration 120, Current loss 2.972238063812256 Accuracy 79.25553968765836\n",
      "Training:: Epoch 15, Iteration 130, Current loss 2.5413029193878174 Accuracy 85.2081316553727\n",
      "Training:: Epoch 15, Iteration 140, Current loss 3.302353858947754 Accuracy 81.14487464297049\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 66.17240962359232\n",
      "Starting Training\n",
      "Training:: Epoch 16, Iteration 0, Current loss 2.26847243309021 Accuracy 88.6439545758183\n",
      "Training:: Epoch 16, Iteration 10, Current loss 2.836221218109131 Accuracy 85.42094455852155\n",
      "Training:: Epoch 16, Iteration 20, Current loss 2.2311956882476807 Accuracy 86.86131386861314\n",
      "Training:: Epoch 16, Iteration 30, Current loss 2.23732590675354 Accuracy 88.51656905626807\n",
      "Training:: Epoch 16, Iteration 40, Current loss 2.238842725753784 Accuracy 88.06117414898866\n",
      "Training:: Epoch 16, Iteration 50, Current loss 3.8586857318878174 Accuracy 73.29320722269992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 16, Iteration 60, Current loss 3.0693767070770264 Accuracy 85.24846190250828\n",
      "Training:: Epoch 16, Iteration 70, Current loss 3.2262818813323975 Accuracy 78.56136505948653\n",
      "Training:: Epoch 16, Iteration 80, Current loss 2.7120676040649414 Accuracy 84.63367350671203\n",
      "Training:: Epoch 16, Iteration 90, Current loss 2.879209518432617 Accuracy 83.92449200432848\n",
      "Training:: Epoch 16, Iteration 100, Current loss 2.5752837657928467 Accuracy 86.71246994160083\n",
      "Training:: Epoch 16, Iteration 110, Current loss 2.8540892601013184 Accuracy 81.10187227784377\n",
      "Training:: Epoch 16, Iteration 120, Current loss 2.4854822158813477 Accuracy 84.22286666255322\n",
      "Training:: Epoch 16, Iteration 130, Current loss 2.413097858428955 Accuracy 85.86827296504715\n",
      "Training:: Epoch 16, Iteration 140, Current loss 2.400545358657837 Accuracy 86.54295194789235\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 16, Probability Accuracy 61.9752416682462\n",
      "Starting Training\n",
      "Training:: Epoch 17, Iteration 0, Current loss 2.4926271438598633 Accuracy 87.67154884096414\n",
      "Training:: Epoch 17, Iteration 10, Current loss 2.7522594928741455 Accuracy 90.10416666666667\n",
      "Training:: Epoch 17, Iteration 20, Current loss 2.4881093502044678 Accuracy 87.9325259515571\n",
      "Training:: Epoch 17, Iteration 30, Current loss 1.841248631477356 Accuracy 89.93989828941285\n",
      "Training:: Epoch 17, Iteration 40, Current loss 2.789036273956299 Accuracy 81.81236673773988\n",
      "Training:: Epoch 17, Iteration 50, Current loss 4.002374172210693 Accuracy 72.16556688662267\n",
      "Training:: Epoch 17, Iteration 60, Current loss 3.489389419555664 Accuracy 78.04423879923259\n",
      "Training:: Epoch 17, Iteration 70, Current loss 2.857424020767212 Accuracy 83.27002477291495\n",
      "Training:: Epoch 17, Iteration 80, Current loss 3.431720495223999 Accuracy 81.94608527548061\n",
      "Training:: Epoch 17, Iteration 90, Current loss 2.1788485050201416 Accuracy 85.9888769636062\n",
      "Training:: Epoch 17, Iteration 100, Current loss 2.4762001037597656 Accuracy 86.58280922431865\n",
      "Training:: Epoch 17, Iteration 110, Current loss 2.689673662185669 Accuracy 81.28302684421898\n",
      "Training:: Epoch 17, Iteration 120, Current loss 2.7222251892089844 Accuracy 81.1418757592259\n",
      "Training:: Epoch 17, Iteration 130, Current loss 2.282818555831909 Accuracy 86.23420537462182\n",
      "Training:: Epoch 17, Iteration 140, Current loss 2.6669681072235107 Accuracy 84.1221098265896\n",
      "Calculating Validation Data Accuracy\n",
      "Training:: Epoch 18, Iteration 110, Current loss 1.6952592134475708 Accuracy 93.79574468085106\n",
      "Training:: Epoch 18, Iteration 120, Current loss 2.3027126789093018 Accuracy 85.16734279918865\n",
      "Training:: Epoch 18, Iteration 130, Current loss 2.1833550930023193 Accuracy 88.20004699616199\n",
      "Training:: Epoch 18, Iteration 140, Current loss 2.246669054031372 Accuracy 87.52973515885182\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 18, Probability Accuracy 66.0910365427225\n",
      "Starting Training\n",
      "Training:: Epoch 19, Iteration 0, Current loss 2.7161874771118164 Accuracy 83.56235148259118\n",
      "Training:: Epoch 19, Iteration 10, Current loss 2.1080169677734375 Accuracy 87.75091089764823\n",
      "Training:: Epoch 19, Iteration 20, Current loss 1.6518741846084595 Accuracy 90.44495884773663\n",
      "Training:: Epoch 19, Iteration 30, Current loss 2.2740707397460938 Accuracy 89.02063078017548\n",
      "Training:: Epoch 19, Iteration 40, Current loss 2.7972989082336426 Accuracy 86.04524095727243\n",
      "Training:: Epoch 19, Iteration 50, Current loss 2.423415422439575 Accuracy 87.01941747572816\n",
      "Training:: Epoch 19, Iteration 60, Current loss 2.5198721885681152 Accuracy 86.33918543476976\n",
      "Training:: Epoch 19, Iteration 70, Current loss 2.4481987953186035 Accuracy 86.53484923381117\n",
      "Training:: Epoch 19, Iteration 80, Current loss 2.124154567718506 Accuracy 90.69791885642212\n",
      "Training:: Epoch 19, Iteration 90, Current loss 2.338486909866333 Accuracy 87.52953531110528\n",
      "Training:: Epoch 19, Iteration 100, Current loss 2.520429849624634 Accuracy 85.38793990533031\n",
      "Training:: Epoch 19, Iteration 110, Current loss 2.9849917888641357 Accuracy 80.28739386022208\n",
      "Training:: Epoch 19, Iteration 120, Current loss 1.9900015592575073 Accuracy 91.96521739130435\n",
      "Training:: Epoch 19, Iteration 130, Current loss 2.708515167236328 Accuracy 81.69578126078851\n",
      "Training:: Epoch 19, Iteration 140, Current loss 2.2095251083374023 Accuracy 86.95297080242293\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 19, Probability Accuracy 61.38308093516951\n",
      "Starting Training\n",
      "Training:: Epoch 20, Iteration 0, Current loss 1.969892144203186 Accuracy 91.28397375820056\n",
      "Training:: Epoch 20, Iteration 10, Current loss 2.1094329357147217 Accuracy 89.97112394491337\n",
      "Training:: Epoch 20, Iteration 20, Current loss 1.9737904071807861 Accuracy 90.0186940846575\n",
      "Training:: Epoch 20, Iteration 30, Current loss 1.7081042528152466 Accuracy 94.51455647182104\n",
      "Training:: Epoch 20, Iteration 40, Current loss 2.7773993015289307 Accuracy 85.1667939903234\n",
      "Training:: Epoch 20, Iteration 50, Current loss 1.9756683111190796 Accuracy 92.93764087152518\n",
      "Training:: Epoch 20, Iteration 60, Current loss 2.753455877304077 Accuracy 82.97305764411027\n",
      "Training:: Epoch 20, Iteration 70, Current loss 3.0896973609924316 Accuracy 84.19499341238472\n",
      "Training:: Epoch 20, Iteration 80, Current loss 1.728918194770813 Accuracy 92.52089749230092\n",
      "Training:: Epoch 20, Iteration 90, Current loss 2.0178616046905518 Accuracy 90.30311876611687\n",
      "Training:: Epoch 20, Iteration 100, Current loss 1.936936378479004 Accuracy 90.98057354301572\n",
      "Training:: Epoch 20, Iteration 110, Current loss 3.1887199878692627 Accuracy 79.3960536356699\n",
      "Training:: Epoch 20, Iteration 120, Current loss 1.9003716707229614 Accuracy 90.30403537866225\n",
      "Training:: Epoch 20, Iteration 130, Current loss 2.7371435165405273 Accuracy 87.30819458047665\n",
      "Training:: Epoch 20, Iteration 140, Current loss 2.7459492683410645 Accuracy 84.27355676434462\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 20, Probability Accuracy 63.44003882378316\n",
      "Starting Training\n",
      "Training:: Epoch 21, Iteration 0, Current loss 3.004971504211426 Accuracy 82.10903506526746\n",
      "Training:: Epoch 21, Iteration 10, Current loss 1.8006248474121094 Accuracy 89.60154738878143\n",
      "Training:: Epoch 21, Iteration 20, Current loss 1.8097288608551025 Accuracy 90.41369130729645\n",
      "Training:: Epoch 21, Iteration 30, Current loss 2.46706485748291 Accuracy 87.52982674551302\n",
      "Training:: Epoch 21, Iteration 40, Current loss 2.535191535949707 Accuracy 83.67120613937807\n",
      "Training:: Epoch 21, Iteration 50, Current loss 2.204773426055908 Accuracy 90.24139979478318\n",
      "Training:: Epoch 21, Iteration 60, Current loss 1.8231176137924194 Accuracy 91.02839417800048\n",
      "Training:: Epoch 21, Iteration 70, Current loss 2.2985503673553467 Accuracy 90.83917044362038\n",
      "Training:: Epoch 21, Iteration 80, Current loss 2.526153087615967 Accuracy 85.36560033236394\n",
      "Training:: Epoch 21, Iteration 90, Current loss 1.4828485250473022 Accuracy 94.6276759192871\n",
      "Training:: Epoch 21, Iteration 100, Current loss 2.816483736038208 Accuracy 82.38952840698106\n",
      "Training:: Epoch 21, Iteration 110, Current loss 2.4513275623321533 Accuracy 87.70919560914162\n",
      "Training:: Epoch 21, Iteration 120, Current loss 3.7241203784942627 Accuracy 79.59918332258758\n",
      "Training:: Epoch 21, Iteration 130, Current loss 3.4967381954193115 Accuracy 80.96742986133505\n",
      "Training:: Epoch 21, Iteration 140, Current loss 2.2911016941070557 Accuracy 89.0413981816577\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 21, Probability Accuracy 63.82108706592854\n",
      "Starting Training\n",
      "Training:: Epoch 22, Iteration 0, Current loss 2.0919482707977295 Accuracy 89.720367278798\n",
      "Training:: Epoch 22, Iteration 10, Current loss 2.4868245124816895 Accuracy 89.42749513598542\n",
      "Training:: Epoch 22, Iteration 20, Current loss 2.3965554237365723 Accuracy 89.0237295481634\n",
      "Training:: Epoch 22, Iteration 30, Current loss 3.4638073444366455 Accuracy 79.66044901150453\n",
      "Training:: Epoch 22, Iteration 40, Current loss 2.023700475692749 Accuracy 89.65445570277994\n",
      "Training:: Epoch 22, Iteration 50, Current loss 2.135998249053955 Accuracy 90.4797703663204\n",
      "Training:: Epoch 22, Iteration 60, Current loss 2.6612095832824707 Accuracy 87.23348479552604\n",
      "Training:: Epoch 22, Iteration 70, Current loss 1.8538827896118164 Accuracy 90.63654371219016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 22, Iteration 80, Current loss 1.5669827461242676 Accuracy 92.79663562631421\n",
      "Training:: Epoch 22, Iteration 90, Current loss 3.738039016723633 Accuracy 77.42814507234154\n",
      "Training:: Epoch 22, Iteration 100, Current loss 2.78122615814209 Accuracy 84.03939346568704\n",
      "Training:: Epoch 22, Iteration 110, Current loss 1.9842180013656616 Accuracy 90.67087225524688\n",
      "Training:: Epoch 22, Iteration 120, Current loss 2.202498197555542 Accuracy 87.1570209822458\n",
      "Training:: Epoch 22, Iteration 130, Current loss 1.6371365785598755 Accuracy 92.35311840915938\n",
      "Training:: Epoch 22, Iteration 140, Current loss 2.7976722717285156 Accuracy 84.0042002100105\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 22, Probability Accuracy 65.80909025549187\n",
      "Starting Training\n",
      "Training:: Epoch 23, Iteration 0, Current loss 2.2027781009674072 Accuracy 89.46955583880772\n",
      "Training:: Epoch 23, Iteration 10, Current loss 2.207719564437866 Accuracy 91.59157028311928\n",
      "Training:: Epoch 23, Iteration 20, Current loss 2.8096516132354736 Accuracy 86.56670933138295\n",
      "Training:: Epoch 23, Iteration 30, Current loss 2.5555992126464844 Accuracy 86.23979767789402\n",
      "Training:: Epoch 23, Iteration 40, Current loss 2.2361371517181396 Accuracy 89.37094288516322\n",
      "Training:: Epoch 23, Iteration 50, Current loss 1.6139700412750244 Accuracy 93.53472016269119\n",
      "Training:: Epoch 23, Iteration 60, Current loss 2.353905439376831 Accuracy 89.09406709936923\n",
      "Training:: Epoch 23, Iteration 70, Current loss 2.614084243774414 Accuracy 86.26832176169437\n",
      "Training:: Epoch 23, Iteration 80, Current loss 2.2455506324768066 Accuracy 90.25890246000344\n",
      "Training:: Epoch 23, Iteration 90, Current loss 2.089632987976074 Accuracy 90.15587636170402\n",
      "Training:: Epoch 23, Iteration 100, Current loss 2.2533516883850098 Accuracy 89.09375257222817\n",
      "Training:: Epoch 23, Iteration 110, Current loss 1.7467540502548218 Accuracy 91.52363348296805\n",
      "Training:: Epoch 23, Iteration 120, Current loss 2.3968539237976074 Accuracy 86.35184932441655\n",
      "Training:: Epoch 23, Iteration 130, Current loss 2.6966733932495117 Accuracy 87.20969533098354\n",
      "Training:: Epoch 23, Iteration 140, Current loss 2.528623342514038 Accuracy 86.62700676391492\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 23, Probability Accuracy 66.58246132327662\n",
      "Starting Training\n",
      "Training:: Epoch 24, Iteration 0, Current loss 1.9572389125823975 Accuracy 88.27555256367104\n",
      "Training:: Epoch 24, Iteration 10, Current loss 1.7582780122756958 Accuracy 92.17903415783275\n",
      "Training:: Epoch 24, Iteration 20, Current loss 2.1912498474121094 Accuracy 86.07486523250542\n",
      "Training:: Epoch 24, Iteration 30, Current loss 1.9008234739303589 Accuracy 91.1391921814156\n",
      "Training:: Epoch 24, Iteration 40, Current loss 1.4845601320266724 Accuracy 91.91673425250067\n",
      "Training:: Epoch 24, Iteration 50, Current loss 1.7669321298599243 Accuracy 91.47764095917044\n",
      "Training:: Epoch 24, Iteration 60, Current loss 2.2231359481811523 Accuracy 91.6684446340943\n",
      "Training:: Epoch 24, Iteration 70, Current loss 2.7293477058410645 Accuracy 87.75827313959348\n",
      "Training:: Epoch 24, Iteration 80, Current loss 2.7648532390594482 Accuracy 87.31032313861152\n",
      "Training:: Epoch 24, Iteration 90, Current loss 1.722917914390564 Accuracy 91.99130955948456\n",
      "Training:: Epoch 24, Iteration 100, Current loss 1.8508533239364624 Accuracy 92.45901639344262\n",
      "Training:: Epoch 24, Iteration 110, Current loss 2.701939105987549 Accuracy 85.78998347672497\n",
      "Training:: Epoch 24, Iteration 120, Current loss 2.116896152496338 Accuracy 90.94139101999748\n",
      "Training:: Epoch 24, Iteration 130, Current loss 1.4700431823730469 Accuracy 94.58733747880159\n",
      "Training:: Epoch 24, Iteration 140, Current loss 2.057335376739502 Accuracy 89.43856085884231\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 24, Probability Accuracy 66.78475022712567\n",
      "Starting Training\n",
      "Training:: Epoch 25, Iteration 0, Current loss 1.7058513164520264 Accuracy 91.53220987211665\n",
      "Training:: Epoch 25, Iteration 10, Current loss 2.020214319229126 Accuracy 91.31701101301539\n",
      "Training:: Epoch 25, Iteration 20, Current loss 1.8261979818344116 Accuracy 90.92963371695832\n",
      "Training:: Epoch 25, Iteration 30, Current loss 1.5032901763916016 Accuracy 94.89563959495763\n",
      "Training:: Epoch 25, Iteration 40, Current loss 1.286867380142212 Accuracy 95.88258808748786\n",
      "Training:: Epoch 25, Iteration 50, Current loss 2.5758228302001953 Accuracy 86.3624124932687\n",
      "Training:: Epoch 25, Iteration 60, Current loss 1.8397940397262573 Accuracy 93.59088217880402\n",
      "Training:: Epoch 25, Iteration 70, Current loss 1.500321388244629 Accuracy 93.41039662213609\n",
      "Training:: Epoch 25, Iteration 80, Current loss 2.1352779865264893 Accuracy 89.86898150644309\n",
      "Training:: Epoch 25, Iteration 90, Current loss 2.260112762451172 Accuracy 87.43467933491686\n",
      "Training:: Epoch 25, Iteration 100, Current loss 2.5541911125183105 Accuracy 88.73490669593853\n",
      "Training:: Epoch 25, Iteration 110, Current loss 1.7518892288208008 Accuracy 93.68301396557324\n",
      "Training:: Epoch 25, Iteration 120, Current loss 1.861907958984375 Accuracy 93.96004922843889\n",
      "Training:: Epoch 25, Iteration 130, Current loss 2.1514475345611572 Accuracy 91.2033779028853\n",
      "Training:: Epoch 25, Iteration 140, Current loss 1.6106398105621338 Accuracy 91.42217245240761\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 25, Probability Accuracy 64.93400283661985\n",
      "Starting Training\n",
      "Training:: Epoch 26, Iteration 0, Current loss 2.2642178535461426 Accuracy 93.06453508875045\n",
      "Training:: Epoch 26, Iteration 10, Current loss 1.4530130624771118 Accuracy 94.52861471613973\n",
      "Training:: Epoch 26, Iteration 20, Current loss 1.385915756225586 Accuracy 94.91978609625669\n",
      "Training:: Epoch 26, Iteration 30, Current loss 2.2187414169311523 Accuracy 86.4290909090909\n",
      "Training:: Epoch 26, Iteration 40, Current loss 2.228046417236328 Accuracy 86.71906489318823\n",
      "Training:: Epoch 26, Iteration 50, Current loss 1.6033670902252197 Accuracy 94.73422133698493\n",
      "Training:: Epoch 26, Iteration 60, Current loss 2.2172632217407227 Accuracy 87.78484910695956\n",
      "Training:: Epoch 26, Iteration 70, Current loss 6.147192478179932 Accuracy 63.659250585480095\n",
      "Training:: Epoch 26, Iteration 80, Current loss 4.34248161315918 Accuracy 77.52326934264107\n",
      "Training:: Epoch 26, Iteration 90, Current loss 3.9152894020080566 Accuracy 71.69971191323505\n",
      "Training:: Epoch 26, Iteration 100, Current loss 3.946908950805664 Accuracy 74.46971633018144\n",
      "Training:: Epoch 26, Iteration 110, Current loss 3.0095059871673584 Accuracy 82.26953740392419\n",
      "Training:: Epoch 26, Iteration 120, Current loss 3.3673510551452637 Accuracy 79.23196212863206\n",
      "Training:: Epoch 26, Iteration 130, Current loss 3.3666915893554688 Accuracy 81.5805637070907\n",
      "Training:: Epoch 26, Iteration 140, Current loss 3.4577713012695312 Accuracy 81.85253987572865\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 26, Probability Accuracy 58.04988921496219\n",
      "Starting Training\n",
      "Training:: Epoch 27, Iteration 0, Current loss 2.944603204727173 Accuracy 82.04329846752614\n",
      "Training:: Epoch 27, Iteration 10, Current loss 2.857962131500244 Accuracy 81.70947529535562\n",
      "Training:: Epoch 27, Iteration 20, Current loss 2.321477174758911 Accuracy 87.56150364101555\n",
      "Training:: Epoch 27, Iteration 30, Current loss 2.360668659210205 Accuracy 89.11959453936075\n",
      "Training:: Epoch 27, Iteration 40, Current loss 1.6925753355026245 Accuracy 93.48114563750967\n",
      "Training:: Epoch 27, Iteration 50, Current loss 3.4702277183532715 Accuracy 78.47860538827258\n",
      "Training:: Epoch 27, Iteration 60, Current loss 4.010103225708008 Accuracy 73.61320322221495\n",
      "Training:: Epoch 27, Iteration 70, Current loss 2.9249556064605713 Accuracy 83.56350184956844\n",
      "Training:: Epoch 27, Iteration 80, Current loss 3.8167126178741455 Accuracy 77.00877083593133\n",
      "Training:: Epoch 27, Iteration 90, Current loss 4.136462211608887 Accuracy 72.98374186203706\n",
      "Training:: Epoch 27, Iteration 100, Current loss 2.2405266761779785 Accuracy 90.3915681107911\n",
      "Training:: Epoch 27, Iteration 110, Current loss 2.2153303623199463 Accuracy 90.7072329569613\n",
      "Training:: Epoch 27, Iteration 120, Current loss 1.9581736326217651 Accuracy 90.42178507001117\n",
      "Training:: Epoch 27, Iteration 130, Current loss 2.2169246673583984 Accuracy 89.74096492789745\n",
      "Training:: Epoch 27, Iteration 140, Current loss 2.2558505535125732 Accuracy 92.87296154620495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 27, Probability Accuracy 67.98099987581618\n",
      "Starting Training\n",
      "Training:: Epoch 28, Iteration 0, Current loss 1.6177830696105957 Accuracy 93.42869112454366\n",
      "Training:: Epoch 28, Iteration 10, Current loss 1.8444932699203491 Accuracy 92.82852564102564\n",
      "Training:: Epoch 28, Iteration 20, Current loss 1.794857382774353 Accuracy 90.14645902855024\n",
      "Training:: Epoch 28, Iteration 30, Current loss 1.7891204357147217 Accuracy 94.06193675241684\n",
      "Training:: Epoch 28, Iteration 40, Current loss 2.087045907974243 Accuracy 89.14756446991404\n",
      "Training:: Epoch 28, Iteration 50, Current loss 1.805161476135254 Accuracy 93.3851590106007\n",
      "Training:: Epoch 28, Iteration 60, Current loss 1.9109272956848145 Accuracy 92.89296948057468\n",
      "Training:: Epoch 28, Iteration 70, Current loss 2.6215109825134277 Accuracy 85.70546024806349\n",
      "Training:: Epoch 28, Iteration 80, Current loss 1.9363934993743896 Accuracy 92.6915545448803\n",
      "Training:: Epoch 28, Iteration 90, Current loss 1.5839765071868896 Accuracy 94.91099071207431\n",
      "Training:: Epoch 28, Iteration 100, Current loss 1.5537583827972412 Accuracy 95.13147318865542\n",
      "Training:: Epoch 28, Iteration 110, Current loss 2.317869186401367 Accuracy 93.44244043642841\n",
      "Training:: Epoch 28, Iteration 120, Current loss 1.3007415533065796 Accuracy 95.50944400106411\n",
      "Training:: Epoch 28, Iteration 130, Current loss 1.6162278652191162 Accuracy 95.48436527318871\n",
      "Training:: Epoch 28, Iteration 140, Current loss 1.9710705280303955 Accuracy 89.04729956390473\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 28, Probability Accuracy 65.6496120889679\n",
      "Starting Training\n",
      "Training:: Epoch 29, Iteration 0, Current loss 1.3149250745773315 Accuracy 95.3586918590159\n",
      "Training:: Epoch 29, Iteration 10, Current loss 1.696763277053833 Accuracy 93.09567271555889\n",
      "Training:: Epoch 29, Iteration 20, Current loss 1.5911598205566406 Accuracy 93.1048951048951\n",
      "Training:: Epoch 29, Iteration 30, Current loss 1.2280737161636353 Accuracy 96.72986606366325\n",
      "Training:: Epoch 29, Iteration 40, Current loss 2.0474815368652344 Accuracy 90.4705293455137\n",
      "Training:: Epoch 29, Iteration 50, Current loss 1.3188838958740234 Accuracy 95.34703172713628\n",
      "Training:: Epoch 29, Iteration 60, Current loss 1.3926111459732056 Accuracy 95.84761758021648\n",
      "Training:: Epoch 29, Iteration 70, Current loss 1.3574928045272827 Accuracy 96.24173529752929\n",
      "Training:: Epoch 29, Iteration 80, Current loss 1.4168312549591064 Accuracy 94.81631472374892\n",
      "Training:: Epoch 29, Iteration 90, Current loss 1.9493334293365479 Accuracy 92.58841234010534\n",
      "Training:: Epoch 29, Iteration 100, Current loss 1.2592631578445435 Accuracy 95.16975308641975\n",
      "Training:: Epoch 29, Iteration 110, Current loss 1.262626051902771 Accuracy 97.2323127486594\n",
      "Training:: Epoch 29, Iteration 120, Current loss 1.7729696035385132 Accuracy 91.15657761244505\n",
      "Training:: Epoch 29, Iteration 130, Current loss 1.5081098079681396 Accuracy 96.27489072365226\n",
      "Training:: Epoch 29, Iteration 140, Current loss 1.629148244857788 Accuracy 94.69012178619757\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 29, Probability Accuracy 68.43860090588828\n",
      "Starting Training\n",
      "Training:: Epoch 30, Iteration 0, Current loss 1.1113728284835815 Accuracy 96.70423834948257\n",
      "Training:: Epoch 30, Iteration 10, Current loss 1.3343490362167358 Accuracy 94.79895561357702\n",
      "Training:: Epoch 30, Iteration 20, Current loss 1.081197738647461 Accuracy 96.73403395311237\n",
      "Training:: Epoch 30, Iteration 30, Current loss 1.6409475803375244 Accuracy 94.37878637401927\n",
      "Training:: Epoch 30, Iteration 40, Current loss 1.6859500408172607 Accuracy 94.69481661929896\n",
      "Training:: Epoch 30, Iteration 50, Current loss 1.161436676979065 Accuracy 96.16360177552315\n",
      "Training:: Epoch 30, Iteration 60, Current loss 1.5415163040161133 Accuracy 93.84674624566807\n",
      "Training:: Epoch 30, Iteration 70, Current loss 1.543095350265503 Accuracy 92.96467431808018\n",
      "Training:: Epoch 30, Iteration 80, Current loss 1.38485848903656 Accuracy 93.99198931909213\n",
      "Training:: Epoch 30, Iteration 90, Current loss 1.308359980583191 Accuracy 96.18656642595707\n",
      "Training:: Epoch 30, Iteration 100, Current loss 1.5899940729141235 Accuracy 93.66567573747999\n",
      "Training:: Epoch 30, Iteration 110, Current loss 1.7636948823928833 Accuracy 92.83290762837863\n",
      "Training:: Epoch 30, Iteration 120, Current loss 1.9405038356781006 Accuracy 91.47653194263364\n",
      "Training:: Epoch 30, Iteration 130, Current loss 1.8368809223175049 Accuracy 92.53956292388847\n",
      "Training:: Epoch 30, Iteration 140, Current loss 1.2590030431747437 Accuracy 95.99647429949898\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 30, Probability Accuracy 67.19815162190602\n",
      "Starting Training\n",
      "Training:: Epoch 31, Iteration 0, Current loss 1.1325308084487915 Accuracy 96.40990542558487\n",
      "Training:: Epoch 31, Iteration 10, Current loss 1.5775465965270996 Accuracy 93.34205281514699\n",
      "Training:: Epoch 31, Iteration 20, Current loss 1.5101460218429565 Accuracy 95.40436360914636\n",
      "Training:: Epoch 31, Iteration 30, Current loss 1.234800100326538 Accuracy 96.10284167794316\n",
      "Training:: Epoch 31, Iteration 40, Current loss 1.5803413391113281 Accuracy 95.00104144969798\n",
      "Training:: Epoch 31, Iteration 50, Current loss 1.0548412799835205 Accuracy 96.72502263809237\n",
      "Training:: Epoch 31, Iteration 60, Current loss 1.1637018918991089 Accuracy 95.55681408639242\n",
      "Training:: Epoch 31, Iteration 70, Current loss 1.6523699760437012 Accuracy 95.10701763566905\n",
      "Training:: Epoch 31, Iteration 80, Current loss 1.4037938117980957 Accuracy 95.50512074851436\n",
      "Training:: Epoch 31, Iteration 90, Current loss 1.122389793395996 Accuracy 96.55221440250232\n",
      "Training:: Epoch 31, Iteration 100, Current loss 1.5027682781219482 Accuracy 95.95952827660145\n",
      "Training:: Epoch 31, Iteration 110, Current loss 1.5054265260696411 Accuracy 94.84647430952013\n",
      "Training:: Epoch 31, Iteration 120, Current loss 1.3227863311767578 Accuracy 95.81878413465843\n",
      "Training:: Epoch 31, Iteration 130, Current loss 1.4082759618759155 Accuracy 95.02471169686986\n",
      "Training:: Epoch 31, Iteration 140, Current loss 1.487259030342102 Accuracy 94.6627246557224\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 31, Probability Accuracy 65.45532977339721\n",
      "Starting Training\n",
      "Training:: Epoch 32, Iteration 0, Current loss 0.9435107707977295 Accuracy 96.98642625607779\n",
      "Training:: Epoch 32, Iteration 10, Current loss 1.2803809642791748 Accuracy 95.59904365536173\n",
      "Training:: Epoch 32, Iteration 20, Current loss 1.3361717462539673 Accuracy 95.99301233759144\n",
      "Training:: Epoch 32, Iteration 30, Current loss 1.2971603870391846 Accuracy 95.73030256489018\n",
      "Training:: Epoch 32, Iteration 40, Current loss 1.760188102722168 Accuracy 93.32337393702598\n",
      "Training:: Epoch 32, Iteration 50, Current loss 1.5243728160858154 Accuracy 94.20438255171001\n",
      "Training:: Epoch 32, Iteration 60, Current loss 1.5650105476379395 Accuracy 94.29421057522535\n",
      "Training:: Epoch 32, Iteration 70, Current loss 1.3839739561080933 Accuracy 96.22252131000448\n",
      "Training:: Epoch 32, Iteration 80, Current loss 1.5866286754608154 Accuracy 96.72164842505524\n",
      "Training:: Epoch 32, Iteration 90, Current loss 1.8511956930160522 Accuracy 90.11524530786961\n",
      "Training:: Epoch 32, Iteration 100, Current loss 1.3278825283050537 Accuracy 95.49342791571041\n",
      "Training:: Epoch 32, Iteration 110, Current loss 1.430309772491455 Accuracy 95.39821474997834\n",
      "Training:: Epoch 32, Iteration 120, Current loss 2.062699556350708 Accuracy 90.86891757696127\n",
      "Training:: Epoch 32, Iteration 130, Current loss 1.590811014175415 Accuracy 93.05762842424534\n",
      "Training:: Epoch 32, Iteration 140, Current loss 1.411881923675537 Accuracy 94.47447020338501\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 32, Probability Accuracy 66.27845606834032\n",
      "Starting Training\n",
      "Training:: Epoch 33, Iteration 0, Current loss 1.3251383304595947 Accuracy 95.38811918770165\n",
      "Training:: Epoch 33, Iteration 10, Current loss 1.489493727684021 Accuracy 92.62288241675708\n",
      "Training:: Epoch 33, Iteration 20, Current loss 1.340810775756836 Accuracy 96.12315425699026\n",
      "Training:: Epoch 33, Iteration 30, Current loss 1.8272883892059326 Accuracy 91.64486203615604\n",
      "Training:: Epoch 33, Iteration 40, Current loss 1.7730892896652222 Accuracy 90.73710512225594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 33, Iteration 50, Current loss 1.4957717657089233 Accuracy 95.06887276876681\n",
      "Training:: Epoch 33, Iteration 60, Current loss 1.5947777032852173 Accuracy 95.03158528025669\n",
      "Training:: Epoch 33, Iteration 70, Current loss 3.2825164794921875 Accuracy 75.66985978064696\n",
      "Training:: Epoch 33, Iteration 80, Current loss 1.992293357849121 Accuracy 91.05211406096362\n",
      "Training:: Epoch 33, Iteration 90, Current loss 1.53650963306427 Accuracy 85.68959557552714\n",
      "Training:: Epoch 33, Iteration 100, Current loss 1.409597396850586 Accuracy 93.32681017612525\n",
      "Training:: Epoch 33, Iteration 110, Current loss 1.9070916175842285 Accuracy 91.52780378206329\n",
      "Training:: Epoch 33, Iteration 120, Current loss 1.4237364530563354 Accuracy 95.94691535150646\n",
      "Training:: Epoch 33, Iteration 130, Current loss 1.7874016761779785 Accuracy 91.75364910383831\n",
      "Training:: Epoch 33, Iteration 140, Current loss 1.133253574371338 Accuracy 94.2188165176671\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 33, Probability Accuracy 67.04096105203303\n",
      "Starting Training\n",
      "Training:: Epoch 34, Iteration 0, Current loss 1.8469398021697998 Accuracy 89.86080586080585\n",
      "Training:: Epoch 34, Iteration 10, Current loss 1.99239981174469 Accuracy 92.98592087660063\n",
      "Training:: Epoch 34, Iteration 20, Current loss 1.7451443672180176 Accuracy 92.7626720375965\n",
      "Training:: Epoch 34, Iteration 30, Current loss 1.1972132921218872 Accuracy 95.83986562150056\n",
      "Training:: Epoch 34, Iteration 40, Current loss 1.5143684148788452 Accuracy 95.92053919829726\n",
      "Training:: Epoch 34, Iteration 50, Current loss 1.479933261871338 Accuracy 95.15605058591484\n",
      "Training:: Epoch 34, Iteration 60, Current loss 1.3398585319519043 Accuracy 94.66780031580225\n",
      "Training:: Epoch 34, Iteration 70, Current loss 1.3285846710205078 Accuracy 94.02134890622756\n",
      "Training:: Epoch 34, Iteration 80, Current loss 1.8990511894226074 Accuracy 91.44005166104614\n",
      "Training:: Epoch 34, Iteration 90, Current loss 1.8921390771865845 Accuracy 91.14647300688846\n",
      "Training:: Epoch 34, Iteration 100, Current loss 1.71452796459198 Accuracy 90.9998228311581\n",
      "Training:: Epoch 34, Iteration 110, Current loss 1.4884133338928223 Accuracy 94.03723634564236\n",
      "Training:: Epoch 34, Iteration 120, Current loss 1.6994357109069824 Accuracy 91.62687886825817\n",
      "Training:: Epoch 34, Iteration 130, Current loss 1.3405945301055908 Accuracy 94.8093965395977\n",
      "Training:: Epoch 34, Iteration 140, Current loss 1.753074049949646 Accuracy 94.58841027401178\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 34, Probability Accuracy 67.07282400538566\n",
      "Starting Training\n",
      "Training:: Epoch 35, Iteration 0, Current loss 1.5807547569274902 Accuracy 91.56637490882568\n",
      "Training:: Epoch 35, Iteration 10, Current loss 1.3442680835723877 Accuracy 94.86162318570028\n",
      "Training:: Epoch 35, Iteration 20, Current loss 1.342183232307434 Accuracy 95.83453652902108\n",
      "Training:: Epoch 35, Iteration 30, Current loss 1.2354614734649658 Accuracy 96.56573116691285\n",
      "Training:: Epoch 35, Iteration 40, Current loss 1.1732971668243408 Accuracy 95.30667991060342\n",
      "Training:: Epoch 35, Iteration 50, Current loss 1.3868626356124878 Accuracy 96.09012078042738\n",
      "Training:: Epoch 35, Iteration 60, Current loss 1.4337449073791504 Accuracy 94.92645745250998\n",
      "Training:: Epoch 35, Iteration 70, Current loss 1.443061113357544 Accuracy 94.36043870465312\n",
      "Training:: Epoch 35, Iteration 80, Current loss 1.2500890493392944 Accuracy 96.1969411957144\n",
      "Training:: Epoch 35, Iteration 90, Current loss 1.1121485233306885 Accuracy 97.02628481757552\n",
      "Training:: Epoch 35, Iteration 100, Current loss 1.4922677278518677 Accuracy 94.18379216750678\n",
      "Training:: Epoch 35, Iteration 110, Current loss 1.4907071590423584 Accuracy 93.42290267145758\n",
      "Training:: Epoch 35, Iteration 120, Current loss 1.6391854286193848 Accuracy 92.81674685574407\n",
      "Training:: Epoch 35, Iteration 130, Current loss 1.2433483600616455 Accuracy 96.00317628374802\n",
      "Training:: Epoch 35, Iteration 140, Current loss 0.8957301378250122 Accuracy 97.20419086664388\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 35, Probability Accuracy 66.34340747325146\n",
      "Starting Training\n",
      "Training:: Epoch 36, Iteration 0, Current loss 0.937520444393158 Accuracy 97.23861222563134\n",
      "Training:: Epoch 36, Iteration 10, Current loss 1.314204454421997 Accuracy 94.94183631076028\n",
      "Training:: Epoch 36, Iteration 20, Current loss 1.1407676935195923 Accuracy 95.73966642494561\n",
      "Training:: Epoch 36, Iteration 30, Current loss 1.744522213935852 Accuracy 94.02111239244212\n",
      "Training:: Epoch 36, Iteration 40, Current loss 1.2533037662506104 Accuracy 96.50078210995886\n",
      "Training:: Epoch 36, Iteration 50, Current loss 1.1942875385284424 Accuracy 96.05386052303861\n",
      "Training:: Epoch 36, Iteration 60, Current loss 1.0893480777740479 Accuracy 97.16385826225643\n",
      "Training:: Epoch 36, Iteration 70, Current loss 1.317520022392273 Accuracy 95.60744422610102\n",
      "Training:: Epoch 36, Iteration 80, Current loss 1.0217232704162598 Accuracy 97.1533180778032\n",
      "Training:: Epoch 36, Iteration 90, Current loss 0.9641014933586121 Accuracy 97.12045423820467\n",
      "Training:: Epoch 36, Iteration 100, Current loss 1.4024361371994019 Accuracy 95.75150300601203\n",
      "Training:: Epoch 36, Iteration 110, Current loss 0.948868453502655 Accuracy 96.8240382016398\n",
      "Training:: Epoch 36, Iteration 120, Current loss 1.2740792036056519 Accuracy 95.576598780646\n",
      "Training:: Epoch 36, Iteration 130, Current loss 1.310370922088623 Accuracy 96.84047295987594\n",
      "Training:: Epoch 36, Iteration 140, Current loss 1.3906550407409668 Accuracy 94.99457406402604\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 36, Probability Accuracy 67.33630611964784\n",
      "Starting Training\n",
      "Training:: Epoch 37, Iteration 0, Current loss 1.5258381366729736 Accuracy 96.06100393899607\n",
      "Training:: Epoch 37, Iteration 10, Current loss 1.0971355438232422 Accuracy 96.6626545573914\n",
      "Training:: Epoch 37, Iteration 20, Current loss 1.04140305519104 Accuracy 97.11590458451\n",
      "Training:: Epoch 37, Iteration 30, Current loss 1.375661015510559 Accuracy 96.10720040709015\n",
      "Training:: Epoch 37, Iteration 40, Current loss 1.2746310234069824 Accuracy 93.86421319796955\n",
      "Training:: Epoch 37, Iteration 50, Current loss 1.098429560661316 Accuracy 97.16732987118095\n",
      "Training:: Epoch 37, Iteration 60, Current loss 1.641648292541504 Accuracy 92.11225314701467\n",
      "Training:: Epoch 37, Iteration 70, Current loss 1.0179632902145386 Accuracy 97.57035981474884\n",
      "Training:: Epoch 37, Iteration 80, Current loss 1.6189637184143066 Accuracy 94.90251927515592\n",
      "Training:: Epoch 37, Iteration 90, Current loss 1.0105713605880737 Accuracy 97.85823559408465\n",
      "Training:: Epoch 37, Iteration 100, Current loss 1.1313011646270752 Accuracy 96.57142857142857\n",
      "Training:: Epoch 37, Iteration 110, Current loss 1.114310622215271 Accuracy 96.89818468823994\n",
      "Training:: Epoch 37, Iteration 120, Current loss 1.1862961053848267 Accuracy 97.00757230179728\n",
      "Training:: Epoch 37, Iteration 130, Current loss 1.041479229927063 Accuracy 95.59284685142809\n",
      "Training:: Epoch 37, Iteration 140, Current loss 0.9877094626426697 Accuracy 97.18007513294629\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 37, Probability Accuracy 67.28802149033653\n",
      "Starting Training\n",
      "Training:: Epoch 38, Iteration 0, Current loss 1.0626858472824097 Accuracy 96.82170951903278\n",
      "Training:: Epoch 38, Iteration 10, Current loss 1.1764076948165894 Accuracy 95.96219931271477\n",
      "Training:: Epoch 38, Iteration 20, Current loss 1.0647542476654053 Accuracy 95.88058923217312\n",
      "Training:: Epoch 38, Iteration 30, Current loss 1.492486834526062 Accuracy 91.9757174392936\n",
      "Training:: Epoch 38, Iteration 40, Current loss 1.0887560844421387 Accuracy 98.03861320223281\n",
      "Training:: Epoch 38, Iteration 50, Current loss 1.2529698610305786 Accuracy 95.4280831063398\n",
      "Training:: Epoch 38, Iteration 60, Current loss 1.3443922996520996 Accuracy 93.90367811420442\n",
      "Training:: Epoch 38, Iteration 70, Current loss 0.958892285823822 Accuracy 97.5184893193678\n",
      "Training:: Epoch 38, Iteration 80, Current loss 1.3090277910232544 Accuracy 96.47658785350023\n",
      "Training:: Epoch 38, Iteration 90, Current loss 1.317286729812622 Accuracy 94.80052608505042\n",
      "Training:: Epoch 38, Iteration 100, Current loss 0.755138635635376 Accuracy 98.38939857288482\n",
      "Training:: Epoch 38, Iteration 110, Current loss 1.4112299680709839 Accuracy 96.0722301571108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 38, Iteration 120, Current loss 1.0427796840667725 Accuracy 97.0740800778084\n",
      "Training:: Epoch 38, Iteration 130, Current loss 1.0552843809127808 Accuracy 96.755012508611\n",
      "Training:: Epoch 38, Iteration 140, Current loss 1.3140252828598022 Accuracy 93.9518005024658\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 38, Probability Accuracy 67.1378571101772\n",
      "Starting Training\n",
      "Training:: Epoch 39, Iteration 0, Current loss 1.4747594594955444 Accuracy 93.57528200098088\n",
      "Training:: Epoch 39, Iteration 10, Current loss 0.8413489460945129 Accuracy 97.28271532273887\n",
      "Training:: Epoch 39, Iteration 20, Current loss 1.4132814407348633 Accuracy 95.14939234729077\n",
      "Training:: Epoch 39, Iteration 30, Current loss 1.2232366800308228 Accuracy 97.01115633951454\n",
      "Training:: Epoch 39, Iteration 40, Current loss 0.9232543110847473 Accuracy 98.05241644626112\n",
      "Training:: Epoch 39, Iteration 50, Current loss 1.259474277496338 Accuracy 96.91401648998823\n",
      "Training:: Epoch 39, Iteration 60, Current loss 1.0944983959197998 Accuracy 96.89321487182389\n",
      "Training:: Epoch 39, Iteration 70, Current loss 0.6940237879753113 Accuracy 97.9351228934923\n",
      "Training:: Epoch 39, Iteration 80, Current loss 1.2693085670471191 Accuracy 95.73738363547281\n",
      "Training:: Epoch 39, Iteration 90, Current loss 0.9043492078781128 Accuracy 97.40356698465366\n",
      "Training:: Epoch 39, Iteration 100, Current loss 1.6730661392211914 Accuracy 94.84037407287971\n",
      "Training:: Epoch 39, Iteration 110, Current loss 1.1671301126480103 Accuracy 96.99853157121879\n",
      "Training:: Epoch 39, Iteration 120, Current loss 1.323778748512268 Accuracy 95.17931998136935\n",
      "Training:: Epoch 39, Iteration 130, Current loss 1.0615134239196777 Accuracy 97.76333397056013\n",
      "Training:: Epoch 39, Iteration 140, Current loss 1.3332781791687012 Accuracy 96.66042245332989\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 39, Probability Accuracy 67.53124203426167\n",
      "Starting Training\n",
      "Training:: Epoch 40, Iteration 0, Current loss 0.9335407018661499 Accuracy 96.30514012818901\n",
      "Training:: Epoch 40, Iteration 10, Current loss 0.949830949306488 Accuracy 98.08379159816316\n",
      "Training:: Epoch 40, Iteration 20, Current loss 1.4249355792999268 Accuracy 94.17822614543925\n",
      "Training:: Epoch 40, Iteration 30, Current loss 1.0390031337738037 Accuracy 96.6884531590414\n",
      "Training:: Epoch 40, Iteration 40, Current loss 1.3527523279190063 Accuracy 95.25160035653512\n",
      "Training:: Epoch 40, Iteration 50, Current loss 1.1963897943496704 Accuracy 96.14577743084354\n",
      "Training:: Epoch 40, Iteration 60, Current loss 1.7455344200134277 Accuracy 91.96188108556039\n",
      "Training:: Epoch 40, Iteration 70, Current loss 0.9572710394859314 Accuracy 96.61335841956726\n",
      "Training:: Epoch 40, Iteration 80, Current loss 0.9840068221092224 Accuracy 96.94679718009839\n",
      "Training:: Epoch 40, Iteration 90, Current loss 1.3736733198165894 Accuracy 94.03131115459882\n",
      "Training:: Epoch 40, Iteration 100, Current loss 1.295333743095398 Accuracy 95.02192416817127\n",
      "Training:: Epoch 40, Iteration 110, Current loss 1.0581648349761963 Accuracy 97.76774921427996\n",
      "Training:: Epoch 40, Iteration 120, Current loss 0.9848601222038269 Accuracy 97.58890898131405\n",
      "Training:: Epoch 40, Iteration 130, Current loss 1.2999229431152344 Accuracy 97.14865962632007\n",
      "Training:: Epoch 40, Iteration 140, Current loss 0.9551848769187927 Accuracy 97.45867258820627\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 40, Probability Accuracy 66.99602611781776\n",
      "Starting Training\n",
      "Training:: Epoch 41, Iteration 0, Current loss 0.7595937848091125 Accuracy 97.25744766505636\n",
      "Training:: Epoch 41, Iteration 10, Current loss 1.033602237701416 Accuracy 96.85584732672281\n",
      "Training:: Epoch 41, Iteration 20, Current loss 1.2969887256622314 Accuracy 95.7337774646083\n",
      "Training:: Epoch 41, Iteration 30, Current loss 1.1842092275619507 Accuracy 96.47862338123115\n",
      "Training:: Epoch 41, Iteration 40, Current loss 1.38103449344635 Accuracy 95.85162930436792\n",
      "Training:: Epoch 41, Iteration 50, Current loss 1.5299879312515259 Accuracy 92.67123703209367\n",
      "Training:: Epoch 41, Iteration 60, Current loss 2.219064950942993 Accuracy 90.08931761343337\n",
      "Training:: Epoch 41, Iteration 70, Current loss 1.4031018018722534 Accuracy 93.71128214755022\n",
      "Training:: Epoch 41, Iteration 80, Current loss 1.5549519062042236 Accuracy 91.44046884639111\n",
      "Training:: Epoch 41, Iteration 90, Current loss 1.4632362127304077 Accuracy 96.11923769092336\n",
      "Training:: Epoch 41, Iteration 100, Current loss 3.115342617034912 Accuracy 76.98790978842129\n",
      "Training:: Epoch 41, Iteration 110, Current loss 1.3542810678482056 Accuracy 95.52462818938464\n",
      "Training:: Epoch 41, Iteration 120, Current loss 2.8831369876861572 Accuracy 86.4946128789777\n",
      "Training:: Epoch 41, Iteration 130, Current loss 3.0835514068603516 Accuracy 85.84864391951007\n",
      "Training:: Epoch 41, Iteration 140, Current loss 1.8347917795181274 Accuracy 89.53882352941176\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 41, Probability Accuracy 58.43110085686835\n",
      "Starting Training\n",
      "Training:: Epoch 42, Iteration 0, Current loss 3.144265651702881 Accuracy 78.56743144935646\n",
      "Training:: Epoch 42, Iteration 10, Current loss 2.4234437942504883 Accuracy 84.74139948513924\n",
      "Training:: Epoch 42, Iteration 20, Current loss 6.625941276550293 Accuracy 54.809948381041764\n",
      "Training:: Epoch 42, Iteration 30, Current loss 2.599499225616455 Accuracy 83.13285579132315\n",
      "Training:: Epoch 42, Iteration 40, Current loss 2.6329267024993896 Accuracy 87.94847726324572\n",
      "Training:: Epoch 42, Iteration 50, Current loss 3.220203399658203 Accuracy 83.48236218925027\n",
      "Training:: Epoch 42, Iteration 60, Current loss 4.867580890655518 Accuracy 77.57441884926897\n",
      "Training:: Epoch 42, Iteration 70, Current loss 4.764608860015869 Accuracy 66.6840085842492\n",
      "Training:: Epoch 42, Iteration 80, Current loss 2.142265558242798 Accuracy 88.44075260208166\n",
      "Training:: Epoch 42, Iteration 90, Current loss 3.4105727672576904 Accuracy 79.145067389744\n",
      "Training:: Epoch 42, Iteration 100, Current loss 2.467057943344116 Accuracy 85.96491228070175\n",
      "Training:: Epoch 42, Iteration 110, Current loss 3.4131417274475098 Accuracy 79.62838292715767\n",
      "Training:: Epoch 42, Iteration 120, Current loss 2.842073678970337 Accuracy 79.46494921971761\n",
      "Training:: Epoch 42, Iteration 130, Current loss 2.0344650745391846 Accuracy 91.84777215441103\n",
      "Training:: Epoch 42, Iteration 140, Current loss 2.5418386459350586 Accuracy 85.3836784409257\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 42, Probability Accuracy 63.1460009542546\n",
      "Starting Training\n",
      "Training:: Epoch 43, Iteration 0, Current loss 1.887315273284912 Accuracy 91.28565346956151\n",
      "Training:: Epoch 43, Iteration 10, Current loss 3.118323564529419 Accuracy 87.16775725288089\n",
      "Training:: Epoch 43, Iteration 20, Current loss 2.132524251937866 Accuracy 89.05812324929973\n",
      "Training:: Epoch 43, Iteration 30, Current loss 1.810738205909729 Accuracy 91.8443569197338\n",
      "Training:: Epoch 43, Iteration 40, Current loss 1.5836780071258545 Accuracy 93.71597358971388\n",
      "Training:: Epoch 43, Iteration 50, Current loss 1.1076422929763794 Accuracy 97.0828800515734\n",
      "Training:: Epoch 43, Iteration 60, Current loss 1.465188980102539 Accuracy 95.62425760056503\n",
      "Training:: Epoch 43, Iteration 70, Current loss 2.4255423545837402 Accuracy 89.29332677488112\n",
      "Training:: Epoch 43, Iteration 80, Current loss 3.0353217124938965 Accuracy 83.27013050972667\n",
      "Training:: Epoch 43, Iteration 90, Current loss 1.9334843158721924 Accuracy 90.85677749360613\n",
      "Training:: Epoch 43, Iteration 100, Current loss 2.1040515899658203 Accuracy 91.93426994480683\n",
      "Training:: Epoch 43, Iteration 110, Current loss 1.6784675121307373 Accuracy 92.66534510722653\n",
      "Training:: Epoch 43, Iteration 120, Current loss 1.3615186214447021 Accuracy 95.8979808714134\n",
      "Training:: Epoch 43, Iteration 130, Current loss 1.1374272108078003 Accuracy 96.97133585722013\n",
      "Training:: Epoch 43, Iteration 140, Current loss 1.1564950942993164 Accuracy 95.96367974549311\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 43, Probability Accuracy 64.1974784148916\n",
      "Starting Training\n",
      "Training:: Epoch 44, Iteration 0, Current loss 1.2573370933532715 Accuracy 95.86530044796869\n",
      "Training:: Epoch 44, Iteration 10, Current loss 1.444361686706543 Accuracy 95.76562354907605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 44, Iteration 20, Current loss 1.360611915588379 Accuracy 96.35634906175989\n",
      "Training:: Epoch 44, Iteration 30, Current loss 1.3309746980667114 Accuracy 96.48847848187056\n",
      "Training:: Epoch 44, Iteration 40, Current loss 1.1124647855758667 Accuracy 97.28546409807356\n",
      "Training:: Epoch 44, Iteration 50, Current loss 1.1702522039413452 Accuracy 97.0638259779787\n",
      "Training:: Epoch 44, Iteration 60, Current loss 1.2851933240890503 Accuracy 96.8586387434555\n",
      "Training:: Epoch 44, Iteration 70, Current loss 0.8603113889694214 Accuracy 97.48401610229695\n",
      "Training:: Epoch 44, Iteration 80, Current loss 1.6422569751739502 Accuracy 95.36971830985915\n",
      "Training:: Epoch 44, Iteration 90, Current loss 1.5028117895126343 Accuracy 93.89897492898605\n",
      "Training:: Epoch 44, Iteration 100, Current loss 1.0957705974578857 Accuracy 96.654216005722\n",
      "Training:: Epoch 44, Iteration 110, Current loss 1.2088444232940674 Accuracy 95.93963907902925\n",
      "Training:: Epoch 44, Iteration 120, Current loss 0.9511327147483826 Accuracy 97.64303216522111\n",
      "Training:: Epoch 44, Iteration 130, Current loss 1.1928538084030151 Accuracy 96.59375\n",
      "Training:: Epoch 44, Iteration 140, Current loss 1.090442419052124 Accuracy 97.0910138248848\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 44, Probability Accuracy 66.63752704266041\n",
      "Starting Training\n",
      "Training:: Epoch 45, Iteration 0, Current loss 0.8993566036224365 Accuracy 97.65575232513696\n",
      "Training:: Epoch 45, Iteration 10, Current loss 1.012507438659668 Accuracy 97.58839528558477\n",
      "Training:: Epoch 45, Iteration 20, Current loss 1.011340856552124 Accuracy 96.86024810626853\n",
      "Training:: Epoch 45, Iteration 30, Current loss 1.1801209449768066 Accuracy 94.54292225879969\n",
      "Training:: Epoch 45, Iteration 40, Current loss 1.246677279472351 Accuracy 96.52540272614623\n",
      "Training:: Epoch 45, Iteration 50, Current loss 0.9268006682395935 Accuracy 97.51289248645473\n",
      "Training:: Epoch 45, Iteration 60, Current loss 1.0555005073547363 Accuracy 97.9110200304092\n",
      "Training:: Epoch 45, Iteration 70, Current loss 0.9410814046859741 Accuracy 97.69662368343342\n",
      "Training:: Epoch 45, Iteration 80, Current loss 1.3069953918457031 Accuracy 96.65208293703633\n",
      "Training:: Epoch 45, Iteration 90, Current loss 1.1253209114074707 Accuracy 97.69531914893616\n",
      "Training:: Epoch 45, Iteration 100, Current loss 1.4455879926681519 Accuracy 95.51526717557252\n",
      "Training:: Epoch 45, Iteration 110, Current loss 0.8905489444732666 Accuracy 97.52653617684292\n",
      "Training:: Epoch 45, Iteration 120, Current loss 0.8600436449050903 Accuracy 98.19439640262885\n",
      "Training:: Epoch 45, Iteration 130, Current loss 0.8826442956924438 Accuracy 98.05394056847545\n",
      "Training:: Epoch 45, Iteration 140, Current loss 1.2751356363296509 Accuracy 95.66146413972501\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 45, Probability Accuracy 67.36408407898091\n",
      "Starting Training\n",
      "Training:: Epoch 46, Iteration 0, Current loss 1.0781625509262085 Accuracy 97.3885588912078\n",
      "Training:: Epoch 46, Iteration 10, Current loss 1.165420651435852 Accuracy 96.39150662054158\n",
      "Training:: Epoch 46, Iteration 20, Current loss 0.9767554998397827 Accuracy 97.56982799431908\n",
      "Training:: Epoch 46, Iteration 30, Current loss 0.7517567873001099 Accuracy 97.02048417132217\n",
      "Training:: Epoch 46, Iteration 40, Current loss 1.162868857383728 Accuracy 97.53324339949894\n",
      "Training:: Epoch 46, Iteration 50, Current loss 0.8425751328468323 Accuracy 97.70340288475003\n",
      "Training:: Epoch 46, Iteration 60, Current loss 1.058923363685608 Accuracy 97.39489295847305\n",
      "Training:: Epoch 46, Iteration 70, Current loss 0.8525084853172302 Accuracy 98.23706231212934\n",
      "Training:: Epoch 46, Iteration 80, Current loss 0.791127622127533 Accuracy 97.73458353295462\n",
      "Training:: Epoch 46, Iteration 90, Current loss 0.9674546122550964 Accuracy 96.89432362954214\n",
      "Training:: Epoch 46, Iteration 100, Current loss 0.7786924839019775 Accuracy 98.72057490177642\n",
      "Training:: Epoch 46, Iteration 110, Current loss 0.8485413789749146 Accuracy 97.90009130037825\n",
      "Training:: Epoch 46, Iteration 120, Current loss 1.2977547645568848 Accuracy 95.89305402425579\n",
      "Training:: Epoch 46, Iteration 130, Current loss 0.8878923654556274 Accuracy 97.48263888888889\n",
      "Training:: Epoch 46, Iteration 140, Current loss 0.9184077978134155 Accuracy 98.23586609251285\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 46, Probability Accuracy 67.3585284871143\n",
      "Starting Training\n",
      "Training:: Epoch 47, Iteration 0, Current loss 0.9495128393173218 Accuracy 98.38252326370862\n",
      "Training:: Epoch 47, Iteration 10, Current loss 0.8836984038352966 Accuracy 97.87193103979065\n",
      "Training:: Epoch 47, Iteration 20, Current loss 0.7556579113006592 Accuracy 98.39655294200749\n",
      "Training:: Epoch 47, Iteration 30, Current loss 0.8707934617996216 Accuracy 99.00697232199451\n",
      "Training:: Epoch 47, Iteration 40, Current loss 0.8674956560134888 Accuracy 97.8823135515192\n",
      "Training:: Epoch 47, Iteration 50, Current loss 1.0229690074920654 Accuracy 97.92666364870982\n",
      "Training:: Epoch 47, Iteration 60, Current loss 0.95859295129776 Accuracy 97.28178090216754\n",
      "Training:: Epoch 47, Iteration 70, Current loss 0.7995136976242065 Accuracy 98.23555901256806\n",
      "Training:: Epoch 47, Iteration 80, Current loss 1.0259878635406494 Accuracy 98.05133344468571\n",
      "Training:: Epoch 47, Iteration 90, Current loss 0.6374566555023193 Accuracy 98.63639094131075\n",
      "Training:: Epoch 47, Iteration 100, Current loss 0.9881842136383057 Accuracy 98.28166884321946\n",
      "Training:: Epoch 47, Iteration 110, Current loss 0.8442406058311462 Accuracy 97.9892887343794\n",
      "Training:: Epoch 47, Iteration 120, Current loss 0.7741566896438599 Accuracy 98.5413075188967\n",
      "Training:: Epoch 47, Iteration 130, Current loss 1.2272576093673706 Accuracy 96.3670267489712\n",
      "Training:: Epoch 47, Iteration 140, Current loss 0.869792640209198 Accuracy 97.78997330169089\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 47, Probability Accuracy 66.70280524709312\n",
      "Starting Training\n",
      "Training:: Epoch 48, Iteration 0, Current loss 1.0833404064178467 Accuracy 97.18794594140854\n",
      "Training:: Epoch 48, Iteration 10, Current loss 1.2036330699920654 Accuracy 97.30860896355287\n",
      "Training:: Epoch 48, Iteration 20, Current loss 1.0464518070220947 Accuracy 95.99775312456116\n",
      "Training:: Epoch 48, Iteration 30, Current loss 0.830238938331604 Accuracy 98.13125152476213\n",
      "Training:: Epoch 48, Iteration 40, Current loss 0.7529558539390564 Accuracy 98.40395818370442\n",
      "Training:: Epoch 48, Iteration 50, Current loss 0.9350789785385132 Accuracy 98.65217177431256\n",
      "Training:: Epoch 48, Iteration 60, Current loss 1.0956244468688965 Accuracy 97.55672949933965\n",
      "Training:: Epoch 48, Iteration 70, Current loss 1.0052974224090576 Accuracy 96.45971700837424\n",
      "Training:: Epoch 48, Iteration 80, Current loss 0.9216804504394531 Accuracy 98.18535320803629\n",
      "Training:: Epoch 48, Iteration 90, Current loss 0.7358691096305847 Accuracy 97.81424073625575\n",
      "Training:: Epoch 48, Iteration 100, Current loss 0.8076742887496948 Accuracy 98.63187875089004\n",
      "Training:: Epoch 48, Iteration 110, Current loss 0.9971030354499817 Accuracy 97.1995257778205\n",
      "Training:: Epoch 48, Iteration 120, Current loss 0.7729400992393494 Accuracy 97.83341623832422\n",
      "Training:: Epoch 48, Iteration 130, Current loss 1.0554708242416382 Accuracy 96.60597626502764\n",
      "Training:: Epoch 48, Iteration 140, Current loss 1.1084821224212646 Accuracy 96.23638945873442\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 48, Probability Accuracy 66.96963705645135\n",
      "Starting Training\n",
      "Training:: Epoch 49, Iteration 0, Current loss 0.8981884717941284 Accuracy 98.40405904059041\n",
      "Training:: Epoch 49, Iteration 10, Current loss 1.1854405403137207 Accuracy 97.6470588235294\n",
      "Training:: Epoch 49, Iteration 20, Current loss 0.7982777953147888 Accuracy 98.29485320058014\n",
      "Training:: Epoch 49, Iteration 30, Current loss 0.7071936726570129 Accuracy 98.96395855834233\n",
      "Training:: Epoch 49, Iteration 40, Current loss 0.8754376173019409 Accuracy 97.94051693615012\n",
      "Training:: Epoch 49, Iteration 50, Current loss 0.7173011898994446 Accuracy 98.58714408973253\n",
      "Training:: Epoch 49, Iteration 60, Current loss 0.8604411482810974 Accuracy 98.3149708360337\n",
      "Training:: Epoch 49, Iteration 70, Current loss 0.7036964297294617 Accuracy 98.46725735818312\n",
      "Training:: Epoch 49, Iteration 80, Current loss 0.8557020425796509 Accuracy 97.64860456687205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 49, Iteration 90, Current loss 0.9233090281486511 Accuracy 97.92069791085895\n",
      "Training:: Epoch 49, Iteration 100, Current loss 0.9554712772369385 Accuracy 97.36306020823902\n",
      "Training:: Epoch 49, Iteration 110, Current loss 0.8602346777915955 Accuracy 97.8329009943796\n",
      "Training:: Epoch 49, Iteration 120, Current loss 1.0310910940170288 Accuracy 96.78827699489928\n",
      "Training:: Epoch 49, Iteration 130, Current loss 0.9324406385421753 Accuracy 97.5646339056641\n",
      "Training:: Epoch 49, Iteration 140, Current loss 0.6744258999824524 Accuracy 98.30888535924507\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 49, Probability Accuracy 67.27339721174648\n",
      "Starting Training\n",
      "Training:: Epoch 50, Iteration 0, Current loss 1.243818998336792 Accuracy 96.88358045030203\n",
      "Training:: Epoch 50, Iteration 10, Current loss 0.6178680062294006 Accuracy 98.6304347826087\n",
      "Training:: Epoch 50, Iteration 20, Current loss 0.9300528168678284 Accuracy 96.89378194686289\n",
      "Training:: Epoch 50, Iteration 30, Current loss 1.4088596105575562 Accuracy 95.18599562363238\n",
      "Training:: Epoch 50, Iteration 40, Current loss 0.7748520970344543 Accuracy 98.03524992776654\n",
      "Training:: Epoch 50, Iteration 50, Current loss 0.9287124872207642 Accuracy 97.17463848720801\n",
      "Training:: Epoch 50, Iteration 60, Current loss 0.9534927606582642 Accuracy 96.57657657657657\n",
      "Training:: Epoch 50, Iteration 70, Current loss 0.7547405958175659 Accuracy 97.72079772079772\n",
      "Training:: Epoch 50, Iteration 80, Current loss 0.9876484274864197 Accuracy 96.76885523975011\n",
      "Training:: Epoch 50, Iteration 90, Current loss 0.758948564529419 Accuracy 98.78332261734012\n",
      "Training:: Epoch 50, Iteration 100, Current loss 0.9753729701042175 Accuracy 97.99218845355792\n",
      "Training:: Epoch 50, Iteration 110, Current loss 0.8382216691970825 Accuracy 98.390263054574\n",
      "Training:: Epoch 50, Iteration 120, Current loss 0.8966538906097412 Accuracy 98.4389181339626\n",
      "Training:: Epoch 50, Iteration 130, Current loss 0.9773639440536499 Accuracy 98.09462086843811\n",
      "Training:: Epoch 50, Iteration 140, Current loss 0.7633789777755737 Accuracy 98.53686923268768\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 50, Probability Accuracy 66.60411179158034\n",
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 0.6746971011161804 Accuracy 99.15957826109101\n",
      "Training:: Epoch 51, Iteration 10, Current loss 1.1255367994308472 Accuracy 97.08931088813334\n",
      "Training:: Epoch 51, Iteration 20, Current loss 0.8817496299743652 Accuracy 97.75938347366919\n",
      "Training:: Epoch 51, Iteration 30, Current loss 0.9213791489601135 Accuracy 97.42744825991444\n",
      "Training:: Epoch 51, Iteration 40, Current loss 0.7718737721443176 Accuracy 97.71023719067666\n",
      "Training:: Epoch 51, Iteration 50, Current loss 0.9775645732879639 Accuracy 97.49068046514216\n",
      "Training:: Epoch 51, Iteration 60, Current loss 0.7719621658325195 Accuracy 98.06354207693317\n",
      "Training:: Epoch 51, Iteration 70, Current loss 0.8976278901100159 Accuracy 98.18032664628235\n",
      "Training:: Epoch 51, Iteration 80, Current loss 0.7794535756111145 Accuracy 97.48553182997405\n",
      "Training:: Epoch 51, Iteration 90, Current loss 0.7570692300796509 Accuracy 97.77614531855215\n",
      "Training:: Epoch 51, Iteration 100, Current loss 0.6986298561096191 Accuracy 98.10007080481472\n",
      "Training:: Epoch 51, Iteration 110, Current loss 0.8565163016319275 Accuracy 98.78718176590517\n",
      "Training:: Epoch 51, Iteration 120, Current loss 0.8617051839828491 Accuracy 97.9218461146172\n",
      "Training:: Epoch 51, Iteration 130, Current loss 1.2996995449066162 Accuracy 96.80167597765363\n",
      "Training:: Epoch 51, Iteration 140, Current loss 0.8010023236274719 Accuracy 97.75420225957564\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 51, Probability Accuracy 65.96325792979039\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 1.081425666809082 Accuracy 97.21119395758691\n",
      "Training:: Epoch 52, Iteration 10, Current loss 1.493120551109314 Accuracy 93.5602575896964\n",
      "Training:: Epoch 52, Iteration 20, Current loss 1.131351113319397 Accuracy 98.00165952150464\n",
      "Training:: Epoch 52, Iteration 30, Current loss 0.7512445449829102 Accuracy 98.29917139119058\n",
      "Training:: Epoch 52, Iteration 40, Current loss 0.8925870060920715 Accuracy 96.52137963619182\n",
      "Training:: Epoch 52, Iteration 50, Current loss 0.9439095258712769 Accuracy 98.01241260114699\n",
      "Training:: Epoch 52, Iteration 60, Current loss 1.1590139865875244 Accuracy 96.1110732073412\n",
      "Training:: Epoch 52, Iteration 70, Current loss 0.8430277109146118 Accuracy 98.73568229558876\n",
      "Training:: Epoch 52, Iteration 80, Current loss 0.9964718818664551 Accuracy 97.99307958477509\n",
      "Training:: Epoch 52, Iteration 90, Current loss 1.026603102684021 Accuracy 97.63022191320471\n",
      "Training:: Epoch 52, Iteration 100, Current loss 0.7169921398162842 Accuracy 98.51548269581056\n",
      "Training:: Epoch 52, Iteration 110, Current loss 0.8806489706039429 Accuracy 98.23748874308504\n",
      "Training:: Epoch 52, Iteration 120, Current loss 0.9816369414329529 Accuracy 97.18514923562242\n",
      "Training:: Epoch 52, Iteration 130, Current loss 1.0066629648208618 Accuracy 97.09857841994874\n",
      "Training:: Epoch 52, Iteration 140, Current loss 0.7792778015136719 Accuracy 98.65657682169883\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 52, Probability Accuracy 67.40526081869817\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 0.9784627556800842 Accuracy 98.21493294671983\n",
      "Training:: Epoch 53, Iteration 10, Current loss 1.2310283184051514 Accuracy 94.77370689655173\n",
      "Training:: Epoch 53, Iteration 20, Current loss 0.8266114592552185 Accuracy 97.32567608108687\n",
      "Training:: Epoch 53, Iteration 30, Current loss 0.8991926312446594 Accuracy 98.20615298819592\n",
      "Training:: Epoch 53, Iteration 40, Current loss 0.9016457796096802 Accuracy 97.3750832778148\n",
      "Training:: Epoch 53, Iteration 50, Current loss 0.6337246298789978 Accuracy 98.52702901752835\n",
      "Training:: Epoch 53, Iteration 60, Current loss 1.0016636848449707 Accuracy 96.59919028340082\n",
      "Training:: Epoch 53, Iteration 70, Current loss 1.5865588188171387 Accuracy 93.14616019818332\n",
      "Training:: Epoch 53, Iteration 80, Current loss 0.9357773065567017 Accuracy 97.6368088377655\n",
      "Training:: Epoch 53, Iteration 90, Current loss 0.9448867440223694 Accuracy 97.04595185995623\n",
      "Training:: Epoch 53, Iteration 100, Current loss 0.8612242341041565 Accuracy 97.83924276514858\n",
      "Training:: Epoch 53, Iteration 110, Current loss 0.9151942133903503 Accuracy 98.52056053296577\n",
      "Training:: Epoch 53, Iteration 120, Current loss 0.7040219306945801 Accuracy 97.07652099920988\n",
      "Training:: Epoch 53, Iteration 130, Current loss 1.0816106796264648 Accuracy 96.7258883248731\n",
      "Training:: Epoch 53, Iteration 140, Current loss 1.0348620414733887 Accuracy 97.41720313348328\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 53, Probability Accuracy 67.22690997980379\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 1.06557035446167 Accuracy 96.97158166682614\n",
      "Training:: Epoch 54, Iteration 10, Current loss 0.7389270067214966 Accuracy 97.96405567316788\n",
      "Training:: Epoch 54, Iteration 20, Current loss 0.860085129737854 Accuracy 97.66857708908229\n",
      "Training:: Epoch 54, Iteration 30, Current loss 0.7721018195152283 Accuracy 98.37143512500955\n",
      "Training:: Epoch 54, Iteration 40, Current loss 0.7502771615982056 Accuracy 98.47187458832828\n",
      "Training:: Epoch 54, Iteration 50, Current loss 0.7364827990531921 Accuracy 98.34587782251006\n",
      "Training:: Epoch 54, Iteration 60, Current loss 0.9156204462051392 Accuracy 97.37873378037098\n",
      "Training:: Epoch 54, Iteration 70, Current loss 0.8434263467788696 Accuracy 98.26373023850863\n",
      "Training:: Epoch 54, Iteration 80, Current loss 0.8432528972625732 Accuracy 97.82528395973775\n",
      "Training:: Epoch 54, Iteration 90, Current loss 0.9882997870445251 Accuracy 97.68686345589505\n",
      "Training:: Epoch 54, Iteration 100, Current loss 0.6016178131103516 Accuracy 98.24102251978088\n",
      "Training:: Epoch 54, Iteration 110, Current loss 0.8425925374031067 Accuracy 97.98680716182643\n",
      "Training:: Epoch 54, Iteration 120, Current loss 0.8473615050315857 Accuracy 98.17875993271852\n",
      "Training:: Epoch 54, Iteration 130, Current loss 0.8189197182655334 Accuracy 98.06446253530487\n",
      "Training:: Epoch 54, Iteration 140, Current loss 0.844113290309906 Accuracy 98.39380196523054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 54, Probability Accuracy 67.94023163550088\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 0.5942917466163635 Accuracy 98.52790140362889\n",
      "Training:: Epoch 55, Iteration 10, Current loss 0.715894877910614 Accuracy 98.45182867623261\n",
      "Training:: Epoch 55, Iteration 20, Current loss 0.6883230805397034 Accuracy 97.98458540042523\n",
      "Training:: Epoch 55, Iteration 30, Current loss 0.825450599193573 Accuracy 98.05389221556887\n",
      "Training:: Epoch 55, Iteration 40, Current loss 0.8789615631103516 Accuracy 98.67439561427159\n",
      "Training:: Epoch 55, Iteration 50, Current loss 0.8084027767181396 Accuracy 98.37837837837837\n",
      "Training:: Epoch 55, Iteration 60, Current loss 0.8414090871810913 Accuracy 98.19129975575946\n",
      "Training:: Epoch 55, Iteration 70, Current loss 0.6622308492660522 Accuracy 97.78977388470157\n",
      "Training:: Epoch 55, Iteration 80, Current loss 0.7829093337059021 Accuracy 97.9369339147937\n",
      "Training:: Epoch 55, Iteration 90, Current loss 0.7323395609855652 Accuracy 98.44313981300283\n",
      "Training:: Epoch 55, Iteration 100, Current loss 0.9578783512115479 Accuracy 97.97861241523213\n",
      "Training:: Epoch 55, Iteration 110, Current loss 0.7948738932609558 Accuracy 98.74459765383824\n",
      "Training:: Epoch 55, Iteration 120, Current loss 0.9207801818847656 Accuracy 97.98753339269813\n",
      "Training:: Epoch 55, Iteration 130, Current loss 0.8209195137023926 Accuracy 98.49243236801335\n",
      "Training:: Epoch 55, Iteration 140, Current loss 0.6009946465492249 Accuracy 98.29712272460364\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 67.0573010281113\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 0.7693051099777222 Accuracy 97.29414054895854\n",
      "Training:: Epoch 56, Iteration 10, Current loss 0.8986786603927612 Accuracy 97.8676678945877\n",
      "Training:: Epoch 56, Iteration 20, Current loss 0.7846457958221436 Accuracy 98.47385272145144\n",
      "Training:: Epoch 56, Iteration 30, Current loss 0.6398921608924866 Accuracy 98.41075218108936\n",
      "Training:: Epoch 56, Iteration 40, Current loss 0.7555752396583557 Accuracy 98.81438848920864\n",
      "Training:: Epoch 56, Iteration 50, Current loss 0.744297206401825 Accuracy 98.51121141525917\n",
      "Training:: Epoch 56, Iteration 60, Current loss 0.7040621638298035 Accuracy 98.49270954468199\n",
      "Training:: Epoch 56, Iteration 70, Current loss 0.8504965305328369 Accuracy 98.12058440222238\n",
      "Training:: Epoch 56, Iteration 80, Current loss 0.9603087902069092 Accuracy 98.15289095847885\n",
      "Training:: Epoch 56, Iteration 90, Current loss 0.9643685817718506 Accuracy 98.2893305646198\n",
      "Training:: Epoch 56, Iteration 100, Current loss 0.8845494389533997 Accuracy 97.59179948044654\n",
      "Training:: Epoch 56, Iteration 110, Current loss 0.8470108509063721 Accuracy 98.09071786850762\n",
      "Training:: Epoch 56, Iteration 120, Current loss 0.6987465620040894 Accuracy 97.97485046861019\n",
      "Training:: Epoch 56, Iteration 130, Current loss 0.5925604104995728 Accuracy 98.65917946367179\n",
      "Training:: Epoch 56, Iteration 140, Current loss 0.8628258109092712 Accuracy 97.85419843017675\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 56, Probability Accuracy 66.8906332721129\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 0.7528330087661743 Accuracy 97.91444122284959\n",
      "Training:: Epoch 57, Iteration 10, Current loss 0.9082783460617065 Accuracy 97.8270955768636\n",
      "Training:: Epoch 57, Iteration 20, Current loss 0.748681366443634 Accuracy 97.67739779436337\n",
      "Training:: Epoch 57, Iteration 30, Current loss 0.8977547287940979 Accuracy 97.5183930865351\n",
      "Training:: Epoch 57, Iteration 40, Current loss 0.7368722558021545 Accuracy 98.46034080096878\n",
      "Training:: Epoch 57, Iteration 50, Current loss 0.7177196145057678 Accuracy 98.62611654615057\n",
      "Training:: Epoch 57, Iteration 60, Current loss 0.8106880187988281 Accuracy 97.6429180887372\n",
      "Training:: Epoch 57, Iteration 70, Current loss 0.8658326864242554 Accuracy 97.66000641094134\n",
      "Training:: Epoch 57, Iteration 80, Current loss 0.7403669357299805 Accuracy 97.88649706457926\n",
      "Training:: Epoch 57, Iteration 90, Current loss 0.7567195296287537 Accuracy 98.72175597159458\n",
      "Training:: Epoch 57, Iteration 100, Current loss 0.872823178768158 Accuracy 98.20233870498575\n",
      "Training:: Epoch 57, Iteration 110, Current loss 1.019453525543213 Accuracy 97.38648363252376\n",
      "Training:: Epoch 57, Iteration 120, Current loss 0.7007550597190857 Accuracy 97.88390379278447\n",
      "Training:: Epoch 57, Iteration 130, Current loss 0.6109277606010437 Accuracy 98.90453199172313\n",
      "Training:: Epoch 57, Iteration 140, Current loss 0.6158522367477417 Accuracy 98.50704357186315\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 57, Probability Accuracy 66.61089288165282\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 0.6034632325172424 Accuracy 98.81258023106547\n",
      "Training:: Epoch 58, Iteration 10, Current loss 0.6686054468154907 Accuracy 98.34997828918802\n",
      "Training:: Epoch 58, Iteration 20, Current loss 0.7781088352203369 Accuracy 98.436582501468\n",
      "Training:: Epoch 58, Iteration 30, Current loss 0.7036230564117432 Accuracy 98.55767116661536\n",
      "Training:: Epoch 58, Iteration 40, Current loss 0.8450905084609985 Accuracy 98.4259828009828\n",
      "Training:: Epoch 58, Iteration 50, Current loss 0.7592711448669434 Accuracy 98.9196694662693\n",
      "Training:: Epoch 58, Iteration 60, Current loss 0.6165677905082703 Accuracy 98.87932759655793\n",
      "Training:: Epoch 58, Iteration 70, Current loss 0.6531360745429993 Accuracy 98.35418721769379\n",
      "Training:: Epoch 58, Iteration 80, Current loss 0.7679060101509094 Accuracy 97.87395021563138\n",
      "Training:: Epoch 58, Iteration 90, Current loss 0.5183142423629761 Accuracy 98.83844339622641\n",
      "Training:: Epoch 58, Iteration 100, Current loss 1.0217772722244263 Accuracy 96.10992479806889\n",
      "Training:: Epoch 58, Iteration 110, Current loss 0.6175165772438049 Accuracy 98.06934594168636\n",
      "Training:: Epoch 58, Iteration 120, Current loss 0.8324291110038757 Accuracy 97.42214352671597\n",
      "Training:: Epoch 58, Iteration 130, Current loss 0.6829056739807129 Accuracy 97.51217163340645\n",
      "Training:: Epoch 58, Iteration 140, Current loss 0.586253821849823 Accuracy 98.1868348443043\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 58, Probability Accuracy 66.78033843358453\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 0.6831018924713135 Accuracy 98.01369863013699\n",
      "Training:: Epoch 59, Iteration 10, Current loss 0.8038479089736938 Accuracy 98.41643214180704\n",
      "Training:: Epoch 59, Iteration 20, Current loss 0.639962911605835 Accuracy 98.32052250410983\n",
      "Training:: Epoch 59, Iteration 30, Current loss 0.9369063973426819 Accuracy 97.12750303883308\n",
      "Training:: Epoch 59, Iteration 40, Current loss 0.4943881034851074 Accuracy 98.55888981141027\n",
      "Training:: Epoch 59, Iteration 50, Current loss 0.865041196346283 Accuracy 97.07000986054373\n",
      "Training:: Epoch 59, Iteration 60, Current loss 0.9355221390724182 Accuracy 96.68586818283833\n",
      "Training:: Epoch 59, Iteration 70, Current loss 0.7421354651451111 Accuracy 96.75627918717657\n",
      "Training:: Epoch 59, Iteration 80, Current loss 0.779068648815155 Accuracy 98.11007651885313\n",
      "Training:: Epoch 59, Iteration 90, Current loss 1.0314313173294067 Accuracy 96.43589058184087\n",
      "Training:: Epoch 59, Iteration 100, Current loss 0.9087342023849487 Accuracy 98.23999130859906\n",
      "Training:: Epoch 59, Iteration 110, Current loss 0.66246497631073 Accuracy 98.02802802802803\n",
      "Training:: Epoch 59, Iteration 120, Current loss 0.9408894777297974 Accuracy 97.66308671439937\n",
      "Training:: Epoch 59, Iteration 130, Current loss 0.9503850340843201 Accuracy 97.37942808365344\n",
      "Training:: Epoch 59, Iteration 140, Current loss 0.7189016938209534 Accuracy 98.05760770316262\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 59, Probability Accuracy 66.78687442401585\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 1.0468409061431885 Accuracy 96.30454288657738\n",
      "Training:: Epoch 60, Iteration 10, Current loss 1.069881796836853 Accuracy 96.46719538572458\n",
      "Training:: Epoch 60, Iteration 20, Current loss 0.9150869250297546 Accuracy 95.73527411236566\n",
      "Training:: Epoch 60, Iteration 30, Current loss 3.0105655193328857 Accuracy 84.63676061929337\n",
      "Training:: Epoch 60, Iteration 40, Current loss 4.2048187255859375 Accuracy 83.5416820770767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 60, Iteration 50, Current loss 6.541080951690674 Accuracy 59.134312696747116\n",
      "Training:: Epoch 60, Iteration 60, Current loss 8.248635292053223 Accuracy 46.9898156503803\n",
      "Training:: Epoch 60, Iteration 70, Current loss 4.804670333862305 Accuracy 72.11577995230655\n",
      "Training:: Epoch 60, Iteration 80, Current loss 7.578023910522461 Accuracy 34.73880379709976\n",
      "Training:: Epoch 60, Iteration 90, Current loss 2.6664159297943115 Accuracy 86.42399534071055\n",
      "Training:: Epoch 60, Iteration 100, Current loss 4.976252555847168 Accuracy 67.03650502749022\n",
      "Training:: Epoch 60, Iteration 110, Current loss 5.549400329589844 Accuracy 65.79954093343535\n",
      "Training:: Epoch 60, Iteration 120, Current loss 2.482548713684082 Accuracy 83.80353849306647\n",
      "Training:: Epoch 60, Iteration 130, Current loss 2.924994707107544 Accuracy 75.17688679245283\n",
      "Training:: Epoch 60, Iteration 140, Current loss 1.9680074453353882 Accuracy 91.42666730137573\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 61.81682560016732\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 2.1155612468719482 Accuracy 91.9554303905963\n",
      "Training:: Epoch 61, Iteration 10, Current loss 1.6278440952301025 Accuracy 94.04265955674055\n",
      "Training:: Epoch 61, Iteration 20, Current loss 3.1111435890197754 Accuracy 83.93326988747691\n",
      "Training:: Epoch 61, Iteration 30, Current loss 1.9865015745162964 Accuracy 91.31508241957738\n",
      "Training:: Epoch 61, Iteration 40, Current loss 2.378101348876953 Accuracy 86.39857517810273\n",
      "Training:: Epoch 61, Iteration 50, Current loss 1.1677929162979126 Accuracy 95.47944415614369\n",
      "Training:: Epoch 61, Iteration 60, Current loss 1.9190813302993774 Accuracy 88.34841628959276\n",
      "Training:: Epoch 61, Iteration 70, Current loss 1.8172340393066406 Accuracy 92.01423909819044\n",
      "Training:: Epoch 61, Iteration 80, Current loss 1.4482753276824951 Accuracy 93.28456474030725\n",
      "Training:: Epoch 61, Iteration 90, Current loss 1.9858317375183105 Accuracy 92.64856183384435\n",
      "Training:: Epoch 61, Iteration 100, Current loss 1.9167999029159546 Accuracy 90.97636667462402\n",
      "Training:: Epoch 61, Iteration 110, Current loss 1.4483065605163574 Accuracy 95.12218495448012\n",
      "Training:: Epoch 61, Iteration 120, Current loss 1.4314202070236206 Accuracy 93.50569259962049\n",
      "Training:: Epoch 61, Iteration 130, Current loss 1.4022434949874878 Accuracy 95.2598798434658\n",
      "Training:: Epoch 61, Iteration 140, Current loss 1.2837988138198853 Accuracy 96.33519229376404\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 61, Probability Accuracy 67.98622866816123\n",
      "Starting Training\n",
      "Training:: Epoch 62, Iteration 0, Current loss 1.2745729684829712 Accuracy 95.62450278440731\n",
      "Training:: Epoch 62, Iteration 10, Current loss 1.2327135801315308 Accuracy 95.35618092319123\n",
      "Training:: Epoch 62, Iteration 20, Current loss 2.1567881107330322 Accuracy 89.04884318766067\n",
      "Training:: Epoch 62, Iteration 30, Current loss 1.1633824110031128 Accuracy 95.91280653950953\n",
      "Training:: Epoch 62, Iteration 40, Current loss 1.1181039810180664 Accuracy 97.11646378313046\n",
      "Training:: Epoch 62, Iteration 50, Current loss 1.1531970500946045 Accuracy 96.06302727313306\n",
      "Training:: Epoch 62, Iteration 60, Current loss 1.3774346113204956 Accuracy 93.83833047153612\n",
      "Training:: Epoch 62, Iteration 70, Current loss 1.7719831466674805 Accuracy 93.17287144303228\n",
      "Training:: Epoch 62, Iteration 80, Current loss 1.275530219078064 Accuracy 96.91487801300542\n",
      "Training:: Epoch 62, Iteration 90, Current loss 1.0656652450561523 Accuracy 97.32371578805447\n",
      "Training:: Epoch 62, Iteration 100, Current loss 1.5832313299179077 Accuracy 94.87360905640959\n",
      "Training:: Epoch 62, Iteration 110, Current loss 0.8588120341300964 Accuracy 97.52593161736458\n",
      "Training:: Epoch 62, Iteration 120, Current loss 1.2030967473983765 Accuracy 97.31148012364316\n",
      "Training:: Epoch 62, Iteration 130, Current loss 0.7357351779937744 Accuracy 97.43965601485391\n",
      "Training:: Epoch 62, Iteration 140, Current loss 0.8976718187332153 Accuracy 97.54573599774838\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 62, Probability Accuracy 68.09921960274251\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 0.9925940632820129 Accuracy 97.6551003939811\n",
      "Training:: Epoch 63, Iteration 10, Current loss 1.2025059461593628 Accuracy 96.19558806181432\n",
      "Training:: Epoch 63, Iteration 20, Current loss 1.1212835311889648 Accuracy 96.92153254140874\n",
      "Training:: Epoch 63, Iteration 30, Current loss 0.9980568885803223 Accuracy 97.91644743765127\n",
      "Training:: Epoch 63, Iteration 40, Current loss 0.7637984156608582 Accuracy 97.9950781702374\n",
      "Training:: Epoch 63, Iteration 50, Current loss 0.9839253425598145 Accuracy 96.96050144264252\n",
      "Training:: Epoch 63, Iteration 60, Current loss 0.7792341709136963 Accuracy 98.27218798594713\n",
      "Training:: Epoch 63, Iteration 70, Current loss 0.8767493367195129 Accuracy 98.45106934543098\n",
      "Training:: Epoch 63, Iteration 80, Current loss 0.9530327916145325 Accuracy 98.00593389912372\n",
      "Training:: Epoch 63, Iteration 90, Current loss 0.8050960898399353 Accuracy 98.58008596998097\n",
      "Training:: Epoch 63, Iteration 100, Current loss 0.8903421759605408 Accuracy 97.49360613810742\n",
      "Training:: Epoch 63, Iteration 110, Current loss 0.9064192771911621 Accuracy 98.07622062583708\n",
      "Training:: Epoch 63, Iteration 120, Current loss 0.6569701433181763 Accuracy 98.71461841633823\n",
      "Training:: Epoch 63, Iteration 130, Current loss 1.0280505418777466 Accuracy 95.52307692307693\n",
      "Training:: Epoch 63, Iteration 140, Current loss 0.8328901529312134 Accuracy 98.3188349134982\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 63, Probability Accuracy 68.54587284884215\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 1.1018564701080322 Accuracy 98.16886300950046\n",
      "Training:: Epoch 64, Iteration 10, Current loss 0.8976588845252991 Accuracy 97.75985065671044\n",
      "Training:: Epoch 64, Iteration 20, Current loss 0.6922609806060791 Accuracy 98.932384341637\n",
      "Training:: Epoch 64, Iteration 30, Current loss 0.8264293074607849 Accuracy 98.57979762116102\n",
      "Training:: Epoch 64, Iteration 40, Current loss 0.6838796734809875 Accuracy 98.31086845248251\n",
      "Training:: Epoch 64, Iteration 50, Current loss 1.1690349578857422 Accuracy 98.2126998223801\n",
      "Training:: Epoch 64, Iteration 60, Current loss 1.0251749753952026 Accuracy 97.86499215070644\n",
      "Training:: Epoch 64, Iteration 70, Current loss 0.8404203057289124 Accuracy 98.27682824320539\n",
      "Training:: Epoch 64, Iteration 80, Current loss 0.7653831839561462 Accuracy 98.67636458947645\n",
      "Training:: Epoch 64, Iteration 90, Current loss 0.6736608147621155 Accuracy 98.80106191658817\n",
      "Training:: Epoch 64, Iteration 100, Current loss 0.7023270130157471 Accuracy 98.32937515355007\n",
      "Training:: Epoch 64, Iteration 110, Current loss 0.6812569499015808 Accuracy 98.7646835922698\n",
      "Training:: Epoch 64, Iteration 120, Current loss 0.8190544247627258 Accuracy 97.49878413117487\n",
      "Training:: Epoch 64, Iteration 130, Current loss 0.8728657960891724 Accuracy 98.56931881115008\n",
      "Training:: Epoch 64, Iteration 140, Current loss 0.7167700529098511 Accuracy 98.75757575757575\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 64, Probability Accuracy 68.45502258184695\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 0.7390310764312744 Accuracy 98.56189674159486\n",
      "Training:: Epoch 65, Iteration 10, Current loss 0.7302678823471069 Accuracy 98.62270112614438\n",
      "Training:: Epoch 65, Iteration 20, Current loss 0.6215890645980835 Accuracy 98.68684516880093\n",
      "Training:: Epoch 65, Iteration 30, Current loss 0.9710843563079834 Accuracy 98.08539845386893\n",
      "Training:: Epoch 65, Iteration 40, Current loss 0.6700180768966675 Accuracy 98.38582232757648\n",
      "Training:: Epoch 65, Iteration 50, Current loss 0.9314503073692322 Accuracy 97.03748488512697\n",
      "Training:: Epoch 65, Iteration 60, Current loss 0.7831680774688721 Accuracy 98.90922042651708\n",
      "Training:: Epoch 65, Iteration 70, Current loss 0.8104608058929443 Accuracy 98.49501596195192\n",
      "Training:: Epoch 65, Iteration 80, Current loss 0.668386697769165 Accuracy 98.83293365307753\n",
      "Training:: Epoch 65, Iteration 90, Current loss 0.6683787107467651 Accuracy 98.31918060054277\n",
      "Training:: Epoch 65, Iteration 100, Current loss 0.6593660116195679 Accuracy 98.31031681559708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 65, Iteration 110, Current loss 0.6177629828453064 Accuracy 98.81922607970223\n",
      "Training:: Epoch 65, Iteration 120, Current loss 0.6695812344551086 Accuracy 99.04804402796371\n",
      "Training:: Epoch 65, Iteration 130, Current loss 0.7363218665122986 Accuracy 98.74809676873626\n",
      "Training:: Epoch 65, Iteration 140, Current loss 0.8439977765083313 Accuracy 98.04588761423294\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 68.1086150889875\n",
      "Starting Training\n",
      "Training:: Epoch 66, Iteration 0, Current loss 0.6593851447105408 Accuracy 98.69831546707503\n",
      "Training:: Epoch 66, Iteration 10, Current loss 0.8029894232749939 Accuracy 98.45201238390094\n",
      "Training:: Epoch 66, Iteration 20, Current loss 0.5222232937812805 Accuracy 99.18578475980651\n",
      "Training:: Epoch 66, Iteration 30, Current loss 0.897393524646759 Accuracy 97.99851742031134\n",
      "Training:: Epoch 66, Iteration 40, Current loss 0.55270916223526 Accuracy 99.2152466367713\n",
      "Training:: Epoch 66, Iteration 50, Current loss 0.9845759868621826 Accuracy 98.52521875921738\n",
      "Training:: Epoch 66, Iteration 60, Current loss 0.7554141879081726 Accuracy 98.9421771138589\n",
      "Training:: Epoch 66, Iteration 70, Current loss 0.9073306322097778 Accuracy 98.21961510754348\n",
      "Training:: Epoch 66, Iteration 80, Current loss 0.7214062213897705 Accuracy 99.02293235243405\n",
      "Training:: Epoch 66, Iteration 90, Current loss 0.8473285436630249 Accuracy 98.3911333571684\n",
      "Training:: Epoch 66, Iteration 100, Current loss 0.6034838557243347 Accuracy 98.8564652068951\n",
      "Training:: Epoch 66, Iteration 110, Current loss 0.7299370169639587 Accuracy 98.35486649440138\n",
      "Training:: Epoch 66, Iteration 120, Current loss 0.7674084901809692 Accuracy 98.72337922108483\n",
      "Training:: Epoch 66, Iteration 130, Current loss 0.6063426733016968 Accuracy 98.5614443797321\n",
      "Training:: Epoch 66, Iteration 140, Current loss 0.6139723062515259 Accuracy 99.11921343711593\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 66, Probability Accuracy 67.59872613546494\n",
      "Starting Training\n",
      "Training:: Epoch 67, Iteration 0, Current loss 1.128616213798523 Accuracy 98.03802833959065\n",
      "Training:: Epoch 67, Iteration 10, Current loss 0.7676318883895874 Accuracy 98.7596057467424\n",
      "Training:: Epoch 67, Iteration 20, Current loss 0.7116070985794067 Accuracy 98.8585047666834\n",
      "Training:: Epoch 67, Iteration 30, Current loss 0.7817819118499756 Accuracy 99.22663908181863\n",
      "Training:: Epoch 67, Iteration 40, Current loss 0.5888298153877258 Accuracy 98.93668438859352\n",
      "Training:: Epoch 67, Iteration 50, Current loss 0.7748427391052246 Accuracy 98.56054836252856\n",
      "Training:: Epoch 67, Iteration 60, Current loss 0.7858719825744629 Accuracy 98.54591328597039\n",
      "Training:: Epoch 67, Iteration 70, Current loss 0.7590655088424683 Accuracy 98.46472710524296\n",
      "Training:: Epoch 67, Iteration 80, Current loss 0.6983087658882141 Accuracy 98.40380549682875\n",
      "Training:: Epoch 67, Iteration 90, Current loss 0.7647978663444519 Accuracy 98.28824397442203\n",
      "Training:: Epoch 67, Iteration 100, Current loss 0.6883844137191772 Accuracy 98.62604540023895\n",
      "Training:: Epoch 67, Iteration 110, Current loss 0.7811922430992126 Accuracy 98.74213836477988\n",
      "Training:: Epoch 67, Iteration 120, Current loss 1.0152032375335693 Accuracy 98.48161061747835\n",
      "Training:: Epoch 67, Iteration 130, Current loss 0.5630379915237427 Accuracy 98.96622404420283\n",
      "Training:: Epoch 67, Iteration 140, Current loss 0.9064139127731323 Accuracy 98.89666965230845\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 67, Probability Accuracy 67.87691422819756\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 0.8283421397209167 Accuracy 99.00560790888593\n",
      "Training:: Epoch 68, Iteration 10, Current loss 0.6297608017921448 Accuracy 98.82655981682885\n",
      "Training:: Epoch 68, Iteration 20, Current loss 0.6914263367652893 Accuracy 98.72200922375951\n",
      "Training:: Epoch 68, Iteration 30, Current loss 0.5512459874153137 Accuracy 98.63206441022434\n",
      "Training:: Epoch 68, Iteration 40, Current loss 0.7080574631690979 Accuracy 98.50631715668911\n",
      "Training:: Epoch 68, Iteration 50, Current loss 0.6939277052879333 Accuracy 98.94476698572102\n",
      "Training:: Epoch 68, Iteration 60, Current loss 0.6464027166366577 Accuracy 98.97933146210768\n",
      "Training:: Epoch 68, Iteration 70, Current loss 0.6144520044326782 Accuracy 98.6471647560065\n",
      "Training:: Epoch 68, Iteration 80, Current loss 0.48331332206726074 Accuracy 99.04754282055387\n",
      "Training:: Epoch 68, Iteration 90, Current loss 0.7901186347007751 Accuracy 98.68548647775025\n",
      "Training:: Epoch 68, Iteration 100, Current loss 0.811347484588623 Accuracy 98.88281069419833\n",
      "Training:: Epoch 68, Iteration 110, Current loss 0.7896791100502014 Accuracy 98.42566999158755\n",
      "Training:: Epoch 68, Iteration 120, Current loss 0.4866945147514343 Accuracy 98.88903851333154\n",
      "Training:: Epoch 68, Iteration 130, Current loss 0.6320002675056458 Accuracy 98.9306784660767\n",
      "Training:: Epoch 68, Iteration 140, Current loss 0.747573733329773 Accuracy 98.65544681240281\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 68, Probability Accuracy 67.7147399656207\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 0.5104187726974487 Accuracy 99.12151595989329\n",
      "Training:: Epoch 69, Iteration 10, Current loss 0.6804532408714294 Accuracy 98.12416427889207\n",
      "Training:: Epoch 69, Iteration 20, Current loss 0.5677769780158997 Accuracy 98.56749655948956\n",
      "Training:: Epoch 69, Iteration 30, Current loss 0.6736260056495667 Accuracy 99.25025250563282\n",
      "Training:: Epoch 69, Iteration 40, Current loss 0.9658757448196411 Accuracy 98.37269056649944\n",
      "Training:: Epoch 69, Iteration 50, Current loss 0.7724578976631165 Accuracy 98.8316072986609\n",
      "Training:: Epoch 69, Iteration 60, Current loss 0.7692720890045166 Accuracy 98.65185687637782\n",
      "Training:: Epoch 69, Iteration 70, Current loss 0.7037279009819031 Accuracy 98.71068967869569\n",
      "Training:: Epoch 69, Iteration 80, Current loss 0.4837063252925873 Accuracy 99.03861391368655\n",
      "Training:: Epoch 69, Iteration 90, Current loss 0.7535879611968994 Accuracy 99.11358173076923\n",
      "Training:: Epoch 69, Iteration 100, Current loss 0.9598472118377686 Accuracy 97.34963412457611\n",
      "Training:: Epoch 69, Iteration 110, Current loss 0.6721584796905518 Accuracy 98.74362482895883\n",
      "Training:: Epoch 69, Iteration 120, Current loss 0.8634057641029358 Accuracy 98.60467950280282\n",
      "Training:: Epoch 69, Iteration 130, Current loss 0.6122721433639526 Accuracy 99.0420612199956\n",
      "Training:: Epoch 69, Iteration 140, Current loss 0.6134384274482727 Accuracy 98.53462974056495\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 69, Probability Accuracy 67.09929476663247\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 0.8734754323959351 Accuracy 98.73215912541755\n",
      "Training:: Epoch 70, Iteration 10, Current loss 0.8102719783782959 Accuracy 98.68743047830924\n",
      "Training:: Epoch 70, Iteration 20, Current loss 0.5964763164520264 Accuracy 99.11604714415232\n",
      "Training:: Epoch 70, Iteration 30, Current loss 0.7932261824607849 Accuracy 98.84648423119401\n",
      "Training:: Epoch 70, Iteration 40, Current loss 0.7562929391860962 Accuracy 98.71347178827422\n",
      "Training:: Epoch 70, Iteration 50, Current loss 1.069818377494812 Accuracy 94.38920454545455\n",
      "Training:: Epoch 70, Iteration 60, Current loss 1.4123890399932861 Accuracy 92.71950302694576\n",
      "Training:: Epoch 70, Iteration 70, Current loss 0.8200445175170898 Accuracy 97.36680485183479\n",
      "Training:: Epoch 70, Iteration 80, Current loss 0.7523598670959473 Accuracy 97.82821071616442\n",
      "Training:: Epoch 70, Iteration 90, Current loss 0.5953132510185242 Accuracy 98.43707005668372\n",
      "Training:: Epoch 70, Iteration 100, Current loss 0.7803533673286438 Accuracy 98.9222710578602\n",
      "Training:: Epoch 70, Iteration 110, Current loss 0.6456573605537415 Accuracy 98.27238052501683\n",
      "Training:: Epoch 70, Iteration 120, Current loss 0.6596059203147888 Accuracy 98.09535743374475\n",
      "Training:: Epoch 70, Iteration 130, Current loss 0.7351173162460327 Accuracy 97.82896673535473\n",
      "Training:: Epoch 70, Iteration 140, Current loss 0.504027783870697 Accuracy 98.93699766658024\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 67.45395394741142\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 0.49258899688720703 Accuracy 98.73900971772328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 71, Iteration 10, Current loss 0.5556270480155945 Accuracy 99.17780398322851\n",
      "Training:: Epoch 71, Iteration 20, Current loss 0.49734559655189514 Accuracy 98.6701563273679\n",
      "Training:: Epoch 71, Iteration 30, Current loss 0.8820996880531311 Accuracy 97.07816311379786\n",
      "Training:: Epoch 71, Iteration 40, Current loss 0.918388307094574 Accuracy 97.23225806451613\n",
      "Training:: Epoch 71, Iteration 50, Current loss 0.845200777053833 Accuracy 97.14069591527988\n",
      "Training:: Epoch 71, Iteration 60, Current loss 0.9189693331718445 Accuracy 98.49403890399498\n",
      "Training:: Epoch 71, Iteration 70, Current loss 0.7966311573982239 Accuracy 98.82827264709589\n",
      "Training:: Epoch 71, Iteration 80, Current loss 0.6498368382453918 Accuracy 98.7401832460733\n",
      "Training:: Epoch 71, Iteration 90, Current loss 0.8889122605323792 Accuracy 98.33815028901734\n",
      "Training:: Epoch 71, Iteration 100, Current loss 1.1062835454940796 Accuracy 96.66722632639355\n",
      "Training:: Epoch 71, Iteration 110, Current loss 0.8298720121383667 Accuracy 96.57738095238095\n",
      "Training:: Epoch 71, Iteration 120, Current loss 0.6549713611602783 Accuracy 98.67884750527055\n",
      "Training:: Epoch 71, Iteration 130, Current loss 0.46350109577178955 Accuracy 98.97780227489102\n",
      "Training:: Epoch 71, Iteration 140, Current loss 0.846908450126648 Accuracy 98.82918304936653\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 71, Probability Accuracy 68.4762645507487\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 0.8213142156600952 Accuracy 98.2687034860948\n",
      "Training:: Epoch 72, Iteration 10, Current loss 0.8460741639137268 Accuracy 98.9428980288972\n",
      "Training:: Epoch 72, Iteration 20, Current loss 0.6272947788238525 Accuracy 98.75698072419384\n",
      "Training:: Epoch 72, Iteration 30, Current loss 0.7505893111228943 Accuracy 98.69292336039615\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.6479177474975586 Accuracy 99.02863777089783\n",
      "Training:: Epoch 72, Iteration 50, Current loss 0.8924168348312378 Accuracy 97.90502793296089\n",
      "Training:: Epoch 72, Iteration 60, Current loss 0.7243674397468567 Accuracy 98.01786876516182\n",
      "Training:: Epoch 72, Iteration 70, Current loss 0.7214963436126709 Accuracy 98.35656461135797\n",
      "Training:: Epoch 72, Iteration 80, Current loss 0.6054209470748901 Accuracy 99.15573917729478\n",
      "Training:: Epoch 72, Iteration 90, Current loss 0.5588148236274719 Accuracy 99.01341570236504\n",
      "Training:: Epoch 72, Iteration 100, Current loss 0.5164916515350342 Accuracy 98.66003062787136\n",
      "Training:: Epoch 72, Iteration 110, Current loss 0.8060121536254883 Accuracy 98.07282587642844\n",
      "Training:: Epoch 72, Iteration 120, Current loss 0.7094535231590271 Accuracy 98.00216632567096\n",
      "Training:: Epoch 72, Iteration 130, Current loss 0.7607923150062561 Accuracy 96.8986146095718\n",
      "Training:: Epoch 72, Iteration 140, Current loss 0.8038067817687988 Accuracy 97.55060384419119\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 72, Probability Accuracy 67.10770985431277\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 0.6666813492774963 Accuracy 98.14975279066212\n",
      "Training:: Epoch 73, Iteration 10, Current loss 0.8011491298675537 Accuracy 98.12401811292857\n",
      "Training:: Epoch 73, Iteration 20, Current loss 1.0675253868103027 Accuracy 97.30949967750853\n",
      "Training:: Epoch 73, Iteration 30, Current loss 1.0098048448562622 Accuracy 96.00257069408741\n",
      "Training:: Epoch 73, Iteration 40, Current loss 0.6529099941253662 Accuracy 98.0469595325687\n",
      "Training:: Epoch 73, Iteration 50, Current loss 0.8186158537864685 Accuracy 97.43897928994083\n",
      "Training:: Epoch 73, Iteration 60, Current loss 0.8158861398696899 Accuracy 97.72449869224063\n",
      "Training:: Epoch 73, Iteration 70, Current loss 0.6892430186271667 Accuracy 98.39808029055062\n",
      "Training:: Epoch 73, Iteration 80, Current loss 0.9065300226211548 Accuracy 97.72747447847314\n",
      "Training:: Epoch 73, Iteration 90, Current loss 0.9490516185760498 Accuracy 97.11270168627927\n",
      "Training:: Epoch 73, Iteration 100, Current loss 0.6939705014228821 Accuracy 98.6844606668948\n",
      "Training:: Epoch 73, Iteration 110, Current loss 0.700400710105896 Accuracy 97.53022749412641\n",
      "Training:: Epoch 73, Iteration 120, Current loss 0.5833479166030884 Accuracy 97.84400414824518\n",
      "Training:: Epoch 73, Iteration 130, Current loss 1.1655172109603882 Accuracy 95.96686036476612\n",
      "Training:: Epoch 73, Iteration 140, Current loss 0.8414397835731506 Accuracy 97.32961339178956\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 73, Probability Accuracy 65.98262080144315\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 0.858025848865509 Accuracy 97.87265258215963\n",
      "Training:: Epoch 74, Iteration 10, Current loss 0.776884913444519 Accuracy 98.33817721141665\n",
      "Training:: Epoch 74, Iteration 20, Current loss 0.6075884699821472 Accuracy 98.47704110843925\n",
      "Training:: Epoch 74, Iteration 30, Current loss 0.7884525060653687 Accuracy 98.57814950339781\n",
      "Training:: Epoch 74, Iteration 40, Current loss 0.6979101300239563 Accuracy 98.81521819118404\n",
      "Training:: Epoch 74, Iteration 50, Current loss 0.8657131791114807 Accuracy 98.32214765100672\n",
      "Training:: Epoch 74, Iteration 60, Current loss 0.6065575480461121 Accuracy 98.88262997506695\n",
      "Training:: Epoch 74, Iteration 70, Current loss 0.7295835614204407 Accuracy 97.8304856115108\n",
      "Training:: Epoch 74, Iteration 80, Current loss 0.5720452070236206 Accuracy 98.62818261633011\n",
      "Training:: Epoch 74, Iteration 90, Current loss 0.6107513904571533 Accuracy 98.19581859694676\n",
      "Training:: Epoch 74, Iteration 100, Current loss 0.7823696732521057 Accuracy 97.75392004520413\n",
      "Training:: Epoch 74, Iteration 110, Current loss 0.7530950307846069 Accuracy 98.18302681143363\n",
      "Training:: Epoch 74, Iteration 120, Current loss 0.8391073942184448 Accuracy 98.66478608368594\n",
      "Training:: Epoch 74, Iteration 130, Current loss 0.5381778478622437 Accuracy 98.81894557408985\n",
      "Training:: Epoch 74, Iteration 140, Current loss 0.5930086374282837 Accuracy 98.67975905602772\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 74, Probability Accuracy 67.65346505532716\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 0.6742064356803894 Accuracy 99.01588640517363\n",
      "Training:: Epoch 75, Iteration 10, Current loss 0.5489610433578491 Accuracy 99.24684253215403\n",
      "Training:: Epoch 75, Iteration 20, Current loss 0.5185874104499817 Accuracy 99.19041302657457\n",
      "Training:: Epoch 75, Iteration 30, Current loss 0.5574577450752258 Accuracy 98.91439205955335\n",
      "Training:: Epoch 75, Iteration 40, Current loss 0.4676085412502289 Accuracy 99.09558708872602\n",
      "Training:: Epoch 75, Iteration 50, Current loss 0.7295015454292297 Accuracy 98.69504624350691\n",
      "Training:: Epoch 75, Iteration 60, Current loss 0.6781363487243652 Accuracy 98.66469755707341\n",
      "Training:: Epoch 75, Iteration 70, Current loss 0.8426550030708313 Accuracy 97.45677354373007\n",
      "Training:: Epoch 75, Iteration 80, Current loss 0.9174565076828003 Accuracy 97.41977895558568\n",
      "Training:: Epoch 75, Iteration 90, Current loss 0.9788925051689148 Accuracy 98.01474554936162\n",
      "Training:: Epoch 75, Iteration 100, Current loss 1.0392647981643677 Accuracy 95.85728148092856\n",
      "Training:: Epoch 75, Iteration 110, Current loss 0.8332611918449402 Accuracy 97.45378208166859\n",
      "Training:: Epoch 75, Iteration 120, Current loss 0.7576550841331482 Accuracy 96.47887323943662\n",
      "Training:: Epoch 75, Iteration 130, Current loss 1.086022138595581 Accuracy 97.783727502603\n",
      "Training:: Epoch 75, Iteration 140, Current loss 1.1421122550964355 Accuracy 95.57394746311623\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 66.96310106602004\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 0.7115685939788818 Accuracy 98.09836065573771\n",
      "Training:: Epoch 76, Iteration 10, Current loss 1.0614874362945557 Accuracy 97.6162368325463\n",
      "Training:: Epoch 76, Iteration 20, Current loss 0.9738460183143616 Accuracy 96.65592264302981\n",
      "Training:: Epoch 76, Iteration 30, Current loss 1.391533613204956 Accuracy 94.31804206124708\n",
      "Training:: Epoch 76, Iteration 40, Current loss 1.8071552515029907 Accuracy 92.11674528301887\n",
      "Training:: Epoch 76, Iteration 50, Current loss 6.726408958435059 Accuracy 38.37588689212284\n",
      "Training:: Epoch 76, Iteration 60, Current loss 3.7509806156158447 Accuracy 70.90399165144795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 76, Iteration 70, Current loss 2.3209400177001953 Accuracy 83.30733229329174\n",
      "Training:: Epoch 76, Iteration 80, Current loss 2.9586408138275146 Accuracy 82.21912222341453\n",
      "Training:: Epoch 76, Iteration 90, Current loss 1.7818228006362915 Accuracy 91.44872171613282\n",
      "Training:: Epoch 76, Iteration 100, Current loss 3.834482192993164 Accuracy 82.47517768976182\n",
      "Training:: Epoch 76, Iteration 110, Current loss 2.8967130184173584 Accuracy 86.20080022331814\n",
      "Training:: Epoch 76, Iteration 120, Current loss 2.8060059547424316 Accuracy 88.21588594704684\n",
      "Training:: Epoch 76, Iteration 130, Current loss 2.0777525901794434 Accuracy 91.54907245917235\n",
      "Training:: Epoch 76, Iteration 140, Current loss 1.8032361268997192 Accuracy 92.93227913372293\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 76, Probability Accuracy 63.475414871992626\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 2.1637914180755615 Accuracy 88.60778221630419\n",
      "Training:: Epoch 77, Iteration 10, Current loss 1.3236680030822754 Accuracy 95.811084895898\n",
      "Training:: Epoch 77, Iteration 20, Current loss 1.1182005405426025 Accuracy 97.58573727869259\n",
      "Training:: Epoch 77, Iteration 30, Current loss 1.4972680807113647 Accuracy 95.72336852207293\n",
      "Training:: Epoch 77, Iteration 40, Current loss 1.1277562379837036 Accuracy 96.59023522435264\n",
      "Training:: Epoch 77, Iteration 50, Current loss 1.315630555152893 Accuracy 96.03320296911166\n",
      "Training:: Epoch 77, Iteration 60, Current loss 1.0812894105911255 Accuracy 96.00140919499736\n",
      "Training:: Epoch 77, Iteration 70, Current loss 0.9589412212371826 Accuracy 97.56143786720577\n",
      "Training:: Epoch 77, Iteration 80, Current loss 1.064531683921814 Accuracy 97.65670767428237\n",
      "Training:: Epoch 77, Iteration 90, Current loss 1.0469743013381958 Accuracy 97.61685319289006\n",
      "Training:: Epoch 77, Iteration 100, Current loss 0.8481712937355042 Accuracy 98.2129229203765\n",
      "Training:: Epoch 77, Iteration 110, Current loss 0.745040774345398 Accuracy 98.2286188762801\n",
      "Training:: Epoch 77, Iteration 120, Current loss 0.9859766960144043 Accuracy 97.58224678352474\n",
      "Training:: Epoch 77, Iteration 130, Current loss 0.9061066508293152 Accuracy 98.40650406504065\n",
      "Training:: Epoch 77, Iteration 140, Current loss 0.7227535247802734 Accuracy 98.39165299799527\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 77, Probability Accuracy 68.971692625442\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 0.8431469798088074 Accuracy 98.17108129801628\n",
      "Training:: Epoch 78, Iteration 10, Current loss 0.8987781405448914 Accuracy 97.98001737619461\n",
      "Training:: Epoch 78, Iteration 20, Current loss 0.7254059910774231 Accuracy 98.89934881087203\n",
      "Training:: Epoch 78, Iteration 30, Current loss 0.9781719446182251 Accuracy 98.12870835235053\n",
      "Training:: Epoch 78, Iteration 40, Current loss 0.7410498261451721 Accuracy 97.97974973731971\n",
      "Training:: Epoch 78, Iteration 50, Current loss 1.0333271026611328 Accuracy 98.27992033315228\n",
      "Training:: Epoch 78, Iteration 60, Current loss 0.9665159583091736 Accuracy 97.58412048927\n",
      "Training:: Epoch 78, Iteration 70, Current loss 0.6412733197212219 Accuracy 98.51875196974473\n",
      "Training:: Epoch 78, Iteration 80, Current loss 0.7913069725036621 Accuracy 98.62114610335888\n",
      "Training:: Epoch 78, Iteration 90, Current loss 0.5387811064720154 Accuracy 99.01794616151545\n",
      "Training:: Epoch 78, Iteration 100, Current loss 0.5111988186836243 Accuracy 98.96929353661156\n",
      "Training:: Epoch 78, Iteration 110, Current loss 0.5408735275268555 Accuracy 99.10170964937699\n",
      "Training:: Epoch 78, Iteration 120, Current loss 0.9613784551620483 Accuracy 98.19164185687507\n",
      "Training:: Epoch 78, Iteration 130, Current loss 0.8929545283317566 Accuracy 98.63968148639681\n",
      "Training:: Epoch 78, Iteration 140, Current loss 0.7235383987426758 Accuracy 98.64649681528662\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 78, Probability Accuracy 67.9032215896836\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 0.831867516040802 Accuracy 98.46905837867749\n",
      "Training:: Epoch 79, Iteration 10, Current loss 0.6196945905685425 Accuracy 98.41000502765209\n",
      "Training:: Epoch 79, Iteration 20, Current loss 0.629311203956604 Accuracy 98.45977729212197\n",
      "Training:: Epoch 79, Iteration 30, Current loss 0.47457951307296753 Accuracy 99.04393916877198\n",
      "Training:: Epoch 79, Iteration 40, Current loss 0.7125656008720398 Accuracy 98.51267665272853\n",
      "Training:: Epoch 79, Iteration 50, Current loss 0.697226881980896 Accuracy 99.10614002246349\n",
      "Training:: Epoch 79, Iteration 60, Current loss 0.6709954738616943 Accuracy 98.44349680170576\n",
      "Training:: Epoch 79, Iteration 70, Current loss 0.6238352060317993 Accuracy 98.68597479216949\n",
      "Training:: Epoch 79, Iteration 80, Current loss 0.6852263808250427 Accuracy 99.09707560307264\n",
      "Training:: Epoch 79, Iteration 90, Current loss 0.6700546741485596 Accuracy 98.67384096961791\n",
      "Training:: Epoch 79, Iteration 100, Current loss 0.5955260992050171 Accuracy 98.91122943654136\n",
      "Training:: Epoch 79, Iteration 110, Current loss 0.7294003963470459 Accuracy 99.02204836415363\n",
      "Training:: Epoch 79, Iteration 120, Current loss 0.6346772313117981 Accuracy 98.67091972355131\n",
      "Training:: Epoch 79, Iteration 130, Current loss 0.7462590336799622 Accuracy 98.8405633165545\n",
      "Training:: Epoch 79, Iteration 140, Current loss 0.7270838022232056 Accuracy 98.60692518920366\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 79, Probability Accuracy 67.98982346289846\n",
      "Starting Training\n",
      "Training:: Epoch 80, Iteration 0, Current loss 0.635485827922821 Accuracy 98.54613905799424\n",
      "Training:: Epoch 80, Iteration 10, Current loss 0.5734974145889282 Accuracy 98.61905851834898\n",
      "Training:: Epoch 80, Iteration 20, Current loss 0.7162917256355286 Accuracy 98.58567813011761\n",
      "Training:: Epoch 80, Iteration 30, Current loss 0.4581359922885895 Accuracy 98.96959674341687\n",
      "Training:: Epoch 80, Iteration 40, Current loss 0.6470038294792175 Accuracy 98.6345517767789\n",
      "Training:: Epoch 80, Iteration 50, Current loss 0.6687345504760742 Accuracy 99.03596588802372\n",
      "Training:: Epoch 80, Iteration 60, Current loss 0.7713215351104736 Accuracy 98.7546699875467\n",
      "Training:: Epoch 80, Iteration 70, Current loss 0.7623796463012695 Accuracy 98.39039887045534\n",
      "Training:: Epoch 80, Iteration 80, Current loss 0.8317952752113342 Accuracy 98.75980425018436\n",
      "Training:: Epoch 80, Iteration 90, Current loss 0.6207162737846375 Accuracy 98.62632732175965\n",
      "Training:: Epoch 80, Iteration 100, Current loss 0.8863509297370911 Accuracy 98.82726874177992\n",
      "Training:: Epoch 80, Iteration 110, Current loss 0.8328498601913452 Accuracy 98.40682495631617\n",
      "Training:: Epoch 80, Iteration 120, Current loss 0.6814110279083252 Accuracy 98.6951433586893\n",
      "Training:: Epoch 80, Iteration 130, Current loss 0.5575529336929321 Accuracy 98.71787906774597\n",
      "Training:: Epoch 80, Iteration 140, Current loss 0.6858125925064087 Accuracy 98.99934300298176\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 80, Probability Accuracy 68.13467735083236\n",
      "Starting Training\n",
      "Training:: Epoch 81, Iteration 0, Current loss 0.7557323575019836 Accuracy 99.08566174592634\n",
      "Training:: Epoch 81, Iteration 10, Current loss 0.8011250495910645 Accuracy 99.03566649748535\n",
      "Training:: Epoch 81, Iteration 20, Current loss 0.9507019519805908 Accuracy 97.50379436173992\n",
      "Training:: Epoch 81, Iteration 30, Current loss 0.6294121742248535 Accuracy 98.71987011365056\n",
      "Training:: Epoch 81, Iteration 40, Current loss 0.44501492381095886 Accuracy 99.0793376173999\n",
      "Training:: Epoch 81, Iteration 50, Current loss 0.5151594877243042 Accuracy 99.03669995863129\n",
      "Training:: Epoch 81, Iteration 60, Current loss 0.5301411747932434 Accuracy 98.79777568922306\n",
      "Training:: Epoch 81, Iteration 70, Current loss 0.7452400922775269 Accuracy 98.99589481859536\n",
      "Training:: Epoch 81, Iteration 80, Current loss 0.6919220685958862 Accuracy 98.37331505597442\n",
      "Training:: Epoch 81, Iteration 90, Current loss 0.5045511722564697 Accuracy 98.95599858440487\n",
      "Training:: Epoch 81, Iteration 100, Current loss 0.47420811653137207 Accuracy 99.21165006022117\n",
      "Training:: Epoch 81, Iteration 110, Current loss 0.5787099003791809 Accuracy 99.2492344166749\n",
      "Training:: Epoch 81, Iteration 120, Current loss 0.6807311773300171 Accuracy 99.16148228293211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 81, Iteration 130, Current loss 0.683064877986908 Accuracy 99.05761811397129\n",
      "Training:: Epoch 81, Iteration 140, Current loss 0.4789310693740845 Accuracy 99.15132974786599\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 81, Probability Accuracy 67.768661886679\n",
      "Starting Training\n",
      "Training:: Epoch 82, Iteration 0, Current loss 0.5405555367469788 Accuracy 98.99904886996694\n",
      "Training:: Epoch 82, Iteration 10, Current loss 0.6165996789932251 Accuracy 99.38710468252023\n",
      "Training:: Epoch 82, Iteration 20, Current loss 0.48303842544555664 Accuracy 99.2321353949018\n",
      "Training:: Epoch 82, Iteration 30, Current loss 0.4179251790046692 Accuracy 99.11959692389287\n",
      "Training:: Epoch 82, Iteration 40, Current loss 0.6438714265823364 Accuracy 99.10173528411025\n",
      "Training:: Epoch 82, Iteration 50, Current loss 0.6151978373527527 Accuracy 98.86888546725098\n",
      "Training:: Epoch 82, Iteration 60, Current loss 0.5943899154663086 Accuracy 98.96789957030921\n",
      "Training:: Epoch 82, Iteration 70, Current loss 0.6210804581642151 Accuracy 98.79943502824858\n",
      "Training:: Epoch 82, Iteration 80, Current loss 0.6136159896850586 Accuracy 99.1622727048736\n",
      "Training:: Epoch 82, Iteration 90, Current loss 0.7535092830657959 Accuracy 99.152916912198\n",
      "Training:: Epoch 82, Iteration 100, Current loss 0.5415509939193726 Accuracy 98.77284763934638\n",
      "Training:: Epoch 82, Iteration 110, Current loss 0.5529916286468506 Accuracy 99.10697902012356\n",
      "Training:: Epoch 82, Iteration 120, Current loss 0.7695080041885376 Accuracy 98.80239520958084\n",
      "Training:: Epoch 82, Iteration 130, Current loss 0.46476292610168457 Accuracy 99.16558581928231\n",
      "Training:: Epoch 82, Iteration 140, Current loss 0.6331630945205688 Accuracy 98.71656471152693\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 82, Probability Accuracy 68.0158857247433\n",
      "Starting Training\n",
      "Training:: Epoch 83, Iteration 0, Current loss 0.5522007942199707 Accuracy 99.13885132730766\n",
      "Training:: Epoch 83, Iteration 10, Current loss 0.50179523229599 Accuracy 99.47611265980967\n",
      "Training:: Epoch 83, Iteration 20, Current loss 0.7404702305793762 Accuracy 99.11132543400386\n",
      "Training:: Epoch 83, Iteration 30, Current loss 0.7093791961669922 Accuracy 98.74267201481024\n",
      "Training:: Epoch 83, Iteration 40, Current loss 0.5855473875999451 Accuracy 98.89195051884856\n",
      "Training:: Epoch 83, Iteration 50, Current loss 0.6138572692871094 Accuracy 99.08675799086758\n",
      "Training:: Epoch 83, Iteration 60, Current loss 0.45792335271835327 Accuracy 99.24124079446553\n",
      "Training:: Epoch 83, Iteration 70, Current loss 0.6145551204681396 Accuracy 99.04276131917273\n",
      "Training:: Epoch 83, Iteration 80, Current loss 0.5759462714195251 Accuracy 99.19129082426127\n",
      "Training:: Epoch 83, Iteration 90, Current loss 0.4584064483642578 Accuracy 99.36739192945461\n",
      "Training:: Epoch 83, Iteration 100, Current loss 0.6200852990150452 Accuracy 99.23618884572332\n",
      "Training:: Epoch 83, Iteration 110, Current loss 0.5871187448501587 Accuracy 99.15229465068694\n",
      "Training:: Epoch 83, Iteration 120, Current loss 0.6357653737068176 Accuracy 98.71536061662691\n",
      "Training:: Epoch 83, Iteration 130, Current loss 0.676545262336731 Accuracy 98.88737511353315\n",
      "Training:: Epoch 83, Iteration 140, Current loss 0.4815070927143097 Accuracy 99.15055325807533\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 83, Probability Accuracy 67.96237230308695\n",
      "Starting Training\n",
      "Training:: Epoch 84, Iteration 0, Current loss 0.5464593768119812 Accuracy 98.83895936357773\n",
      "Training:: Epoch 84, Iteration 10, Current loss 0.5083678364753723 Accuracy 99.11608357028062\n",
      "Training:: Epoch 84, Iteration 20, Current loss 0.4723285734653473 Accuracy 99.15807038834951\n",
      "Training:: Epoch 84, Iteration 30, Current loss 0.5611066222190857 Accuracy 98.9703043118507\n",
      "Training:: Epoch 84, Iteration 40, Current loss 0.6721303462982178 Accuracy 99.27421322273726\n",
      "Training:: Epoch 84, Iteration 50, Current loss 0.6285651922225952 Accuracy 98.88934878587196\n",
      "Training:: Epoch 84, Iteration 60, Current loss 0.5721814632415771 Accuracy 99.25560860413779\n",
      "Training:: Epoch 84, Iteration 70, Current loss 0.5574270486831665 Accuracy 99.2512523068811\n",
      "Training:: Epoch 84, Iteration 80, Current loss 0.6868150234222412 Accuracy 99.3159974055074\n",
      "Training:: Epoch 84, Iteration 90, Current loss 0.812832236289978 Accuracy 98.74244492103723\n",
      "Training:: Epoch 84, Iteration 100, Current loss 0.6427602767944336 Accuracy 98.9761404150288\n",
      "Training:: Epoch 84, Iteration 110, Current loss 0.8182072043418884 Accuracy 98.79850849330202\n",
      "Training:: Epoch 84, Iteration 120, Current loss 0.745915949344635 Accuracy 99.12860576923077\n",
      "Training:: Epoch 84, Iteration 130, Current loss 0.7515426278114319 Accuracy 98.61943687556767\n",
      "Training:: Epoch 84, Iteration 140, Current loss 0.4517870545387268 Accuracy 99.04171829822387\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 84, Probability Accuracy 68.2119654376826\n",
      "Starting Training\n",
      "Training:: Epoch 85, Iteration 0, Current loss 0.6868939995765686 Accuracy 99.01192504258944\n",
      "Training:: Epoch 85, Iteration 10, Current loss 0.4684503674507141 Accuracy 99.41907261592301\n",
      "Training:: Epoch 85, Iteration 20, Current loss 0.5305089950561523 Accuracy 99.11192557090499\n",
      "Training:: Epoch 85, Iteration 30, Current loss 0.3649730682373047 Accuracy 99.12089112154636\n",
      "Training:: Epoch 85, Iteration 40, Current loss 0.4563332200050354 Accuracy 99.39698928462974\n",
      "Training:: Epoch 85, Iteration 50, Current loss 0.4454893171787262 Accuracy 99.27211899272119\n",
      "Training:: Epoch 85, Iteration 60, Current loss 0.5914058685302734 Accuracy 99.149177538287\n",
      "Training:: Epoch 85, Iteration 70, Current loss 0.7212953567504883 Accuracy 99.12565010929373\n",
      "Training:: Epoch 85, Iteration 80, Current loss 0.7193324565887451 Accuracy 98.68076347305389\n",
      "Training:: Epoch 85, Iteration 90, Current loss 0.5438367128372192 Accuracy 98.77541219785498\n",
      "Training:: Epoch 85, Iteration 100, Current loss 0.449273020029068 Accuracy 99.36937310708211\n",
      "Training:: Epoch 85, Iteration 110, Current loss 0.5366150736808777 Accuracy 99.08256880733946\n",
      "Training:: Epoch 85, Iteration 120, Current loss 0.4782068133354187 Accuracy 99.22266139657444\n",
      "Training:: Epoch 85, Iteration 130, Current loss 0.7197635173797607 Accuracy 98.85867407701468\n",
      "Training:: Epoch 85, Iteration 140, Current loss 0.43284738063812256 Accuracy 98.9486096099047\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 85, Probability Accuracy 67.81661971646874\n",
      "Starting Training\n",
      "Training:: Epoch 86, Iteration 0, Current loss 0.5472274422645569 Accuracy 99.06669538373178\n",
      "Training:: Epoch 86, Iteration 10, Current loss 0.6013290882110596 Accuracy 99.23485797474355\n",
      "Training:: Epoch 86, Iteration 20, Current loss 0.6594058275222778 Accuracy 99.2260277146962\n",
      "Training:: Epoch 86, Iteration 30, Current loss 0.4306805431842804 Accuracy 99.33631655689075\n",
      "Training:: Epoch 86, Iteration 40, Current loss 0.6636754870414734 Accuracy 98.82544045982756\n",
      "Training:: Epoch 86, Iteration 50, Current loss 0.46066707372665405 Accuracy 99.37725335955425\n",
      "Training:: Epoch 86, Iteration 60, Current loss 0.4352392256259918 Accuracy 99.32741762278867\n",
      "Training:: Epoch 86, Iteration 70, Current loss 0.7076793313026428 Accuracy 98.74587458745874\n",
      "Training:: Epoch 86, Iteration 80, Current loss 0.6383986473083496 Accuracy 98.8714592032502\n",
      "Training:: Epoch 86, Iteration 90, Current loss 0.4883815348148346 Accuracy 99.24005970959425\n",
      "Training:: Epoch 86, Iteration 100, Current loss 0.549026608467102 Accuracy 99.18057663125948\n",
      "Training:: Epoch 86, Iteration 110, Current loss 0.5649187564849854 Accuracy 99.05150836516928\n",
      "Training:: Epoch 86, Iteration 120, Current loss 0.6437541842460632 Accuracy 98.83\n",
      "Training:: Epoch 86, Iteration 130, Current loss 0.4840638041496277 Accuracy 99.36687770743086\n",
      "Training:: Epoch 86, Iteration 140, Current loss 0.5254594683647156 Accuracy 98.77492877492878\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 86, Probability Accuracy 66.68001098046392\n",
      "Starting Training\n",
      "Training:: Epoch 87, Iteration 0, Current loss 0.46987321972846985 Accuracy 99.181762825392\n",
      "Training:: Epoch 87, Iteration 10, Current loss 0.5073303580284119 Accuracy 99.27778156298972\n",
      "Training:: Epoch 87, Iteration 20, Current loss 0.42972028255462646 Accuracy 99.03938520653217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 87, Iteration 30, Current loss 0.5148950219154358 Accuracy 99.02514950438272\n",
      "Training:: Epoch 87, Iteration 40, Current loss 0.44468289613723755 Accuracy 99.2263404102195\n",
      "Training:: Epoch 87, Iteration 50, Current loss 0.4944378137588501 Accuracy 99.06517094017094\n",
      "Training:: Epoch 87, Iteration 60, Current loss 0.48295313119888306 Accuracy 99.31921331316188\n",
      "Training:: Epoch 87, Iteration 70, Current loss 0.5235454440116882 Accuracy 99.17456306590861\n",
      "Training:: Epoch 87, Iteration 80, Current loss 0.4071255326271057 Accuracy 99.08448036368229\n",
      "Training:: Epoch 87, Iteration 90, Current loss 0.5646202564239502 Accuracy 98.8327331429281\n",
      "Training:: Epoch 87, Iteration 100, Current loss 0.4072774350643158 Accuracy 99.29460217315106\n",
      "Training:: Epoch 87, Iteration 110, Current loss 0.7864325642585754 Accuracy 98.87736656835696\n",
      "Training:: Epoch 87, Iteration 120, Current loss 0.7080487608909607 Accuracy 99.07175112895133\n",
      "Training:: Epoch 87, Iteration 130, Current loss 0.4394160807132721 Accuracy 99.06481780070945\n",
      "Training:: Epoch 87, Iteration 140, Current loss 0.39992788434028625 Accuracy 99.09222199298536\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 87, Probability Accuracy 68.32111647788548\n",
      "Starting Training\n",
      "Training:: Epoch 88, Iteration 0, Current loss 0.4524680972099304 Accuracy 99.3340505097075\n",
      "Training:: Epoch 88, Iteration 10, Current loss 0.6341386437416077 Accuracy 98.96089174381258\n",
      "Training:: Epoch 88, Iteration 20, Current loss 0.6661299467086792 Accuracy 99.00036796271311\n",
      "Training:: Epoch 88, Iteration 30, Current loss 0.4946655035018921 Accuracy 98.32275182412585\n",
      "Training:: Epoch 88, Iteration 40, Current loss 0.5390352010726929 Accuracy 99.22773068217123\n",
      "Training:: Epoch 88, Iteration 50, Current loss 0.5601046085357666 Accuracy 99.20106524633822\n",
      "Training:: Epoch 88, Iteration 60, Current loss 0.5670298933982849 Accuracy 99.13820119084926\n",
      "Training:: Epoch 88, Iteration 70, Current loss 0.6801389455795288 Accuracy 99.1477143247801\n",
      "Training:: Epoch 88, Iteration 80, Current loss 0.7222035527229309 Accuracy 98.68894838888376\n",
      "Training:: Epoch 88, Iteration 90, Current loss 0.6790163516998291 Accuracy 99.05555917410278\n",
      "Training:: Epoch 88, Iteration 100, Current loss 0.42642268538475037 Accuracy 99.10216565536975\n",
      "Training:: Epoch 88, Iteration 110, Current loss 0.5161231160163879 Accuracy 99.31843998485422\n",
      "Training:: Epoch 88, Iteration 120, Current loss 0.6353572010993958 Accuracy 99.04282612985988\n",
      "Training:: Epoch 88, Iteration 130, Current loss 0.6093037128448486 Accuracy 99.16081614743308\n",
      "Training:: Epoch 88, Iteration 140, Current loss 0.5209046006202698 Accuracy 99.28870919436957\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 88, Probability Accuracy 67.5318139334244\n",
      "Starting Training\n",
      "Training:: Epoch 89, Iteration 0, Current loss 0.4619334936141968 Accuracy 99.492519607197\n",
      "Training:: Epoch 89, Iteration 10, Current loss 0.42505019903182983 Accuracy 99.38259769145223\n",
      "Training:: Epoch 89, Iteration 20, Current loss 0.7522932291030884 Accuracy 98.5187969924812\n",
      "Training:: Epoch 89, Iteration 30, Current loss 0.5106873512268066 Accuracy 99.19923126201154\n",
      "Training:: Epoch 89, Iteration 40, Current loss 0.37700605392456055 Accuracy 99.48611994066539\n",
      "Training:: Epoch 89, Iteration 50, Current loss 0.4037435054779053 Accuracy 99.1469238258612\n",
      "Training:: Epoch 89, Iteration 60, Current loss 0.7312996983528137 Accuracy 98.96940838823299\n",
      "Training:: Epoch 89, Iteration 70, Current loss 0.44780242443084717 Accuracy 98.73250787603487\n",
      "Training:: Epoch 89, Iteration 80, Current loss 0.5184308886528015 Accuracy 99.10729467777588\n",
      "Training:: Epoch 89, Iteration 90, Current loss 0.6275990009307861 Accuracy 98.83274704189319\n",
      "Training:: Epoch 89, Iteration 100, Current loss 0.5315929651260376 Accuracy 99.10087811436495\n",
      "Training:: Epoch 89, Iteration 110, Current loss 0.5033356547355652 Accuracy 98.92939077236808\n",
      "Training:: Epoch 89, Iteration 120, Current loss 0.4710733890533447 Accuracy 98.5083683066188\n",
      "Training:: Epoch 89, Iteration 130, Current loss 0.5825303196907043 Accuracy 98.85834832785258\n",
      "Training:: Epoch 89, Iteration 140, Current loss 0.5352244973182678 Accuracy 98.55613893376413\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 89, Probability Accuracy 67.08221949163067\n",
      "Starting Training\n",
      "Training:: Epoch 90, Iteration 0, Current loss 0.4264357089996338 Accuracy 99.18913440097303\n",
      "Training:: Epoch 90, Iteration 10, Current loss 0.35813048481941223 Accuracy 99.34764427097853\n",
      "Training:: Epoch 90, Iteration 20, Current loss 0.3860006034374237 Accuracy 99.1698784464868\n",
      "Training:: Epoch 90, Iteration 30, Current loss 0.38718053698539734 Accuracy 99.20043219881146\n",
      "Training:: Epoch 90, Iteration 40, Current loss 0.45570680499076843 Accuracy 98.94751881131536\n",
      "Training:: Epoch 90, Iteration 50, Current loss 0.38699203729629517 Accuracy 99.33455331891533\n",
      "Training:: Epoch 90, Iteration 60, Current loss 0.47099432349205017 Accuracy 98.79714321513595\n",
      "Training:: Epoch 90, Iteration 70, Current loss 0.4144253134727478 Accuracy 99.31577651981294\n",
      "Training:: Epoch 90, Iteration 80, Current loss 0.47866976261138916 Accuracy 98.97940586841625\n",
      "Training:: Epoch 90, Iteration 90, Current loss 0.5544564723968506 Accuracy 99.06758320288573\n",
      "Training:: Epoch 90, Iteration 100, Current loss 0.41482827067375183 Accuracy 99.39011890118901\n",
      "Training:: Epoch 90, Iteration 110, Current loss 0.587370753288269 Accuracy 98.79583719282951\n",
      "Training:: Epoch 90, Iteration 120, Current loss 0.42098286747932434 Accuracy 99.18874807987712\n",
      "Training:: Epoch 90, Iteration 130, Current loss 0.7157635688781738 Accuracy 97.89643344216225\n",
      "Training:: Epoch 90, Iteration 140, Current loss 0.5920801758766174 Accuracy 98.632918245804\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 90, Probability Accuracy 65.98245740168237\n",
      "Starting Training\n",
      "Training:: Epoch 91, Iteration 0, Current loss 1.8531595468521118 Accuracy 85.23887300939158\n",
      "Training:: Epoch 91, Iteration 10, Current loss 3.264565944671631 Accuracy 81.67577413479053\n",
      "Training:: Epoch 91, Iteration 20, Current loss 9.405956268310547 Accuracy 59.36213991769547\n",
      "Training:: Epoch 91, Iteration 30, Current loss 5.720942974090576 Accuracy 51.029919687070866\n",
      "Training:: Epoch 91, Iteration 40, Current loss 5.374956130981445 Accuracy 61.14767932489451\n",
      "Training:: Epoch 91, Iteration 50, Current loss 5.025574207305908 Accuracy 64.83924154987633\n",
      "Training:: Epoch 91, Iteration 60, Current loss 6.736597061157227 Accuracy 68.81449484041276\n",
      "Training:: Epoch 91, Iteration 70, Current loss 5.338295936584473 Accuracy 73.17945839764276\n",
      "Training:: Epoch 91, Iteration 80, Current loss 5.682406902313232 Accuracy 64.01875799346313\n",
      "Training:: Epoch 91, Iteration 90, Current loss 3.072028398513794 Accuracy 80.66946050607395\n",
      "Training:: Epoch 91, Iteration 100, Current loss 4.029180526733398 Accuracy 74.51530612244898\n",
      "Training:: Epoch 91, Iteration 110, Current loss 6.385695934295654 Accuracy 65.95545865130042\n",
      "Training:: Epoch 91, Iteration 120, Current loss 1.6496509313583374 Accuracy 93.2140653917335\n",
      "Training:: Epoch 91, Iteration 130, Current loss 1.9630283117294312 Accuracy 90.64966258721263\n",
      "Training:: Epoch 91, Iteration 140, Current loss 1.7555259466171265 Accuracy 92.00857888847445\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 91, Probability Accuracy 64.34511009875881\n",
      "Starting Training\n",
      "Training:: Epoch 92, Iteration 0, Current loss 2.6675150394439697 Accuracy 81.29863661831538\n",
      "Training:: Epoch 92, Iteration 10, Current loss 1.5383683443069458 Accuracy 92.0001846466325\n",
      "Training:: Epoch 92, Iteration 20, Current loss 1.652911901473999 Accuracy 95.52484369858506\n",
      "Training:: Epoch 92, Iteration 30, Current loss 4.185575008392334 Accuracy 84.56917710890066\n",
      "Training:: Epoch 92, Iteration 40, Current loss 2.976539373397827 Accuracy 80.62537402752842\n",
      "Training:: Epoch 92, Iteration 50, Current loss 1.856682300567627 Accuracy 92.92869770182675\n",
      "Training:: Epoch 92, Iteration 60, Current loss 1.8454965353012085 Accuracy 94.030014240333\n",
      "Training:: Epoch 92, Iteration 70, Current loss 2.0561747550964355 Accuracy 89.46842699557023\n",
      "Training:: Epoch 92, Iteration 80, Current loss 1.8943560123443604 Accuracy 91.55076495132128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 92, Iteration 90, Current loss 1.6632229089736938 Accuracy 93.63316936050597\n",
      "Training:: Epoch 92, Iteration 100, Current loss 1.3144694566726685 Accuracy 94.49465142014017\n",
      "Training:: Epoch 92, Iteration 110, Current loss 1.186171054840088 Accuracy 96.81994530305921\n",
      "Training:: Epoch 92, Iteration 120, Current loss 1.7142467498779297 Accuracy 92.44208720068637\n",
      "Training:: Epoch 92, Iteration 130, Current loss 1.0153151750564575 Accuracy 96.68195786754553\n",
      "Training:: Epoch 92, Iteration 140, Current loss 1.179618239402771 Accuracy 95.69770088777601\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 92, Probability Accuracy 65.65124608657572\n",
      "Starting Training\n",
      "Training:: Epoch 93, Iteration 0, Current loss 1.4774198532104492 Accuracy 92.95150636378283\n",
      "Training:: Epoch 93, Iteration 10, Current loss 0.9249299168586731 Accuracy 97.73239865894544\n",
      "Training:: Epoch 93, Iteration 20, Current loss 0.6579246520996094 Accuracy 98.44076707091224\n",
      "Training:: Epoch 93, Iteration 30, Current loss 1.3245631456375122 Accuracy 96.83392636700312\n",
      "Training:: Epoch 93, Iteration 40, Current loss 1.109374761581421 Accuracy 96.12583911707817\n",
      "Training:: Epoch 93, Iteration 50, Current loss 1.219362735748291 Accuracy 95.25743219531759\n",
      "Training:: Epoch 93, Iteration 60, Current loss 0.6632702350616455 Accuracy 98.66680032076985\n",
      "Training:: Epoch 93, Iteration 70, Current loss 0.821563720703125 Accuracy 98.0262220322075\n",
      "Training:: Epoch 93, Iteration 80, Current loss 0.8495097160339355 Accuracy 98.00696594427245\n",
      "Training:: Epoch 93, Iteration 90, Current loss 0.8771246671676636 Accuracy 97.95009956659248\n",
      "Training:: Epoch 93, Iteration 100, Current loss 0.8451710939407349 Accuracy 97.43146649909669\n",
      "Training:: Epoch 93, Iteration 110, Current loss 0.7922186851501465 Accuracy 98.26332794830371\n",
      "Training:: Epoch 93, Iteration 120, Current loss 0.8333513736724854 Accuracy 98.0852533115701\n",
      "Training:: Epoch 93, Iteration 130, Current loss 0.9373235702514648 Accuracy 98.726318484383\n",
      "Training:: Epoch 93, Iteration 140, Current loss 0.6916325688362122 Accuracy 98.3840575572432\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 93, Probability Accuracy 67.36081608376526\n",
      "Starting Training\n",
      "Training:: Epoch 94, Iteration 0, Current loss 0.9309065937995911 Accuracy 98.12203295168948\n",
      "Training:: Epoch 94, Iteration 10, Current loss 0.7596298456192017 Accuracy 98.70746663460382\n",
      "Training:: Epoch 94, Iteration 20, Current loss 0.7004827857017517 Accuracy 98.61542231661494\n",
      "Training:: Epoch 94, Iteration 30, Current loss 0.6890463829040527 Accuracy 98.4490769810373\n",
      "Training:: Epoch 94, Iteration 40, Current loss 0.6527708172798157 Accuracy 98.40305892937472\n",
      "Training:: Epoch 94, Iteration 50, Current loss 0.7946180701255798 Accuracy 98.49155826183227\n",
      "Training:: Epoch 94, Iteration 60, Current loss 0.8722683191299438 Accuracy 98.71341583186015\n",
      "Training:: Epoch 94, Iteration 70, Current loss 0.9691141843795776 Accuracy 98.51271229022208\n",
      "Training:: Epoch 94, Iteration 80, Current loss 0.6184744238853455 Accuracy 98.79053279115838\n",
      "Training:: Epoch 94, Iteration 90, Current loss 0.6590290665626526 Accuracy 98.88493761678018\n",
      "Training:: Epoch 94, Iteration 100, Current loss 0.6250028014183044 Accuracy 98.55548260013131\n",
      "Training:: Epoch 94, Iteration 110, Current loss 0.9295350909233093 Accuracy 98.00078657577346\n",
      "Training:: Epoch 94, Iteration 120, Current loss 0.7229851484298706 Accuracy 98.50840601693844\n",
      "Training:: Epoch 94, Iteration 130, Current loss 0.7997810244560242 Accuracy 97.9453237410072\n",
      "Training:: Epoch 94, Iteration 140, Current loss 0.5701072812080383 Accuracy 98.65133902767965\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 94, Probability Accuracy 67.6976646906189\n",
      "Starting Training\n",
      "Training:: Epoch 95, Iteration 0, Current loss 0.5835639238357544 Accuracy 99.10537342166356\n",
      "Training:: Epoch 95, Iteration 10, Current loss 0.7702198624610901 Accuracy 98.5434843793977\n",
      "Training:: Epoch 95, Iteration 20, Current loss 0.9745417237281799 Accuracy 98.70210907275677\n",
      "Training:: Epoch 95, Iteration 30, Current loss 0.8415367007255554 Accuracy 98.34045234927527\n",
      "Training:: Epoch 95, Iteration 40, Current loss 0.8390042781829834 Accuracy 98.39347800023978\n",
      "Training:: Epoch 95, Iteration 50, Current loss 0.7715751528739929 Accuracy 99.14133500470072\n",
      "Training:: Epoch 95, Iteration 60, Current loss 0.9960314631462097 Accuracy 98.60704336024001\n",
      "Training:: Epoch 95, Iteration 70, Current loss 0.7310335636138916 Accuracy 98.62850327966608\n",
      "Training:: Epoch 95, Iteration 80, Current loss 0.6937111616134644 Accuracy 98.31411853310932\n",
      "Training:: Epoch 95, Iteration 90, Current loss 0.7126702070236206 Accuracy 98.46526655896608\n",
      "Training:: Epoch 95, Iteration 100, Current loss 0.5727266073226929 Accuracy 98.58753907994641\n",
      "Training:: Epoch 95, Iteration 110, Current loss 0.7018335461616516 Accuracy 98.92383723615538\n",
      "Training:: Epoch 95, Iteration 120, Current loss 0.7401736378669739 Accuracy 98.62647038106643\n",
      "Training:: Epoch 95, Iteration 130, Current loss 0.7114222049713135 Accuracy 98.61121105274519\n",
      "Training:: Epoch 95, Iteration 140, Current loss 0.5789370536804199 Accuracy 99.03846153846153\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 95, Probability Accuracy 67.48720579873071\n",
      "Starting Training\n",
      "Training:: Epoch 96, Iteration 0, Current loss 0.6622126698493958 Accuracy 98.91983479826327\n",
      "Training:: Epoch 96, Iteration 10, Current loss 0.5483278632164001 Accuracy 98.85080245690509\n",
      "Training:: Epoch 96, Iteration 20, Current loss 0.678006112575531 Accuracy 98.41773490713712\n",
      "Training:: Epoch 96, Iteration 30, Current loss 0.6625662446022034 Accuracy 98.94069756684245\n",
      "Training:: Epoch 96, Iteration 40, Current loss 0.44428375363349915 Accuracy 99.15093723466788\n",
      "Training:: Epoch 96, Iteration 50, Current loss 0.7057783007621765 Accuracy 98.86253416806278\n",
      "Training:: Epoch 96, Iteration 60, Current loss 0.604686975479126 Accuracy 99.38267736274756\n",
      "Training:: Epoch 96, Iteration 70, Current loss 0.7793947458267212 Accuracy 98.97189501963216\n",
      "Training:: Epoch 96, Iteration 80, Current loss 0.6621864438056946 Accuracy 99.03318214370971\n",
      "Training:: Epoch 96, Iteration 90, Current loss 0.609883725643158 Accuracy 98.74181405661837\n",
      "Training:: Epoch 96, Iteration 100, Current loss 0.5852347016334534 Accuracy 99.27739522230723\n",
      "Training:: Epoch 96, Iteration 110, Current loss 0.47487813234329224 Accuracy 99.11849556576557\n",
      "Training:: Epoch 96, Iteration 120, Current loss 0.6956862807273865 Accuracy 98.6829048263419\n",
      "Training:: Epoch 96, Iteration 130, Current loss 0.6478017568588257 Accuracy 99.0045841519319\n",
      "Training:: Epoch 96, Iteration 140, Current loss 0.6477765440940857 Accuracy 98.88584929122109\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 96, Probability Accuracy 67.43124138066261\n",
      "Starting Training\n",
      "Training:: Epoch 97, Iteration 0, Current loss 0.7010119557380676 Accuracy 99.20997649516845\n",
      "Training:: Epoch 97, Iteration 10, Current loss 0.7592838406562805 Accuracy 98.8269218166328\n",
      "Training:: Epoch 97, Iteration 20, Current loss 0.6381443738937378 Accuracy 99.0864538230956\n",
      "Training:: Epoch 97, Iteration 30, Current loss 0.614909827709198 Accuracy 98.82509001326511\n",
      "Training:: Epoch 97, Iteration 40, Current loss 0.4100435674190521 Accuracy 98.8534359206457\n",
      "Training:: Epoch 97, Iteration 50, Current loss 0.7126919031143188 Accuracy 98.67684478371501\n",
      "Training:: Epoch 97, Iteration 60, Current loss 0.5368202924728394 Accuracy 98.7492310846832\n",
      "Training:: Epoch 97, Iteration 70, Current loss 0.5805956125259399 Accuracy 98.66048862679023\n",
      "Training:: Epoch 97, Iteration 80, Current loss 0.5143234729766846 Accuracy 99.02377473768097\n",
      "Training:: Epoch 97, Iteration 90, Current loss 0.7443867325782776 Accuracy 99.00353584056573\n",
      "Training:: Epoch 97, Iteration 100, Current loss 0.5778294205665588 Accuracy 99.41289989399581\n",
      "Training:: Epoch 97, Iteration 110, Current loss 0.5748621225357056 Accuracy 99.15302318041822\n",
      "Training:: Epoch 97, Iteration 120, Current loss 0.6689058542251587 Accuracy 98.69410368025326\n",
      "Training:: Epoch 97, Iteration 130, Current loss 0.7283366322517395 Accuracy 98.79395760578251\n",
      "Training:: Epoch 97, Iteration 140, Current loss 0.5304144620895386 Accuracy 98.94172981447609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 97, Probability Accuracy 67.3907182399885\n",
      "Starting Training\n",
      "Training:: Epoch 98, Iteration 0, Current loss 0.4360431432723999 Accuracy 99.17747107207585\n",
      "Training:: Epoch 98, Iteration 10, Current loss 0.4909144639968872 Accuracy 99.2842535787321\n",
      "Training:: Epoch 98, Iteration 20, Current loss 0.6540205478668213 Accuracy 99.21513225021585\n",
      "Training:: Epoch 98, Iteration 30, Current loss 0.5513595342636108 Accuracy 98.94533493447501\n",
      "Training:: Epoch 98, Iteration 40, Current loss 0.5391235947608948 Accuracy 99.23132110279799\n",
      "Training:: Epoch 98, Iteration 50, Current loss 0.7823745012283325 Accuracy 98.79767827529021\n",
      "Training:: Epoch 98, Iteration 60, Current loss 0.5759575963020325 Accuracy 98.80131019583246\n",
      "Training:: Epoch 98, Iteration 70, Current loss 0.41101256012916565 Accuracy 99.5329830706363\n",
      "Training:: Epoch 98, Iteration 80, Current loss 0.7269703149795532 Accuracy 98.84043357701033\n",
      "Training:: Epoch 98, Iteration 90, Current loss 0.6731001734733582 Accuracy 99.1522240703976\n",
      "Training:: Epoch 98, Iteration 100, Current loss 0.7013549208641052 Accuracy 99.17677324513046\n",
      "Training:: Epoch 98, Iteration 110, Current loss 0.5835822820663452 Accuracy 98.86356039842235\n",
      "Training:: Epoch 98, Iteration 120, Current loss 0.4936385750770569 Accuracy 99.32692307692308\n",
      "Training:: Epoch 98, Iteration 130, Current loss 0.5910910367965698 Accuracy 98.85555211877514\n",
      "Training:: Epoch 98, Iteration 140, Current loss 0.6952400803565979 Accuracy 99.05505341002466\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 98, Probability Accuracy 67.47462401715043\n",
      "Starting Training\n",
      "Training:: Epoch 99, Iteration 0, Current loss 0.4571748673915863 Accuracy 99.46801436361218\n",
      "Training:: Epoch 99, Iteration 10, Current loss 0.5281127691268921 Accuracy 99.45657964837507\n",
      "Training:: Epoch 99, Iteration 20, Current loss 0.4732266664505005 Accuracy 99.30316914852997\n",
      "Training:: Epoch 99, Iteration 30, Current loss 0.3712960481643677 Accuracy 99.22818791946308\n",
      "Training:: Epoch 99, Iteration 40, Current loss 0.7872492671012878 Accuracy 99.257065948856\n",
      "Training:: Epoch 99, Iteration 50, Current loss 0.5597550868988037 Accuracy 99.0837762591749\n",
      "Training:: Epoch 99, Iteration 60, Current loss 0.44466784596443176 Accuracy 99.3429886821408\n",
      "Training:: Epoch 99, Iteration 70, Current loss 0.5786234140396118 Accuracy 99.09941852219544\n",
      "Training:: Epoch 99, Iteration 80, Current loss 0.6498819589614868 Accuracy 98.94058037770613\n",
      "Training:: Epoch 99, Iteration 90, Current loss 0.6596323847770691 Accuracy 99.15677701436603\n",
      "Training:: Epoch 99, Iteration 100, Current loss 0.5364949703216553 Accuracy 99.43879340582252\n",
      "Training:: Epoch 99, Iteration 110, Current loss 0.5302300453186035 Accuracy 99.37958797063699\n",
      "Training:: Epoch 99, Iteration 120, Current loss 0.6037470698356628 Accuracy 99.11725516322451\n",
      "Training:: Epoch 99, Iteration 130, Current loss 0.5599900484085083 Accuracy 99.27578402845134\n",
      "Training:: Epoch 99, Iteration 140, Current loss 0.6313056349754333 Accuracy 98.7554846429996\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 99, Probability Accuracy 67.45763044202903\n",
      "Starting Training\n",
      "Training:: Epoch 100, Iteration 0, Current loss 0.4919354319572449 Accuracy 98.87757463550105\n",
      "Training:: Epoch 100, Iteration 10, Current loss 0.4854937493801117 Accuracy 99.06258838740959\n",
      "Training:: Epoch 100, Iteration 20, Current loss 0.643767237663269 Accuracy 98.395873483732\n",
      "Training:: Epoch 100, Iteration 30, Current loss 0.5177839398384094 Accuracy 99.39010444461385\n",
      "Training:: Epoch 100, Iteration 40, Current loss 0.45143306255340576 Accuracy 99.40327961283654\n",
      "Training:: Epoch 100, Iteration 50, Current loss 0.5391346216201782 Accuracy 99.12712623097583\n",
      "Training:: Epoch 100, Iteration 60, Current loss 0.5461761355400085 Accuracy 99.11276841969911\n",
      "Training:: Epoch 100, Iteration 70, Current loss 0.5023736953735352 Accuracy 99.30011490650789\n",
      "Training:: Epoch 100, Iteration 80, Current loss 0.4711802899837494 Accuracy 99.16333435865887\n",
      "Training:: Epoch 100, Iteration 90, Current loss 0.5702593326568604 Accuracy 99.17614295660246\n",
      "Training:: Epoch 100, Iteration 100, Current loss 0.4238493740558624 Accuracy 99.43753570330009\n",
      "Training:: Epoch 100, Iteration 110, Current loss 0.47906816005706787 Accuracy 99.22621426236059\n",
      "Training:: Epoch 100, Iteration 120, Current loss 0.497184693813324 Accuracy 98.7058764288232\n",
      "Training:: Epoch 100, Iteration 130, Current loss 0.4841088652610779 Accuracy 99.29432519847104\n",
      "Training:: Epoch 100, Iteration 140, Current loss 0.5657258033752441 Accuracy 98.91896285458832\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 100, Probability Accuracy 67.28508029464246\n",
      "Starting Training\n",
      "Training:: Epoch 101, Iteration 0, Current loss 0.40848293900489807 Accuracy 99.33433433433433\n",
      "Training:: Epoch 101, Iteration 10, Current loss 0.6915724873542786 Accuracy 98.9351403678606\n",
      "Training:: Epoch 101, Iteration 20, Current loss 0.8198502659797668 Accuracy 99.10339565051507\n",
      "Training:: Epoch 101, Iteration 30, Current loss 0.39153623580932617 Accuracy 99.4927195866604\n",
      "Training:: Epoch 101, Iteration 40, Current loss 0.6456488966941833 Accuracy 99.16929115888448\n",
      "Training:: Epoch 101, Iteration 50, Current loss 0.5678090453147888 Accuracy 99.03549635850666\n",
      "Training:: Epoch 101, Iteration 60, Current loss 0.606844961643219 Accuracy 99.20975909160657\n",
      "Training:: Epoch 101, Iteration 70, Current loss 0.5643529295921326 Accuracy 99.07217814179677\n",
      "Training:: Epoch 101, Iteration 80, Current loss 0.5659434199333191 Accuracy 98.91919191919192\n",
      "Training:: Epoch 101, Iteration 90, Current loss 0.4307808578014374 Accuracy 99.34841790511894\n",
      "Training:: Epoch 101, Iteration 100, Current loss 0.6185203194618225 Accuracy 99.09257561869845\n",
      "Training:: Epoch 101, Iteration 110, Current loss 0.6237599849700928 Accuracy 98.77216916780355\n",
      "Training:: Epoch 101, Iteration 120, Current loss 0.46799325942993164 Accuracy 99.23650724001754\n",
      "Training:: Epoch 101, Iteration 130, Current loss 0.5642016530036926 Accuracy 99.12838057999349\n",
      "Training:: Epoch 101, Iteration 140, Current loss 0.3734944462776184 Accuracy 99.19354838709677\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 101, Probability Accuracy 67.3718455676181\n",
      "Starting Training\n",
      "Training:: Epoch 102, Iteration 0, Current loss 0.4210268557071686 Accuracy 99.31275851097678\n",
      "Training:: Epoch 102, Iteration 10, Current loss 0.42126935720443726 Accuracy 99.37843157567596\n",
      "Training:: Epoch 102, Iteration 20, Current loss 0.5637795329093933 Accuracy 99.30381191508788\n",
      "Training:: Epoch 102, Iteration 30, Current loss 0.5919517278671265 Accuracy 99.08630221130221\n",
      "Training:: Epoch 102, Iteration 40, Current loss 0.6443628668785095 Accuracy 99.18443002780351\n",
      "Training:: Epoch 102, Iteration 50, Current loss 0.7821550965309143 Accuracy 98.97790055248619\n",
      "Training:: Epoch 102, Iteration 60, Current loss 0.5228676795959473 Accuracy 99.20050940993349\n",
      "Training:: Epoch 102, Iteration 70, Current loss 0.5024797916412354 Accuracy 99.11187508952872\n",
      "Training:: Epoch 102, Iteration 80, Current loss 0.5005048513412476 Accuracy 99.30232558139535\n",
      "Training:: Epoch 102, Iteration 90, Current loss 0.43809211254119873 Accuracy 99.09140369967356\n",
      "Training:: Epoch 102, Iteration 100, Current loss 0.6741558313369751 Accuracy 99.03235560931358\n",
      "Training:: Epoch 102, Iteration 110, Current loss 0.601219117641449 Accuracy 99.18039309302141\n",
      "Training:: Epoch 102, Iteration 120, Current loss 0.4141590893268585 Accuracy 99.6144139091419\n",
      "Training:: Epoch 102, Iteration 130, Current loss 0.7461360096931458 Accuracy 99.11632897809758\n",
      "Training:: Epoch 102, Iteration 140, Current loss 0.5233227014541626 Accuracy 99.35016010548127\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 102, Probability Accuracy 66.80778959339604\n",
      "Starting Training\n",
      "Training:: Epoch 103, Iteration 0, Current loss 0.37585124373435974 Accuracy 99.40482938442354\n",
      "Training:: Epoch 103, Iteration 10, Current loss 0.41212373971939087 Accuracy 99.4341147938561\n",
      "Training:: Epoch 103, Iteration 20, Current loss 0.46143099665641785 Accuracy 99.22486879289463\n",
      "Training:: Epoch 103, Iteration 30, Current loss 0.4196433424949646 Accuracy 99.4875\n",
      "Training:: Epoch 103, Iteration 40, Current loss 0.4765857756137848 Accuracy 99.4335246930008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 103, Iteration 50, Current loss 0.682835042476654 Accuracy 99.24740445210138\n",
      "Training:: Epoch 103, Iteration 60, Current loss 0.6873112916946411 Accuracy 98.65009642168417\n",
      "Training:: Epoch 103, Iteration 70, Current loss 0.42660894989967346 Accuracy 99.13718723037101\n",
      "Training:: Epoch 103, Iteration 80, Current loss 0.37791138887405396 Accuracy 99.26897588303615\n",
      "Training:: Epoch 103, Iteration 90, Current loss 0.581752598285675 Accuracy 98.9833739277772\n",
      "Training:: Epoch 103, Iteration 100, Current loss 0.397438645362854 Accuracy 99.38070404172099\n",
      "Training:: Epoch 103, Iteration 110, Current loss 0.541903555393219 Accuracy 99.04252566203743\n",
      "Training:: Epoch 103, Iteration 120, Current loss 0.5007880926132202 Accuracy 99.17997578976141\n",
      "Training:: Epoch 103, Iteration 130, Current loss 0.6139792799949646 Accuracy 99.03285638579756\n",
      "Training:: Epoch 103, Iteration 140, Current loss 0.416760116815567 Accuracy 99.0836822622732\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 103, Probability Accuracy 66.60059869672351\n",
      "Starting Training\n",
      "Training:: Epoch 104, Iteration 0, Current loss 0.40759024024009705 Accuracy 99.39562432007736\n",
      "Training:: Epoch 104, Iteration 10, Current loss 0.38335782289505005 Accuracy 99.43801652892562\n",
      "Training:: Epoch 104, Iteration 20, Current loss 0.5040462613105774 Accuracy 99.14080037154578\n",
      "Training:: Epoch 104, Iteration 30, Current loss 0.5736564993858337 Accuracy 99.2731979291119\n",
      "Training:: Epoch 104, Iteration 40, Current loss 0.6450163722038269 Accuracy 99.27701909411111\n",
      "Training:: Epoch 104, Iteration 50, Current loss 0.4428400695323944 Accuracy 99.30281903607154\n",
      "Training:: Epoch 104, Iteration 60, Current loss 0.6737078428268433 Accuracy 98.55690687965148\n",
      "Training:: Epoch 104, Iteration 70, Current loss 0.37863460183143616 Accuracy 99.43229722893598\n",
      "Training:: Epoch 104, Iteration 80, Current loss 0.4520053267478943 Accuracy 99.40187520206919\n",
      "Training:: Epoch 104, Iteration 90, Current loss 0.6924697756767273 Accuracy 99.11720658942224\n",
      "Training:: Epoch 104, Iteration 100, Current loss 0.530600905418396 Accuracy 99.28805190331286\n",
      "Training:: Epoch 104, Iteration 110, Current loss 0.6087455153465271 Accuracy 98.86762640449439\n",
      "Training:: Epoch 104, Iteration 120, Current loss 0.544795036315918 Accuracy 99.39506293296907\n",
      "Training:: Epoch 104, Iteration 130, Current loss 0.5345042943954468 Accuracy 99.20801680943914\n",
      "Training:: Epoch 104, Iteration 140, Current loss 0.5425671339035034 Accuracy 99.06809060608121\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 104, Probability Accuracy 66.57862142889823\n",
      "Starting Training\n",
      "Training:: Epoch 105, Iteration 0, Current loss 0.49417707324028015 Accuracy 99.04358618675623\n",
      "Training:: Epoch 105, Iteration 10, Current loss 0.6681135296821594 Accuracy 98.95746455379484\n",
      "Training:: Epoch 105, Iteration 20, Current loss 0.4100353419780731 Accuracy 99.140469180359\n",
      "Training:: Epoch 105, Iteration 30, Current loss 0.527844250202179 Accuracy 98.9371238314765\n",
      "Training:: Epoch 105, Iteration 40, Current loss 0.5332542061805725 Accuracy 98.95513912549687\n",
      "Training:: Epoch 105, Iteration 50, Current loss 0.5169446468353271 Accuracy 98.6650631389056\n",
      "Training:: Epoch 105, Iteration 60, Current loss 0.5674834847450256 Accuracy 99.11293074036165\n",
      "Training:: Epoch 105, Iteration 70, Current loss 0.7291256785392761 Accuracy 98.73425614166355\n",
      "Training:: Epoch 105, Iteration 80, Current loss 0.5391557812690735 Accuracy 98.67099523181496\n",
      "Training:: Epoch 105, Iteration 90, Current loss 0.4943305552005768 Accuracy 99.22567835870285\n",
      "Training:: Epoch 105, Iteration 100, Current loss 0.597001314163208 Accuracy 98.55517983399939\n",
      "Training:: Epoch 105, Iteration 110, Current loss 0.42896509170532227 Accuracy 99.22269376814832\n",
      "Training:: Epoch 105, Iteration 120, Current loss 0.47589218616485596 Accuracy 98.6685975894609\n",
      "Training:: Epoch 105, Iteration 130, Current loss 0.37818753719329834 Accuracy 99.37745882149497\n",
      "Training:: Epoch 105, Iteration 140, Current loss 0.5009849071502686 Accuracy 98.77782627673506\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 105, Probability Accuracy 67.10591245694417\n",
      "Starting Training\n",
      "Training:: Epoch 106, Iteration 0, Current loss 0.5251623392105103 Accuracy 98.95682710183846\n",
      "Training:: Epoch 106, Iteration 10, Current loss 0.6568952202796936 Accuracy 98.93162393162393\n",
      "Training:: Epoch 106, Iteration 20, Current loss 0.40546363592147827 Accuracy 99.34249613815503\n",
      "Training:: Epoch 106, Iteration 30, Current loss 0.4248344898223877 Accuracy 99.06525416184456\n",
      "Training:: Epoch 106, Iteration 40, Current loss 0.44537216424942017 Accuracy 99.03060517933804\n",
      "Training:: Epoch 106, Iteration 50, Current loss 0.5579808950424194 Accuracy 98.77170932358318\n",
      "Training:: Epoch 106, Iteration 60, Current loss 0.38858762383461 Accuracy 99.14186269803169\n",
      "Training:: Epoch 106, Iteration 70, Current loss 0.46360844373703003 Accuracy 99.1359785252915\n",
      "Training:: Epoch 106, Iteration 80, Current loss 0.5503462553024292 Accuracy 98.86142168427531\n",
      "Training:: Epoch 106, Iteration 90, Current loss 0.4631222188472748 Accuracy 99.21990301496943\n",
      "Training:: Epoch 106, Iteration 100, Current loss 0.620070219039917 Accuracy 98.93329986823116\n",
      "Training:: Epoch 106, Iteration 110, Current loss 0.9332081079483032 Accuracy 96.61285008237232\n",
      "Training:: Epoch 106, Iteration 120, Current loss 0.9962184429168701 Accuracy 94.5829750644884\n",
      "Training:: Epoch 106, Iteration 130, Current loss 0.7446003556251526 Accuracy 95.82922142003844\n",
      "Training:: Epoch 106, Iteration 140, Current loss 0.587296724319458 Accuracy 98.81285183625015\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 106, Probability Accuracy 67.04610814449768\n",
      "Starting Training\n",
      "Training:: Epoch 107, Iteration 0, Current loss 0.5415935516357422 Accuracy 98.36350974930362\n",
      "Training:: Epoch 107, Iteration 10, Current loss 0.7591122388839722 Accuracy 97.89220404234841\n",
      "Training:: Epoch 107, Iteration 20, Current loss 0.6496012806892395 Accuracy 97.61786951297094\n",
      "Training:: Epoch 107, Iteration 30, Current loss 0.5867225527763367 Accuracy 98.06834313526669\n",
      "Training:: Epoch 107, Iteration 40, Current loss 0.6609473824501038 Accuracy 97.62323943661971\n",
      "Training:: Epoch 107, Iteration 50, Current loss 0.7171736359596252 Accuracy 98.13552976413328\n",
      "Training:: Epoch 107, Iteration 60, Current loss 0.6283025145530701 Accuracy 97.57838001973734\n",
      "Training:: Epoch 107, Iteration 70, Current loss 0.5621810555458069 Accuracy 97.7353538774128\n",
      "Training:: Epoch 107, Iteration 80, Current loss 0.7701085805892944 Accuracy 98.12598234796276\n",
      "Training:: Epoch 107, Iteration 90, Current loss 0.5182088017463684 Accuracy 98.5998074735276\n",
      "Training:: Epoch 107, Iteration 100, Current loss 0.7648292183876038 Accuracy 97.80102476515799\n",
      "Training:: Epoch 107, Iteration 110, Current loss 0.4965604543685913 Accuracy 98.9238845144357\n",
      "Training:: Epoch 107, Iteration 120, Current loss 0.3877643048763275 Accuracy 98.92502756339582\n",
      "Training:: Epoch 107, Iteration 130, Current loss 0.3703252077102661 Accuracy 99.1921354876387\n",
      "Training:: Epoch 107, Iteration 140, Current loss 0.46424880623817444 Accuracy 98.64663819167826\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 107, Probability Accuracy 67.10052026483834\n",
      "Starting Training\n",
      "Training:: Epoch 108, Iteration 0, Current loss 0.7365248799324036 Accuracy 98.24159021406727\n",
      "Training:: Epoch 108, Iteration 10, Current loss 0.5089057087898254 Accuracy 98.30670164559982\n",
      "Training:: Epoch 108, Iteration 20, Current loss 0.5318314433097839 Accuracy 98.63923848463553\n",
      "Training:: Epoch 108, Iteration 30, Current loss 0.5919155478477478 Accuracy 98.41763592101704\n",
      "Training:: Epoch 108, Iteration 40, Current loss 0.490805059671402 Accuracy 99.28635147190009\n",
      "Training:: Epoch 108, Iteration 50, Current loss 0.678059458732605 Accuracy 98.80156427400026\n",
      "Training:: Epoch 108, Iteration 60, Current loss 0.5363538265228271 Accuracy 99.10032853171595\n",
      "Training:: Epoch 108, Iteration 70, Current loss 0.6306464672088623 Accuracy 98.67931647889601\n",
      "Training:: Epoch 108, Iteration 80, Current loss 0.33503836393356323 Accuracy 99.21381440404323\n",
      "Training:: Epoch 108, Iteration 90, Current loss 0.4789729118347168 Accuracy 99.05262659277155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 108, Iteration 100, Current loss 0.4557134509086609 Accuracy 98.70328772717991\n",
      "Training:: Epoch 108, Iteration 110, Current loss 0.3167456388473511 Accuracy 99.19068056407112\n",
      "Training:: Epoch 108, Iteration 120, Current loss 0.5826885104179382 Accuracy 99.12049252418646\n",
      "Training:: Epoch 108, Iteration 130, Current loss 0.47465866804122925 Accuracy 99.03514445881845\n",
      "Training:: Epoch 108, Iteration 140, Current loss 0.5031968951225281 Accuracy 98.85704389990475\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 108, Probability Accuracy 67.23303747083314\n",
      "Starting Training\n",
      "Training:: Epoch 109, Iteration 0, Current loss 0.41716697812080383 Accuracy 99.28274820687052\n",
      "Training:: Epoch 109, Iteration 10, Current loss 0.4573659896850586 Accuracy 99.33945411491005\n",
      "Training:: Epoch 109, Iteration 20, Current loss 0.5135369300842285 Accuracy 98.76761601509523\n",
      "Training:: Epoch 109, Iteration 30, Current loss 0.4213758111000061 Accuracy 99.32805672355512\n",
      "Training:: Epoch 109, Iteration 40, Current loss 0.4399271309375763 Accuracy 99.11092294665538\n",
      "Training:: Epoch 109, Iteration 50, Current loss 0.6877811551094055 Accuracy 98.80200213342086\n",
      "Training:: Epoch 109, Iteration 60, Current loss 0.6923919320106506 Accuracy 98.42662632375189\n",
      "Training:: Epoch 109, Iteration 70, Current loss 0.42073196172714233 Accuracy 98.99156404537962\n",
      "Training:: Epoch 109, Iteration 80, Current loss 0.6012653112411499 Accuracy 98.42881573321932\n",
      "Training:: Epoch 109, Iteration 90, Current loss 0.41444262862205505 Accuracy 99.52469331401929\n",
      "Training:: Epoch 109, Iteration 100, Current loss 0.4673057198524475 Accuracy 98.83320581484315\n",
      "Training:: Epoch 109, Iteration 110, Current loss 0.5646460652351379 Accuracy 98.98021308980213\n",
      "Training:: Epoch 109, Iteration 120, Current loss 0.45257827639579773 Accuracy 99.21648287869994\n",
      "Training:: Epoch 109, Iteration 130, Current loss 0.8038997054100037 Accuracy 98.81389197030464\n",
      "Training:: Epoch 109, Iteration 140, Current loss 0.3968806862831116 Accuracy 99.40081416091112\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 109, Probability Accuracy 67.47176452133674\n",
      "Starting Training\n",
      "Training:: Epoch 110, Iteration 0, Current loss 0.3593546152114868 Accuracy 99.11845321875694\n",
      "Training:: Epoch 110, Iteration 10, Current loss 0.4963267743587494 Accuracy 99.36613458045066\n",
      "Training:: Epoch 110, Iteration 20, Current loss 0.4976026117801666 Accuracy 99.36686752248711\n",
      "Training:: Epoch 110, Iteration 30, Current loss 0.545070230960846 Accuracy 99.06457793305533\n",
      "Training:: Epoch 110, Iteration 40, Current loss 0.45062410831451416 Accuracy 99.34833901352009\n",
      "Training:: Epoch 110, Iteration 50, Current loss 0.4923744201660156 Accuracy 99.22301206811044\n",
      "Training:: Epoch 110, Iteration 60, Current loss 0.38868531584739685 Accuracy 99.40261245597611\n",
      "Training:: Epoch 110, Iteration 70, Current loss 0.763651430606842 Accuracy 98.24449815545096\n",
      "Training:: Epoch 110, Iteration 80, Current loss 0.5664937496185303 Accuracy 98.02243211334121\n",
      "Training:: Epoch 110, Iteration 90, Current loss 0.45682552456855774 Accuracy 99.16555144454065\n",
      "Training:: Epoch 110, Iteration 100, Current loss 0.5086663365364075 Accuracy 98.77237851662404\n",
      "Training:: Epoch 110, Iteration 110, Current loss 0.5315384864807129 Accuracy 98.98652824125571\n",
      "Training:: Epoch 110, Iteration 120, Current loss 0.5592278242111206 Accuracy 99.2611580217129\n",
      "Training:: Epoch 110, Iteration 130, Current loss 0.634461522102356 Accuracy 98.75076173065204\n",
      "Training:: Epoch 110, Iteration 140, Current loss 0.46439865231513977 Accuracy 98.81614492293023\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 110, Probability Accuracy 66.84569833789763\n",
      "Starting Training\n",
      "Training:: Epoch 111, Iteration 0, Current loss 0.6593482494354248 Accuracy 98.77597279939555\n",
      "Training:: Epoch 111, Iteration 10, Current loss 0.3228132426738739 Accuracy 99.37114524126255\n",
      "Training:: Epoch 111, Iteration 20, Current loss 0.5643013715744019 Accuracy 99.14160682226212\n",
      "Training:: Epoch 111, Iteration 30, Current loss 0.5443387627601624 Accuracy 99.29374836515825\n",
      "Training:: Epoch 111, Iteration 40, Current loss 0.5376723408699036 Accuracy 99.04769400283331\n",
      "Training:: Epoch 111, Iteration 50, Current loss 0.6423826813697815 Accuracy 99.17494270435446\n",
      "Training:: Epoch 111, Iteration 60, Current loss 0.5329982042312622 Accuracy 99.34840035221603\n",
      "Training:: Epoch 111, Iteration 70, Current loss 0.5781022310256958 Accuracy 99.16377971568511\n",
      "Training:: Epoch 111, Iteration 80, Current loss 0.4021347761154175 Accuracy 99.45370117454247\n",
      "Training:: Epoch 111, Iteration 90, Current loss 0.5259172320365906 Accuracy 99.34937026204263\n",
      "Training:: Epoch 111, Iteration 100, Current loss 0.49215608835220337 Accuracy 99.29868634531847\n",
      "Training:: Epoch 111, Iteration 110, Current loss 0.5374071002006531 Accuracy 99.21028723181215\n",
      "Training:: Epoch 111, Iteration 120, Current loss 0.4258198142051697 Accuracy 99.34110491637101\n",
      "Training:: Epoch 111, Iteration 130, Current loss 0.49478331208229065 Accuracy 99.14490561073472\n",
      "Training:: Epoch 111, Iteration 140, Current loss 0.3640388250350952 Accuracy 99.37437714538811\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 111, Probability Accuracy 67.04635324413886\n",
      "Starting Training\n",
      "Training:: Epoch 112, Iteration 0, Current loss 0.44075024127960205 Accuracy 99.25173703901658\n",
      "Training:: Epoch 112, Iteration 10, Current loss 0.5260593295097351 Accuracy 99.2453925409955\n",
      "Training:: Epoch 112, Iteration 20, Current loss 0.32294708490371704 Accuracy 99.44424432666084\n",
      "Training:: Epoch 112, Iteration 30, Current loss 0.6144582629203796 Accuracy 99.43413124898225\n",
      "Training:: Epoch 112, Iteration 40, Current loss 0.41051530838012695 Accuracy 99.4223888960731\n",
      "Training:: Epoch 112, Iteration 50, Current loss 0.5137583613395691 Accuracy 99.48738978880459\n",
      "Training:: Epoch 112, Iteration 60, Current loss 0.31273984909057617 Accuracy 99.28907235408784\n",
      "Training:: Epoch 112, Iteration 70, Current loss 0.40302953124046326 Accuracy 98.91038094433651\n",
      "Training:: Epoch 112, Iteration 80, Current loss 0.3781939148902893 Accuracy 99.18315181151367\n",
      "Training:: Epoch 112, Iteration 90, Current loss 0.4684966802597046 Accuracy 99.28902627511592\n",
      "Training:: Epoch 112, Iteration 100, Current loss 0.38261282444000244 Accuracy 99.13888123449986\n",
      "Training:: Epoch 112, Iteration 110, Current loss 0.4034925401210785 Accuracy 99.3091780628789\n",
      "Training:: Epoch 112, Iteration 120, Current loss 0.44769468903541565 Accuracy 99.19954721862872\n",
      "Training:: Epoch 112, Iteration 130, Current loss 0.47702306509017944 Accuracy 98.95888477148404\n",
      "Training:: Epoch 112, Iteration 140, Current loss 0.4579581022262573 Accuracy 99.04370438721584\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 112, Probability Accuracy 67.29153458519336\n",
      "Starting Training\n",
      "Training:: Epoch 113, Iteration 0, Current loss 0.5583848357200623 Accuracy 98.87828397126833\n",
      "Training:: Epoch 113, Iteration 10, Current loss 0.38914549350738525 Accuracy 99.39806454600176\n",
      "Training:: Epoch 113, Iteration 20, Current loss 0.5045587420463562 Accuracy 99.14853987893301\n",
      "Training:: Epoch 113, Iteration 30, Current loss 0.4165063500404358 Accuracy 99.15736632154542\n",
      "Training:: Epoch 113, Iteration 40, Current loss 0.43855229020118713 Accuracy 99.07253477994576\n",
      "Training:: Epoch 113, Iteration 50, Current loss 0.40146923065185547 Accuracy 99.27995391705069\n",
      "Training:: Epoch 113, Iteration 60, Current loss 0.5801805257797241 Accuracy 99.24754634678298\n",
      "Training:: Epoch 113, Iteration 70, Current loss 0.3487473130226135 Accuracy 99.41108876759552\n",
      "Training:: Epoch 113, Iteration 80, Current loss 0.48347440361976624 Accuracy 99.25677797550507\n",
      "Training:: Epoch 113, Iteration 90, Current loss 0.4825748801231384 Accuracy 99.1623117122656\n",
      "Training:: Epoch 113, Iteration 100, Current loss 0.4572625756263733 Accuracy 98.99493313398123\n",
      "Training:: Epoch 113, Iteration 110, Current loss 0.4409693479537964 Accuracy 99.19503881062877\n",
      "Training:: Epoch 113, Iteration 120, Current loss 0.6011772155761719 Accuracy 98.66373211759158\n",
      "Training:: Epoch 113, Iteration 130, Current loss 0.6348056793212891 Accuracy 98.74247635425624\n",
      "Training:: Epoch 113, Iteration 140, Current loss 0.5690461993217468 Accuracy 98.6134124008023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 113, Probability Accuracy 66.39847319263525\n",
      "Starting Training\n",
      "Training:: Epoch 114, Iteration 0, Current loss 0.40857359766960144 Accuracy 99.25199264255058\n",
      "Training:: Epoch 114, Iteration 10, Current loss 0.38798052072525024 Accuracy 98.69198842912841\n",
      "Training:: Epoch 114, Iteration 20, Current loss 0.49707159399986267 Accuracy 99.19644138326876\n",
      "Training:: Epoch 114, Iteration 30, Current loss 0.5123744606971741 Accuracy 98.84526558891454\n",
      "Training:: Epoch 114, Iteration 40, Current loss 0.3755198121070862 Accuracy 99.31969783981977\n",
      "Training:: Epoch 114, Iteration 50, Current loss 0.35586223006248474 Accuracy 99.25491548809934\n",
      "Training:: Epoch 114, Iteration 60, Current loss 0.6692327857017517 Accuracy 98.72271825396825\n",
      "Training:: Epoch 114, Iteration 70, Current loss 0.6952630877494812 Accuracy 98.46379758274031\n",
      "Training:: Epoch 114, Iteration 80, Current loss 0.40394726395606995 Accuracy 99.13443043385338\n",
      "Training:: Epoch 114, Iteration 90, Current loss 0.7394803762435913 Accuracy 97.55331264972706\n",
      "Training:: Epoch 114, Iteration 100, Current loss 0.392913281917572 Accuracy 99.08664424793457\n",
      "Training:: Epoch 114, Iteration 110, Current loss 0.4162299633026123 Accuracy 98.85325971543853\n",
      "Training:: Epoch 114, Iteration 120, Current loss 0.36399775743484497 Accuracy 99.32493249324932\n",
      "Training:: Epoch 114, Iteration 130, Current loss 0.5999430418014526 Accuracy 99.38055908513341\n",
      "Training:: Epoch 114, Iteration 140, Current loss 0.3925810158252716 Accuracy 99.30839880491314\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 114, Probability Accuracy 66.77674363884731\n",
      "Starting Training\n",
      "Training:: Epoch 115, Iteration 0, Current loss 0.45974019169807434 Accuracy 99.30661031499703\n",
      "Training:: Epoch 115, Iteration 10, Current loss 0.42137712240219116 Accuracy 99.32111861137898\n",
      "Training:: Epoch 115, Iteration 20, Current loss 0.5819292664527893 Accuracy 98.84855052831212\n",
      "Training:: Epoch 115, Iteration 30, Current loss 0.577716588973999 Accuracy 99.16839916839916\n",
      "Training:: Epoch 115, Iteration 40, Current loss 0.6702064275741577 Accuracy 97.90794979079497\n",
      "Training:: Epoch 115, Iteration 50, Current loss 1.1147133111953735 Accuracy 97.98005302360814\n",
      "Training:: Epoch 115, Iteration 60, Current loss 1.8292748928070068 Accuracy 92.57145876261426\n",
      "Training:: Epoch 115, Iteration 70, Current loss 7.472676753997803 Accuracy 60.53869499241274\n",
      "Training:: Epoch 115, Iteration 80, Current loss 4.497003078460693 Accuracy 72.01461084574319\n",
      "Training:: Epoch 115, Iteration 90, Current loss 3.6201062202453613 Accuracy 83.40464044758927\n",
      "Training:: Epoch 115, Iteration 100, Current loss 3.614408016204834 Accuracy 75.16621040281579\n",
      "Training:: Epoch 115, Iteration 110, Current loss 2.323316812515259 Accuracy 80.86480735618208\n",
      "Training:: Epoch 115, Iteration 120, Current loss 1.9984862804412842 Accuracy 90.21584252467149\n",
      "Training:: Epoch 115, Iteration 130, Current loss 1.7101937532424927 Accuracy 96.20366025340216\n",
      "Training:: Epoch 115, Iteration 140, Current loss 1.3446886539459229 Accuracy 93.26108841120389\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 115, Probability Accuracy 62.03602637925738\n",
      "Starting Training\n",
      "Training:: Epoch 116, Iteration 0, Current loss 1.785498857498169 Accuracy 91.43126177024482\n",
      "Training:: Epoch 116, Iteration 10, Current loss 3.5251989364624023 Accuracy 77.9980994615141\n",
      "Training:: Epoch 116, Iteration 20, Current loss 1.2443362474441528 Accuracy 94.91077329808328\n",
      "Training:: Epoch 116, Iteration 30, Current loss 1.1293039321899414 Accuracy 96.16996951219512\n",
      "Training:: Epoch 116, Iteration 40, Current loss 1.2108224630355835 Accuracy 96.11650485436893\n",
      "Training:: Epoch 116, Iteration 50, Current loss 1.5173810720443726 Accuracy 94.87675205413242\n",
      "Training:: Epoch 116, Iteration 60, Current loss 0.8020261526107788 Accuracy 97.97010834653763\n",
      "Training:: Epoch 116, Iteration 70, Current loss 1.3385906219482422 Accuracy 93.96307692307693\n",
      "Training:: Epoch 116, Iteration 80, Current loss 0.8789416551589966 Accuracy 98.35963777490298\n",
      "Training:: Epoch 116, Iteration 90, Current loss 0.9695261716842651 Accuracy 97.71554900515844\n",
      "Training:: Epoch 116, Iteration 100, Current loss 1.0014686584472656 Accuracy 96.40196860867252\n",
      "Training:: Epoch 116, Iteration 110, Current loss 1.9179598093032837 Accuracy 89.32602739726028\n",
      "Training:: Epoch 116, Iteration 120, Current loss 1.5802783966064453 Accuracy 92.51064662979098\n",
      "Training:: Epoch 116, Iteration 130, Current loss 2.0392935276031494 Accuracy 89.3933236574746\n",
      "Training:: Epoch 116, Iteration 140, Current loss 1.3094756603240967 Accuracy 94.86774344569288\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 116, Probability Accuracy 64.6540173465186\n",
      "Starting Training\n",
      "Training:: Epoch 117, Iteration 0, Current loss 1.1918957233428955 Accuracy 96.21039936917296\n",
      "Training:: Epoch 117, Iteration 10, Current loss 1.3526395559310913 Accuracy 95.84142293480303\n",
      "Training:: Epoch 117, Iteration 20, Current loss 1.003595232963562 Accuracy 97.70173646578141\n",
      "Training:: Epoch 117, Iteration 30, Current loss 1.4511038064956665 Accuracy 93.26604181687094\n",
      "Training:: Epoch 117, Iteration 40, Current loss 1.0726933479309082 Accuracy 95.6475170399221\n",
      "Training:: Epoch 117, Iteration 50, Current loss 0.8198924660682678 Accuracy 97.68283852280956\n",
      "Training:: Epoch 117, Iteration 60, Current loss 2.0380842685699463 Accuracy 89.09254037426301\n",
      "Training:: Epoch 117, Iteration 70, Current loss 1.144791841506958 Accuracy 95.55630026809652\n",
      "Training:: Epoch 117, Iteration 80, Current loss 1.1563103199005127 Accuracy 96.3444247555293\n",
      "Training:: Epoch 117, Iteration 90, Current loss 1.0096373558044434 Accuracy 95.9664514705071\n",
      "Training:: Epoch 117, Iteration 100, Current loss 1.2387140989303589 Accuracy 96.92867132867133\n",
      "Training:: Epoch 117, Iteration 110, Current loss 0.7814968228340149 Accuracy 98.09964936582831\n",
      "Training:: Epoch 117, Iteration 120, Current loss 0.7343682646751404 Accuracy 98.36415796862602\n",
      "Training:: Epoch 117, Iteration 130, Current loss 1.3472824096679688 Accuracy 97.89561019761653\n",
      "Training:: Epoch 117, Iteration 140, Current loss 0.7476998567581177 Accuracy 98.59849285160927\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 117, Probability Accuracy 65.74364865129837\n",
      "Starting Training\n",
      "Training:: Epoch 118, Iteration 0, Current loss 0.9679440259933472 Accuracy 97.35166151711962\n",
      "Training:: Epoch 118, Iteration 10, Current loss 0.5596417784690857 Accuracy 98.77145085803433\n",
      "Training:: Epoch 118, Iteration 20, Current loss 0.6952460408210754 Accuracy 98.71298181101845\n",
      "Training:: Epoch 118, Iteration 30, Current loss 1.1198800802230835 Accuracy 96.36749867233138\n",
      "Training:: Epoch 118, Iteration 40, Current loss 0.8233306407928467 Accuracy 96.80081325121091\n",
      "Training:: Epoch 118, Iteration 50, Current loss 0.9053629636764526 Accuracy 95.18907743991731\n",
      "Training:: Epoch 118, Iteration 60, Current loss 0.5807545185089111 Accuracy 98.29435423804559\n",
      "Training:: Epoch 118, Iteration 70, Current loss 0.5401374101638794 Accuracy 99.15059026778002\n",
      "Training:: Epoch 118, Iteration 80, Current loss 0.7589114904403687 Accuracy 98.56190958164642\n",
      "Training:: Epoch 118, Iteration 90, Current loss 0.647365391254425 Accuracy 98.09075255748\n",
      "Training:: Epoch 118, Iteration 100, Current loss 1.1948943138122559 Accuracy 96.33365273084875\n",
      "Training:: Epoch 118, Iteration 110, Current loss 1.2225914001464844 Accuracy 95.19705072952905\n",
      "Training:: Epoch 118, Iteration 120, Current loss 0.997567355632782 Accuracy 96.98167239404353\n",
      "Training:: Epoch 118, Iteration 130, Current loss 1.1076219081878662 Accuracy 96.98635857461025\n",
      "Training:: Epoch 118, Iteration 140, Current loss 0.6282204985618591 Accuracy 98.72837498151708\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 118, Probability Accuracy 68.38304498722214\n",
      "Starting Training\n",
      "Training:: Epoch 119, Iteration 0, Current loss 0.5637558102607727 Accuracy 98.97652016857315\n",
      "Training:: Epoch 119, Iteration 10, Current loss 0.642696738243103 Accuracy 98.71955680986726\n",
      "Training:: Epoch 119, Iteration 20, Current loss 0.7718812823295593 Accuracy 98.14761215629522\n",
      "Training:: Epoch 119, Iteration 30, Current loss 0.558728814125061 Accuracy 98.17554240631164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 119, Iteration 40, Current loss 0.626442015171051 Accuracy 99.06086387434554\n",
      "Training:: Epoch 119, Iteration 50, Current loss 0.8413012027740479 Accuracy 98.81322407459734\n",
      "Training:: Epoch 119, Iteration 60, Current loss 0.6224347949028015 Accuracy 99.10169554965003\n",
      "Training:: Epoch 119, Iteration 70, Current loss 0.7577076554298401 Accuracy 98.93664596273292\n",
      "Training:: Epoch 119, Iteration 80, Current loss 0.6511445641517639 Accuracy 98.98020395920815\n",
      "Training:: Epoch 119, Iteration 90, Current loss 0.7645769715309143 Accuracy 98.72431989343717\n",
      "Training:: Epoch 119, Iteration 100, Current loss 0.5971558094024658 Accuracy 98.8758484162896\n",
      "Training:: Epoch 119, Iteration 110, Current loss 0.4509885311126709 Accuracy 99.23005631995437\n",
      "Training:: Epoch 119, Iteration 120, Current loss 0.7980319261550903 Accuracy 98.33195142174316\n",
      "Training:: Epoch 119, Iteration 130, Current loss 0.8819818496704102 Accuracy 97.65186804829357\n",
      "Training:: Epoch 119, Iteration 140, Current loss 0.7151485681533813 Accuracy 98.60613071139387\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 119, Probability Accuracy 68.74366825926967\n",
      "Starting Training\n",
      "Training:: Epoch 120, Iteration 0, Current loss 0.5742725729942322 Accuracy 98.63182684759448\n",
      "Training:: Epoch 120, Iteration 10, Current loss 0.66805100440979 Accuracy 98.73034194199971\n",
      "Training:: Epoch 120, Iteration 20, Current loss 0.70167475938797 Accuracy 98.91131775912905\n",
      "Training:: Epoch 120, Iteration 30, Current loss 0.4186750054359436 Accuracy 99.12126537785589\n",
      "Training:: Epoch 120, Iteration 40, Current loss 0.49850594997406006 Accuracy 98.98180690510647\n",
      "Training:: Epoch 120, Iteration 50, Current loss 0.6025306582450867 Accuracy 98.8513680059157\n",
      "Training:: Epoch 120, Iteration 60, Current loss 0.4360620379447937 Accuracy 99.15954712787386\n",
      "Training:: Epoch 120, Iteration 70, Current loss 0.6551457643508911 Accuracy 98.72272485364556\n",
      "Training:: Epoch 120, Iteration 80, Current loss 0.6404213309288025 Accuracy 98.95672994698135\n",
      "Training:: Epoch 120, Iteration 90, Current loss 0.43532106280326843 Accuracy 99.30297397769517\n",
      "Training:: Epoch 120, Iteration 100, Current loss 0.7031187415122986 Accuracy 98.29750552233178\n",
      "Training:: Epoch 120, Iteration 110, Current loss 0.38862645626068115 Accuracy 99.05030800821355\n",
      "Training:: Epoch 120, Iteration 120, Current loss 0.40798234939575195 Accuracy 98.94200626959248\n",
      "Training:: Epoch 120, Iteration 130, Current loss 0.6081557869911194 Accuracy 99.16757940854326\n",
      "Training:: Epoch 120, Iteration 140, Current loss 0.5405325889587402 Accuracy 99.07318308432515\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 120, Probability Accuracy 68.21972692631978\n",
      "Starting Training\n",
      "Training:: Epoch 121, Iteration 0, Current loss 0.563696026802063 Accuracy 99.23011604868384\n",
      "Training:: Epoch 121, Iteration 10, Current loss 0.5394777059555054 Accuracy 99.16278335044625\n",
      "Training:: Epoch 121, Iteration 20, Current loss 0.5469036102294922 Accuracy 99.15846213079588\n",
      "Training:: Epoch 121, Iteration 30, Current loss 0.5360392928123474 Accuracy 99.32635803353877\n",
      "Training:: Epoch 121, Iteration 40, Current loss 0.7349690198898315 Accuracy 99.28729913265649\n",
      "Training:: Epoch 121, Iteration 50, Current loss 0.4846034049987793 Accuracy 98.98645258404416\n",
      "Training:: Epoch 121, Iteration 60, Current loss 0.6301060914993286 Accuracy 98.96789993877373\n",
      "Training:: Epoch 121, Iteration 70, Current loss 0.6220781803131104 Accuracy 99.05448608938167\n",
      "Training:: Epoch 121, Iteration 80, Current loss 0.5454983115196228 Accuracy 99.08442493656128\n",
      "Training:: Epoch 121, Iteration 90, Current loss 0.6381620764732361 Accuracy 98.83092641713331\n",
      "Training:: Epoch 121, Iteration 100, Current loss 0.5235221982002258 Accuracy 99.09692190282371\n",
      "Training:: Epoch 121, Iteration 110, Current loss 0.4980952739715576 Accuracy 99.15327282867717\n",
      "Training:: Epoch 121, Iteration 120, Current loss 0.5514538288116455 Accuracy 98.91839378238342\n",
      "Training:: Epoch 121, Iteration 130, Current loss 0.46006685495376587 Accuracy 99.20408163265306\n",
      "Training:: Epoch 121, Iteration 140, Current loss 0.5542953610420227 Accuracy 99.20312881935956\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 121, Probability Accuracy 68.24881208373911\n",
      "Starting Training\n",
      "Training:: Epoch 122, Iteration 0, Current loss 0.6509209871292114 Accuracy 99.36269915651359\n",
      "Training:: Epoch 122, Iteration 10, Current loss 0.6850557327270508 Accuracy 98.89201256821565\n",
      "Training:: Epoch 122, Iteration 20, Current loss 0.4658553898334503 Accuracy 99.33593933970705\n",
      "Training:: Epoch 122, Iteration 30, Current loss 0.5517821311950684 Accuracy 99.04384328358209\n",
      "Training:: Epoch 122, Iteration 40, Current loss 0.4990594685077667 Accuracy 99.16111735708726\n",
      "Training:: Epoch 122, Iteration 50, Current loss 0.49056899547576904 Accuracy 99.26911832589629\n",
      "Training:: Epoch 122, Iteration 60, Current loss 0.4399557113647461 Accuracy 99.3736337649235\n",
      "Training:: Epoch 122, Iteration 70, Current loss 0.4520082473754883 Accuracy 99.13959723174762\n",
      "Training:: Epoch 122, Iteration 80, Current loss 0.5990728139877319 Accuracy 98.84657789839709\n",
      "Training:: Epoch 122, Iteration 90, Current loss 0.4023260474205017 Accuracy 99.49259392137769\n",
      "Training:: Epoch 122, Iteration 100, Current loss 0.34388467669487 Accuracy 99.4667739825947\n",
      "Training:: Epoch 122, Iteration 110, Current loss 0.4191247224807739 Accuracy 99.35358038450822\n",
      "Training:: Epoch 122, Iteration 120, Current loss 0.6500771045684814 Accuracy 99.13803569260654\n",
      "Training:: Epoch 122, Iteration 130, Current loss 0.42113277316093445 Accuracy 99.31134066422304\n",
      "Training:: Epoch 122, Iteration 140, Current loss 0.35960453748703003 Accuracy 99.21432486753152\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 122, Probability Accuracy 67.85689775750168\n",
      "Starting Training\n",
      "Training:: Epoch 123, Iteration 0, Current loss 0.6606997847557068 Accuracy 98.96907216494846\n",
      "Training:: Epoch 123, Iteration 10, Current loss 0.3771117329597473 Accuracy 99.49746405472058\n",
      "Training:: Epoch 123, Iteration 20, Current loss 0.41084492206573486 Accuracy 99.34914575380186\n",
      "Training:: Epoch 123, Iteration 30, Current loss 0.4751589298248291 Accuracy 99.36189608021878\n",
      "Training:: Epoch 123, Iteration 40, Current loss 0.3191676437854767 Accuracy 99.50241139095154\n",
      "Training:: Epoch 123, Iteration 50, Current loss 0.4746842086315155 Accuracy 99.22712540513588\n",
      "Training:: Epoch 123, Iteration 60, Current loss 0.5031648874282837 Accuracy 99.3717277486911\n",
      "Training:: Epoch 123, Iteration 70, Current loss 0.3385360538959503 Accuracy 99.62770252085762\n",
      "Training:: Epoch 123, Iteration 80, Current loss 0.5111198425292969 Accuracy 99.17996649325457\n",
      "Training:: Epoch 123, Iteration 90, Current loss 0.3480827212333679 Accuracy 99.42005529705307\n",
      "Training:: Epoch 123, Iteration 100, Current loss 0.5222675800323486 Accuracy 99.02234636871508\n",
      "Training:: Epoch 123, Iteration 110, Current loss 0.487541139125824 Accuracy 99.205542138045\n",
      "Training:: Epoch 123, Iteration 120, Current loss 0.5684808492660522 Accuracy 99.28115444450405\n",
      "Training:: Epoch 123, Iteration 130, Current loss 0.5909256339073181 Accuracy 98.97654093836246\n",
      "Training:: Epoch 123, Iteration 140, Current loss 0.5026613473892212 Accuracy 99.347168179853\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 123, Probability Accuracy 67.83344989182936\n",
      "Starting Training\n",
      "Training:: Epoch 124, Iteration 0, Current loss 0.37629133462905884 Accuracy 99.38908420263962\n",
      "Training:: Epoch 124, Iteration 10, Current loss 0.42761725187301636 Accuracy 99.50491096382656\n",
      "Training:: Epoch 124, Iteration 20, Current loss 0.6899498701095581 Accuracy 99.14971848787775\n",
      "Training:: Epoch 124, Iteration 30, Current loss 0.6266976594924927 Accuracy 99.0233977619532\n",
      "Training:: Epoch 124, Iteration 40, Current loss 0.4626324474811554 Accuracy 99.39256666323485\n",
      "Training:: Epoch 124, Iteration 50, Current loss 0.5560680627822876 Accuracy 99.30108414506631\n",
      "Training:: Epoch 124, Iteration 60, Current loss 0.37633395195007324 Accuracy 99.47384310173804\n",
      "Training:: Epoch 124, Iteration 70, Current loss 0.41168761253356934 Accuracy 99.62610745346663\n",
      "Training:: Epoch 124, Iteration 80, Current loss 0.3555784225463867 Accuracy 99.69975605179208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 124, Iteration 90, Current loss 0.7393378019332886 Accuracy 98.95540377661712\n",
      "Training:: Epoch 124, Iteration 100, Current loss 0.4644353985786438 Accuracy 99.12802219579865\n",
      "Training:: Epoch 124, Iteration 110, Current loss 0.6329156756401062 Accuracy 99.29392651837041\n",
      "Training:: Epoch 124, Iteration 120, Current loss 0.47300657629966736 Accuracy 99.44440216150392\n",
      "Training:: Epoch 124, Iteration 130, Current loss 0.6706408858299255 Accuracy 99.224062075034\n",
      "Training:: Epoch 124, Iteration 140, Current loss 0.4138261079788208 Accuracy 99.17576170986813\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 124, Probability Accuracy 67.94521532820475\n",
      "Starting Training\n",
      "Training:: Epoch 125, Iteration 0, Current loss 0.32070934772491455 Accuracy 99.43919067517044\n",
      "Training:: Epoch 125, Iteration 10, Current loss 0.6719176769256592 Accuracy 99.22922605849435\n",
      "Training:: Epoch 125, Iteration 20, Current loss 0.519336462020874 Accuracy 99.51256092988376\n",
      "Training:: Epoch 125, Iteration 30, Current loss 0.379266619682312 Accuracy 99.22340273476583\n",
      "Training:: Epoch 125, Iteration 40, Current loss 0.4348796010017395 Accuracy 99.36668777707409\n",
      "Training:: Epoch 125, Iteration 50, Current loss 0.508734405040741 Accuracy 99.2354513064133\n",
      "Training:: Epoch 125, Iteration 60, Current loss 0.6050624251365662 Accuracy 99.16833000665336\n",
      "Training:: Epoch 125, Iteration 70, Current loss 0.43283113837242126 Accuracy 99.39499304589708\n",
      "Training:: Epoch 125, Iteration 80, Current loss 0.4256385266780853 Accuracy 99.36224965501809\n",
      "Training:: Epoch 125, Iteration 90, Current loss 0.3684763014316559 Accuracy 99.52521434498018\n",
      "Training:: Epoch 125, Iteration 100, Current loss 0.4999924600124359 Accuracy 99.41409342834521\n",
      "Training:: Epoch 125, Iteration 110, Current loss 0.5853274464607239 Accuracy 99.24216088437738\n",
      "Training:: Epoch 125, Iteration 120, Current loss 0.34381118416786194 Accuracy 99.4772218073189\n",
      "Training:: Epoch 125, Iteration 130, Current loss 0.6762232184410095 Accuracy 99.28253918739765\n",
      "Training:: Epoch 125, Iteration 140, Current loss 0.33504679799079895 Accuracy 99.45614129556836\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 125, Probability Accuracy 67.58083386165923\n",
      "Starting Training\n",
      "Training:: Epoch 126, Iteration 0, Current loss 0.50931316614151 Accuracy 99.05110478514301\n",
      "Training:: Epoch 126, Iteration 10, Current loss 0.3414810299873352 Accuracy 99.62634056388606\n",
      "Training:: Epoch 126, Iteration 20, Current loss 0.5670360922813416 Accuracy 99.22993848898906\n",
      "Training:: Epoch 126, Iteration 30, Current loss 0.45475539565086365 Accuracy 99.42557134033353\n",
      "Training:: Epoch 126, Iteration 40, Current loss 0.38852909207344055 Accuracy 99.46461665179491\n",
      "Training:: Epoch 126, Iteration 50, Current loss 0.4364042282104492 Accuracy 99.37676583014792\n",
      "Training:: Epoch 126, Iteration 60, Current loss 0.4783400297164917 Accuracy 99.38380281690141\n",
      "Training:: Epoch 126, Iteration 70, Current loss 0.35702481865882874 Accuracy 99.57681106661813\n",
      "Training:: Epoch 126, Iteration 80, Current loss 0.4073762893676758 Accuracy 99.53051643192488\n",
      "Training:: Epoch 126, Iteration 90, Current loss 0.5536555647850037 Accuracy 99.32885906040268\n",
      "Training:: Epoch 126, Iteration 100, Current loss 0.35485565662384033 Accuracy 99.44644549763034\n",
      "Training:: Epoch 126, Iteration 110, Current loss 0.5253888964653015 Accuracy 99.09826589595376\n",
      "Training:: Epoch 126, Iteration 120, Current loss 0.5070669651031494 Accuracy 99.43738656987296\n",
      "Training:: Epoch 126, Iteration 130, Current loss 0.5311795473098755 Accuracy 99.06755805770584\n",
      "Training:: Epoch 126, Iteration 140, Current loss 0.3419651687145233 Accuracy 99.59418953193452\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 126, Probability Accuracy 67.29619147837568\n",
      "Starting Training\n",
      "Training:: Epoch 127, Iteration 0, Current loss 0.45052850246429443 Accuracy 99.37609091246703\n",
      "Training:: Epoch 127, Iteration 10, Current loss 0.33333325386047363 Accuracy 99.41539048103196\n",
      "Training:: Epoch 127, Iteration 20, Current loss 0.36351779103279114 Accuracy 99.23586347427407\n",
      "Training:: Epoch 127, Iteration 30, Current loss 0.4134531617164612 Accuracy 99.1421568627451\n",
      "Training:: Epoch 127, Iteration 40, Current loss 0.41503220796585083 Accuracy 99.27800140566099\n",
      "Training:: Epoch 127, Iteration 50, Current loss 0.5744823813438416 Accuracy 99.13540649839632\n",
      "Training:: Epoch 127, Iteration 60, Current loss 0.5239805579185486 Accuracy 99.22883787661407\n",
      "Training:: Epoch 127, Iteration 70, Current loss 0.38199731707572937 Accuracy 99.62001556562743\n",
      "Training:: Epoch 127, Iteration 80, Current loss 0.28365346789360046 Accuracy 99.61230615307653\n",
      "Training:: Epoch 127, Iteration 90, Current loss 0.550920307636261 Accuracy 99.28176259341939\n",
      "Training:: Epoch 127, Iteration 100, Current loss 0.4572966694831848 Accuracy 99.4589195038708\n",
      "Training:: Epoch 127, Iteration 110, Current loss 0.4636945426464081 Accuracy 99.22111897259052\n",
      "Training:: Epoch 127, Iteration 120, Current loss 0.5108756422996521 Accuracy 98.85422826729746\n",
      "Training:: Epoch 127, Iteration 130, Current loss 0.4593902826309204 Accuracy 99.34904931778897\n",
      "Training:: Epoch 127, Iteration 140, Current loss 0.48686450719833374 Accuracy 99.33536718239267\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 127, Probability Accuracy 67.85828665546833\n",
      "Starting Training\n",
      "Training:: Epoch 128, Iteration 0, Current loss 0.5190333127975464 Accuracy 99.53559606738797\n",
      "Training:: Epoch 128, Iteration 10, Current loss 0.6338213682174683 Accuracy 99.16262251403559\n",
      "Training:: Epoch 128, Iteration 20, Current loss 0.35890617966651917 Accuracy 99.61678661557156\n",
      "Training:: Epoch 128, Iteration 30, Current loss 0.47640979290008545 Accuracy 99.23277581709375\n",
      "Training:: Epoch 128, Iteration 40, Current loss 0.45604896545410156 Accuracy 99.16699333594669\n",
      "Training:: Epoch 128, Iteration 50, Current loss 0.3497041165828705 Accuracy 99.5934023782125\n",
      "Training:: Epoch 128, Iteration 60, Current loss 0.5971404910087585 Accuracy 99.11373707533235\n",
      "Training:: Epoch 128, Iteration 70, Current loss 0.43193379044532776 Accuracy 99.27758192221002\n",
      "Training:: Epoch 128, Iteration 80, Current loss 0.42444318532943726 Accuracy 99.15751162633956\n",
      "Training:: Epoch 128, Iteration 90, Current loss 0.6049940586090088 Accuracy 99.17960479887086\n",
      "Training:: Epoch 128, Iteration 100, Current loss 0.4177757203578949 Accuracy 99.45181030086691\n",
      "Training:: Epoch 128, Iteration 110, Current loss 0.42131975293159485 Accuracy 99.5035799522673\n",
      "Training:: Epoch 128, Iteration 120, Current loss 0.4532730281352997 Accuracy 99.57561919888818\n",
      "Training:: Epoch 128, Iteration 130, Current loss 0.550764799118042 Accuracy 99.44502254595908\n",
      "Training:: Epoch 128, Iteration 140, Current loss 0.6034889817237854 Accuracy 99.19309287501008\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 128, Probability Accuracy 67.10493205837946\n",
      "Starting Training\n",
      "Training:: Epoch 129, Iteration 0, Current loss 0.48083174228668213 Accuracy 99.18934568616098\n",
      "Training:: Epoch 129, Iteration 10, Current loss 0.4638315737247467 Accuracy 99.5213843038596\n",
      "Training:: Epoch 129, Iteration 20, Current loss 0.5754643082618713 Accuracy 99.19258881523032\n",
      "Training:: Epoch 129, Iteration 30, Current loss 0.44496357440948486 Accuracy 99.28080549784242\n",
      "Training:: Epoch 129, Iteration 40, Current loss 0.4439164102077484 Accuracy 99.45567165112932\n",
      "Training:: Epoch 129, Iteration 50, Current loss 0.33532753586769104 Accuracy 99.43531126189717\n",
      "Training:: Epoch 129, Iteration 60, Current loss 0.5133858919143677 Accuracy 99.3467573573978\n",
      "Training:: Epoch 129, Iteration 70, Current loss 0.4043330252170563 Accuracy 99.46645674801015\n",
      "Training:: Epoch 129, Iteration 80, Current loss 0.3721480369567871 Accuracy 99.49225819424895\n",
      "Training:: Epoch 129, Iteration 90, Current loss 0.6084019541740417 Accuracy 99.2103185048697\n",
      "Training:: Epoch 129, Iteration 100, Current loss 0.33261680603027344 Accuracy 99.43119667468825\n",
      "Training:: Epoch 129, Iteration 110, Current loss 0.3783860504627228 Accuracy 99.50961757773892\n",
      "Training:: Epoch 129, Iteration 120, Current loss 0.49147331714630127 Accuracy 99.45997458703938\n",
      "Training:: Epoch 129, Iteration 130, Current loss 0.3963348865509033 Accuracy 99.61406377759607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 129, Iteration 140, Current loss 0.38705024123191833 Accuracy 99.12041516404257\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 129, Probability Accuracy 67.33843031653802\n",
      "Starting Training\n",
      "Training:: Epoch 130, Iteration 0, Current loss 0.41341182589530945 Accuracy 99.21193653462225\n",
      "Training:: Epoch 130, Iteration 10, Current loss 0.3320269286632538 Accuracy 99.62398575103899\n",
      "Training:: Epoch 130, Iteration 20, Current loss 0.4091413915157318 Accuracy 99.44174176569105\n",
      "Training:: Epoch 130, Iteration 30, Current loss 0.3156035542488098 Accuracy 99.43493615865253\n",
      "Training:: Epoch 130, Iteration 40, Current loss 0.7169132828712463 Accuracy 99.26955463122654\n",
      "Training:: Epoch 130, Iteration 50, Current loss 0.5374307632446289 Accuracy 99.28876244665719\n",
      "Training:: Epoch 130, Iteration 60, Current loss 0.5339712500572205 Accuracy 99.45138511678435\n",
      "Training:: Epoch 130, Iteration 70, Current loss 0.48644256591796875 Accuracy 98.79840391765666\n",
      "Training:: Epoch 130, Iteration 80, Current loss 0.339578777551651 Accuracy 99.20471759359447\n",
      "Training:: Epoch 130, Iteration 90, Current loss 0.5687544941902161 Accuracy 99.32002632156174\n",
      "Training:: Epoch 130, Iteration 100, Current loss 0.3674044609069824 Accuracy 99.37800963081862\n",
      "Training:: Epoch 130, Iteration 110, Current loss 0.3994276523590088 Accuracy 99.16210331859867\n",
      "Training:: Epoch 130, Iteration 120, Current loss 0.3317479193210602 Accuracy 99.3440743806916\n",
      "Training:: Epoch 130, Iteration 130, Current loss 0.4918223023414612 Accuracy 99.26442121564072\n",
      "Training:: Epoch 130, Iteration 140, Current loss 0.46069005131721497 Accuracy 99.34338158483808\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 130, Probability Accuracy 68.01425172713547\n",
      "Starting Training\n",
      "Training:: Epoch 131, Iteration 0, Current loss 0.33660855889320374 Accuracy 99.45931894983104\n",
      "Training:: Epoch 131, Iteration 10, Current loss 0.4307368993759155 Accuracy 99.1009341855728\n",
      "Training:: Epoch 131, Iteration 20, Current loss 0.36117973923683167 Accuracy 99.39874428008939\n",
      "Training:: Epoch 131, Iteration 30, Current loss 0.528537929058075 Accuracy 98.80536367330353\n",
      "Training:: Epoch 131, Iteration 40, Current loss 0.5399284958839417 Accuracy 98.9619087484303\n",
      "Training:: Epoch 131, Iteration 50, Current loss 0.470929890871048 Accuracy 99.53461029735952\n",
      "Training:: Epoch 131, Iteration 60, Current loss 0.42160657048225403 Accuracy 99.20544022906228\n",
      "Training:: Epoch 131, Iteration 70, Current loss 0.24622413516044617 Accuracy 99.54583861427967\n",
      "Training:: Epoch 131, Iteration 80, Current loss 0.41632306575775146 Accuracy 99.3338948013169\n",
      "Training:: Epoch 131, Iteration 90, Current loss 0.3656176030635834 Accuracy 99.18533604887983\n",
      "Training:: Epoch 131, Iteration 100, Current loss 0.47896692156791687 Accuracy 98.56468191499214\n",
      "Training:: Epoch 131, Iteration 110, Current loss 0.5466454029083252 Accuracy 99.31792873051225\n",
      "Training:: Epoch 131, Iteration 120, Current loss 0.5041879415512085 Accuracy 99.10081354964557\n",
      "Training:: Epoch 131, Iteration 130, Current loss 0.36336028575897217 Accuracy 99.35652077439397\n",
      "Training:: Epoch 131, Iteration 140, Current loss 0.363991379737854 Accuracy 99.31823816213536\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 131, Probability Accuracy 67.92062366420696\n",
      "Starting Training\n",
      "Training:: Epoch 132, Iteration 0, Current loss 0.47417569160461426 Accuracy 99.382936406752\n",
      "Training:: Epoch 132, Iteration 10, Current loss 0.4490955173969269 Accuracy 99.51284056053407\n",
      "Training:: Epoch 132, Iteration 20, Current loss 0.46918919682502747 Accuracy 99.17747548244226\n",
      "Training:: Epoch 132, Iteration 30, Current loss 0.38680461049079895 Accuracy 99.38948417642662\n",
      "Training:: Epoch 132, Iteration 40, Current loss 0.4841949939727783 Accuracy 99.40387481371089\n",
      "Training:: Epoch 132, Iteration 50, Current loss 0.29910075664520264 Accuracy 99.27217008235971\n",
      "Training:: Epoch 132, Iteration 60, Current loss 0.37988102436065674 Accuracy 99.16107382550335\n",
      "Training:: Epoch 132, Iteration 70, Current loss 0.46697571873664856 Accuracy 99.18032786885246\n",
      "Training:: Epoch 132, Iteration 80, Current loss 0.5451053380966187 Accuracy 99.30611831442464\n",
      "Training:: Epoch 132, Iteration 90, Current loss 0.39314767718315125 Accuracy 99.40238128074289\n",
      "Training:: Epoch 132, Iteration 100, Current loss 0.4410861134529114 Accuracy 99.09909909909909\n",
      "Training:: Epoch 132, Iteration 110, Current loss 0.475752055644989 Accuracy 99.10989688904556\n",
      "Training:: Epoch 132, Iteration 120, Current loss 0.3774733543395996 Accuracy 99.29420068227267\n",
      "Training:: Epoch 132, Iteration 130, Current loss 0.3239910900592804 Accuracy 99.406989743839\n",
      "Training:: Epoch 132, Iteration 140, Current loss 0.48991531133651733 Accuracy 99.25365993684815\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 132, Probability Accuracy 67.20771050791181\n",
      "Starting Training\n",
      "Training:: Epoch 133, Iteration 0, Current loss 0.37912672758102417 Accuracy 99.53318951798282\n",
      "Training:: Epoch 133, Iteration 10, Current loss 0.5145542025566101 Accuracy 99.30630005675727\n",
      "Training:: Epoch 133, Iteration 20, Current loss 0.6574901938438416 Accuracy 99.08867219538868\n",
      "Training:: Epoch 133, Iteration 30, Current loss 0.46888023614883423 Accuracy 99.22862180429033\n",
      "Training:: Epoch 133, Iteration 40, Current loss 0.3784105181694031 Accuracy 99.53880542115836\n",
      "Training:: Epoch 133, Iteration 50, Current loss 0.36253610253334045 Accuracy 99.36530324400564\n",
      "Training:: Epoch 133, Iteration 60, Current loss 0.41970738768577576 Accuracy 99.17557750387724\n",
      "Training:: Epoch 133, Iteration 70, Current loss 0.45034149289131165 Accuracy 99.28339535988597\n",
      "Training:: Epoch 133, Iteration 80, Current loss 0.4172113835811615 Accuracy 99.54717491213842\n",
      "Training:: Epoch 133, Iteration 90, Current loss 0.5150107145309448 Accuracy 99.01071723000824\n",
      "Training:: Epoch 133, Iteration 100, Current loss 0.3039630651473999 Accuracy 99.28092825625102\n",
      "Training:: Epoch 133, Iteration 110, Current loss 0.3509370684623718 Accuracy 99.12774193548387\n",
      "Training:: Epoch 133, Iteration 120, Current loss 0.36220741271972656 Accuracy 99.3999399939994\n",
      "Training:: Epoch 133, Iteration 130, Current loss 0.5260313749313354 Accuracy 99.1819421299803\n",
      "Training:: Epoch 133, Iteration 140, Current loss 0.5548775792121887 Accuracy 99.27492447129909\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 133, Probability Accuracy 67.27519460911509\n",
      "Starting Training\n",
      "Training:: Epoch 134, Iteration 0, Current loss 0.42271116375923157 Accuracy 99.14671421293937\n",
      "Training:: Epoch 134, Iteration 10, Current loss 0.38213130831718445 Accuracy 99.49844627378292\n",
      "Training:: Epoch 134, Iteration 20, Current loss 0.4494778513908386 Accuracy 98.90594382756369\n",
      "Training:: Epoch 134, Iteration 30, Current loss 0.38894614577293396 Accuracy 99.29251520761703\n",
      "Training:: Epoch 134, Iteration 40, Current loss 0.46446722745895386 Accuracy 99.36860714736709\n",
      "Training:: Epoch 134, Iteration 50, Current loss 0.32673296332359314 Accuracy 99.3406993134179\n",
      "Training:: Epoch 134, Iteration 60, Current loss 0.39188194274902344 Accuracy 99.43533415841584\n",
      "Training:: Epoch 134, Iteration 70, Current loss 0.43743017315864563 Accuracy 99.22431027588965\n",
      "Training:: Epoch 134, Iteration 80, Current loss 0.39643150568008423 Accuracy 99.4268684089867\n",
      "Training:: Epoch 134, Iteration 90, Current loss 0.33602699637413025 Accuracy 99.171362280411\n",
      "Training:: Epoch 134, Iteration 100, Current loss 0.30254754424095154 Accuracy 99.43035157988429\n",
      "Training:: Epoch 134, Iteration 110, Current loss 0.48310762643814087 Accuracy 99.35785753397381\n",
      "Training:: Epoch 134, Iteration 120, Current loss 0.34625738859176636 Accuracy 99.4806566612385\n",
      "Training:: Epoch 134, Iteration 130, Current loss 0.38428735733032227 Accuracy 99.10153712924875\n",
      "Training:: Epoch 134, Iteration 140, Current loss 0.3226053714752197 Accuracy 99.19562419562419\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 134, Probability Accuracy 67.37764625912588\n",
      "Starting Training\n",
      "Training:: Epoch 135, Iteration 0, Current loss 0.2823158800601959 Accuracy 99.40961655508217\n",
      "Training:: Epoch 135, Iteration 10, Current loss 0.5933613777160645 Accuracy 99.10503418272219\n",
      "Training:: Epoch 135, Iteration 20, Current loss 0.45167863368988037 Accuracy 98.69804707060591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 135, Iteration 30, Current loss 0.42130184173583984 Accuracy 99.34454995871181\n",
      "Training:: Epoch 135, Iteration 40, Current loss 0.5294716954231262 Accuracy 98.60161352285824\n",
      "Training:: Epoch 135, Iteration 50, Current loss 0.5737283229827881 Accuracy 98.27783147350848\n",
      "Training:: Epoch 135, Iteration 60, Current loss 0.4309258759021759 Accuracy 99.29876060013046\n",
      "Training:: Epoch 135, Iteration 70, Current loss 0.29994329810142517 Accuracy 99.37930103899609\n",
      "Training:: Epoch 135, Iteration 80, Current loss 0.4665156602859497 Accuracy 99.15904396577162\n",
      "Training:: Epoch 135, Iteration 90, Current loss 0.5321930050849915 Accuracy 99.08947519357613\n",
      "Training:: Epoch 135, Iteration 100, Current loss 0.4479236602783203 Accuracy 99.11581569115816\n",
      "Training:: Epoch 135, Iteration 110, Current loss 0.44496363401412964 Accuracy 99.55121732301133\n",
      "Training:: Epoch 135, Iteration 120, Current loss 0.3856486976146698 Accuracy 99.23754572149812\n",
      "Training:: Epoch 135, Iteration 130, Current loss 0.38447296619415283 Accuracy 99.20624243239607\n",
      "Training:: Epoch 135, Iteration 140, Current loss 0.3613567352294922 Accuracy 99.38175547798292\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 135, Probability Accuracy 66.95060098432016\n",
      "Starting Training\n",
      "Training:: Epoch 136, Iteration 0, Current loss 0.5236545205116272 Accuracy 99.17895575926664\n",
      "Training:: Epoch 136, Iteration 10, Current loss 0.31198906898498535 Accuracy 99.36465474001004\n",
      "Training:: Epoch 136, Iteration 20, Current loss 0.4824068546295166 Accuracy 98.76639489542715\n",
      "Training:: Epoch 136, Iteration 30, Current loss 0.4709756076335907 Accuracy 99.01309546403492\n",
      "Training:: Epoch 136, Iteration 40, Current loss 0.41428372263908386 Accuracy 99.2708122341503\n",
      "Training:: Epoch 136, Iteration 50, Current loss 0.3548734188079834 Accuracy 99.17014925373134\n",
      "Training:: Epoch 136, Iteration 60, Current loss 0.48476213216781616 Accuracy 98.59592819175609\n",
      "Training:: Epoch 136, Iteration 70, Current loss 0.5708621740341187 Accuracy 99.11561001598295\n",
      "Training:: Epoch 136, Iteration 80, Current loss 0.621268093585968 Accuracy 98.20200951877314\n",
      "Training:: Epoch 136, Iteration 90, Current loss 0.3738895654678345 Accuracy 98.92815191855912\n",
      "Training:: Epoch 136, Iteration 100, Current loss 6.805932521820068 Accuracy 58.84124182327899\n",
      "Training:: Epoch 136, Iteration 110, Current loss 12.283748626708984 Accuracy 35.72392245970319\n",
      "Training:: Epoch 136, Iteration 120, Current loss 5.553243160247803 Accuracy 57.61044305832422\n",
      "Training:: Epoch 136, Iteration 130, Current loss 5.843407154083252 Accuracy 71.92749778956676\n",
      "Training:: Epoch 136, Iteration 140, Current loss 5.486372470855713 Accuracy 61.119727047146405\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 136, Probability Accuracy 43.71237720507977\n",
      "Starting Training\n",
      "Training:: Epoch 137, Iteration 0, Current loss 6.0514678955078125 Accuracy 52.25938642153062\n",
      "Training:: Epoch 137, Iteration 10, Current loss 4.424468994140625 Accuracy 74.1312408381478\n",
      "Training:: Epoch 137, Iteration 20, Current loss 4.036473751068115 Accuracy 70.97516099356025\n",
      "Training:: Epoch 137, Iteration 30, Current loss 2.143033981323242 Accuracy 87.84036930487723\n",
      "Training:: Epoch 137, Iteration 40, Current loss 2.968611001968384 Accuracy 84.49458483754513\n",
      "Training:: Epoch 137, Iteration 50, Current loss 4.588681221008301 Accuracy 62.273846483245904\n",
      "Training:: Epoch 137, Iteration 60, Current loss 3.6797852516174316 Accuracy 84.2399593289273\n",
      "Training:: Epoch 137, Iteration 70, Current loss 1.8872017860412598 Accuracy 91.18743237286098\n",
      "Training:: Epoch 137, Iteration 80, Current loss 3.065714120864868 Accuracy 88.45468582967098\n",
      "Training:: Epoch 137, Iteration 90, Current loss 1.332261085510254 Accuracy 95.4802987380891\n",
      "Training:: Epoch 137, Iteration 100, Current loss 2.149120569229126 Accuracy 90.31339031339031\n",
      "Training:: Epoch 137, Iteration 110, Current loss 1.7034450769424438 Accuracy 94.32305696897137\n",
      "Training:: Epoch 137, Iteration 120, Current loss 2.2469401359558105 Accuracy 91.44702842377261\n",
      "Training:: Epoch 137, Iteration 130, Current loss 1.4872323274612427 Accuracy 95.54905782975959\n",
      "Training:: Epoch 137, Iteration 140, Current loss 1.8583446741104126 Accuracy 92.76602284953123\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 137, Probability Accuracy 61.309796142458445\n",
      "Starting Training\n",
      "Training:: Epoch 138, Iteration 0, Current loss 1.3246221542358398 Accuracy 95.82306699518949\n",
      "Training:: Epoch 138, Iteration 10, Current loss 2.1610634326934814 Accuracy 91.80237372343362\n",
      "Training:: Epoch 138, Iteration 20, Current loss 1.3999407291412354 Accuracy 95.2265576971757\n",
      "Training:: Epoch 138, Iteration 30, Current loss 1.6200642585754395 Accuracy 92.82990083905416\n",
      "Training:: Epoch 138, Iteration 40, Current loss 1.1740360260009766 Accuracy 96.89591227928727\n",
      "Training:: Epoch 138, Iteration 50, Current loss 1.4743200540542603 Accuracy 95.69426579107711\n",
      "Training:: Epoch 138, Iteration 60, Current loss 0.9611169099807739 Accuracy 97.01329049218636\n",
      "Training:: Epoch 138, Iteration 70, Current loss 0.8298963904380798 Accuracy 97.87883491127322\n",
      "Training:: Epoch 138, Iteration 80, Current loss 0.7704087495803833 Accuracy 98.08167225260188\n",
      "Training:: Epoch 138, Iteration 90, Current loss 0.9530947804450989 Accuracy 97.84406379208505\n",
      "Training:: Epoch 138, Iteration 100, Current loss 1.0324382781982422 Accuracy 97.09826335458342\n",
      "Training:: Epoch 138, Iteration 110, Current loss 1.0997194051742554 Accuracy 97.33813581656857\n",
      "Training:: Epoch 138, Iteration 120, Current loss 0.9521825909614563 Accuracy 95.87219899217327\n",
      "Training:: Epoch 138, Iteration 130, Current loss 1.0319998264312744 Accuracy 96.6932811087677\n",
      "Training:: Epoch 138, Iteration 140, Current loss 0.9028196334838867 Accuracy 98.7633389847412\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 138, Probability Accuracy 66.84496303897411\n",
      "Starting Training\n",
      "Training:: Epoch 139, Iteration 0, Current loss 1.1789453029632568 Accuracy 97.73239789526434\n",
      "Training:: Epoch 139, Iteration 10, Current loss 1.0644237995147705 Accuracy 98.08713410483323\n",
      "Training:: Epoch 139, Iteration 20, Current loss 0.7684366106987 Accuracy 97.53005104561173\n",
      "Training:: Epoch 139, Iteration 30, Current loss 0.9657776355743408 Accuracy 98.62002759944801\n",
      "Training:: Epoch 139, Iteration 40, Current loss 0.8680185079574585 Accuracy 98.30095009843362\n",
      "Training:: Epoch 139, Iteration 50, Current loss 1.0944766998291016 Accuracy 96.26622587388789\n",
      "Training:: Epoch 139, Iteration 60, Current loss 0.8627355098724365 Accuracy 97.94350842418235\n",
      "Training:: Epoch 139, Iteration 70, Current loss 1.060772180557251 Accuracy 98.39547607006574\n",
      "Training:: Epoch 139, Iteration 80, Current loss 0.8797773122787476 Accuracy 97.88227767114523\n",
      "Training:: Epoch 139, Iteration 90, Current loss 0.7474374771118164 Accuracy 98.10365723248022\n",
      "Training:: Epoch 139, Iteration 100, Current loss 0.9669474363327026 Accuracy 97.38863287250383\n",
      "Training:: Epoch 139, Iteration 110, Current loss 1.2348248958587646 Accuracy 98.10577843992763\n",
      "Training:: Epoch 139, Iteration 120, Current loss 0.594993531703949 Accuracy 98.6342552825975\n",
      "Training:: Epoch 139, Iteration 130, Current loss 0.6814990043640137 Accuracy 98.79351947604275\n",
      "Training:: Epoch 139, Iteration 140, Current loss 1.0689462423324585 Accuracy 97.83050847457628\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 139, Probability Accuracy 68.06261805632717\n",
      "Starting Training\n",
      "Training:: Epoch 140, Iteration 0, Current loss 0.8463375568389893 Accuracy 98.69364502504665\n",
      "Training:: Epoch 140, Iteration 10, Current loss 0.7018105983734131 Accuracy 99.13504908463784\n",
      "Training:: Epoch 140, Iteration 20, Current loss 0.7664797306060791 Accuracy 98.86229406279142\n",
      "Training:: Epoch 140, Iteration 30, Current loss 0.8141137957572937 Accuracy 98.9342926961716\n",
      "Training:: Epoch 140, Iteration 40, Current loss 0.5760003924369812 Accuracy 98.97757940626946\n",
      "Training:: Epoch 140, Iteration 50, Current loss 0.8142072558403015 Accuracy 98.49632560768796\n",
      "Training:: Epoch 140, Iteration 60, Current loss 0.5063936114311218 Accuracy 98.73725671918443\n",
      "Training:: Epoch 140, Iteration 70, Current loss 0.5717939734458923 Accuracy 98.75533018324306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 140, Iteration 80, Current loss 0.6184672713279724 Accuracy 98.73189711074285\n",
      "Training:: Epoch 140, Iteration 90, Current loss 0.8270182013511658 Accuracy 98.62356243774337\n",
      "Training:: Epoch 140, Iteration 100, Current loss 0.5723561644554138 Accuracy 98.94598795414805\n",
      "Training:: Epoch 140, Iteration 110, Current loss 0.6190896034240723 Accuracy 98.53451369781074\n",
      "Training:: Epoch 140, Iteration 120, Current loss 0.6948848962783813 Accuracy 98.74901652242329\n",
      "Training:: Epoch 140, Iteration 130, Current loss 0.7767261266708374 Accuracy 98.62967914438502\n",
      "Training:: Epoch 140, Iteration 140, Current loss 0.5749860405921936 Accuracy 98.77608238529142\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 140, Probability Accuracy 67.45689514310551\n",
      "Starting Training\n",
      "Training:: Epoch 141, Iteration 0, Current loss 0.59666907787323 Accuracy 99.16093202945792\n",
      "Training:: Epoch 141, Iteration 10, Current loss 0.515842616558075 Accuracy 98.8148412840212\n",
      "Training:: Epoch 141, Iteration 20, Current loss 0.6377782225608826 Accuracy 98.94199421787538\n",
      "Training:: Epoch 141, Iteration 30, Current loss 0.6533200144767761 Accuracy 98.29678735339112\n",
      "Training:: Epoch 141, Iteration 40, Current loss 0.4856889843940735 Accuracy 99.2362960357735\n",
      "Training:: Epoch 141, Iteration 50, Current loss 0.6446390748023987 Accuracy 98.85601820530168\n",
      "Training:: Epoch 141, Iteration 60, Current loss 0.5192639231681824 Accuracy 99.20876722295485\n",
      "Training:: Epoch 141, Iteration 70, Current loss 0.6275777816772461 Accuracy 99.06088877976921\n",
      "Training:: Epoch 141, Iteration 80, Current loss 0.6360319256782532 Accuracy 98.8127666135581\n",
      "Training:: Epoch 141, Iteration 90, Current loss 0.8785440325737 Accuracy 98.48233341237847\n",
      "Training:: Epoch 141, Iteration 100, Current loss 0.5768673419952393 Accuracy 99.37901498929337\n",
      "Training:: Epoch 141, Iteration 110, Current loss 0.7305790185928345 Accuracy 97.95071164997364\n",
      "Training:: Epoch 141, Iteration 120, Current loss 0.6131076812744141 Accuracy 99.02695595003287\n",
      "Training:: Epoch 141, Iteration 130, Current loss 0.5019826889038086 Accuracy 99.02590673575129\n",
      "Training:: Epoch 141, Iteration 140, Current loss 0.45571547746658325 Accuracy 99.26873857404021\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 141, Probability Accuracy 67.78614566108276\n",
      "Starting Training\n",
      "Training:: Epoch 142, Iteration 0, Current loss 0.7574283480644226 Accuracy 99.05865878325812\n",
      "Training:: Epoch 142, Iteration 10, Current loss 0.5649273991584778 Accuracy 99.26178307779671\n",
      "Training:: Epoch 142, Iteration 20, Current loss 0.5172196626663208 Accuracy 99.45041373348154\n",
      "Training:: Epoch 142, Iteration 30, Current loss 0.5796019434928894 Accuracy 99.00420840495525\n",
      "Training:: Epoch 142, Iteration 40, Current loss 0.7271949052810669 Accuracy 99.21773985001293\n",
      "Training:: Epoch 142, Iteration 50, Current loss 0.43968310952186584 Accuracy 99.34859928865444\n",
      "Training:: Epoch 142, Iteration 60, Current loss 0.5563980340957642 Accuracy 99.01170223227456\n",
      "Training:: Epoch 142, Iteration 70, Current loss 0.5915378332138062 Accuracy 98.8306197715211\n",
      "Training:: Epoch 142, Iteration 80, Current loss 0.40803655982017517 Accuracy 99.3470289818675\n",
      "Training:: Epoch 142, Iteration 90, Current loss 0.48239219188690186 Accuracy 99.10858572694573\n",
      "Training:: Epoch 142, Iteration 100, Current loss 0.5452373623847961 Accuracy 99.25618297898717\n",
      "Training:: Epoch 142, Iteration 110, Current loss 0.5178258419036865 Accuracy 99.0919760331089\n",
      "Training:: Epoch 142, Iteration 120, Current loss 0.5905170440673828 Accuracy 99.3103448275862\n",
      "Training:: Epoch 142, Iteration 130, Current loss 0.4860600233078003 Accuracy 99.33394579696831\n",
      "Training:: Epoch 142, Iteration 140, Current loss 0.7661524415016174 Accuracy 98.8719651855245\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 142, Probability Accuracy 67.95224151791842\n",
      "Starting Training\n",
      "Training:: Epoch 143, Iteration 0, Current loss 0.416416734457016 Accuracy 99.2878612716763\n",
      "Training:: Epoch 143, Iteration 10, Current loss 0.43615245819091797 Accuracy 99.44181147972617\n",
      "Training:: Epoch 143, Iteration 20, Current loss 0.6324499249458313 Accuracy 99.23215898825654\n",
      "Training:: Epoch 143, Iteration 30, Current loss 0.37789446115493774 Accuracy 99.34731012658227\n",
      "Training:: Epoch 143, Iteration 40, Current loss 0.7658694386482239 Accuracy 99.20336218793125\n",
      "Training:: Epoch 143, Iteration 50, Current loss 0.6247433423995972 Accuracy 99.03941702550513\n",
      "Training:: Epoch 143, Iteration 60, Current loss 0.3900955319404602 Accuracy 99.38157407896823\n",
      "Training:: Epoch 143, Iteration 70, Current loss 0.4088038206100464 Accuracy 99.37737616397598\n",
      "Training:: Epoch 143, Iteration 80, Current loss 0.7407588958740234 Accuracy 99.01210653753027\n",
      "Training:: Epoch 143, Iteration 90, Current loss 0.39202868938446045 Accuracy 99.47985834440017\n",
      "Training:: Epoch 143, Iteration 100, Current loss 0.5525307655334473 Accuracy 99.00447356814315\n",
      "Training:: Epoch 143, Iteration 110, Current loss 0.44638821482658386 Accuracy 99.30287679509225\n",
      "Training:: Epoch 143, Iteration 120, Current loss 0.5645091533660889 Accuracy 99.12884642405508\n",
      "Training:: Epoch 143, Iteration 130, Current loss 0.665632426738739 Accuracy 99.23596589995175\n",
      "Training:: Epoch 143, Iteration 140, Current loss 0.5582202672958374 Accuracy 99.09622541201489\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 143, Probability Accuracy 67.66776253439565\n",
      "Starting Training\n",
      "Training:: Epoch 144, Iteration 0, Current loss 0.5990915298461914 Accuracy 99.343753728672\n",
      "Training:: Epoch 144, Iteration 10, Current loss 0.4853956997394562 Accuracy 99.20515574650913\n",
      "Training:: Epoch 144, Iteration 20, Current loss 0.500409722328186 Accuracy 99.31566588393157\n",
      "Training:: Epoch 144, Iteration 30, Current loss 0.6401513814926147 Accuracy 98.98144151920587\n",
      "Training:: Epoch 144, Iteration 40, Current loss 0.5824989080429077 Accuracy 99.15262397688974\n",
      "Training:: Epoch 144, Iteration 50, Current loss 0.9824196100234985 Accuracy 98.91411648568608\n",
      "Training:: Epoch 144, Iteration 60, Current loss 0.5540869832038879 Accuracy 99.17931585445567\n",
      "Training:: Epoch 144, Iteration 70, Current loss 0.5887889862060547 Accuracy 99.20658416720943\n",
      "Training:: Epoch 144, Iteration 80, Current loss 0.5284776091575623 Accuracy 98.99237472766885\n",
      "Training:: Epoch 144, Iteration 90, Current loss 0.4994979500770569 Accuracy 99.31600547195623\n",
      "Training:: Epoch 144, Iteration 100, Current loss 0.5950733423233032 Accuracy 99.19941395008111\n",
      "Training:: Epoch 144, Iteration 110, Current loss 0.42351818084716797 Accuracy 99.54922651367687\n",
      "Training:: Epoch 144, Iteration 120, Current loss 0.48571649193763733 Accuracy 99.2383659487628\n",
      "Training:: Epoch 144, Iteration 130, Current loss 0.48623865842819214 Accuracy 99.24771628156905\n",
      "Training:: Epoch 144, Iteration 140, Current loss 0.40009090304374695 Accuracy 99.40132843908408\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 144, Probability Accuracy 67.9287119523657\n",
      "Starting Training\n",
      "Training:: Epoch 145, Iteration 0, Current loss 0.6453685760498047 Accuracy 99.32779183230936\n",
      "Training:: Epoch 145, Iteration 10, Current loss 0.5493283271789551 Accuracy 99.36054165883017\n",
      "Training:: Epoch 145, Iteration 20, Current loss 0.5012990832328796 Accuracy 99.52286282306163\n",
      "Training:: Epoch 145, Iteration 30, Current loss 0.5157107710838318 Accuracy 99.25282585094834\n",
      "Training:: Epoch 145, Iteration 40, Current loss 0.49217724800109863 Accuracy 99.25150432953377\n",
      "Training:: Epoch 145, Iteration 50, Current loss 0.5947672128677368 Accuracy 99.31658545845725\n",
      "Training:: Epoch 145, Iteration 60, Current loss 0.48373737931251526 Accuracy 99.39851727514338\n",
      "Training:: Epoch 145, Iteration 70, Current loss 0.4028226137161255 Accuracy 99.32235001554243\n",
      "Training:: Epoch 145, Iteration 80, Current loss 0.5524824857711792 Accuracy 98.87730553327987\n",
      "Training:: Epoch 145, Iteration 90, Current loss 0.5009651184082031 Accuracy 99.5290576297519\n",
      "Training:: Epoch 145, Iteration 100, Current loss 0.4174121916294098 Accuracy 99.49372215471851\n",
      "Training:: Epoch 145, Iteration 110, Current loss 0.3983096778392792 Accuracy 99.39012889028608\n",
      "Training:: Epoch 145, Iteration 120, Current loss 0.3564712703227997 Accuracy 99.43211151264842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 145, Iteration 130, Current loss 0.5691803693771362 Accuracy 99.50562743241822\n",
      "Training:: Epoch 145, Iteration 140, Current loss 0.4882022738456726 Accuracy 99.13441448612079\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 145, Probability Accuracy 67.72740344708136\n",
      "Starting Training\n",
      "Training:: Epoch 146, Iteration 0, Current loss 0.5308578610420227 Accuracy 99.15273132664437\n",
      "Training:: Epoch 146, Iteration 10, Current loss 0.3576907515525818 Accuracy 99.46634089886196\n",
      "Training:: Epoch 146, Iteration 20, Current loss 0.42141062021255493 Accuracy 99.47514075770589\n",
      "Training:: Epoch 146, Iteration 30, Current loss 0.46576598286628723 Accuracy 99.43864136999929\n",
      "Training:: Epoch 146, Iteration 40, Current loss 0.5341756939888 Accuracy 99.49096197797631\n",
      "Training:: Epoch 146, Iteration 50, Current loss 0.2977522909641266 Accuracy 99.46904592479063\n",
      "Training:: Epoch 146, Iteration 60, Current loss 0.4780900776386261 Accuracy 99.18089325745818\n",
      "Training:: Epoch 146, Iteration 70, Current loss 0.4174765944480896 Accuracy 99.30583443290689\n",
      "Training:: Epoch 146, Iteration 80, Current loss 0.5405609011650085 Accuracy 99.21546252050496\n",
      "Training:: Epoch 146, Iteration 90, Current loss 0.3754268288612366 Accuracy 99.38988221337175\n",
      "Training:: Epoch 146, Iteration 100, Current loss 0.4869459271430969 Accuracy 99.43279789454579\n",
      "Training:: Epoch 146, Iteration 110, Current loss 0.47725382447242737 Accuracy 99.39540507859734\n",
      "Training:: Epoch 146, Iteration 120, Current loss 0.36244532465934753 Accuracy 99.32348474389066\n",
      "Training:: Epoch 146, Iteration 130, Current loss 0.4356636106967926 Accuracy 99.39578352532058\n",
      "Training:: Epoch 146, Iteration 140, Current loss 0.5096011161804199 Accuracy 99.28212009392821\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 146, Probability Accuracy 67.44578395937228\n",
      "Starting Training\n",
      "Training:: Epoch 147, Iteration 0, Current loss 0.45326030254364014 Accuracy 99.45623482208848\n",
      "Training:: Epoch 147, Iteration 10, Current loss 0.3825739324092865 Accuracy 99.58664794883794\n",
      "Training:: Epoch 147, Iteration 20, Current loss 0.5914343595504761 Accuracy 99.04510475762513\n",
      "Training:: Epoch 147, Iteration 30, Current loss 0.5518948435783386 Accuracy 99.39037825927286\n",
      "Training:: Epoch 147, Iteration 40, Current loss 0.44824057817459106 Accuracy 99.3244317278652\n",
      "Training:: Epoch 147, Iteration 50, Current loss 0.3280058801174164 Accuracy 99.56423741547708\n",
      "Training:: Epoch 147, Iteration 60, Current loss 0.3442559242248535 Accuracy 99.43664857113592\n",
      "Training:: Epoch 147, Iteration 70, Current loss 0.37447676062583923 Accuracy 99.40997083001857\n",
      "Training:: Epoch 147, Iteration 80, Current loss 0.4603426456451416 Accuracy 99.40654712526323\n",
      "Training:: Epoch 147, Iteration 90, Current loss 0.44060784578323364 Accuracy 99.33684593023256\n",
      "Training:: Epoch 147, Iteration 100, Current loss 0.44953224062919617 Accuracy 99.53611875359908\n",
      "Training:: Epoch 147, Iteration 110, Current loss 0.4052293002605438 Accuracy 99.38466655579577\n",
      "Training:: Epoch 147, Iteration 120, Current loss 0.6247694492340088 Accuracy 99.08903908316192\n",
      "Training:: Epoch 147, Iteration 130, Current loss 0.4971950352191925 Accuracy 99.26221121223544\n",
      "Training:: Epoch 147, Iteration 140, Current loss 0.4917406737804413 Accuracy 99.23041199495788\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 147, Probability Accuracy 67.14480160001045\n",
      "Starting Training\n",
      "Training:: Epoch 148, Iteration 0, Current loss 0.42099061608314514 Accuracy 99.39698492462311\n",
      "Training:: Epoch 148, Iteration 10, Current loss 0.41136854887008667 Accuracy 99.36720905890189\n",
      "Training:: Epoch 148, Iteration 20, Current loss 0.4675094485282898 Accuracy 99.50046623151725\n",
      "Training:: Epoch 148, Iteration 30, Current loss 0.40284621715545654 Accuracy 99.19555187507393\n",
      "Training:: Epoch 148, Iteration 40, Current loss 0.3943522274494171 Accuracy 99.25332362046986\n",
      "Training:: Epoch 148, Iteration 50, Current loss 0.6411591172218323 Accuracy 99.30524628128619\n",
      "Training:: Epoch 148, Iteration 60, Current loss 0.5929845571517944 Accuracy 99.24384271928561\n",
      "Training:: Epoch 148, Iteration 70, Current loss 0.4022250175476074 Accuracy 99.32970027247957\n",
      "Training:: Epoch 148, Iteration 80, Current loss 0.427462637424469 Accuracy 99.16756846749355\n",
      "Training:: Epoch 148, Iteration 90, Current loss 0.3588908612728119 Accuracy 99.54458888796887\n",
      "Training:: Epoch 148, Iteration 100, Current loss 0.511023998260498 Accuracy 99.27663734115347\n",
      "Training:: Epoch 148, Iteration 110, Current loss 0.4178219437599182 Accuracy 99.59681998864282\n",
      "Training:: Epoch 148, Iteration 120, Current loss 0.37990638613700867 Accuracy 99.47368421052632\n",
      "Training:: Epoch 148, Iteration 130, Current loss 0.420153945684433 Accuracy 99.55998447004012\n",
      "Training:: Epoch 148, Iteration 140, Current loss 0.36611804366111755 Accuracy 99.52294712663986\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 148, Probability Accuracy 67.46310433401526\n",
      "Starting Training\n",
      "Training:: Epoch 149, Iteration 0, Current loss 0.41819441318511963 Accuracy 99.35173017958826\n",
      "Training:: Epoch 149, Iteration 10, Current loss 0.36952149868011475 Accuracy 99.38680006454736\n",
      "Training:: Epoch 149, Iteration 20, Current loss 0.5758711695671082 Accuracy 98.97608425363856\n",
      "Training:: Epoch 149, Iteration 30, Current loss 0.528887152671814 Accuracy 99.29968167348795\n",
      "Training:: Epoch 149, Iteration 40, Current loss 0.5899863243103027 Accuracy 99.52726383568343\n",
      "Training:: Epoch 149, Iteration 50, Current loss 0.3880957365036011 Accuracy 99.49250288350635\n",
      "Training:: Epoch 149, Iteration 60, Current loss 0.47838085889816284 Accuracy 99.4533221194281\n",
      "Training:: Epoch 149, Iteration 70, Current loss 0.3765055537223816 Accuracy 99.5794513457557\n",
      "Training:: Epoch 149, Iteration 80, Current loss 0.37526199221611023 Accuracy 99.20757810365242\n",
      "Training:: Epoch 149, Iteration 90, Current loss 0.31275925040245056 Accuracy 99.65150661785412\n",
      "Training:: Epoch 149, Iteration 100, Current loss 0.34097352623939514 Accuracy 99.42146008490802\n",
      "Training:: Epoch 149, Iteration 110, Current loss 0.4894937574863434 Accuracy 99.24739482825164\n",
      "Training:: Epoch 149, Iteration 120, Current loss 0.644942045211792 Accuracy 99.27797833935018\n",
      "Training:: Epoch 149, Iteration 130, Current loss 0.38729286193847656 Accuracy 99.34809787462046\n",
      "Training:: Epoch 149, Iteration 140, Current loss 0.33380624651908875 Accuracy 99.49663567723047\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 149, Probability Accuracy 67.43998326786449\n",
      "Starting Training\n",
      "Training:: Epoch 150, Iteration 0, Current loss 0.3421635627746582 Accuracy 99.52851573125396\n",
      "Training:: Epoch 150, Iteration 10, Current loss 0.3301008939743042 Accuracy 99.55421686746988\n",
      "Training:: Epoch 150, Iteration 20, Current loss 0.5285912752151489 Accuracy 99.51908804844743\n",
      "Training:: Epoch 150, Iteration 30, Current loss 0.3952479958534241 Accuracy 99.58358848744642\n",
      "Training:: Epoch 150, Iteration 40, Current loss 0.4046919643878937 Accuracy 99.44784937330904\n",
      "Training:: Epoch 150, Iteration 50, Current loss 0.48417291045188904 Accuracy 99.53651909860956\n",
      "Training:: Epoch 150, Iteration 60, Current loss 0.3670494556427002 Accuracy 99.24007599240076\n",
      "Training:: Epoch 150, Iteration 70, Current loss 0.3489830195903778 Accuracy 99.54333643988817\n",
      "Training:: Epoch 150, Iteration 80, Current loss 0.49862003326416016 Accuracy 98.95002099958\n",
      "Training:: Epoch 150, Iteration 90, Current loss 0.3117105960845947 Accuracy 99.38460047487523\n",
      "Training:: Epoch 150, Iteration 100, Current loss 0.4768884479999542 Accuracy 99.3984266543267\n",
      "Training:: Epoch 150, Iteration 110, Current loss 0.44226330518722534 Accuracy 99.44678391544876\n",
      "Training:: Epoch 150, Iteration 120, Current loss 0.5310090780258179 Accuracy 99.13457377758546\n",
      "Training:: Epoch 150, Iteration 130, Current loss 0.3933827579021454 Accuracy 99.44256847295921\n",
      "Training:: Epoch 150, Iteration 140, Current loss 0.38668352365493774 Accuracy 99.54520141080378\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 150, Probability Accuracy 67.57748416656318\n",
      "Starting Training\n",
      "Training:: Epoch 151, Iteration 0, Current loss 0.3947698473930359 Accuracy 99.35862813934648\n",
      "Training:: Epoch 151, Iteration 10, Current loss 0.4643736779689789 Accuracy 99.37252007013011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 151, Iteration 20, Current loss 0.31271281838417053 Accuracy 99.51072183630323\n",
      "Training:: Epoch 151, Iteration 30, Current loss 0.3826983869075775 Accuracy 99.22586520947176\n",
      "Training:: Epoch 151, Iteration 40, Current loss 0.5206665396690369 Accuracy 99.24570694912533\n",
      "Training:: Epoch 151, Iteration 50, Current loss 0.3504335284233093 Accuracy 99.48619502004404\n",
      "Training:: Epoch 151, Iteration 60, Current loss 0.35556867718696594 Accuracy 99.19738254690455\n",
      "Training:: Epoch 151, Iteration 70, Current loss 0.42428621649742126 Accuracy 99.40964454387273\n",
      "Training:: Epoch 151, Iteration 80, Current loss 0.46214547753334045 Accuracy 99.48833497629442\n",
      "Training:: Epoch 151, Iteration 90, Current loss 0.3819141983985901 Accuracy 99.496090356212\n",
      "Training:: Epoch 151, Iteration 100, Current loss 0.42634016275405884 Accuracy 99.37063539960342\n",
      "Training:: Epoch 151, Iteration 110, Current loss 0.3318380117416382 Accuracy 99.53716308194936\n",
      "Training:: Epoch 151, Iteration 120, Current loss 0.40335631370544434 Accuracy 99.5503130512989\n",
      "Training:: Epoch 151, Iteration 130, Current loss 0.37099647521972656 Accuracy 99.51636232146086\n",
      "Training:: Epoch 151, Iteration 140, Current loss 0.36190831661224365 Accuracy 99.52899390008494\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 151, Probability Accuracy 67.05092843744077\n",
      "Starting Training\n",
      "Training:: Epoch 152, Iteration 0, Current loss 0.386875182390213 Accuracy 99.55421469740634\n",
      "Training:: Epoch 152, Iteration 10, Current loss 0.5538820624351501 Accuracy 99.41976127320955\n",
      "Training:: Epoch 152, Iteration 20, Current loss 0.33371099829673767 Accuracy 99.59941377625793\n",
      "Training:: Epoch 152, Iteration 30, Current loss 0.5059415102005005 Accuracy 99.57822696164253\n",
      "Training:: Epoch 152, Iteration 40, Current loss 0.48276078701019287 Accuracy 99.35682517960998\n",
      "Training:: Epoch 152, Iteration 50, Current loss 0.4342609643936157 Accuracy 99.27835051546391\n",
      "Training:: Epoch 152, Iteration 60, Current loss 0.507117509841919 Accuracy 98.76291972366519\n",
      "Training:: Epoch 152, Iteration 70, Current loss 0.3645693063735962 Accuracy 99.41947453468191\n",
      "Training:: Epoch 152, Iteration 80, Current loss 0.40050238370895386 Accuracy 99.4720324903083\n",
      "Training:: Epoch 152, Iteration 90, Current loss 0.5483043193817139 Accuracy 99.10714285714286\n",
      "Training:: Epoch 152, Iteration 100, Current loss 0.519405722618103 Accuracy 99.35407569141194\n",
      "Training:: Epoch 152, Iteration 110, Current loss 0.4961542785167694 Accuracy 99.31226603987116\n",
      "Training:: Epoch 152, Iteration 120, Current loss 0.6230604648590088 Accuracy 99.21308440055547\n",
      "Training:: Epoch 152, Iteration 130, Current loss 0.5563026070594788 Accuracy 99.27381995743083\n",
      "Training:: Epoch 152, Iteration 140, Current loss 0.4205356538295746 Accuracy 99.34301773852106\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 152, Probability Accuracy 67.30762946163047\n",
      "Starting Training\n",
      "Training:: Epoch 153, Iteration 0, Current loss 0.4442465603351593 Accuracy 99.37158921779395\n",
      "Training:: Epoch 153, Iteration 10, Current loss 0.5655062198638916 Accuracy 99.23859839950276\n",
      "Training:: Epoch 153, Iteration 20, Current loss 0.3691863715648651 Accuracy 99.42591155934834\n",
      "Training:: Epoch 153, Iteration 30, Current loss 0.37287211418151855 Accuracy 99.53232002672458\n",
      "Training:: Epoch 153, Iteration 40, Current loss 0.47819775342941284 Accuracy 99.11610650371017\n",
      "Training:: Epoch 153, Iteration 50, Current loss 0.518217921257019 Accuracy 99.08765652951699\n",
      "Training:: Epoch 153, Iteration 60, Current loss 0.5503233075141907 Accuracy 99.10834249419811\n",
      "Training:: Epoch 153, Iteration 70, Current loss 0.3591615557670593 Accuracy 99.5222056865356\n",
      "Training:: Epoch 153, Iteration 80, Current loss 0.4992504119873047 Accuracy 98.92661555312158\n",
      "Training:: Epoch 153, Iteration 90, Current loss 0.37742504477500916 Accuracy 99.45922868934112\n",
      "Training:: Epoch 153, Iteration 100, Current loss 0.34824782609939575 Accuracy 99.56773891430424\n",
      "Training:: Epoch 153, Iteration 110, Current loss 0.4312400221824646 Accuracy 99.32329555066825\n",
      "Training:: Epoch 153, Iteration 120, Current loss 0.4156149625778198 Accuracy 99.14351095129128\n",
      "Training:: Epoch 153, Iteration 130, Current loss 0.45622900128364563 Accuracy 99.50543274634694\n",
      "Training:: Epoch 153, Iteration 140, Current loss 0.31904110312461853 Accuracy 99.67523355118078\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 153, Probability Accuracy 66.86963640285231\n",
      "Starting Training\n",
      "Training:: Epoch 154, Iteration 0, Current loss 0.5272900462150574 Accuracy 99.3563579277865\n",
      "Training:: Epoch 154, Iteration 10, Current loss 0.5230032205581665 Accuracy 99.20990566037736\n",
      "Training:: Epoch 154, Iteration 20, Current loss 0.3400038182735443 Accuracy 99.09800520381613\n",
      "Training:: Epoch 154, Iteration 30, Current loss 0.2993496358394623 Accuracy 99.53152111046847\n",
      "Training:: Epoch 154, Iteration 40, Current loss 0.3655199706554413 Accuracy 99.49311675594724\n",
      "Training:: Epoch 154, Iteration 50, Current loss 0.41580846905708313 Accuracy 99.20963214655045\n",
      "Training:: Epoch 154, Iteration 60, Current loss 0.32642194628715515 Accuracy 99.58929395477618\n",
      "Training:: Epoch 154, Iteration 70, Current loss 0.6722723841667175 Accuracy 99.10650097565986\n",
      "Training:: Epoch 154, Iteration 80, Current loss 0.5341435074806213 Accuracy 99.2313037435942\n",
      "Training:: Epoch 154, Iteration 90, Current loss 0.37355539202690125 Accuracy 99.44116238224493\n",
      "Training:: Epoch 154, Iteration 100, Current loss 0.37903085350990295 Accuracy 99.53192105390146\n",
      "Training:: Epoch 154, Iteration 110, Current loss 0.3879825174808502 Accuracy 99.5436206098287\n",
      "Training:: Epoch 154, Iteration 120, Current loss 0.36994466185569763 Accuracy 99.61786372007367\n",
      "Training:: Epoch 154, Iteration 130, Current loss 0.3846375048160553 Accuracy 99.21454793818833\n",
      "Training:: Epoch 154, Iteration 140, Current loss 0.39957964420318604 Accuracy 99.45015616866216\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 154, Probability Accuracy 66.89700586278342\n",
      "Starting Training\n",
      "Training:: Epoch 155, Iteration 0, Current loss 0.5409474968910217 Accuracy 99.02404668477715\n",
      "Training:: Epoch 155, Iteration 10, Current loss 0.33791816234588623 Accuracy 99.3979459331838\n",
      "Training:: Epoch 155, Iteration 20, Current loss 0.43401169776916504 Accuracy 98.96038434275813\n",
      "Training:: Epoch 155, Iteration 30, Current loss 0.5328055024147034 Accuracy 99.49909721008795\n",
      "Training:: Epoch 155, Iteration 40, Current loss 0.38496580719947815 Accuracy 99.30904522613065\n",
      "Training:: Epoch 155, Iteration 50, Current loss 0.27142155170440674 Accuracy 99.37627443924673\n",
      "Training:: Epoch 155, Iteration 60, Current loss 0.4116143584251404 Accuracy 99.41186954197113\n",
      "Training:: Epoch 155, Iteration 70, Current loss 0.3287579119205475 Accuracy 99.37449177456683\n",
      "Training:: Epoch 155, Iteration 80, Current loss 0.3144875168800354 Accuracy 99.37185277474951\n",
      "Training:: Epoch 155, Iteration 90, Current loss 0.44009682536125183 Accuracy 99.07456025203466\n",
      "Training:: Epoch 155, Iteration 100, Current loss 0.36179018020629883 Accuracy 99.61747533722568\n",
      "Training:: Epoch 155, Iteration 110, Current loss 0.49549517035484314 Accuracy 98.9689797382105\n",
      "Training:: Epoch 155, Iteration 120, Current loss 0.37751513719558716 Accuracy 99.30529894316754\n",
      "Training:: Epoch 155, Iteration 130, Current loss 0.3797428607940674 Accuracy 99.189991899919\n",
      "Training:: Epoch 155, Iteration 140, Current loss 0.38646218180656433 Accuracy 99.00038201961034\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 155, Probability Accuracy 67.33900221570076\n",
      "Starting Training\n",
      "Training:: Epoch 156, Iteration 0, Current loss 0.440121591091156 Accuracy 99.43397400012441\n",
      "Training:: Epoch 156, Iteration 10, Current loss 0.40838292241096497 Accuracy 99.20711661187391\n",
      "Training:: Epoch 156, Iteration 20, Current loss 0.4736490249633789 Accuracy 99.33085954133463\n",
      "Training:: Epoch 156, Iteration 30, Current loss 0.5306262969970703 Accuracy 99.13503527040645\n",
      "Training:: Epoch 156, Iteration 40, Current loss 0.37270891666412354 Accuracy 99.32141458958633\n",
      "Training:: Epoch 156, Iteration 50, Current loss 0.2521882951259613 Accuracy 99.51851349518513\n",
      "Training:: Epoch 156, Iteration 60, Current loss 0.23004110157489777 Accuracy 99.53582240161452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 156, Iteration 70, Current loss 0.3050261437892914 Accuracy 99.51743603751252\n",
      "Training:: Epoch 156, Iteration 80, Current loss 0.4423988461494446 Accuracy 99.23604427333974\n",
      "Training:: Epoch 156, Iteration 90, Current loss 0.4127361476421356 Accuracy 99.47483361064891\n",
      "Training:: Epoch 156, Iteration 100, Current loss 0.27338406443595886 Accuracy 99.30924873158506\n",
      "Training:: Epoch 156, Iteration 110, Current loss 0.47186478972435 Accuracy 99.09399773499433\n",
      "Training:: Epoch 156, Iteration 120, Current loss 0.5528319478034973 Accuracy 99.0527645242773\n",
      "Training:: Epoch 156, Iteration 130, Current loss 0.4697485864162445 Accuracy 99.00477302731797\n",
      "Training:: Epoch 156, Iteration 140, Current loss 0.3586413562297821 Accuracy 99.47899967938442\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 156, Probability Accuracy 66.89667906326186\n",
      "Starting Training\n",
      "Training:: Epoch 157, Iteration 0, Current loss 0.4044498801231384 Accuracy 99.23848878394332\n",
      "Training:: Epoch 157, Iteration 10, Current loss 0.3774847686290741 Accuracy 99.1317125300561\n",
      "Training:: Epoch 157, Iteration 20, Current loss 0.4032526910305023 Accuracy 99.38203790167536\n",
      "Training:: Epoch 157, Iteration 30, Current loss 0.3404431939125061 Accuracy 99.49195893626712\n",
      "Training:: Epoch 157, Iteration 40, Current loss 0.4009001851081848 Accuracy 98.95450461488197\n",
      "Training:: Epoch 157, Iteration 50, Current loss 0.44833803176879883 Accuracy 99.4003618506074\n",
      "Training:: Epoch 157, Iteration 60, Current loss 0.35163241624832153 Accuracy 99.53476077187418\n",
      "Training:: Epoch 157, Iteration 70, Current loss 0.40278372168540955 Accuracy 99.3809037528721\n",
      "Training:: Epoch 157, Iteration 80, Current loss 0.5173988938331604 Accuracy 99.08682524486339\n",
      "Training:: Epoch 157, Iteration 90, Current loss 0.3312261700630188 Accuracy 99.41612604263207\n",
      "Training:: Epoch 157, Iteration 100, Current loss 0.3567187190055847 Accuracy 99.39353400222966\n",
      "Training:: Epoch 157, Iteration 110, Current loss 0.26355454325675964 Accuracy 99.56245589273112\n",
      "Training:: Epoch 157, Iteration 120, Current loss 0.3455744981765747 Accuracy 99.55960671855797\n",
      "Training:: Epoch 157, Iteration 130, Current loss 0.38769620656967163 Accuracy 99.52253844129478\n",
      "Training:: Epoch 157, Iteration 140, Current loss 0.32387131452560425 Accuracy 99.47755417956657\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 157, Probability Accuracy 67.67683122111909\n",
      "Starting Training\n",
      "Training:: Epoch 158, Iteration 0, Current loss 0.36286836862564087 Accuracy 99.27480916030534\n",
      "Training:: Epoch 158, Iteration 10, Current loss 0.3602314293384552 Accuracy 99.37291527685123\n",
      "Training:: Epoch 158, Iteration 20, Current loss 0.4942949414253235 Accuracy 99.38091026600449\n",
      "Training:: Epoch 158, Iteration 30, Current loss 0.2641961872577667 Accuracy 99.40219447597427\n",
      "Training:: Epoch 158, Iteration 40, Current loss 0.2672681212425232 Accuracy 99.34839270199826\n",
      "Training:: Epoch 158, Iteration 50, Current loss 0.4451773762702942 Accuracy 99.41583729987019\n",
      "Training:: Epoch 158, Iteration 60, Current loss 0.4373739957809448 Accuracy 99.15368484122229\n",
      "Training:: Epoch 158, Iteration 70, Current loss 0.36430177092552185 Accuracy 99.18347482831342\n",
      "Training:: Epoch 158, Iteration 80, Current loss 0.4094349443912506 Accuracy 99.05940235872947\n",
      "Training:: Epoch 158, Iteration 90, Current loss 0.6170962452888489 Accuracy 98.80758807588076\n",
      "Training:: Epoch 158, Iteration 100, Current loss 0.48885464668273926 Accuracy 98.83845778428501\n",
      "Training:: Epoch 158, Iteration 110, Current loss 0.38190963864326477 Accuracy 99.31908793620929\n",
      "Training:: Epoch 158, Iteration 120, Current loss 0.41237977147102356 Accuracy 99.25270893012828\n",
      "Training:: Epoch 158, Iteration 130, Current loss 0.5259020328521729 Accuracy 98.97984573276935\n",
      "Training:: Epoch 158, Iteration 140, Current loss 1.2028234004974365 Accuracy 95.9335175485627\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 158, Probability Accuracy 60.87654167674299\n",
      "Starting Training\n",
      "Training:: Epoch 159, Iteration 0, Current loss 4.381637096405029 Accuracy 73.8902114549783\n",
      "Training:: Epoch 159, Iteration 10, Current loss 0.9230543971061707 Accuracy 95.2398695854681\n",
      "Training:: Epoch 159, Iteration 20, Current loss 2.5125300884246826 Accuracy 83.76023924067091\n",
      "Training:: Epoch 159, Iteration 30, Current loss 1.9242823123931885 Accuracy 91.49360358863599\n",
      "Training:: Epoch 159, Iteration 40, Current loss 3.551802635192871 Accuracy 77.18608764000786\n",
      "Training:: Epoch 159, Iteration 50, Current loss 1.903444766998291 Accuracy 86.48481091134532\n",
      "Training:: Epoch 159, Iteration 60, Current loss 6.505209445953369 Accuracy 66.97195487943702\n",
      "Training:: Epoch 159, Iteration 70, Current loss 4.60748291015625 Accuracy 71.78667802868911\n",
      "Training:: Epoch 159, Iteration 80, Current loss 5.2653703689575195 Accuracy 68.04606310270574\n",
      "Training:: Epoch 159, Iteration 90, Current loss 2.4728283882141113 Accuracy 88.2240071396698\n",
      "Training:: Epoch 159, Iteration 100, Current loss 1.5568203926086426 Accuracy 92.52140563681769\n",
      "Training:: Epoch 159, Iteration 110, Current loss 2.3506643772125244 Accuracy 89.36252343663836\n",
      "Training:: Epoch 159, Iteration 120, Current loss 4.154760360717773 Accuracy 77.79651193424311\n",
      "Training:: Epoch 159, Iteration 130, Current loss 2.0043540000915527 Accuracy 91.9139503249534\n",
      "Training:: Epoch 159, Iteration 140, Current loss 2.253579616546631 Accuracy 88.0427408412483\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 159, Probability Accuracy 59.70708958882084\n",
      "Starting Training\n",
      "Training:: Epoch 160, Iteration 0, Current loss 1.86440908908844 Accuracy 87.93432347731299\n",
      "Training:: Epoch 160, Iteration 10, Current loss 1.9539985656738281 Accuracy 92.85670861450544\n",
      "Training:: Epoch 160, Iteration 20, Current loss 1.5214009284973145 Accuracy 94.27431589593752\n",
      "Training:: Epoch 160, Iteration 30, Current loss 1.143013596534729 Accuracy 96.8436069666528\n",
      "Training:: Epoch 160, Iteration 40, Current loss 0.7062146067619324 Accuracy 98.65632307635202\n",
      "Training:: Epoch 160, Iteration 50, Current loss 0.7510841488838196 Accuracy 97.59438873362924\n",
      "Training:: Epoch 160, Iteration 60, Current loss 0.8655168414115906 Accuracy 98.10986414648553\n",
      "Training:: Epoch 160, Iteration 70, Current loss 1.0809593200683594 Accuracy 96.7542308663804\n",
      "Training:: Epoch 160, Iteration 80, Current loss 1.1277168989181519 Accuracy 96.00047053287848\n",
      "Training:: Epoch 160, Iteration 90, Current loss 0.9956904649734497 Accuracy 96.08805112646053\n",
      "Training:: Epoch 160, Iteration 100, Current loss 0.7835195660591125 Accuracy 95.74078449905483\n",
      "Training:: Epoch 160, Iteration 110, Current loss 0.8135340809822083 Accuracy 98.46648670380793\n",
      "Training:: Epoch 160, Iteration 120, Current loss 0.9492206573486328 Accuracy 97.14202994457771\n",
      "Training:: Epoch 160, Iteration 130, Current loss 0.6130663156509399 Accuracy 98.52724594992637\n",
      "Training:: Epoch 160, Iteration 140, Current loss 0.7481120824813843 Accuracy 98.43007643048956\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 160, Probability Accuracy 67.61768050771573\n",
      "Starting Training\n",
      "Training:: Epoch 161, Iteration 0, Current loss 0.7658815979957581 Accuracy 98.60991800078095\n",
      "Training:: Epoch 161, Iteration 10, Current loss 0.8429964780807495 Accuracy 98.43777122027426\n",
      "Training:: Epoch 161, Iteration 20, Current loss 0.5576987862586975 Accuracy 98.94829691952775\n",
      "Training:: Epoch 161, Iteration 30, Current loss 0.7047154903411865 Accuracy 98.93481427530953\n",
      "Training:: Epoch 161, Iteration 40, Current loss 0.7310298681259155 Accuracy 98.70509607351713\n",
      "Training:: Epoch 161, Iteration 50, Current loss 0.666281521320343 Accuracy 98.79395784822975\n",
      "Training:: Epoch 161, Iteration 60, Current loss 0.7851452231407166 Accuracy 97.87168604293535\n",
      "Training:: Epoch 161, Iteration 70, Current loss 0.9303455948829651 Accuracy 98.01796663339512\n",
      "Training:: Epoch 161, Iteration 80, Current loss 0.5602210760116577 Accuracy 98.858592263792\n",
      "Training:: Epoch 161, Iteration 90, Current loss 0.4906204640865326 Accuracy 99.19019635946682\n",
      "Training:: Epoch 161, Iteration 100, Current loss 0.8750941157341003 Accuracy 98.73459598048134\n",
      "Training:: Epoch 161, Iteration 110, Current loss 0.5817853808403015 Accuracy 98.97603085682799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 161, Iteration 120, Current loss 0.604710578918457 Accuracy 98.61531019470311\n",
      "Training:: Epoch 161, Iteration 130, Current loss 0.6041221618652344 Accuracy 99.28796544881521\n",
      "Training:: Epoch 161, Iteration 140, Current loss 0.4926254451274872 Accuracy 99.27310942768315\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 161, Probability Accuracy 68.55722913221655\n",
      "Starting Training\n",
      "Training:: Epoch 162, Iteration 0, Current loss 0.43985629081726074 Accuracy 99.41718200827512\n",
      "Training:: Epoch 162, Iteration 10, Current loss 0.46176382899284363 Accuracy 99.31129476584022\n",
      "Training:: Epoch 162, Iteration 20, Current loss 0.5242752432823181 Accuracy 99.23466598636976\n",
      "Training:: Epoch 162, Iteration 30, Current loss 0.41471266746520996 Accuracy 98.66432811656773\n",
      "Training:: Epoch 162, Iteration 40, Current loss 0.5941358208656311 Accuracy 98.08834729626808\n",
      "Training:: Epoch 162, Iteration 50, Current loss 0.4089396297931671 Accuracy 99.31431864016133\n",
      "Training:: Epoch 162, Iteration 60, Current loss 0.42514073848724365 Accuracy 99.13841195066014\n",
      "Training:: Epoch 162, Iteration 70, Current loss 0.5353475213050842 Accuracy 99.21022940955247\n",
      "Training:: Epoch 162, Iteration 80, Current loss 0.7086132764816284 Accuracy 99.3006993006993\n",
      "Training:: Epoch 162, Iteration 90, Current loss 0.5364540815353394 Accuracy 98.79062114356232\n",
      "Training:: Epoch 162, Iteration 100, Current loss 0.47611063718795776 Accuracy 99.45593594415645\n",
      "Training:: Epoch 162, Iteration 110, Current loss 0.6625382304191589 Accuracy 98.71984244214673\n",
      "Training:: Epoch 162, Iteration 120, Current loss 0.83358234167099 Accuracy 98.98944531776331\n",
      "Training:: Epoch 162, Iteration 130, Current loss 0.6832523941993713 Accuracy 99.11543959971408\n",
      "Training:: Epoch 162, Iteration 140, Current loss 0.48261114954948425 Accuracy 99.21545469883584\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 162, Probability Accuracy 68.07070634448591\n",
      "Starting Training\n",
      "Training:: Epoch 163, Iteration 0, Current loss 0.46251407265663147 Accuracy 99.35153583617748\n",
      "Training:: Epoch 163, Iteration 10, Current loss 0.4083777070045471 Accuracy 99.37365985780386\n",
      "Training:: Epoch 163, Iteration 20, Current loss 0.5967221856117249 Accuracy 98.97832267235252\n",
      "Training:: Epoch 163, Iteration 30, Current loss 0.43954703211784363 Accuracy 99.38731637405948\n",
      "Training:: Epoch 163, Iteration 40, Current loss 0.5479531288146973 Accuracy 99.19322538193053\n",
      "Training:: Epoch 163, Iteration 50, Current loss 0.4403955340385437 Accuracy 99.26590538336052\n",
      "Training:: Epoch 163, Iteration 60, Current loss 0.6390392780303955 Accuracy 99.14004914004914\n",
      "Training:: Epoch 163, Iteration 70, Current loss 0.4404570460319519 Accuracy 99.46375739644971\n",
      "Training:: Epoch 163, Iteration 80, Current loss 0.41975483298301697 Accuracy 99.15040612454486\n",
      "Training:: Epoch 163, Iteration 90, Current loss 0.6016489267349243 Accuracy 99.43973567016234\n",
      "Training:: Epoch 163, Iteration 100, Current loss 0.4009927213191986 Accuracy 99.35489073461818\n",
      "Training:: Epoch 163, Iteration 110, Current loss 0.32039323449134827 Accuracy 99.43775100401606\n",
      "Training:: Epoch 163, Iteration 120, Current loss 0.4056556522846222 Accuracy 99.22055916407794\n",
      "Training:: Epoch 163, Iteration 130, Current loss 0.358676016330719 Accuracy 99.33401639344262\n",
      "Training:: Epoch 163, Iteration 140, Current loss 0.502063512802124 Accuracy 99.27044025157232\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 163, Probability Accuracy 67.80126013895516\n",
      "Starting Training\n",
      "Training:: Epoch 164, Iteration 0, Current loss 0.45038366317749023 Accuracy 99.45294922133498\n",
      "Training:: Epoch 164, Iteration 10, Current loss 0.3841690719127655 Accuracy 99.43070179857928\n",
      "Training:: Epoch 164, Iteration 20, Current loss 0.44938358664512634 Accuracy 99.29735603042376\n",
      "Training:: Epoch 164, Iteration 30, Current loss 0.3484771251678467 Accuracy 99.37739810323608\n",
      "Training:: Epoch 164, Iteration 40, Current loss 0.39630961418151855 Accuracy 99.2978417266187\n",
      "Training:: Epoch 164, Iteration 50, Current loss 0.3843882977962494 Accuracy 99.27494908350306\n",
      "Training:: Epoch 164, Iteration 60, Current loss 0.5627192258834839 Accuracy 99.04794096107261\n",
      "Training:: Epoch 164, Iteration 70, Current loss 0.4808826148509979 Accuracy 99.38271604938272\n",
      "Training:: Epoch 164, Iteration 80, Current loss 0.4969414174556732 Accuracy 99.45874042475116\n",
      "Training:: Epoch 164, Iteration 90, Current loss 0.5389224290847778 Accuracy 99.27608944954129\n",
      "Training:: Epoch 164, Iteration 100, Current loss 0.5260759592056274 Accuracy 99.34047175667287\n",
      "Training:: Epoch 164, Iteration 110, Current loss 0.7146575450897217 Accuracy 99.05264291096996\n",
      "Training:: Epoch 164, Iteration 120, Current loss 0.4793505072593689 Accuracy 99.3304721030043\n",
      "Training:: Epoch 164, Iteration 130, Current loss 0.36881566047668457 Accuracy 99.51743274218845\n",
      "Training:: Epoch 164, Iteration 140, Current loss 0.593239426612854 Accuracy 99.22868161691187\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 164, Probability Accuracy 67.53614402708514\n",
      "Starting Training\n",
      "Training:: Epoch 165, Iteration 0, Current loss 0.596662163734436 Accuracy 99.20029988754217\n",
      "Training:: Epoch 165, Iteration 10, Current loss 0.3060389459133148 Accuracy 99.5475113122172\n",
      "Training:: Epoch 165, Iteration 20, Current loss 0.4263565242290497 Accuracy 99.36783074177924\n",
      "Training:: Epoch 165, Iteration 30, Current loss 0.3432212173938751 Accuracy 99.58946110273865\n",
      "Training:: Epoch 165, Iteration 40, Current loss 0.37922343611717224 Accuracy 99.4456029703474\n",
      "Training:: Epoch 165, Iteration 50, Current loss 0.3731156885623932 Accuracy 99.38985228002569\n",
      "Training:: Epoch 165, Iteration 60, Current loss 0.4793321490287781 Accuracy 99.64736546225788\n",
      "Training:: Epoch 165, Iteration 70, Current loss 0.3547429144382477 Accuracy 99.46187972395663\n",
      "Training:: Epoch 165, Iteration 80, Current loss 0.5461386442184448 Accuracy 99.4311230731588\n",
      "Training:: Epoch 165, Iteration 90, Current loss 0.503033459186554 Accuracy 99.25697462498248\n",
      "Training:: Epoch 165, Iteration 100, Current loss 0.40983569622039795 Accuracy 99.21244959677419\n",
      "Training:: Epoch 165, Iteration 110, Current loss 0.5582477450370789 Accuracy 99.11053259572621\n",
      "Training:: Epoch 165, Iteration 120, Current loss 0.38547152280807495 Accuracy 99.52374378796245\n",
      "Training:: Epoch 165, Iteration 130, Current loss 0.47765666246414185 Accuracy 99.41026145075683\n",
      "Training:: Epoch 165, Iteration 140, Current loss 0.4251177906990051 Accuracy 99.42938153464469\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 165, Probability Accuracy 68.09840260393858\n",
      "Starting Training\n",
      "Training:: Epoch 166, Iteration 0, Current loss 0.4817364513874054 Accuracy 99.43921040825482\n",
      "Training:: Epoch 166, Iteration 10, Current loss 0.5343396663665771 Accuracy 99.42388786600938\n",
      "Training:: Epoch 166, Iteration 20, Current loss 0.4021304249763489 Accuracy 99.41338085690948\n",
      "Training:: Epoch 166, Iteration 30, Current loss 0.4574710726737976 Accuracy 99.5211350872007\n",
      "Training:: Epoch 166, Iteration 40, Current loss 0.35858550667762756 Accuracy 99.48067745859456\n",
      "Training:: Epoch 166, Iteration 50, Current loss 0.5296422839164734 Accuracy 99.27837477449212\n",
      "Training:: Epoch 166, Iteration 60, Current loss 0.39511626958847046 Accuracy 99.65737629307505\n",
      "Training:: Epoch 166, Iteration 70, Current loss 0.5343760848045349 Accuracy 99.35141877143748\n",
      "Training:: Epoch 166, Iteration 80, Current loss 0.4287753105163574 Accuracy 99.55245134603648\n",
      "Training:: Epoch 166, Iteration 90, Current loss 0.49411842226982117 Accuracy 99.38958910303509\n",
      "Training:: Epoch 166, Iteration 100, Current loss 0.36997273564338684 Accuracy 99.29577464788733\n",
      "Training:: Epoch 166, Iteration 110, Current loss 0.4705221652984619 Accuracy 99.35326647295904\n",
      "Training:: Epoch 166, Iteration 120, Current loss 0.3412325084209442 Accuracy 99.57330242848865\n",
      "Training:: Epoch 166, Iteration 130, Current loss 0.38730552792549133 Accuracy 99.47847478474785\n",
      "Training:: Epoch 166, Iteration 140, Current loss 0.4038384258747101 Accuracy 99.32711795703284\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 166, Probability Accuracy 67.45991803867999\n",
      "Starting Training\n",
      "Training:: Epoch 167, Iteration 0, Current loss 0.4189166724681854 Accuracy 99.60676792092067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 167, Iteration 10, Current loss 0.4228510856628418 Accuracy 99.44394428851348\n",
      "Training:: Epoch 167, Iteration 20, Current loss 0.4298604428768158 Accuracy 99.41871193947223\n",
      "Training:: Epoch 167, Iteration 30, Current loss 0.3377554416656494 Accuracy 99.16706904876871\n",
      "Training:: Epoch 167, Iteration 40, Current loss 0.41482093930244446 Accuracy 99.42826650404025\n",
      "Training:: Epoch 167, Iteration 50, Current loss 0.3995693325996399 Accuracy 99.4743560862056\n",
      "Training:: Epoch 167, Iteration 60, Current loss 0.5178619623184204 Accuracy 99.2990792990793\n",
      "Training:: Epoch 167, Iteration 70, Current loss 0.5106486678123474 Accuracy 99.49121943213524\n",
      "Training:: Epoch 167, Iteration 80, Current loss 0.3431316912174225 Accuracy 99.57798165137615\n",
      "Training:: Epoch 167, Iteration 90, Current loss 0.3227280378341675 Accuracy 99.52926072536643\n",
      "Training:: Epoch 167, Iteration 100, Current loss 0.452070415019989 Accuracy 99.27080354589648\n",
      "Training:: Epoch 167, Iteration 110, Current loss 0.3513164520263672 Accuracy 99.39725752172384\n",
      "Training:: Epoch 167, Iteration 120, Current loss 0.2628062665462494 Accuracy 99.61137019764601\n",
      "Training:: Epoch 167, Iteration 130, Current loss 0.3525868058204651 Accuracy 99.51862039779941\n",
      "Training:: Epoch 167, Iteration 140, Current loss 0.3922494351863861 Accuracy 99.63397097912762\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 167, Probability Accuracy 67.49578428617181\n",
      "Starting Training\n",
      "Training:: Epoch 168, Iteration 0, Current loss 0.4447454810142517 Accuracy 99.32822197882439\n",
      "Training:: Epoch 168, Iteration 10, Current loss 0.5842311978340149 Accuracy 99.29039888184066\n",
      "Training:: Epoch 168, Iteration 20, Current loss 0.27830958366394043 Accuracy 99.68275047956323\n",
      "Training:: Epoch 168, Iteration 30, Current loss 0.39399027824401855 Accuracy 99.50228746669347\n",
      "Training:: Epoch 168, Iteration 40, Current loss 0.5031733512878418 Accuracy 99.3191148493041\n",
      "Training:: Epoch 168, Iteration 50, Current loss 0.435301274061203 Accuracy 99.47108002047432\n",
      "Training:: Epoch 168, Iteration 60, Current loss 0.475588321685791 Accuracy 99.23095604306646\n",
      "Training:: Epoch 168, Iteration 70, Current loss 0.3811078667640686 Accuracy 99.59256163811116\n",
      "Training:: Epoch 168, Iteration 80, Current loss 0.5100839734077454 Accuracy 99.22031632880375\n",
      "Training:: Epoch 168, Iteration 90, Current loss 0.48488423228263855 Accuracy 99.12877431674424\n",
      "Training:: Epoch 168, Iteration 100, Current loss 0.6144579648971558 Accuracy 99.31008049060942\n",
      "Training:: Epoch 168, Iteration 110, Current loss 0.31713107228279114 Accuracy 99.58366693354684\n",
      "Training:: Epoch 168, Iteration 120, Current loss 0.44807523488998413 Accuracy 99.43164362519201\n",
      "Training:: Epoch 168, Iteration 130, Current loss 0.3270963132381439 Accuracy 99.64051596532036\n",
      "Training:: Epoch 168, Iteration 140, Current loss 0.3549226224422455 Accuracy 99.55960402002485\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 168, Probability Accuracy 67.53687932600867\n",
      "Starting Training\n",
      "Training:: Epoch 169, Iteration 0, Current loss 0.37952950596809387 Accuracy 99.58266313089597\n",
      "Training:: Epoch 169, Iteration 10, Current loss 0.4611970782279968 Accuracy 99.57824004575023\n",
      "Training:: Epoch 169, Iteration 20, Current loss 0.3919231593608856 Accuracy 99.4656881678898\n",
      "Training:: Epoch 169, Iteration 30, Current loss 0.34774166345596313 Accuracy 99.49091511112951\n",
      "Training:: Epoch 169, Iteration 40, Current loss 0.38465529680252075 Accuracy 99.5421974522293\n",
      "Training:: Epoch 169, Iteration 50, Current loss 0.4495030343532562 Accuracy 99.31591587220726\n",
      "Training:: Epoch 169, Iteration 60, Current loss 0.3794213831424713 Accuracy 99.60394028638164\n",
      "Training:: Epoch 169, Iteration 70, Current loss 0.3884193003177643 Accuracy 99.47181574162265\n",
      "Training:: Epoch 169, Iteration 80, Current loss 0.37795931100845337 Accuracy 99.50987784647866\n",
      "Training:: Epoch 169, Iteration 90, Current loss 0.5295109748840332 Accuracy 99.54585842630073\n",
      "Training:: Epoch 169, Iteration 100, Current loss 0.31441524624824524 Accuracy 99.40626909979918\n",
      "Training:: Epoch 169, Iteration 110, Current loss 0.3240812420845032 Accuracy 99.45039325310339\n",
      "Training:: Epoch 169, Iteration 120, Current loss 0.45058465003967285 Accuracy 99.56265769554247\n",
      "Training:: Epoch 169, Iteration 130, Current loss 0.37668800354003906 Accuracy 99.4685970124273\n",
      "Training:: Epoch 169, Iteration 140, Current loss 0.47341859340667725 Accuracy 99.40634275894392\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 169, Probability Accuracy 67.3519107968026\n",
      "Starting Training\n",
      "Training:: Epoch 170, Iteration 0, Current loss 0.31718578934669495 Accuracy 99.48759880076315\n",
      "Training:: Epoch 170, Iteration 10, Current loss 0.2857625484466553 Accuracy 99.63411389036722\n",
      "Training:: Epoch 170, Iteration 20, Current loss 0.301283061504364 Accuracy 99.543191265817\n",
      "Training:: Epoch 170, Iteration 30, Current loss 0.7206323146820068 Accuracy 99.25809822361546\n",
      "Training:: Epoch 170, Iteration 40, Current loss 0.45296087861061096 Accuracy 99.59828357527618\n",
      "Training:: Epoch 170, Iteration 50, Current loss 0.41172176599502563 Accuracy 99.54635667706266\n",
      "Training:: Epoch 170, Iteration 60, Current loss 0.45937639474868774 Accuracy 99.31017287234043\n",
      "Training:: Epoch 170, Iteration 70, Current loss 0.3999021649360657 Accuracy 99.5742021172702\n",
      "Training:: Epoch 170, Iteration 80, Current loss 0.4371670186519623 Accuracy 99.3718363115204\n",
      "Training:: Epoch 170, Iteration 90, Current loss 0.310586154460907 Accuracy 99.50734337237405\n",
      "Training:: Epoch 170, Iteration 100, Current loss 0.3149271309375763 Accuracy 99.42911014507935\n",
      "Training:: Epoch 170, Iteration 110, Current loss 0.4528197944164276 Accuracy 99.42518078991284\n",
      "Training:: Epoch 170, Iteration 120, Current loss 0.4232035279273987 Accuracy 99.41686488145477\n",
      "Training:: Epoch 170, Iteration 130, Current loss 0.36853283643722534 Accuracy 99.53347156398104\n",
      "Training:: Epoch 170, Iteration 140, Current loss 0.3922359347343445 Accuracy 99.72220614618901\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 170, Probability Accuracy 67.15738338159073\n",
      "Starting Training\n",
      "Training:: Epoch 171, Iteration 0, Current loss 0.38690322637557983 Accuracy 99.63239799044234\n",
      "Training:: Epoch 171, Iteration 10, Current loss 0.4188883304595947 Accuracy 99.4417096890383\n",
      "Training:: Epoch 171, Iteration 20, Current loss 0.41113927960395813 Accuracy 99.20598704024825\n",
      "Training:: Epoch 171, Iteration 30, Current loss 0.4343741238117218 Accuracy 99.29587283881763\n",
      "Training:: Epoch 171, Iteration 40, Current loss 0.4218788146972656 Accuracy 99.31019919757865\n",
      "Training:: Epoch 171, Iteration 50, Current loss 0.3614949882030487 Accuracy 99.6261867742896\n",
      "Training:: Epoch 171, Iteration 60, Current loss 0.4274924099445343 Accuracy 99.47753396029258\n",
      "Training:: Epoch 171, Iteration 70, Current loss 0.3641156256198883 Accuracy 99.47936835764382\n",
      "Training:: Epoch 171, Iteration 80, Current loss 0.3896391689777374 Accuracy 99.39997073027952\n",
      "Training:: Epoch 171, Iteration 90, Current loss 0.3657289445400238 Accuracy 99.55829109341057\n",
      "Training:: Epoch 171, Iteration 100, Current loss 0.41068893671035767 Accuracy 99.30760353891525\n",
      "Training:: Epoch 171, Iteration 110, Current loss 0.3408030867576599 Accuracy 99.47301317467063\n",
      "Training:: Epoch 171, Iteration 120, Current loss 0.2663459777832031 Accuracy 99.57494251271689\n",
      "Training:: Epoch 171, Iteration 130, Current loss 0.32909610867500305 Accuracy 99.50601118914415\n",
      "Training:: Epoch 171, Iteration 140, Current loss 0.47255176305770874 Accuracy 99.52444359901084\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 171, Probability Accuracy 66.99782351518637\n",
      "Starting Training\n",
      "Training:: Epoch 172, Iteration 0, Current loss 0.4800867438316345 Accuracy 99.52600723462642\n",
      "Training:: Epoch 172, Iteration 10, Current loss 0.30956101417541504 Accuracy 99.61529583750097\n",
      "Training:: Epoch 172, Iteration 20, Current loss 0.26963090896606445 Accuracy 99.52176223370851\n",
      "Training:: Epoch 172, Iteration 30, Current loss 0.42576298117637634 Accuracy 99.62318208713525\n",
      "Training:: Epoch 172, Iteration 40, Current loss 0.26976755261421204 Accuracy 99.5355626326964\n",
      "Training:: Epoch 172, Iteration 50, Current loss 0.5862654447555542 Accuracy 99.37299395387026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 172, Iteration 60, Current loss 0.419661283493042 Accuracy 99.49400798934754\n",
      "Training:: Epoch 172, Iteration 70, Current loss 0.3330440819263458 Accuracy 99.54406580493537\n",
      "Training:: Epoch 172, Iteration 80, Current loss 0.40505459904670715 Accuracy 99.36547297790095\n",
      "Training:: Epoch 172, Iteration 90, Current loss 0.3562377393245697 Accuracy 99.546790788829\n",
      "Training:: Epoch 172, Iteration 100, Current loss 0.3187128007411957 Accuracy 99.63037828472423\n",
      "Training:: Epoch 172, Iteration 110, Current loss 0.46187859773635864 Accuracy 99.62742980561555\n",
      "Training:: Epoch 172, Iteration 120, Current loss 0.29066911339759827 Accuracy 99.66774254794001\n",
      "Training:: Epoch 172, Iteration 130, Current loss 0.30803382396698 Accuracy 99.55651122161\n",
      "Training:: Epoch 172, Iteration 140, Current loss 0.41404280066490173 Accuracy 99.51733216322948\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 172, Probability Accuracy 66.89708756266381\n",
      "Starting Training\n",
      "Training:: Epoch 173, Iteration 0, Current loss 0.33970409631729126 Accuracy 99.58244817229507\n",
      "Training:: Epoch 173, Iteration 10, Current loss 0.49612879753112793 Accuracy 99.39040551285449\n",
      "Training:: Epoch 173, Iteration 20, Current loss 0.3705207407474518 Accuracy 99.41801385681293\n",
      "Training:: Epoch 173, Iteration 30, Current loss 0.34596720337867737 Accuracy 99.2084432717678\n",
      "Training:: Epoch 173, Iteration 40, Current loss 0.40070921182632446 Accuracy 99.61664063609257\n",
      "Training:: Epoch 173, Iteration 50, Current loss 0.43420302867889404 Accuracy 99.45558541795879\n",
      "Training:: Epoch 173, Iteration 60, Current loss 0.39498817920684814 Accuracy 99.34797072521623\n",
      "Training:: Epoch 173, Iteration 70, Current loss 0.39549723267555237 Accuracy 99.63808155597303\n",
      "Training:: Epoch 173, Iteration 80, Current loss 0.3697439432144165 Accuracy 99.52224359372092\n",
      "Training:: Epoch 173, Iteration 90, Current loss 0.3899010419845581 Accuracy 99.51601338432123\n",
      "Training:: Epoch 173, Iteration 100, Current loss 0.5385686159133911 Accuracy 99.39994606256742\n",
      "Training:: Epoch 173, Iteration 110, Current loss 0.31633350253105164 Accuracy 99.69095713709858\n",
      "Training:: Epoch 173, Iteration 120, Current loss 0.26973822712898254 Accuracy 99.53610964681076\n",
      "Training:: Epoch 173, Iteration 130, Current loss 0.40228110551834106 Accuracy 99.4501848516447\n",
      "Training:: Epoch 173, Iteration 140, Current loss 0.24042664468288422 Accuracy 99.42219304005253\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 173, Probability Accuracy 66.90264315453042\n",
      "Starting Training\n",
      "Training:: Epoch 174, Iteration 0, Current loss 0.3295764625072479 Accuracy 99.5904391029945\n",
      "Training:: Epoch 174, Iteration 10, Current loss 0.4002264142036438 Accuracy 99.27992799279927\n",
      "Training:: Epoch 174, Iteration 20, Current loss 0.4764139950275421 Accuracy 99.39480242079031\n",
      "Training:: Epoch 174, Iteration 30, Current loss 0.2831548750400543 Accuracy 99.45335441608313\n",
      "Training:: Epoch 174, Iteration 40, Current loss 0.38327139616012573 Accuracy 99.57161125319693\n",
      "Training:: Epoch 174, Iteration 50, Current loss 0.27950161695480347 Accuracy 99.72452763552887\n",
      "Training:: Epoch 174, Iteration 60, Current loss 0.29898348450660706 Accuracy 99.55156950672645\n",
      "Training:: Epoch 174, Iteration 70, Current loss 0.26343265175819397 Accuracy 99.56121376923552\n",
      "Training:: Epoch 174, Iteration 80, Current loss 0.5521650314331055 Accuracy 98.94295041193844\n",
      "Training:: Epoch 174, Iteration 90, Current loss 0.3697722852230072 Accuracy 99.60254372019078\n",
      "Training:: Epoch 174, Iteration 100, Current loss 0.3092701733112335 Accuracy 99.5635593220339\n",
      "Training:: Epoch 174, Iteration 110, Current loss 0.25485020875930786 Accuracy 99.56231903575517\n",
      "Training:: Epoch 174, Iteration 120, Current loss 0.3838194012641907 Accuracy 99.51049711737191\n",
      "Training:: Epoch 174, Iteration 130, Current loss 0.24689629673957825 Accuracy 99.69206706095117\n",
      "Training:: Epoch 174, Iteration 140, Current loss 0.32748734951019287 Accuracy 99.64258766530321\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 174, Probability Accuracy 67.15967097824169\n",
      "Starting Training\n",
      "Training:: Epoch 175, Iteration 0, Current loss 0.29867732524871826 Accuracy 99.66221715774051\n",
      "Training:: Epoch 175, Iteration 10, Current loss 0.31197091937065125 Accuracy 99.59075495966013\n",
      "Training:: Epoch 175, Iteration 20, Current loss 0.4827468991279602 Accuracy 99.3389270277142\n",
      "Training:: Epoch 175, Iteration 30, Current loss 0.32011327147483826 Accuracy 99.34897696380025\n",
      "Training:: Epoch 175, Iteration 40, Current loss 0.4973687529563904 Accuracy 99.3670183231538\n",
      "Training:: Epoch 175, Iteration 50, Current loss 0.35967499017715454 Accuracy 99.31430170724882\n",
      "Training:: Epoch 175, Iteration 60, Current loss 0.3703775107860565 Accuracy 99.45238819592333\n",
      "Training:: Epoch 175, Iteration 70, Current loss 0.32362955808639526 Accuracy 99.3542108987969\n",
      "Training:: Epoch 175, Iteration 80, Current loss 0.2487049698829651 Accuracy 99.68577782074833\n",
      "Training:: Epoch 175, Iteration 90, Current loss 0.41078054904937744 Accuracy 99.45715393563397\n",
      "Training:: Epoch 175, Iteration 100, Current loss 0.4530212879180908 Accuracy 99.3711162686232\n",
      "Training:: Epoch 175, Iteration 110, Current loss 0.34424108266830444 Accuracy 99.44692412187075\n",
      "Training:: Epoch 175, Iteration 120, Current loss 0.28292983770370483 Accuracy 99.68582375478927\n",
      "Training:: Epoch 175, Iteration 130, Current loss 0.397134393453598 Accuracy 99.61845562967456\n",
      "Training:: Epoch 175, Iteration 140, Current loss 0.3297654986381531 Accuracy 99.44674965421854\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 175, Probability Accuracy 66.27584167216779\n",
      "Starting Training\n",
      "Training:: Epoch 176, Iteration 0, Current loss 0.4469987154006958 Accuracy 99.46752793296089\n",
      "Training:: Epoch 176, Iteration 10, Current loss 0.32080206274986267 Accuracy 99.41425477456193\n",
      "Training:: Epoch 176, Iteration 20, Current loss 0.42548322677612305 Accuracy 99.42163100057837\n",
      "Training:: Epoch 176, Iteration 30, Current loss 0.24317634105682373 Accuracy 99.57262455359756\n",
      "Training:: Epoch 176, Iteration 40, Current loss 0.29327014088630676 Accuracy 99.65940372323053\n",
      "Training:: Epoch 176, Iteration 50, Current loss 0.2692418396472931 Accuracy 99.63179758308156\n",
      "Training:: Epoch 176, Iteration 60, Current loss 0.5380294322967529 Accuracy 99.23521731020332\n",
      "Training:: Epoch 176, Iteration 70, Current loss 0.3710392713546753 Accuracy 99.52315354455865\n",
      "Training:: Epoch 176, Iteration 80, Current loss 0.25555193424224854 Accuracy 99.61485445552583\n",
      "Training:: Epoch 176, Iteration 90, Current loss 0.358442485332489 Accuracy 99.6138628021166\n",
      "Training:: Epoch 176, Iteration 100, Current loss 0.41922539472579956 Accuracy 99.15023423030831\n",
      "Training:: Epoch 176, Iteration 110, Current loss 0.4157508313655853 Accuracy 99.51335083562785\n",
      "Training:: Epoch 176, Iteration 120, Current loss 0.37428194284439087 Accuracy 99.47093946035825\n",
      "Training:: Epoch 176, Iteration 130, Current loss 0.2600051462650299 Accuracy 99.4326141776408\n",
      "Training:: Epoch 176, Iteration 140, Current loss 0.3126191198825836 Accuracy 99.5620056220174\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 176, Probability Accuracy 67.12609232740083\n",
      "Starting Training\n",
      "Training:: Epoch 177, Iteration 0, Current loss 0.3877452313899994 Accuracy 99.56375974471867\n",
      "Training:: Epoch 177, Iteration 10, Current loss 0.3275565803050995 Accuracy 99.51391701869105\n",
      "Training:: Epoch 177, Iteration 20, Current loss 0.29744991660118103 Accuracy 99.49160236041762\n",
      "Training:: Epoch 177, Iteration 30, Current loss 0.35551947355270386 Accuracy 99.2436433859028\n",
      "Training:: Epoch 177, Iteration 40, Current loss 0.4928045868873596 Accuracy 99.43611352088239\n",
      "Training:: Epoch 177, Iteration 50, Current loss 0.3687690794467926 Accuracy 99.48411958480949\n",
      "Training:: Epoch 177, Iteration 60, Current loss 0.45875558257102966 Accuracy 99.29957515214146\n",
      "Training:: Epoch 177, Iteration 70, Current loss 0.4167460799217224 Accuracy 99.38611876722626\n",
      "Training:: Epoch 177, Iteration 80, Current loss 0.3247707784175873 Accuracy 99.52197378565921\n",
      "Training:: Epoch 177, Iteration 90, Current loss 0.3776925802230835 Accuracy 99.44136407220577\n",
      "Training:: Epoch 177, Iteration 100, Current loss 0.30145546793937683 Accuracy 99.42653974079596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 177, Iteration 110, Current loss 0.3301791548728943 Accuracy 99.65766068132237\n",
      "Training:: Epoch 177, Iteration 120, Current loss 0.2765507996082306 Accuracy 99.6851200937317\n",
      "Training:: Epoch 177, Iteration 130, Current loss 0.2663963735103607 Accuracy 99.57271364317842\n",
      "Training:: Epoch 177, Iteration 140, Current loss 0.27813518047332764 Accuracy 99.53995934524447\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 177, Probability Accuracy 65.98164040287845\n",
      "Starting Training\n",
      "Training:: Epoch 178, Iteration 0, Current loss 0.3862082362174988 Accuracy 99.39707149009475\n",
      "Training:: Epoch 178, Iteration 10, Current loss 0.2520947754383087 Accuracy 99.65519042892221\n",
      "Training:: Epoch 178, Iteration 20, Current loss 0.477072149515152 Accuracy 99.11198815984213\n",
      "Training:: Epoch 178, Iteration 30, Current loss 0.3405613303184509 Accuracy 99.39313778884308\n",
      "Training:: Epoch 178, Iteration 40, Current loss 0.2843129634857178 Accuracy 99.53175992054108\n",
      "Training:: Epoch 178, Iteration 50, Current loss 0.22524672746658325 Accuracy 99.72174071708785\n",
      "Training:: Epoch 178, Iteration 60, Current loss 0.3216930031776428 Accuracy 99.17825104430597\n",
      "Training:: Epoch 178, Iteration 70, Current loss 0.5107192993164062 Accuracy 99.28866832092639\n",
      "Training:: Epoch 178, Iteration 80, Current loss 0.3534952700138092 Accuracy 99.26585094549499\n",
      "Training:: Epoch 178, Iteration 90, Current loss 0.3310256600379944 Accuracy 99.33218658302135\n",
      "Training:: Epoch 178, Iteration 100, Current loss 0.3897377550601959 Accuracy 99.2057889163431\n",
      "Training:: Epoch 178, Iteration 110, Current loss 0.6927489638328552 Accuracy 98.95805503606732\n",
      "Training:: Epoch 178, Iteration 120, Current loss 0.3385804295539856 Accuracy 99.51372379511562\n",
      "Training:: Epoch 178, Iteration 130, Current loss 0.267597496509552 Accuracy 99.64480372343684\n",
      "Training:: Epoch 178, Iteration 140, Current loss 0.3932550549507141 Accuracy 99.21315160882395\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 178, Probability Accuracy 66.06105268661886\n",
      "Starting Training\n",
      "Training:: Epoch 179, Iteration 0, Current loss 0.38841986656188965 Accuracy 99.46176567115195\n",
      "Training:: Epoch 179, Iteration 10, Current loss 0.41489851474761963 Accuracy 99.16465157177402\n",
      "Training:: Epoch 179, Iteration 20, Current loss 0.28253841400146484 Accuracy 99.56898256158804\n",
      "Training:: Epoch 179, Iteration 30, Current loss 0.4113645851612091 Accuracy 99.48278834549738\n",
      "Training:: Epoch 179, Iteration 40, Current loss 0.3470209836959839 Accuracy 99.6386292834891\n",
      "Training:: Epoch 179, Iteration 50, Current loss 0.3410307765007019 Accuracy 99.37331615321541\n",
      "Training:: Epoch 179, Iteration 60, Current loss 0.32814353704452515 Accuracy 99.45062491416014\n",
      "Training:: Epoch 179, Iteration 70, Current loss 0.3642074167728424 Accuracy 98.99300699300699\n",
      "Training:: Epoch 179, Iteration 80, Current loss 0.30045509338378906 Accuracy 99.36028033575096\n",
      "Training:: Epoch 179, Iteration 90, Current loss 0.349457323551178 Accuracy 99.6150682678072\n",
      "Training:: Epoch 179, Iteration 100, Current loss 0.3358730673789978 Accuracy 99.51360115294541\n",
      "Training:: Epoch 179, Iteration 110, Current loss 0.3702809810638428 Accuracy 99.37293506843773\n",
      "Training:: Epoch 179, Iteration 120, Current loss 0.43500956892967224 Accuracy 99.2888834862943\n",
      "Training:: Epoch 179, Iteration 130, Current loss 0.32256123423576355 Accuracy 99.34052757793765\n",
      "Training:: Epoch 179, Iteration 140, Current loss 0.2356157898902893 Accuracy 99.59423150737511\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 179, Probability Accuracy 66.82821456349387\n",
      "Starting Training\n",
      "Training:: Epoch 180, Iteration 0, Current loss 0.3048560917377472 Accuracy 99.12299239222317\n",
      "Training:: Epoch 180, Iteration 10, Current loss 0.3060690462589264 Accuracy 99.51964050515224\n",
      "Training:: Epoch 180, Iteration 20, Current loss 0.4161379039287567 Accuracy 99.37852290793467\n",
      "Training:: Epoch 180, Iteration 30, Current loss 0.29136431217193604 Accuracy 99.5292633213627\n",
      "Training:: Epoch 180, Iteration 40, Current loss 0.34231942892074585 Accuracy 99.57220315375814\n",
      "Training:: Epoch 180, Iteration 50, Current loss 0.3914848566055298 Accuracy 99.38069403214249\n",
      "Training:: Epoch 180, Iteration 60, Current loss 0.3508301377296448 Accuracy 99.3803822726318\n",
      "Training:: Epoch 180, Iteration 70, Current loss 0.24991711974143982 Accuracy 99.42168674698796\n",
      "Training:: Epoch 180, Iteration 80, Current loss 0.3198026418685913 Accuracy 99.39903846153847\n",
      "Training:: Epoch 180, Iteration 90, Current loss 0.4389684200286865 Accuracy 99.26948864204944\n",
      "Training:: Epoch 180, Iteration 100, Current loss 0.3548443913459778 Accuracy 99.32198393396713\n",
      "Training:: Epoch 180, Iteration 110, Current loss 0.35391777753829956 Accuracy 98.96059291395517\n",
      "Training:: Epoch 180, Iteration 120, Current loss 0.4245874285697937 Accuracy 99.4096697077621\n",
      "Training:: Epoch 180, Iteration 130, Current loss 0.3227007985115051 Accuracy 99.57964373083416\n",
      "Training:: Epoch 180, Iteration 140, Current loss 0.40436291694641113 Accuracy 99.24490668186351\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 180, Probability Accuracy 67.26293962705638\n",
      "Starting Training\n",
      "Training:: Epoch 181, Iteration 0, Current loss 0.37997767329216003 Accuracy 99.50736626853185\n",
      "Training:: Epoch 181, Iteration 10, Current loss 0.343558669090271 Accuracy 99.44578655677135\n",
      "Training:: Epoch 181, Iteration 20, Current loss 0.27575060725212097 Accuracy 99.25153477419897\n",
      "Training:: Epoch 181, Iteration 30, Current loss 0.4689314365386963 Accuracy 99.40465767816494\n",
      "Training:: Epoch 181, Iteration 40, Current loss 0.3294502794742584 Accuracy 99.57357227680018\n",
      "Training:: Epoch 181, Iteration 50, Current loss 0.3192936182022095 Accuracy 99.54903041539309\n",
      "Training:: Epoch 181, Iteration 60, Current loss 0.3719801604747772 Accuracy 99.19354838709677\n",
      "Training:: Epoch 181, Iteration 70, Current loss 0.2929762303829193 Accuracy 99.5089576859631\n",
      "Training:: Epoch 181, Iteration 80, Current loss 0.31015223264694214 Accuracy 99.4129712143716\n",
      "Training:: Epoch 181, Iteration 90, Current loss 0.39534294605255127 Accuracy 99.40553467714383\n",
      "Training:: Epoch 181, Iteration 100, Current loss 0.2609269618988037 Accuracy 99.4873269129414\n",
      "Training:: Epoch 181, Iteration 110, Current loss 0.3047143816947937 Accuracy 99.49014744384732\n",
      "Training:: Epoch 181, Iteration 120, Current loss 0.3652263581752777 Accuracy 98.75045603794236\n",
      "Training:: Epoch 181, Iteration 130, Current loss 0.34909725189208984 Accuracy 99.56764666755355\n",
      "Training:: Epoch 181, Iteration 140, Current loss 0.31239885091781616 Accuracy 99.52391496899911\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 181, Probability Accuracy 67.0032974071726\n",
      "Starting Training\n",
      "Training:: Epoch 182, Iteration 0, Current loss 0.43702226877212524 Accuracy 99.40985541457657\n",
      "Training:: Epoch 182, Iteration 10, Current loss 0.326992005109787 Accuracy 99.44342115800433\n",
      "Training:: Epoch 182, Iteration 20, Current loss 0.2913327217102051 Accuracy 99.42949407965554\n",
      "Training:: Epoch 182, Iteration 30, Current loss 0.41192948818206787 Accuracy 99.45026178010471\n",
      "Training:: Epoch 182, Iteration 40, Current loss 0.43907445669174194 Accuracy 99.40878378378379\n",
      "Training:: Epoch 182, Iteration 50, Current loss 0.5324268937110901 Accuracy 98.20465512347431\n",
      "Training:: Epoch 182, Iteration 60, Current loss 0.48518288135528564 Accuracy 97.85763648928818\n",
      "Training:: Epoch 182, Iteration 70, Current loss 0.9328773021697998 Accuracy 94.08383784198023\n",
      "Training:: Epoch 182, Iteration 80, Current loss 1.3269766569137573 Accuracy 94.12887348701723\n",
      "Training:: Epoch 182, Iteration 90, Current loss 1.3668400049209595 Accuracy 91.09386548410939\n",
      "Training:: Epoch 182, Iteration 100, Current loss 6.482337474822998 Accuracy 61.60967597431686\n",
      "Training:: Epoch 182, Iteration 110, Current loss 4.493381023406982 Accuracy 79.18710581639803\n",
      "Training:: Epoch 182, Iteration 120, Current loss 5.134130477905273 Accuracy 68.53037908949281\n",
      "Training:: Epoch 182, Iteration 130, Current loss 3.2044715881347656 Accuracy 81.60339404579035\n",
      "Training:: Epoch 182, Iteration 140, Current loss 1.9530410766601562 Accuracy 91.54709105139577\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 182, Probability Accuracy 51.991843083941724\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 183, Iteration 0, Current loss 3.1072120666503906 Accuracy 79.47403841176165\n",
      "Training:: Epoch 183, Iteration 10, Current loss 2.592122793197632 Accuracy 82.44830319187261\n",
      "Training:: Epoch 183, Iteration 20, Current loss 2.6747169494628906 Accuracy 86.62970662970663\n",
      "Training:: Epoch 183, Iteration 30, Current loss 3.372126340866089 Accuracy 84.2355131487084\n",
      "Training:: Epoch 183, Iteration 40, Current loss 2.024667978286743 Accuracy 92.6005481075476\n",
      "Training:: Epoch 183, Iteration 50, Current loss 2.756181240081787 Accuracy 84.94514277410448\n",
      "Training:: Epoch 183, Iteration 60, Current loss 1.4788154363632202 Accuracy 94.33811802232854\n",
      "Training:: Epoch 183, Iteration 70, Current loss 2.1763157844543457 Accuracy 90.51781613452849\n",
      "Training:: Epoch 183, Iteration 80, Current loss 2.892791986465454 Accuracy 92.10095669687814\n",
      "Training:: Epoch 183, Iteration 90, Current loss 3.6443355083465576 Accuracy 84.0412653297542\n",
      "Training:: Epoch 183, Iteration 100, Current loss 1.2036106586456299 Accuracy 95.30196579376907\n",
      "Training:: Epoch 183, Iteration 110, Current loss 2.3546652793884277 Accuracy 88.93515305729046\n",
      "Training:: Epoch 183, Iteration 120, Current loss 2.1110541820526123 Accuracy 88.43300616297648\n",
      "Training:: Epoch 183, Iteration 130, Current loss 2.3982348442077637 Accuracy 54.913534223879054\n",
      "Training:: Epoch 183, Iteration 140, Current loss 0.9333933591842651 Accuracy 96.55205999805202\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 183, Probability Accuracy 66.62755965725266\n",
      "Starting Training\n",
      "Training:: Epoch 184, Iteration 0, Current loss 0.7776325345039368 Accuracy 98.07168996903444\n",
      "Training:: Epoch 184, Iteration 10, Current loss 0.8524903655052185 Accuracy 97.03265866618902\n",
      "Training:: Epoch 184, Iteration 20, Current loss 0.7652499079704285 Accuracy 98.24885379521142\n",
      "Training:: Epoch 184, Iteration 30, Current loss 0.8606011271476746 Accuracy 97.81498074353533\n",
      "Training:: Epoch 184, Iteration 40, Current loss 0.9258183836936951 Accuracy 98.03837953091684\n",
      "Training:: Epoch 184, Iteration 50, Current loss 0.7085345387458801 Accuracy 98.62272727272727\n",
      "Training:: Epoch 184, Iteration 60, Current loss 0.7759019136428833 Accuracy 98.5962392269522\n",
      "Training:: Epoch 184, Iteration 70, Current loss 0.6917036771774292 Accuracy 98.43443128892879\n",
      "Training:: Epoch 184, Iteration 80, Current loss 0.5696115493774414 Accuracy 98.62157314885985\n",
      "Training:: Epoch 184, Iteration 90, Current loss 0.6041012406349182 Accuracy 98.61326866972303\n",
      "Training:: Epoch 184, Iteration 100, Current loss 0.5525946021080017 Accuracy 98.991041961339\n",
      "Training:: Epoch 184, Iteration 110, Current loss 0.6474276185035706 Accuracy 98.4097164576871\n",
      "Training:: Epoch 184, Iteration 120, Current loss 0.537225067615509 Accuracy 98.91629545907851\n",
      "Training:: Epoch 184, Iteration 130, Current loss 1.0971087217330933 Accuracy 98.15085158150852\n",
      "Training:: Epoch 184, Iteration 140, Current loss 0.5204769968986511 Accuracy 99.25337569499602\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 184, Probability Accuracy 67.60501702625507\n",
      "Starting Training\n",
      "Training:: Epoch 185, Iteration 0, Current loss 0.6569899320602417 Accuracy 98.92428630533719\n",
      "Training:: Epoch 185, Iteration 10, Current loss 0.6168433427810669 Accuracy 99.01666343731834\n",
      "Training:: Epoch 185, Iteration 20, Current loss 0.4452274739742279 Accuracy 99.13170100312803\n",
      "Training:: Epoch 185, Iteration 30, Current loss 0.6379841566085815 Accuracy 99.31137890820237\n",
      "Training:: Epoch 185, Iteration 40, Current loss 0.4435074031352997 Accuracy 99.12946250299497\n",
      "Training:: Epoch 185, Iteration 50, Current loss 0.4994567036628723 Accuracy 99.08724832214764\n",
      "Training:: Epoch 185, Iteration 60, Current loss 0.6952693462371826 Accuracy 98.79130627633621\n",
      "Training:: Epoch 185, Iteration 70, Current loss 0.49123209714889526 Accuracy 98.93233856444559\n",
      "Training:: Epoch 185, Iteration 80, Current loss 0.42878618836402893 Accuracy 99.21598157335224\n",
      "Training:: Epoch 185, Iteration 90, Current loss 1.015525460243225 Accuracy 98.12477019242554\n",
      "Training:: Epoch 185, Iteration 100, Current loss 0.6519632339477539 Accuracy 98.35074779758246\n",
      "Training:: Epoch 185, Iteration 110, Current loss 0.5681324601173401 Accuracy 98.63084032175253\n",
      "Training:: Epoch 185, Iteration 120, Current loss 0.48047396540641785 Accuracy 99.26447243094508\n",
      "Training:: Epoch 185, Iteration 130, Current loss 0.6998504996299744 Accuracy 98.8825694212111\n",
      "Training:: Epoch 185, Iteration 140, Current loss 0.4307461380958557 Accuracy 99.1127398977722\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 185, Probability Accuracy 67.17437695671214\n",
      "Starting Training\n",
      "Training:: Epoch 186, Iteration 0, Current loss 0.3741179406642914 Accuracy 99.04163493272404\n",
      "Training:: Epoch 186, Iteration 10, Current loss 0.5982251763343811 Accuracy 99.40807999540256\n",
      "Training:: Epoch 186, Iteration 20, Current loss 0.5191841721534729 Accuracy 99.28368458441867\n",
      "Training:: Epoch 186, Iteration 30, Current loss 0.4525411128997803 Accuracy 99.12478119529882\n",
      "Training:: Epoch 186, Iteration 40, Current loss 0.41074278950691223 Accuracy 99.28212851405623\n",
      "Training:: Epoch 186, Iteration 50, Current loss 0.42277929186820984 Accuracy 99.37300221293337\n",
      "Training:: Epoch 186, Iteration 60, Current loss 0.3915826678276062 Accuracy 99.36910278599743\n",
      "Training:: Epoch 186, Iteration 70, Current loss 0.42270511388778687 Accuracy 99.33319332143232\n",
      "Training:: Epoch 186, Iteration 80, Current loss 0.37878894805908203 Accuracy 99.33916569669208\n",
      "Training:: Epoch 186, Iteration 90, Current loss 0.5535010695457458 Accuracy 99.45630285944422\n",
      "Training:: Epoch 186, Iteration 100, Current loss 0.36706095933914185 Accuracy 99.5723068698209\n",
      "Training:: Epoch 186, Iteration 110, Current loss 0.5746135711669922 Accuracy 99.18156635608159\n",
      "Training:: Epoch 186, Iteration 120, Current loss 0.3683796226978302 Accuracy 99.4280057199428\n",
      "Training:: Epoch 186, Iteration 130, Current loss 0.5491153597831726 Accuracy 98.7047961015645\n",
      "Training:: Epoch 186, Iteration 140, Current loss 0.6319000720977783 Accuracy 99.16451994708626\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 186, Probability Accuracy 67.28026000169936\n",
      "Starting Training\n",
      "Training:: Epoch 187, Iteration 0, Current loss 0.6108183860778809 Accuracy 98.9585305813293\n",
      "Training:: Epoch 187, Iteration 10, Current loss 0.5429690480232239 Accuracy 99.37264742785446\n",
      "Training:: Epoch 187, Iteration 20, Current loss 0.3552800416946411 Accuracy 99.45884657011803\n",
      "Training:: Epoch 187, Iteration 30, Current loss 0.464820921421051 Accuracy 99.35191186001296\n",
      "Training:: Epoch 187, Iteration 40, Current loss 0.3739169239997864 Accuracy 99.28859260733297\n",
      "Training:: Epoch 187, Iteration 50, Current loss 0.45096826553344727 Accuracy 99.35418082936778\n",
      "Training:: Epoch 187, Iteration 60, Current loss 0.5822365283966064 Accuracy 99.38783770110291\n",
      "Training:: Epoch 187, Iteration 70, Current loss 0.39725229144096375 Accuracy 99.54311046774632\n",
      "Training:: Epoch 187, Iteration 80, Current loss 0.4022081196308136 Accuracy 99.39297367023295\n",
      "Training:: Epoch 187, Iteration 90, Current loss 0.5039569735527039 Accuracy 99.19171507956554\n",
      "Training:: Epoch 187, Iteration 100, Current loss 0.5439330339431763 Accuracy 99.26329063516295\n",
      "Training:: Epoch 187, Iteration 110, Current loss 0.44757866859436035 Accuracy 99.5358514724712\n",
      "Training:: Epoch 187, Iteration 120, Current loss 0.38491585850715637 Accuracy 99.4301256313949\n",
      "Training:: Epoch 187, Iteration 130, Current loss 0.40702295303344727 Accuracy 99.36104695919938\n",
      "Training:: Epoch 187, Iteration 140, Current loss 0.4467671513557434 Accuracy 99.2442748091603\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 187, Probability Accuracy 67.53924862254001\n",
      "Starting Training\n",
      "Training:: Epoch 188, Iteration 0, Current loss 0.4410403370857239 Accuracy 99.58459979736575\n",
      "Training:: Epoch 188, Iteration 10, Current loss 0.39525893330574036 Accuracy 99.33479212253829\n",
      "Training:: Epoch 188, Iteration 20, Current loss 0.4875895082950592 Accuracy 99.28918546456808\n",
      "Training:: Epoch 188, Iteration 30, Current loss 0.6565484404563904 Accuracy 98.91116312201079\n",
      "Training:: Epoch 188, Iteration 40, Current loss 0.48366183042526245 Accuracy 99.41460559051662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 188, Iteration 50, Current loss 0.42013514041900635 Accuracy 99.57258712647037\n",
      "Training:: Epoch 188, Iteration 60, Current loss 0.3310494124889374 Accuracy 99.55842707074463\n",
      "Training:: Epoch 188, Iteration 70, Current loss 0.36552050709724426 Accuracy 99.42963632256607\n",
      "Training:: Epoch 188, Iteration 80, Current loss 0.2841032147407532 Accuracy 99.62368058742543\n",
      "Training:: Epoch 188, Iteration 90, Current loss 0.3116050660610199 Accuracy 99.45326144733845\n",
      "Training:: Epoch 188, Iteration 100, Current loss 0.5254781246185303 Accuracy 99.29599499374218\n",
      "Training:: Epoch 188, Iteration 110, Current loss 0.45340976119041443 Accuracy 99.43055480232117\n",
      "Training:: Epoch 188, Iteration 120, Current loss 0.2835119664669037 Accuracy 99.57532002669417\n",
      "Training:: Epoch 188, Iteration 130, Current loss 0.4227016568183899 Accuracy 99.17655057792551\n",
      "Training:: Epoch 188, Iteration 140, Current loss 0.49844810366630554 Accuracy 99.30951666166317\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 188, Probability Accuracy 66.90558435022452\n",
      "Starting Training\n",
      "Training:: Epoch 189, Iteration 0, Current loss 0.2615446150302887 Accuracy 99.59175200664268\n",
      "Training:: Epoch 189, Iteration 10, Current loss 0.5557385087013245 Accuracy 99.40912927731965\n",
      "Training:: Epoch 189, Iteration 20, Current loss 0.38839855790138245 Accuracy 99.57849319615002\n",
      "Training:: Epoch 189, Iteration 30, Current loss 0.46183767914772034 Accuracy 99.5397332331392\n",
      "Training:: Epoch 189, Iteration 40, Current loss 0.3310551643371582 Accuracy 99.45919370698132\n",
      "Training:: Epoch 189, Iteration 50, Current loss 0.3614426553249359 Accuracy 99.45633850303439\n",
      "Training:: Epoch 189, Iteration 60, Current loss 0.4258614778518677 Accuracy 99.4936537942209\n",
      "Training:: Epoch 189, Iteration 70, Current loss 0.3519149422645569 Accuracy 99.55853250114173\n",
      "Training:: Epoch 189, Iteration 80, Current loss 0.4586622714996338 Accuracy 99.49487928078224\n",
      "Training:: Epoch 189, Iteration 90, Current loss 0.6440308690071106 Accuracy 99.51589139128605\n",
      "Training:: Epoch 189, Iteration 100, Current loss 0.5111846327781677 Accuracy 99.41810956571103\n",
      "Training:: Epoch 189, Iteration 110, Current loss 0.553909182548523 Accuracy 99.16344243438252\n",
      "Training:: Epoch 189, Iteration 120, Current loss 0.43405258655548096 Accuracy 99.31878214931183\n",
      "Training:: Epoch 189, Iteration 130, Current loss 0.3933200538158417 Accuracy 99.39124173594293\n",
      "Training:: Epoch 189, Iteration 140, Current loss 0.39890098571777344 Accuracy 99.55148109813199\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 189, Probability Accuracy 67.0477421421055\n",
      "Starting Training\n",
      "Training:: Epoch 190, Iteration 0, Current loss 0.4139641225337982 Accuracy 99.33606717438\n",
      "Training:: Epoch 190, Iteration 10, Current loss 0.4485400915145874 Accuracy 99.45465411624085\n",
      "Training:: Epoch 190, Iteration 20, Current loss 0.5125516653060913 Accuracy 99.53226823628337\n",
      "Training:: Epoch 190, Iteration 30, Current loss 0.35142141580581665 Accuracy 99.52560836334885\n",
      "Training:: Epoch 190, Iteration 40, Current loss 0.32467007637023926 Accuracy 99.60849883773092\n",
      "Training:: Epoch 190, Iteration 50, Current loss 0.3794640302658081 Accuracy 99.53101071975497\n",
      "Training:: Epoch 190, Iteration 60, Current loss 0.35331007838249207 Accuracy 99.66862873337693\n",
      "Training:: Epoch 190, Iteration 70, Current loss 0.27991026639938354 Accuracy 99.45590729225552\n",
      "Training:: Epoch 190, Iteration 80, Current loss 0.5468804240226746 Accuracy 99.28648469260764\n",
      "Training:: Epoch 190, Iteration 90, Current loss 0.32595568895339966 Accuracy 99.48878989264588\n",
      "Training:: Epoch 190, Iteration 100, Current loss 0.4134783446788788 Accuracy 99.45105922375136\n",
      "Training:: Epoch 190, Iteration 110, Current loss 0.33079248666763306 Accuracy 99.46822513557626\n",
      "Training:: Epoch 190, Iteration 120, Current loss 0.4045432209968567 Accuracy 99.50452759268751\n",
      "Training:: Epoch 190, Iteration 130, Current loss 0.7200798988342285 Accuracy 99.210686095932\n",
      "Training:: Epoch 190, Iteration 140, Current loss 0.2624521851539612 Accuracy 99.43578089491419\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 190, Probability Accuracy 67.06040562356617\n",
      "Starting Training\n",
      "Training:: Epoch 191, Iteration 0, Current loss 0.29320162534713745 Accuracy 99.66570906241839\n",
      "Training:: Epoch 191, Iteration 10, Current loss 0.3155568242073059 Accuracy 99.40323078506019\n",
      "Training:: Epoch 191, Iteration 20, Current loss 0.39976072311401367 Accuracy 99.46735237395536\n",
      "Training:: Epoch 191, Iteration 30, Current loss 0.3618152141571045 Accuracy 99.3677555321391\n",
      "Training:: Epoch 191, Iteration 40, Current loss 0.36407944560050964 Accuracy 99.60345898428169\n",
      "Training:: Epoch 191, Iteration 50, Current loss 0.30380213260650635 Accuracy 99.54320517700799\n",
      "Training:: Epoch 191, Iteration 60, Current loss 0.4864192306995392 Accuracy 99.55591807141562\n",
      "Training:: Epoch 191, Iteration 70, Current loss 0.3063180148601532 Accuracy 99.58272367636698\n",
      "Training:: Epoch 191, Iteration 80, Current loss 0.28641974925994873 Accuracy 99.6672139148955\n",
      "Training:: Epoch 191, Iteration 90, Current loss 0.354502409696579 Accuracy 99.53116987766464\n",
      "Training:: Epoch 191, Iteration 100, Current loss 0.3464360535144806 Accuracy 99.34175174637292\n",
      "Training:: Epoch 191, Iteration 110, Current loss 0.3037354350090027 Accuracy 99.67103648058134\n",
      "Training:: Epoch 191, Iteration 120, Current loss 0.41416656970977783 Accuracy 99.68366688185927\n",
      "Training:: Epoch 191, Iteration 130, Current loss 0.39374303817749023 Accuracy 99.47174794830677\n",
      "Training:: Epoch 191, Iteration 140, Current loss 0.32094067335128784 Accuracy 99.65087911163455\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 191, Probability Accuracy 66.7408773913555\n",
      "Starting Training\n",
      "Training:: Epoch 192, Iteration 0, Current loss 0.43622347712516785 Accuracy 99.46705800571561\n",
      "Training:: Epoch 192, Iteration 10, Current loss 0.5495846271514893 Accuracy 99.30081300813008\n",
      "Training:: Epoch 192, Iteration 20, Current loss 0.32138949632644653 Accuracy 99.6145544249152\n",
      "Training:: Epoch 192, Iteration 30, Current loss 0.43947914242744446 Accuracy 99.58818849735599\n",
      "Training:: Epoch 192, Iteration 40, Current loss 0.419560045003891 Accuracy 99.50563746747615\n",
      "Training:: Epoch 192, Iteration 50, Current loss 0.408589243888855 Accuracy 99.38038142753682\n",
      "Training:: Epoch 192, Iteration 60, Current loss 0.3377627432346344 Accuracy 99.43690575094618\n",
      "Training:: Epoch 192, Iteration 70, Current loss 0.41934067010879517 Accuracy 99.5008\n",
      "Training:: Epoch 192, Iteration 80, Current loss 0.2612314224243164 Accuracy 99.6214230041198\n",
      "Training:: Epoch 192, Iteration 90, Current loss 0.3832855820655823 Accuracy 99.36439035733665\n",
      "Training:: Epoch 192, Iteration 100, Current loss 0.36637991666793823 Accuracy 99.37849036209693\n",
      "Training:: Epoch 192, Iteration 110, Current loss 0.4633201062679291 Accuracy 99.13535439795046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-76d53e4fb509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initialize_epoch = 15\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0\n",
    "for epoch in range(1000):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        \n",
    "        loss = 0\n",
    "        for p in predictions:\n",
    "            loss += ce_criterion(p, item_2)\n",
    "            loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(p[:, :, 1:], dim=1), \n",
    "                                                                F.log_softmax(p.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                        max=16) * src_mask_mse[:, :, 1:])\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(predictions[-1], dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "    # Calculating Expectation Step\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Calculating Validation Data Accuracy\")\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, item in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            item_0 = item[0].to(device)\n",
    "            item_1 = item[1].to(device)\n",
    "            item_2 = item[2].to(device)\n",
    "            src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "            src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "            middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "            pred = torch.argmax(predictions[-1], dim=1)\n",
    "            correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "            total += float(torch.sum(src_mask).item())\n",
    "    val_acc = correct * 100.0 / total\n",
    "    if val_acc > best_val_acc:\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-best-model.wt\")\n",
    "    torch.save(model.state_dict(), config.output_dir + \"ms-tcn-last-model.wt\")\n",
    "    print(f\"Validation:: Epoch {epoch}, Probability Accuracy {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\n",
    "\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-speed/final-em-maximized.wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 57.770101729343025\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "\n",
    "print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_labels(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            select_item = np.random.randint(start, i, 1)[0]\n",
    "            unique_ids.append(select_item)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    select_item = np.random.randint(start, len(labels_arr), 1)[0]\n",
    "    unique_ids.append(select_item)\n",
    "    return unique_ids\n",
    "# get_selected_labels(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            unique_ids.append(i - 1)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    unique_ids.append(len(labels_arr) - 1)\n",
    "    return unique_ids\n",
    "# get_boundary(np.array([2, 2, 2, 2, 3, 3, 4, 4, 4, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "boundary_dict = {}\n",
    "for file in glob.glob(\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/*txt\"):\n",
    "    video_id = file.split(\"/\")[-1].split(\".txt\")[0]\n",
    "    data = open(file).read().split(\"\\n\")[0:-1]\n",
    "    data = np.array(data)\n",
    "    boundary = get_boundary(data)\n",
    "    boundary_dict[video_id] = boundary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[5]\n",
    "    for i, count in enumerate(count_all):\n",
    "        video_id = video_ids[i]\n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_selected_labels(labels)\n",
    "\n",
    "        loaded_vidid_selected_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_boundary_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[4]\n",
    "    for i, count in enumerate(count_all):\n",
    "        \n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_boundary(labels)\n",
    "        video_id = video_ids[i]\n",
    "        video_id_boundary_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in video_id_boundary_frames.keys():\n",
    "    if len(video_id_boundary_frames[ele]) != len(loaded_vidid_selected_frames[ele + \".txt\"]):\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(video_id_boundary_frames, open(\"dump_dir/video_id_boundary_frames_dict.pkl\", \"wb\"))\n",
    "# pickle.dump(loaded_vidid_selected_frames, open(\"dump_dir/loaded_vidid_selected_frames_dict.pkl\", \"wb\"))\n",
    "pickle.dump(boundary_dict, open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "    \n",
    "    weights = [1, 1, 1, 1, 0, 0]\n",
    "    ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "    for i, outp_ele in enumerate(outp[1]):\n",
    "        upped_logit = F.upsample(outp_ele, size=outp[0].shape[-1], mode='linear', align_corners=True)\n",
    "        ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i + 1] / sum(weights)\n",
    "    \n",
    "    return ensemble_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/results/c2f-tcn-model/split2_c2ftcn_model.wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration done\n",
      "11 iteration done\n",
      "21 iteration done\n",
      "31 iteration done\n",
      "41 iteration done\n",
      "51 iteration done\n",
      "61 iteration done\n",
      "71 iteration done\n",
      "81 iteration done\n",
      "91 iteration done\n",
      "101 iteration done\n",
      "111 iteration done\n",
      "121 iteration done\n",
      "131 iteration done\n",
      "141 iteration done\n",
      "151 iteration done\n",
      "Train Boundary avergage error = 107.269\n",
      "Train From boundary avergage accuracy = 87.407\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        calculate_element_probb(prob, item_1, item[4], item_2)\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f'{i+1} iteration done')\n",
    "get_boundary_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    idx = 4\n",
    "\n",
    "    cur_vid_feat = torch.softmax(predictions[-1], dim=1).permute(0, 2, 1)[idx]\n",
    "    cur_vidid = item[4][idx]\n",
    "\n",
    "    selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "    \n",
    "    bound_list = video_id_boundary_frames[cur_vidid]\n",
    "    \n",
    "    cumsum_feat = torch.cumsum(cur_vid_feat, dim=0)\n",
    "    prob_each_segment = prob_vals_per_segment(selected_frames, cur_vid_feat, item_2[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min prob 0 = 7.953912266787591e-36\n",
      "Min prob 1 = 2.7495868628582206e-249\n",
      "Min prob 2 = 8.185175464823537e-201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 442)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ymax = 1\n",
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "\n",
    "plt.plot([0, item[1][idx].item()], [0, 0])\n",
    "\n",
    "for pivot in loaded_vidid_selected_frames[cur_vidid + \".txt\"]:\n",
    "    plt.plot([pivot, pivot], [0, ymax], '--')\n",
    "    \n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax])\n",
    "    \n",
    "all_prob = []\n",
    "for i, segment in enumerate(prob_each_segment):\n",
    "    segment = segment.cpu().numpy()\n",
    "    print(f\"Min prob {i} = {np.min(segment).item()}\")\n",
    "    xs = np.arange(loaded_vidid_selected_frames[cur_vidid + \".txt\"][i], \n",
    "                   loaded_vidid_selected_frames[cur_vidid + \".txt\"][i+1])\n",
    "    plt.plot(xs, segment, '*')\n",
    "\n",
    "    \n",
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "\n",
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))\n",
    "loss_arr = -loss_arr[0].detach().cpu().numpy()\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr/np.max(loss_arr), 'k+')\n",
    "\n",
    "plt.ylim([0, ymax])\n",
    "plt.xlim([0, item[1][idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1895, 48])\n"
     ]
    }
   ],
   "source": [
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "print(loss_prob.shape)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1387])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf5b8ecb00>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3987, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([75, 345, 1305, 1415, 1440, 1519],\n",
       " [73, 282, 455, 1312, 1434, 1458],\n",
       " tensor(1520, device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid], loaded_vidid_selected_frames[cur_vidid + \".txt\"], item_1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_features_shortened = cur_vid_feat[:item[1][idx]]\n",
    "cur_features_shortened[:, item[2][idx][:item[1][idx]]].shape\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 47, 47, 47, 47, 47, 47,  4,  4,  4,  4,  4,  4,  4,  4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[2][idx][video_id_boundary_frames[cur_vidid][1]-6:video_id_boundary_frames[cur_vidid][1]+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid][0]-6,video_id_boundary_frames[cur_vidid][0]+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0794, 13.3350],\n",
       "        [ 1.0831, 13.5102],\n",
       "        [ 1.1767, 13.7662],\n",
       "        [ 1.3750, 14.0177],\n",
       "        [ 1.7298, 14.1727],\n",
       "        [ 2.1706, 14.0367],\n",
       "        [ 2.7338, 13.4563],\n",
       "        [ 4.5600, 12.1745],\n",
       "        [ 7.8450, 10.3246],\n",
       "        [11.0151,  8.3563],\n",
       "        [13.4775,  6.6546],\n",
       "        [14.1836,  5.2898],\n",
       "        [11.2142,  4.2791],\n",
       "        [ 5.9829,  3.5694],\n",
       "        [ 2.3440,  2.9389],\n",
       "        [ 0.7406,  2.3515]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_vid_feat[video_id_boundary_frames[cur_vidid][1]-7:video_id_boundary_frames[cur_vidid][1]+9][:,[47,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frames_and_get_classification_value(model_output, count_list, video_ids_list, labels_all, p_criterion):\n",
    "    new_selected_frames = {}\n",
    "#     new_selected_frames_probs_values = {}\n",
    "#     new_selected_frames_labels = {}\n",
    "    \n",
    "    labels_arr = []\n",
    "    probs_arr = []\n",
    "    \n",
    "    for i, video_id in enumerate(video_ids_list):\n",
    "        current_boundary_frames = boundaries_dict[video_id]\n",
    "        current_video_logits = model_output[i].T\n",
    "        current_video_count = count_list[i]\n",
    "        current_selected_frames = loaded_vidid_selected_frames[video_id]\n",
    "        labels = labels_all[i]\n",
    "        cur_seg_start = 0\n",
    "        for seg_num, ele in enumerate(current_boundary_frames):\n",
    "            cur_seg_end = ele + 1\n",
    "            total_seg_len = cur_seg_end - cur_seg_start\n",
    "            per_frames_far_from_boundary = int(0.3 * total_seg_len)\n",
    "            new_frames_start = cur_seg_start + per_frames_far_from_boundary\n",
    "            new_frames_end = cur_seg_end - per_frames_far_from_boundary\n",
    "            \n",
    "            frames_added = list(range(new_frames_start, new_frames_end))\n",
    "            frames_added.append(current_selected_frames[seg_num])\n",
    "            frames_added = list(set(frames_added))\n",
    "            frames_added.sort()\n",
    "            new_selected_frames[video_id] = frames_added\n",
    "\n",
    "            \n",
    "            labels_seg = torch.stack([labels[current_selected_frames[seg_num]]] * len(frames_added))\n",
    "            frames_added = torch.tensor(np.array(frames_added), dtype=torch.long, device=current_video_logits.device)\n",
    "            prob_seg = current_video_logits[frames_added, :]\n",
    "            \n",
    "            labels_arr.append(labels_seg)\n",
    "            probs_arr.append(prob_seg)\n",
    "            \n",
    "            cur_seg_start = cur_seg_end\n",
    "            \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    probs_arr = torch.cat(probs_arr)\n",
    "    loss = p_criterion(probs_arr, labels_arr)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
