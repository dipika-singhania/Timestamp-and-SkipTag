{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import scipy.stats\n",
    "from scipy.stats import binom\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mstcn_model import *\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 8, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 2, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-mstcn-weaksup-split2/', 'project_name': 'breakfast-split-2', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split2.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split2.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split2_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=2,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-mstcn-weaksup-split2/\",\n",
    "    project_name=\"breakfast-split-2\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1261\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 451\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", \n",
    "                                       allow_pickle=True).item()\n",
    "\n",
    "video_id_boundary_frames = pickle.load(open(\"data/breakfast_boundary_annotations.pkl\", \"rb\"))\n",
    "video_id_boundary_frames = dict([(key[:-4], val) for key,val in video_id_boundary_frames.items()])\n",
    "\n",
    "weak_labels = pickle.load(open('data/breakfast_weaklysupervised_labels.pkl', 'rb'))\n",
    "weak_labels = dict([(key[:-4], val) for key,val in weak_labels.items()])\n",
    "\n",
    "loaded_mean_var_actions = pickle.load(open(\"data/breakfast_meanvar_actions.pkl\", \"rb\"))\n",
    "\n",
    "with open('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/len_video.txt', 'r') as openf:\n",
    "    attributes = [item.strip().split()[::-1] for item in openf.readlines()]\n",
    "frame_id_dict = dict([(vidid[:-4], int(framecount)) for vidid,framecount in attributes])\n",
    "\n",
    "df=pd.read_csv(config.label_id_csv)\n",
    "label_id_to_label_name = {}\n",
    "label_name_to_label_id_dict = {}\n",
    "for i, ele in df.iterrows():\n",
    "    label_id_to_label_name[ele.label_id] = ele.label_name\n",
    "    label_name_to_label_id_dict[ele.label_name] = ele.label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weak_labels, loaded_mean_var_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_table = {}\n",
    "prior_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(prior_table, open('data/breakfast_lengthmodel_multinomial_prior.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ele in weak_labels.keys():\n",
    "#     frame_count = frame_id_dict[ele]\n",
    "#     label_list = weak_labels[ele]\n",
    "#     mean_list = np.array([loaded_mean_var_actions[label][0] for label in label_list])\n",
    "#     total_mean = np.sum(mean_list)\n",
    "#     left_sum = np.cumsum(mean_list)\n",
    "#     relative_sum = left_sum / total_mean\n",
    "#     prob_id_ge_boundary = [binom.cdf(np.arange(frame_count)+1, frame_count, rel) for rel in relative_sum]\n",
    "#     prob_id_gt_boundary = [1] + [cdf - binom.pmf(np.arange(frame_count)+1, \n",
    "#                                                  frame_count,\n",
    "#                                                  rel) for rel,cdf in zip(relative_sum, prob_id_ge_boundary)]\n",
    "#     prob_id_gt_boundary[-1] = 0\n",
    "#     prior_table[ele] = np.stack([prob_id_gt_boundary[j-1] - prob_id_gt_boundary[j] \n",
    "#                                     for j in range(1,len(label_list)+1)], axis=0)\n",
    "\n",
    "prior_table = pickle.load(open('data/breakfast_lengthmodel_multinomial_prior.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posterior_loss_weights(framewise_proba, ele):\n",
    "    '''\n",
    "    ele is video_id\n",
    "    '''\n",
    "    global posterior_table\n",
    "    frame_count = frame_id_dict[ele]\n",
    "    label_list = [label_name_to_label_id_dict[item] for item in weak_labels[ele]]\n",
    "    seqlen = len(label_list)\n",
    "    label_seq_prob = framewise_proba[:frame_count,label_list]\n",
    "    label_seq_logprob = torch.log(label_seq_prob + 1e-10)\n",
    "\n",
    "    left_probsum = torch.cumsum(label_seq_prob, dim=1)\n",
    "    right_probsum = torch.sum(label_seq_prob, dim=1, keepdim=True) - torch.cumsum(label_seq_prob, dim=1) + label_seq_prob\n",
    "\n",
    "    left_logprobsum = torch.log(torch.clip(left_probsum, min=1e-20))\n",
    "    right_logprobsum = torch.log(torch.clip(right_probsum, min=1e-20))\n",
    "\n",
    "    left_logprobsum_left_idsum = torch.cumsum(left_logprobsum, dim=0)\n",
    "    left_logprobsum_left_idsum -= left_logprobsum\n",
    "    right_logprobsum_right_idsum = torch.sum(right_logprobsum, dim=0, keepdim=True) - torch.cumsum(right_logprobsum, dim=0)\n",
    "\n",
    "    for k in range(seqlen):\n",
    "        left_logprobsum_left_idsum[:k,k] = -1000\n",
    "        remaining_labelcount = seqlen-k-1\n",
    "        if remaining_labelcount > 0:\n",
    "            right_logprobsum_right_idsum[-remaining_labelcount:,k] = -1000\n",
    "\n",
    "    loglikelihood = label_seq_logprob + 10*(left_logprobsum_left_idsum + right_logprobsum_right_idsum)/(frame_count)\n",
    "    posterior = loglikelihood.T + torch.log(torch.clip(torch.tensor(prior_table[ele]), min=1e-10))/10\n",
    "    posterior = torch.softmax(posterior/2, dim=0)\n",
    "    posterior_table[ele] = posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_boundaries():\n",
    "    estimated_boundary_dict = {}\n",
    "    for ele in weak_labels.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        else:\n",
    "            frame_count = frame_id_dict[ele]\n",
    "            label_list = [label_name_to_label_id_dict[item] for item in weak_labels[ele]]\n",
    "            posterior_id_between_boundary = posterior_table[ele]\n",
    "\n",
    "            prob_boundary_le_id = 1 - np.cumsum(posterior_id_between_boundary, axis=0)\n",
    "            prob_boundary_le_id = np.concatenate([np.zeros((len(label_list), 1)), prob_boundary_le_id], axis=1)\n",
    "            frame_wise_proba = prob_boundary_le_id[:,1:] - prob_boundary_le_id[:,:-1]\n",
    "#             import pdb\n",
    "#             pdb.set_trace()\n",
    "            estimated_boundary_dict[ele] = np.sum(frame_wise_proba * np.arange(frame_count), axis=1)[:-1]\n",
    "            \n",
    "    return estimated_boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_loss(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global posterior_table\n",
    "    loss_arr = []\n",
    "    device = data_feat.device\n",
    "    for iter_num in range(len(data_count)):\n",
    "        ele = video_ids[iter_num]\n",
    "        frame_count = frame_id_dict[ele]\n",
    "        label_list = [label_name_to_label_id_dict[item] for item in weak_labels[ele]]\n",
    "        label_list = torch.tensor(label_list).type(torch.long).to(device)\n",
    "        label_seq_logprob = torch.log(data_feat[iter_num][:frame_count, label_list] + 1e-10)\n",
    "        weights = posterior_table[ele].to(device).T\n",
    "        weighted_loss = torch.sum(weights * label_seq_logprob, dim=1)\n",
    "        loss_arr.append(weighted_loss)\n",
    "    return -torch.mean(torch.cat(loss_arr)), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_err():\n",
    "    err_list = []\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    estimated_boundary_dict = get_estimated_boundaries()\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if ele + \".txt\" not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_list = estimated_boundary_dict[ele]\n",
    "\n",
    "        estimated = np.array(estimated_boundary_list) #.detach().cpu().numpy()\n",
    "        if np.isnan(estimated).any():\n",
    "            print(ele)\n",
    "        actual = np.array(video_id_boundary_frames[ele][:-1])\n",
    "        if len(actual) != len(estimated):\n",
    "            print(ele)\n",
    "            continue\n",
    "\n",
    "        mse_err = (actual - estimated)**2\n",
    "        err_list.append(mse_err)\n",
    "\n",
    "    print(f\"Train Boundary avergage error = {np.sqrt(np.mean(np.concatenate(err_list))):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labels_dir = \"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/length_segmentation_output/\"\n",
    "def get_single_random(output_p, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((output_p.shape[0], output_p.shape[2]), dtype=torch.long, \n",
    "                                        device=output_p.device) * (-100)\n",
    "    for iter_num, cur_vidid in enumerate(video_ids):\n",
    "        pseudo_l = open(pseudo_labels_dir + cur_vidid + \".txt\").read().split(\"\\n\")[0:-1]\n",
    "        pseudo_l = [label_name_to_label_id_dict[ele] for ele in pseudo_l]\n",
    "        abc = torch.tensor(pseudo_l).to(torch.long).to(boundary_target_tensor.device)\n",
    "        frame_idx_tensor = torch.arange(0, len(pseudo_l), 1).to(device)\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = abc\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = MultiStageModel(num_stages=4, num_layers=10, num_f_maps=64, dim=2048, num_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_file=torch.load(f\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-lenassum-mstcn-split2-slup15/\"\n",
    "                       f\"ms-tcn-initial-25-epochs.wt\")\n",
    "model.load_state_dict(loaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Expectation\n",
      "Iter 0\n",
      "Iter 20\n",
      "Iter 40\n",
      "Iter 60\n",
      "Iter 80\n",
      "Iter 100\n",
      "Iter 120\n",
      "Iter 140\n",
      "Train Boundary avergage error = 281.776\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 54.12302688817997\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\"Calculating Expectation\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        for j in range(len(prob)):\n",
    "            generate_posterior_loss_weights(prob[j].cpu(), item[4][j])\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Iter {i}\")\n",
    "\n",
    "get_boundary_err()\n",
    "\n",
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "val_acc = correct * 100.0 / total\n",
    "print(f\"Validation:: Probability Accuracy {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Training:: Epoch 26, Iteration 0, Current loss 1.1014490821535248 Accuracy 61.711366538952745\n",
      "Training:: Epoch 26, Iteration 10, Current loss 0.9540314441316906 Accuracy 56.52804361695957\n",
      "Training:: Epoch 26, Iteration 20, Current loss 1.0059737414446723 Accuracy 58.27196858124693\n",
      "Training:: Epoch 26, Iteration 30, Current loss 1.220347220312781 Accuracy 55.70423506690578\n",
      "Training:: Epoch 26, Iteration 40, Current loss 1.0495563370419834 Accuracy 66.41489777314754\n",
      "Training:: Epoch 26, Iteration 50, Current loss 0.9403711788906621 Accuracy 60.71033630390945\n",
      "Training:: Epoch 26, Iteration 60, Current loss 0.915465453595571 Accuracy 51.49214383633871\n",
      "Training:: Epoch 26, Iteration 70, Current loss 0.8881783782066778 Accuracy 67.03833045819835\n",
      "Training:: Epoch 26, Iteration 80, Current loss 0.9958168625473743 Accuracy 62.61819213313162\n",
      "Training:: Epoch 26, Iteration 90, Current loss 1.0924898720491496 Accuracy 66.01808714716361\n",
      "Training:: Epoch 26, Iteration 100, Current loss 0.9670692006944825 Accuracy 68.44943229143834\n",
      "Training:: Epoch 26, Iteration 110, Current loss 0.8460619349778657 Accuracy 65.44345898004434\n",
      "Training:: Epoch 26, Iteration 120, Current loss 0.910390221245458 Accuracy 76.26336522228475\n",
      "Training:: Epoch 26, Iteration 130, Current loss 0.776501942346721 Accuracy 65.44777641778866\n",
      "Training:: Epoch 26, Iteration 140, Current loss 0.7439984537630613 Accuracy 64.93575207860923\n",
      "Training:: Epoch 26, Iteration 150, Current loss 0.8076425352059424 Accuracy 72.13216605478678\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 26, Probability Accuracy 53.87372084351825\n",
      "Starting Training\n",
      "Training:: Epoch 27, Iteration 0, Current loss 0.9699225594764298 Accuracy 75.12572703021821\n",
      "Training:: Epoch 27, Iteration 10, Current loss 0.7862203268800393 Accuracy 63.9811084872899\n",
      "Training:: Epoch 27, Iteration 20, Current loss 0.8732113403950501 Accuracy 69.7769037770394\n",
      "Training:: Epoch 27, Iteration 30, Current loss 0.8709258208307776 Accuracy 58.94980926022889\n",
      "Training:: Epoch 27, Iteration 40, Current loss 0.9307708033320921 Accuracy 61.68018266278246\n",
      "Training:: Epoch 27, Iteration 50, Current loss 0.8073310243152799 Accuracy 63.89060674774961\n",
      "Training:: Epoch 27, Iteration 60, Current loss 0.9868021384476456 Accuracy 54.0253138292354\n",
      "Training:: Epoch 27, Iteration 70, Current loss 0.9621489895073317 Accuracy 71.44849224333275\n",
      "Training:: Epoch 27, Iteration 80, Current loss 0.7844896057030076 Accuracy 52.766822038962594\n",
      "Training:: Epoch 27, Iteration 90, Current loss 0.653151763522827 Accuracy 71.30036952313918\n",
      "Training:: Epoch 27, Iteration 100, Current loss 0.9786038048574754 Accuracy 68.25692571708753\n",
      "Training:: Epoch 27, Iteration 110, Current loss 0.706382111493143 Accuracy 64.10789614401996\n",
      "Training:: Epoch 27, Iteration 120, Current loss 0.8216204731252863 Accuracy 63.57950887519544\n",
      "Training:: Epoch 27, Iteration 130, Current loss 0.9215658728947314 Accuracy 62.75545885461186\n",
      "Training:: Epoch 27, Iteration 140, Current loss 0.9558436865537311 Accuracy 63.704726826273784\n",
      "Training:: Epoch 27, Iteration 150, Current loss 0.9337189772143581 Accuracy 65.58663232916092\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 27, Probability Accuracy 55.17214235406223\n",
      "Starting Training\n",
      "Training:: Epoch 28, Iteration 0, Current loss 0.9444970688586684 Accuracy 69.2258599783011\n",
      "Training:: Epoch 28, Iteration 10, Current loss 1.361112270157592 Accuracy 65.1072628586198\n",
      "Training:: Epoch 28, Iteration 20, Current loss 0.7720063224236445 Accuracy 67.78360528360528\n",
      "Training:: Epoch 28, Iteration 30, Current loss 0.8439921917049427 Accuracy 53.91803840877915\n",
      "Training:: Epoch 28, Iteration 40, Current loss 0.7796248784983584 Accuracy 80.81664970026016\n",
      "Training:: Epoch 28, Iteration 50, Current loss 0.7734671760987519 Accuracy 65.87666820064427\n",
      "Training:: Epoch 28, Iteration 60, Current loss 0.7200389989418159 Accuracy 68.62808145766346\n",
      "Training:: Epoch 28, Iteration 70, Current loss 0.9905715706524038 Accuracy 67.93478260869566\n",
      "Training:: Epoch 28, Iteration 80, Current loss 0.7649094699810887 Accuracy 64.24238384107726\n",
      "Training:: Epoch 28, Iteration 90, Current loss 0.9848853713820297 Accuracy 64.5507942864771\n",
      "Training:: Epoch 28, Iteration 100, Current loss 0.8414978663550021 Accuracy 65.3107832849824\n",
      "Training:: Epoch 28, Iteration 110, Current loss 0.795819677011902 Accuracy 75.63146067415731\n",
      "Training:: Epoch 28, Iteration 120, Current loss 0.8879134277976931 Accuracy 63.722801243371734\n",
      "Training:: Epoch 28, Iteration 130, Current loss 0.8760370973370601 Accuracy 56.64449742753168\n",
      "Training:: Epoch 28, Iteration 140, Current loss 0.8113282993764792 Accuracy 60.793341597308306\n",
      "Training:: Epoch 28, Iteration 150, Current loss 0.8538144918323981 Accuracy 61.7908787541713\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 28, Probability Accuracy 55.79504495173385\n",
      "Starting Training\n",
      "Training:: Epoch 29, Iteration 0, Current loss 1.0931588377217745 Accuracy 66.79580227540212\n",
      "Training:: Epoch 29, Iteration 10, Current loss 0.8304102218977041 Accuracy 53.36992596374777\n",
      "Training:: Epoch 29, Iteration 20, Current loss 0.782065557319352 Accuracy 62.04318936877076\n",
      "Training:: Epoch 29, Iteration 30, Current loss 1.1332933484258816 Accuracy 58.16623921660061\n",
      "Training:: Epoch 29, Iteration 40, Current loss 0.7803347327537011 Accuracy 59.9667497921862\n",
      "Training:: Epoch 29, Iteration 50, Current loss 1.043109887326694 Accuracy 66.47207627943179\n",
      "Training:: Epoch 29, Iteration 60, Current loss 0.9277621591390782 Accuracy 69.04229253222542\n",
      "Training:: Epoch 29, Iteration 70, Current loss 0.9346928878841769 Accuracy 65.19948787155265\n",
      "Training:: Epoch 29, Iteration 80, Current loss 0.8205157260323719 Accuracy 64.59894984609814\n",
      "Training:: Epoch 29, Iteration 90, Current loss 0.7907119171122585 Accuracy 65.20023215322112\n",
      "Training:: Epoch 29, Iteration 100, Current loss 0.8382022127605988 Accuracy 76.47812005724185\n",
      "Training:: Epoch 29, Iteration 110, Current loss 0.9733492407896664 Accuracy 58.647811557584156\n",
      "Training:: Epoch 29, Iteration 120, Current loss 0.9152385538628228 Accuracy 72.39261052820375\n",
      "Training:: Epoch 29, Iteration 130, Current loss 0.6546768798601835 Accuracy 64.1395676993137\n",
      "Training:: Epoch 29, Iteration 140, Current loss 0.7365155902774092 Accuracy 67.60304541874508\n",
      "Training:: Epoch 29, Iteration 150, Current loss 0.8132390113261933 Accuracy 69.76063080822304\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 29, Probability Accuracy 53.89609313502092\n",
      "Starting Training\n",
      "Training:: Epoch 30, Iteration 0, Current loss 0.8997133064420642 Accuracy 56.68196451811768\n",
      "Training:: Epoch 30, Iteration 10, Current loss 0.935001190899495 Accuracy 61.16531462398597\n",
      "Training:: Epoch 30, Iteration 20, Current loss 0.7469133593968492 Accuracy 65.80422144241345\n",
      "Training:: Epoch 30, Iteration 30, Current loss 0.9963514594139344 Accuracy 64.45030267525874\n",
      "Training:: Epoch 30, Iteration 40, Current loss 1.1883078781433913 Accuracy 54.41396289457487\n",
      "Training:: Epoch 30, Iteration 50, Current loss 0.8394793634660132 Accuracy 63.53483016695452\n",
      "Training:: Epoch 30, Iteration 60, Current loss 0.8653960989155829 Accuracy 59.02656274852871\n",
      "Training:: Epoch 30, Iteration 70, Current loss 0.8110614067483094 Accuracy 64.58894345002376\n",
      "Training:: Epoch 30, Iteration 80, Current loss 0.8919056448365802 Accuracy 58.767572633552014\n",
      "Training:: Epoch 30, Iteration 90, Current loss 0.8489010489271702 Accuracy 60.37907631186889\n",
      "Training:: Epoch 30, Iteration 100, Current loss 0.8031675444422477 Accuracy 65.47708707490875\n",
      "Training:: Epoch 30, Iteration 110, Current loss 0.9072571518317771 Accuracy 68.06067067261097\n",
      "Training:: Epoch 30, Iteration 120, Current loss 0.8396226348878761 Accuracy 49.06806106097664\n",
      "Training:: Epoch 30, Iteration 130, Current loss 1.0213132318652105 Accuracy 59.43742133201992\n",
      "Training:: Epoch 30, Iteration 140, Current loss 0.7396191669776815 Accuracy 61.94858983630267\n",
      "Training:: Epoch 30, Iteration 150, Current loss 0.8149525662513624 Accuracy 62.943650126156435\n",
      "Calculating Expectation\n",
      "Epoch 30 iter 0\n",
      "Epoch 30 iter 20\n",
      "Epoch 30 iter 40\n",
      "Epoch 30 iter 60\n",
      "Epoch 30 iter 80\n",
      "Epoch 30 iter 100\n",
      "Epoch 30 iter 120\n",
      "Epoch 30 iter 140\n",
      "Train Boundary avergage error = 282.904\n",
      "Calculating Validation Data Accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:: Epoch 30, Probability Accuracy 52.91834113601524\n",
      "Starting Training\n",
      "Training:: Epoch 31, Iteration 0, Current loss 1.0626853960928626 Accuracy 61.44437486954707\n",
      "Training:: Epoch 31, Iteration 10, Current loss 1.183922657159757 Accuracy 57.25079654073737\n",
      "Training:: Epoch 31, Iteration 20, Current loss 1.2892270143407643 Accuracy 52.404115996258184\n",
      "Training:: Epoch 31, Iteration 30, Current loss 1.1586382990041377 Accuracy 63.421235253296324\n",
      "Training:: Epoch 31, Iteration 40, Current loss 1.138720157168596 Accuracy 56.26116678318962\n",
      "Training:: Epoch 31, Iteration 50, Current loss 1.1698610412613697 Accuracy 61.05579685933846\n",
      "Training:: Epoch 31, Iteration 60, Current loss 1.1050720021402158 Accuracy 73.29775947076078\n",
      "Training:: Epoch 31, Iteration 70, Current loss 1.1062357854259228 Accuracy 61.29799180535181\n",
      "Training:: Epoch 31, Iteration 80, Current loss 1.0944406808590248 Accuracy 77.49369823550595\n",
      "Training:: Epoch 31, Iteration 90, Current loss 1.0424536718724744 Accuracy 55.901122869375556\n",
      "Training:: Epoch 31, Iteration 100, Current loss 0.782606430777647 Accuracy 55.28157738231392\n",
      "Training:: Epoch 31, Iteration 110, Current loss 0.9964925362423769 Accuracy 60.33740129217516\n",
      "Training:: Epoch 31, Iteration 120, Current loss 0.9450953231426913 Accuracy 76.13971912135398\n",
      "Training:: Epoch 31, Iteration 130, Current loss 1.08024432712256 Accuracy 64.94574998714455\n",
      "Training:: Epoch 31, Iteration 140, Current loss 1.0175745299143837 Accuracy 68.78549029578778\n",
      "Training:: Epoch 31, Iteration 150, Current loss 1.1752537762616364 Accuracy 60.00436871996505\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 31, Probability Accuracy 53.341861043211665\n",
      "Starting Training\n",
      "Training:: Epoch 32, Iteration 0, Current loss 1.0584149687558693 Accuracy 49.43531427681284\n",
      "Training:: Epoch 32, Iteration 10, Current loss 0.9228261515669113 Accuracy 69.73511239647694\n",
      "Training:: Epoch 32, Iteration 20, Current loss 1.02053164078966 Accuracy 66.72419043751903\n",
      "Training:: Epoch 32, Iteration 30, Current loss 0.9450104082980049 Accuracy 65.56825823806321\n",
      "Training:: Epoch 32, Iteration 40, Current loss 1.3888583725805905 Accuracy 57.92352873430842\n",
      "Training:: Epoch 32, Iteration 50, Current loss 0.9251134824380002 Accuracy 69.76408281174771\n",
      "Training:: Epoch 33, Iteration 0, Current loss 1.23974359208177 Accuracy 64.16555258467024\n",
      "Training:: Epoch 33, Iteration 10, Current loss 1.2466993115165617 Accuracy 57.36725285982585\n",
      "Training:: Epoch 33, Iteration 20, Current loss 1.0451517960095296 Accuracy 64.76960575695439\n",
      "Training:: Epoch 33, Iteration 30, Current loss 0.9607529617313622 Accuracy 66.50617878350185\n",
      "Training:: Epoch 33, Iteration 40, Current loss 0.9980335360409451 Accuracy 73.97966010972836\n",
      "Training:: Epoch 33, Iteration 50, Current loss 1.1271960540286263 Accuracy 65.47384978372001\n",
      "Training:: Epoch 33, Iteration 60, Current loss 0.9618196237572367 Accuracy 70.15235457063712\n",
      "Training:: Epoch 33, Iteration 70, Current loss 0.9074139087932909 Accuracy 73.38012958963283\n",
      "Training:: Epoch 33, Iteration 80, Current loss 1.0887961860947708 Accuracy 71.15729129344321\n",
      "Training:: Epoch 33, Iteration 90, Current loss 1.0872282331027416 Accuracy 62.080214929955865\n",
      "Training:: Epoch 33, Iteration 100, Current loss 1.1653947287936028 Accuracy 65.94460006224712\n",
      "Training:: Epoch 33, Iteration 110, Current loss 1.4613707062457957 Accuracy 60.715979137031766\n",
      "Training:: Epoch 33, Iteration 120, Current loss 1.5302556005803623 Accuracy 51.27089923181202\n",
      "Training:: Epoch 33, Iteration 130, Current loss 2.447833297098241 Accuracy 60.26718484333252\n",
      "Training:: Epoch 33, Iteration 140, Current loss 2.1885371708269736 Accuracy 52.703117015840576\n",
      "Training:: Epoch 33, Iteration 150, Current loss 2.135679709854121 Accuracy 57.72956218519953\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 33, Probability Accuracy 45.20942950656668\n",
      "Starting Training\n",
      "Training:: Epoch 34, Iteration 0, Current loss 3.6632767458590543 Accuracy 57.39237504560379\n",
      "Training:: Epoch 34, Iteration 10, Current loss 3.599056251347214 Accuracy 59.74571762953506\n",
      "Training:: Epoch 34, Iteration 20, Current loss 3.568208956710376 Accuracy 37.99037304452467\n",
      "Training:: Epoch 34, Iteration 30, Current loss 2.6040148095308266 Accuracy 63.262910798122064\n",
      "Training:: Epoch 34, Iteration 40, Current loss 4.969276039546361 Accuracy 52.295964668158994\n",
      "Training:: Epoch 34, Iteration 50, Current loss 2.3603711464049573 Accuracy 60.15177326932012\n",
      "Training:: Epoch 34, Iteration 60, Current loss 1.925323374740394 Accuracy 63.100925198040756\n",
      "Training:: Epoch 34, Iteration 70, Current loss 2.4472409690432406 Accuracy 58.92970234325522\n",
      "Training:: Epoch 34, Iteration 80, Current loss 2.9036810755618205 Accuracy 52.61023821591485\n",
      "Training:: Epoch 34, Iteration 90, Current loss 2.061193974146704 Accuracy 64.40330671942344\n",
      "Training:: Epoch 34, Iteration 100, Current loss 1.9971610637052803 Accuracy 71.0099573257468\n",
      "Training:: Epoch 34, Iteration 110, Current loss 2.9148025830860127 Accuracy 67.10599888169982\n",
      "Training:: Epoch 34, Iteration 120, Current loss 1.5716410598187671 Accuracy 73.01634472511144\n",
      "Training:: Epoch 34, Iteration 130, Current loss 1.4620303374715304 Accuracy 69.320536590686\n",
      "Training:: Epoch 34, Iteration 140, Current loss 2.256255645238582 Accuracy 58.798148231381184\n",
      "Training:: Epoch 34, Iteration 150, Current loss 2.6277484775478044 Accuracy 70.84645573913465\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 34, Probability Accuracy 48.95658118241704\n",
      "Starting Training\n",
      "Training:: Epoch 35, Iteration 0, Current loss 2.110498284833336 Accuracy 60.09024428193558\n",
      "Training:: Epoch 35, Iteration 10, Current loss 1.6722388619991349 Accuracy 66.48710601719198\n",
      "Training:: Epoch 35, Iteration 20, Current loss 1.8905879800013634 Accuracy 63.484934621944284\n",
      "Training:: Epoch 35, Iteration 30, Current loss 2.2367790346253056 Accuracy 56.703448275862065\n",
      "Training:: Epoch 35, Iteration 40, Current loss 1.4477922471398337 Accuracy 69.1135655621637\n",
      "Training:: Epoch 35, Iteration 50, Current loss 1.4990243311598699 Accuracy 57.18842811571884\n",
      "Training:: Epoch 35, Iteration 60, Current loss 1.0526962337815746 Accuracy 73.83449883449883\n",
      "Training:: Epoch 35, Iteration 70, Current loss 1.4956394909196695 Accuracy 74.87290529090566\n",
      "Training:: Epoch 35, Iteration 80, Current loss 1.2792111435944165 Accuracy 73.84816753926701\n",
      "Training:: Epoch 35, Iteration 90, Current loss 1.0637229606780925 Accuracy 68.42424242424242\n",
      "Training:: Epoch 35, Iteration 100, Current loss 1.1932005290508634 Accuracy 66.45229410635439\n",
      "Training:: Epoch 35, Iteration 110, Current loss 1.3741169308013896 Accuracy 63.45531354583859\n",
      "Training:: Epoch 35, Iteration 120, Current loss 1.1065926717932875 Accuracy 64.65738809951021\n",
      "Training:: Epoch 35, Iteration 130, Current loss 1.139826515194014 Accuracy 70.95154406226463\n",
      "Training:: Epoch 35, Iteration 140, Current loss 1.3859964270075797 Accuracy 54.04746327246773\n",
      "Training:: Epoch 35, Iteration 150, Current loss 1.158917511581557 Accuracy 69.09534896927707\n",
      "Calculating Expectation\n",
      "Epoch 35 iter 0\n",
      "Epoch 35 iter 20\n",
      "Epoch 35 iter 40\n",
      "Epoch 35 iter 60\n",
      "Epoch 35 iter 80\n",
      "Epoch 35 iter 100\n",
      "Epoch 35 iter 120\n",
      "Epoch 35 iter 140\n",
      "Train Boundary avergage error = 282.907\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 35, Probability Accuracy 54.33887807101131\n",
      "Starting Training\n",
      "Training:: Epoch 36, Iteration 0, Current loss 1.1619252701786238 Accuracy 64.78132615108375\n",
      "Training:: Epoch 36, Iteration 10, Current loss 1.062457812056481 Accuracy 72.6609856804006\n",
      "Training:: Epoch 36, Iteration 20, Current loss 1.1155461151357342 Accuracy 55.87696700704553\n",
      "Training:: Epoch 36, Iteration 30, Current loss 1.250724146173945 Accuracy 56.95313821688378\n",
      "Training:: Epoch 36, Iteration 40, Current loss 1.3096551382647583 Accuracy 65.92008412197687\n",
      "Training:: Epoch 36, Iteration 50, Current loss 1.3193575387711571 Accuracy 56.071286735504366\n",
      "Training:: Epoch 36, Iteration 60, Current loss 1.1220041518986992 Accuracy 68.99411764705883\n",
      "Training:: Epoch 36, Iteration 70, Current loss 1.4114200731121527 Accuracy 64.83551924317351\n",
      "Training:: Epoch 36, Iteration 80, Current loss 1.176314681795808 Accuracy 67.52704294741352\n",
      "Training:: Epoch 36, Iteration 90, Current loss 1.1514954801488806 Accuracy 63.598507851132126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 36, Iteration 100, Current loss 1.4214537931357911 Accuracy 64.90652861662353\n",
      "Training:: Epoch 36, Iteration 110, Current loss 1.1281257763150394 Accuracy 58.58538543897216\n",
      "Training:: Epoch 36, Iteration 120, Current loss 1.4426095170162478 Accuracy 46.77155264469282\n",
      "Training:: Epoch 36, Iteration 130, Current loss 1.293625877013866 Accuracy 58.7297792652801\n",
      "Training:: Epoch 36, Iteration 140, Current loss 1.1036568037811159 Accuracy 70.46768531901714\n",
      "Training:: Epoch 36, Iteration 150, Current loss 1.0619819562648616 Accuracy 63.06132707774799\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 36, Probability Accuracy 55.346045490326055\n",
      "Starting Training\n",
      "Training:: Epoch 37, Iteration 0, Current loss 1.4956295283052707 Accuracy 59.235849056603776\n",
      "Training:: Epoch 37, Iteration 10, Current loss 1.3613786315943064 Accuracy 66.31840008442826\n",
      "Training:: Epoch 37, Iteration 20, Current loss 1.6420109843460846 Accuracy 51.90009303657053\n",
      "Training:: Epoch 37, Iteration 30, Current loss 1.1801071651104782 Accuracy 71.03594080338266\n",
      "Training:: Epoch 37, Iteration 40, Current loss 1.2429926473546724 Accuracy 64.25791252688722\n",
      "Training:: Epoch 37, Iteration 50, Current loss 1.3094758495300254 Accuracy 63.13131313131313\n",
      "Training:: Epoch 37, Iteration 60, Current loss 1.1816485452618417 Accuracy 68.65539719368026\n",
      "Training:: Epoch 37, Iteration 70, Current loss 1.1396323476286578 Accuracy 65.91006792193276\n",
      "Training:: Epoch 37, Iteration 80, Current loss 1.3245758971288433 Accuracy 53.526607646658235\n",
      "Training:: Epoch 37, Iteration 90, Current loss 1.2884945580724851 Accuracy 61.637498363374505\n",
      "Training:: Epoch 37, Iteration 100, Current loss 1.342416548786184 Accuracy 60.036860697281526\n",
      "Training:: Epoch 37, Iteration 110, Current loss 1.1387492594615414 Accuracy 76.24758753791012\n",
      "Training:: Epoch 37, Iteration 120, Current loss 1.205598961456241 Accuracy 76.60989693735839\n",
      "Training:: Epoch 37, Iteration 130, Current loss 1.1955256836110153 Accuracy 76.29317468187077\n",
      "Training:: Epoch 37, Iteration 140, Current loss 1.2938416617813302 Accuracy 67.01195773081201\n",
      "Training:: Epoch 37, Iteration 150, Current loss 1.3535940040143537 Accuracy 58.58959813697976\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 37, Probability Accuracy 55.05655218129842\n",
      "Starting Training\n",
      "Training:: Epoch 38, Iteration 0, Current loss 1.2580436687025063 Accuracy 66.37853949329359\n",
      "Training:: Epoch 38, Iteration 10, Current loss 0.9986720954170996 Accuracy 72.17822514987787\n",
      "Training:: Epoch 38, Iteration 20, Current loss 1.0467373897187164 Accuracy 76.15241507613864\n",
      "Training:: Epoch 38, Iteration 30, Current loss 1.0616171832053256 Accuracy 61.30384167636787\n",
      "Training:: Epoch 38, Iteration 40, Current loss 1.1124349946884098 Accuracy 66.20394775376325\n",
      "Training:: Epoch 38, Iteration 50, Current loss 1.0544418190280727 Accuracy 70.81892629663331\n",
      "Training:: Epoch 38, Iteration 60, Current loss 1.1192876135077183 Accuracy 57.443414399229475\n",
      "Training:: Epoch 38, Iteration 70, Current loss 1.1029576568589499 Accuracy 72.01664706414549\n",
      "Training:: Epoch 38, Iteration 80, Current loss 1.0480001366272056 Accuracy 63.243822798381984\n",
      "Training:: Epoch 38, Iteration 90, Current loss 1.2887474438890252 Accuracy 57.1764705882353\n",
      "Training:: Epoch 38, Iteration 100, Current loss 1.3401504259792656 Accuracy 50.13554001997432\n",
      "Training:: Epoch 38, Iteration 110, Current loss 1.2569594712645866 Accuracy 64.04714233709501\n",
      "Training:: Epoch 38, Iteration 120, Current loss 1.1819737692852095 Accuracy 61.448669902316155\n",
      "Training:: Epoch 38, Iteration 130, Current loss 1.2302911707329784 Accuracy 70.32805271274358\n",
      "Training:: Epoch 38, Iteration 140, Current loss 1.1511924308392727 Accuracy 72.15646940822468\n",
      "Training:: Epoch 38, Iteration 150, Current loss 1.2136655447275562 Accuracy 49.93567201303714\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 38, Probability Accuracy 55.16302771678336\n",
      "Starting Training\n",
      "Training:: Epoch 39, Iteration 0, Current loss 1.4736845034390191 Accuracy 63.93521231818424\n",
      "Training:: Epoch 39, Iteration 10, Current loss 1.082598830190278 Accuracy 68.42004468560485\n",
      "Training:: Epoch 39, Iteration 20, Current loss 1.246860498372469 Accuracy 62.41884622519013\n",
      "Training:: Epoch 39, Iteration 30, Current loss 1.193818334905254 Accuracy 71.31012969955673\n",
      "Training:: Epoch 39, Iteration 40, Current loss 1.3534849694228477 Accuracy 59.079612648898795\n",
      "Training:: Epoch 39, Iteration 50, Current loss 1.1056488354344722 Accuracy 59.65536768869833\n",
      "Training:: Epoch 39, Iteration 60, Current loss 1.2608083607733584 Accuracy 65.31799216082943\n",
      "Training:: Epoch 39, Iteration 70, Current loss 0.9117606654400461 Accuracy 63.014267904141846\n",
      "Training:: Epoch 39, Iteration 80, Current loss 1.1761267761583505 Accuracy 68.79104712867125\n",
      "Training:: Epoch 39, Iteration 90, Current loss 1.2151868732184528 Accuracy 65.27846636919517\n",
      "Training:: Epoch 39, Iteration 100, Current loss 1.334047346871101 Accuracy 67.85500747384155\n",
      "Training:: Epoch 39, Iteration 110, Current loss 1.34757015825798 Accuracy 62.86346714961264\n",
      "Training:: Epoch 39, Iteration 120, Current loss 1.066647309703559 Accuracy 70.30577245858571\n",
      "Training:: Epoch 39, Iteration 130, Current loss 1.1363604711806032 Accuracy 64.27056672760511\n",
      "Training:: Epoch 39, Iteration 140, Current loss 1.263549193782287 Accuracy 64.82932175183667\n",
      "Training:: Epoch 39, Iteration 150, Current loss 1.1751974786456314 Accuracy 64.36583811344805\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 39, Probability Accuracy 55.854393669470106\n",
      "Starting Training\n",
      "Training:: Epoch 40, Iteration 0, Current loss 1.219969927588357 Accuracy 66.9290573372206\n",
      "Training:: Epoch 40, Iteration 10, Current loss 1.4819065310900807 Accuracy 52.48069822087949\n",
      "Training:: Epoch 40, Iteration 20, Current loss 1.2284126453563813 Accuracy 61.271479997283166\n",
      "Training:: Epoch 40, Iteration 30, Current loss 1.1731377784487094 Accuracy 72.87656425393922\n",
      "Training:: Epoch 40, Iteration 40, Current loss 1.057762888039142 Accuracy 67.56497656582872\n",
      "Training:: Epoch 40, Iteration 50, Current loss 1.1796114475862152 Accuracy 59.77882703777336\n",
      "Training:: Epoch 40, Iteration 60, Current loss 1.2293466287613692 Accuracy 65.31879909871506\n",
      "Training:: Epoch 40, Iteration 70, Current loss 0.9482348134950269 Accuracy 75.37320528667871\n",
      "Training:: Epoch 40, Iteration 80, Current loss 1.068535657922138 Accuracy 59.47342109648392\n",
      "Training:: Epoch 40, Iteration 90, Current loss 1.2687100338835902 Accuracy 64.41641403204851\n",
      "Training:: Epoch 40, Iteration 100, Current loss 1.2653395762012432 Accuracy 58.19236057146273\n",
      "Training:: Epoch 40, Iteration 110, Current loss 1.114402215032285 Accuracy 60.2624038606545\n",
      "Training:: Epoch 40, Iteration 120, Current loss 1.253658911664754 Accuracy 56.04570140383236\n",
      "Training:: Epoch 40, Iteration 130, Current loss 1.309950309472015 Accuracy 68.55869436979746\n",
      "Training:: Epoch 40, Iteration 140, Current loss 1.4064819716047925 Accuracy 57.02534919813761\n",
      "Training:: Epoch 40, Iteration 150, Current loss 1.081745567187631 Accuracy 72.07212144462103\n",
      "Calculating Expectation\n",
      "Epoch 40 iter 0\n",
      "Epoch 40 iter 20\n",
      "Epoch 40 iter 40\n",
      "Epoch 40 iter 60\n",
      "Epoch 40 iter 80\n",
      "Epoch 40 iter 100\n",
      "Epoch 40 iter 120\n",
      "Epoch 40 iter 140\n",
      "Train Boundary avergage error = 287.825\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 40, Probability Accuracy 55.33113062932427\n",
      "Starting Training\n",
      "Training:: Epoch 41, Iteration 0, Current loss 1.4069795736905155 Accuracy 67.02191534073852\n",
      "Training:: Epoch 41, Iteration 10, Current loss 1.2928403816183893 Accuracy 52.47809222812814\n",
      "Training:: Epoch 41, Iteration 20, Current loss 1.107663301282304 Accuracy 62.92067307692308\n",
      "Training:: Epoch 41, Iteration 30, Current loss 1.5199306233819712 Accuracy 71.39870647416215\n",
      "Training:: Epoch 41, Iteration 40, Current loss 1.278968597823707 Accuracy 73.58270905041506\n",
      "Training:: Epoch 41, Iteration 50, Current loss 1.3052044903534572 Accuracy 69.02041137993538\n",
      "Training:: Epoch 41, Iteration 60, Current loss 1.4186472466104145 Accuracy 64.54797159022512\n",
      "Training:: Epoch 41, Iteration 70, Current loss 1.026788148719955 Accuracy 62.19561415310103\n",
      "Training:: Epoch 41, Iteration 80, Current loss 1.1773284233288055 Accuracy 74.57042518499938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 41, Iteration 90, Current loss 1.4001638725550327 Accuracy 65.27067767728404\n",
      "Training:: Epoch 41, Iteration 100, Current loss 1.1094283569141898 Accuracy 73.87615092977072\n",
      "Training:: Epoch 41, Iteration 110, Current loss 1.4256904437177083 Accuracy 74.46517979062358\n",
      "Training:: Epoch 41, Iteration 120, Current loss 1.181650136603363 Accuracy 70.22713321055862\n",
      "Training:: Epoch 41, Iteration 130, Current loss 1.7163616873932606 Accuracy 63.77010750949802\n",
      "Training:: Epoch 41, Iteration 140, Current loss 1.3566564220607669 Accuracy 53.65444912963564\n",
      "Training:: Epoch 41, Iteration 150, Current loss 1.356553059241037 Accuracy 70.23090586145648\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 41, Probability Accuracy 55.17711397439616\n",
      "Starting Training\n",
      "Training:: Epoch 42, Iteration 0, Current loss 1.4301478412586066 Accuracy 57.96012915906219\n",
      "Training:: Epoch 42, Iteration 10, Current loss 1.5030271697029491 Accuracy 60.48475693490799\n",
      "Training:: Epoch 42, Iteration 20, Current loss 1.3288155474504193 Accuracy 65.70326248815486\n",
      "Training:: Epoch 42, Iteration 30, Current loss 1.1775159260199737 Accuracy 61.865955826351865\n",
      "Training:: Epoch 42, Iteration 40, Current loss 1.1249239556390682 Accuracy 51.33197231489319\n",
      "Training:: Epoch 42, Iteration 50, Current loss 1.3421367317815578 Accuracy 66.04150281323734\n",
      "Training:: Epoch 42, Iteration 60, Current loss 1.077818089099642 Accuracy 69.95483717613502\n",
      "Training:: Epoch 42, Iteration 70, Current loss 1.027405630035632 Accuracy 69.6272666218939\n",
      "Training:: Epoch 42, Iteration 80, Current loss 1.251708968368832 Accuracy 67.00084423807513\n",
      "Training:: Epoch 42, Iteration 90, Current loss 1.106934532814685 Accuracy 66.71334111240762\n",
      "Training:: Epoch 42, Iteration 100, Current loss 1.203211606259831 Accuracy 55.50410033767487\n",
      "Training:: Epoch 42, Iteration 110, Current loss 1.2564145896813848 Accuracy 66.34494334872011\n",
      "Training:: Epoch 42, Iteration 120, Current loss 1.430618247491302 Accuracy 60.30085791514867\n",
      "Training:: Epoch 42, Iteration 130, Current loss 1.2344168253025425 Accuracy 60.9271523178808\n",
      "Training:: Epoch 42, Iteration 140, Current loss 1.3589476261375588 Accuracy 61.92775559476274\n",
      "Training:: Epoch 42, Iteration 150, Current loss 1.246415869644733 Accuracy 65.90207322452581\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 42, Probability Accuracy 54.54043584538261\n",
      "Starting Training\n",
      "Training:: Epoch 43, Iteration 0, Current loss 1.2142943745972459 Accuracy 71.67593661454508\n",
      "Training:: Epoch 43, Iteration 10, Current loss 1.290916569640511 Accuracy 68.02547770700637\n",
      "Training:: Epoch 43, Iteration 20, Current loss 1.2019384753732982 Accuracy 72.43961352657004\n",
      "Training:: Epoch 43, Iteration 30, Current loss 1.2077555752290028 Accuracy 59.79865115824455\n",
      "Training:: Epoch 43, Iteration 40, Current loss 1.155271292972294 Accuracy 63.8087835211815\n",
      "Training:: Epoch 43, Iteration 50, Current loss 1.292632056564412 Accuracy 54.608735213830755\n",
      "Training:: Epoch 43, Iteration 60, Current loss 1.1681295294197105 Accuracy 68.96300535927988\n",
      "Training:: Epoch 43, Iteration 70, Current loss 1.1244109531819648 Accuracy 70.04327833378414\n",
      "Training:: Epoch 43, Iteration 80, Current loss 1.4182823005495009 Accuracy 64.08142419378375\n",
      "Training:: Epoch 43, Iteration 90, Current loss 1.1427619805336557 Accuracy 65.37829487937948\n",
      "Training:: Epoch 43, Iteration 100, Current loss 1.271116390622126 Accuracy 63.94994927291174\n",
      "Training:: Epoch 43, Iteration 110, Current loss 1.185354764610274 Accuracy 64.06542056074767\n",
      "Training:: Epoch 43, Iteration 120, Current loss 1.4649320766290583 Accuracy 50.69444444444444\n",
      "Training:: Epoch 43, Iteration 130, Current loss 1.3062921342370746 Accuracy 65.27399692271274\n",
      "Training:: Epoch 43, Iteration 140, Current loss 1.3137565863752587 Accuracy 54.64838567005749\n",
      "Training:: Epoch 43, Iteration 150, Current loss 1.1406068774453053 Accuracy 67.61412575366063\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 43, Probability Accuracy 54.72366076977255\n",
      "Starting Training\n",
      "Training:: Epoch 44, Iteration 0, Current loss 1.2249012549010092 Accuracy 65.5017818140016\n",
      "Training:: Epoch 44, Iteration 10, Current loss 1.361039869625163 Accuracy 70.80299961683727\n",
      "Training:: Epoch 44, Iteration 20, Current loss 1.320622190269253 Accuracy 58.08515213417912\n",
      "Training:: Epoch 44, Iteration 30, Current loss 1.0810149756048038 Accuracy 63.398306753297895\n",
      "Training:: Epoch 44, Iteration 40, Current loss 1.3968411944427575 Accuracy 57.78230468534934\n",
      "Training:: Epoch 44, Iteration 50, Current loss 1.26209979023474 Accuracy 68.38551776521226\n",
      "Training:: Epoch 44, Iteration 60, Current loss 1.2032549810877442 Accuracy 61.469344608879496\n",
      "Training:: Epoch 44, Iteration 70, Current loss 1.883202305861201 Accuracy 44.92622950819672\n",
      "Training:: Epoch 44, Iteration 80, Current loss 1.1552502732851813 Accuracy 47.653768408894024\n",
      "Training:: Epoch 44, Iteration 90, Current loss 1.1324717795497563 Accuracy 72.47127872127872\n",
      "Training:: Epoch 44, Iteration 100, Current loss 1.3113746187566935 Accuracy 61.2304352793328\n",
      "Training:: Epoch 44, Iteration 110, Current loss 1.293245670864078 Accuracy 68.96514161220044\n",
      "Training:: Epoch 44, Iteration 120, Current loss 1.3891227142378841 Accuracy 65.0549126557087\n",
      "Training:: Epoch 44, Iteration 130, Current loss 0.9584720565680671 Accuracy 67.22078979508753\n",
      "Training:: Epoch 44, Iteration 140, Current loss 1.5239316815212047 Accuracy 68.24171582951976\n",
      "Training:: Epoch 44, Iteration 150, Current loss 1.3260015727410237 Accuracy 61.536507291296175\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 44, Probability Accuracy 54.38952645316319\n",
      "Starting Training\n",
      "Training:: Epoch 45, Iteration 0, Current loss 1.2064688010620226 Accuracy 72.06774876499647\n",
      "Training:: Epoch 45, Iteration 10, Current loss 1.182163687787248 Accuracy 77.72547933827092\n",
      "Training:: Epoch 45, Iteration 20, Current loss 1.236131571063163 Accuracy 60.490127882711754\n",
      "Training:: Epoch 45, Iteration 30, Current loss 1.1786122576765157 Accuracy 62.179011429419624\n",
      "Training:: Epoch 45, Iteration 40, Current loss 1.1726101568586063 Accuracy 67.86523986578399\n",
      "Training:: Epoch 45, Iteration 50, Current loss 1.0359057004755219 Accuracy 64.1111643507427\n",
      "Training:: Epoch 45, Iteration 60, Current loss 1.2987962031345837 Accuracy 57.90069386349097\n",
      "Training:: Epoch 45, Iteration 70, Current loss 1.2035390297542017 Accuracy 63.86440677966102\n",
      "Training:: Epoch 45, Iteration 80, Current loss 1.1092972698709826 Accuracy 73.92636157569075\n",
      "Training:: Epoch 45, Iteration 90, Current loss 1.1047206919404586 Accuracy 61.763198359815476\n",
      "Training:: Epoch 45, Iteration 100, Current loss 1.0542197971230558 Accuracy 59.34924895230023\n",
      "Training:: Epoch 45, Iteration 110, Current loss 1.24788911741415 Accuracy 63.65079365079365\n",
      "Training:: Epoch 45, Iteration 120, Current loss 1.1879072632762893 Accuracy 62.745002199521\n",
      "Training:: Epoch 45, Iteration 130, Current loss 1.2356427931410177 Accuracy 71.32253100389246\n",
      "Training:: Epoch 45, Iteration 140, Current loss 1.2655324665173868 Accuracy 53.59586773210896\n",
      "Training:: Epoch 45, Iteration 150, Current loss 1.211287330943379 Accuracy 64.63015115811254\n",
      "Calculating Expectation\n",
      "Epoch 45 iter 0\n",
      "Epoch 45 iter 20\n",
      "Epoch 45 iter 40\n",
      "Epoch 45 iter 60\n",
      "Epoch 45 iter 80\n",
      "Epoch 45 iter 100\n",
      "Epoch 45 iter 120\n",
      "Epoch 45 iter 140\n",
      "Train Boundary avergage error = 292.785\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 45, Probability Accuracy 54.232298960102746\n",
      "Starting Training\n",
      "Training:: Epoch 46, Iteration 0, Current loss 1.2903367633493477 Accuracy 69.0336547923637\n",
      "Training:: Epoch 46, Iteration 10, Current loss 1.4061342170113291 Accuracy 56.44583706356312\n",
      "Training:: Epoch 46, Iteration 20, Current loss 1.4954650598066614 Accuracy 61.417181224147235\n",
      "Training:: Epoch 46, Iteration 30, Current loss 1.1746823052025057 Accuracy 72.62622456669179\n",
      "Training:: Epoch 46, Iteration 40, Current loss 1.3267613078693925 Accuracy 72.19251336898395\n",
      "Training:: Epoch 46, Iteration 50, Current loss 1.578676949011064 Accuracy 64.56027247293517\n",
      "Training:: Epoch 46, Iteration 60, Current loss 1.550683577342415 Accuracy 57.17079530638853\n",
      "Training:: Epoch 46, Iteration 70, Current loss 1.4647451307125097 Accuracy 60.190161223646136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 46, Iteration 80, Current loss 1.2788883108993612 Accuracy 63.4454659423318\n",
      "Training:: Epoch 46, Iteration 90, Current loss 1.3013613250745968 Accuracy 68.47122082365881\n",
      "Training:: Epoch 46, Iteration 100, Current loss 1.2724879882624978 Accuracy 73.76103190767142\n",
      "Training:: Epoch 46, Iteration 110, Current loss 1.2754800350000854 Accuracy 65.96181081213068\n",
      "Training:: Epoch 46, Iteration 120, Current loss 1.5873455592675971 Accuracy 62.62128201049785\n",
      "Training:: Epoch 46, Iteration 130, Current loss 1.5922989463952872 Accuracy 58.94777513040118\n",
      "Training:: Epoch 46, Iteration 140, Current loss 1.2707947802633808 Accuracy 71.1438063597532\n",
      "Training:: Epoch 46, Iteration 150, Current loss 1.5449731244494804 Accuracy 58.902199731488174\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 46, Probability Accuracy 53.83529436135394\n",
      "Starting Training\n",
      "Training:: Epoch 47, Iteration 0, Current loss 1.0668766129686518 Accuracy 63.96032329169728\n",
      "Training:: Epoch 47, Iteration 10, Current loss 1.5885501603105412 Accuracy 71.4646025395721\n",
      "Training:: Epoch 47, Iteration 20, Current loss 1.5489090221067106 Accuracy 58.82003092660878\n",
      "Training:: Epoch 47, Iteration 30, Current loss 1.3463040474188535 Accuracy 61.57568887338682\n",
      "Training:: Epoch 47, Iteration 40, Current loss 1.438151400175044 Accuracy 64.96823895120963\n",
      "Training:: Epoch 47, Iteration 50, Current loss 1.440561917582259 Accuracy 56.798722971992454\n",
      "Training:: Epoch 47, Iteration 60, Current loss 1.197228759868464 Accuracy 62.08923512747875\n",
      "Training:: Epoch 47, Iteration 70, Current loss 1.5300979003435344 Accuracy 56.40094864720972\n",
      "Training:: Epoch 47, Iteration 80, Current loss 1.563898234666674 Accuracy 71.73430642536614\n",
      "Training:: Epoch 47, Iteration 90, Current loss 0.9469677575514004 Accuracy 66.35378445259835\n",
      "Training:: Epoch 47, Iteration 100, Current loss 1.273934775625802 Accuracy 60.82165140588283\n",
      "Training:: Epoch 47, Iteration 110, Current loss 1.4287949485208322 Accuracy 59.18383644785178\n",
      "Training:: Epoch 47, Iteration 120, Current loss 1.2857334325204754 Accuracy 64.71347629634943\n",
      "Training:: Epoch 47, Iteration 130, Current loss 1.3508146947929802 Accuracy 65.59491912275479\n",
      "Training:: Epoch 47, Iteration 140, Current loss 1.3061109346066329 Accuracy 54.77866061293984\n",
      "Training:: Epoch 47, Iteration 150, Current loss 1.484145928154795 Accuracy 58.689159738477684\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 47, Probability Accuracy 53.908936487550235\n",
      "Starting Training\n",
      "Training:: Epoch 48, Iteration 0, Current loss 1.3847649194914506 Accuracy 63.418871959369156\n",
      "Training:: Epoch 48, Iteration 10, Current loss 1.3800279927079302 Accuracy 62.53637788224759\n",
      "Training:: Epoch 48, Iteration 20, Current loss 1.2894267046998797 Accuracy 69.16263541595694\n",
      "Training:: Epoch 48, Iteration 30, Current loss 1.374939720720397 Accuracy 74.25283765489951\n",
      "Training:: Epoch 48, Iteration 40, Current loss 1.3070414036542537 Accuracy 51.96226750912165\n",
      "Training:: Epoch 48, Iteration 50, Current loss 1.1435197479460806 Accuracy 61.734493844585586\n",
      "Training:: Epoch 48, Iteration 60, Current loss 1.2757698599822769 Accuracy 68.75645135002988\n",
      "Training:: Epoch 48, Iteration 70, Current loss 1.3302976465947556 Accuracy 69.18895092565384\n",
      "Training:: Epoch 48, Iteration 80, Current loss 1.5131304934814027 Accuracy 59.38071175142458\n",
      "Training:: Epoch 48, Iteration 90, Current loss 1.7570296572782 Accuracy 55.026511702640455\n",
      "Training:: Epoch 48, Iteration 100, Current loss 1.2593244414767555 Accuracy 62.87082530008085\n",
      "Training:: Epoch 48, Iteration 110, Current loss 1.503475218179085 Accuracy 53.40416752630493\n",
      "Training:: Epoch 48, Iteration 120, Current loss 1.4206682605822114 Accuracy 62.004680816721816\n",
      "Training:: Epoch 48, Iteration 130, Current loss 1.474952961278275 Accuracy 63.44363152643831\n",
      "Training:: Epoch 48, Iteration 140, Current loss 1.6607668882143174 Accuracy 60.1726684084971\n",
      "Training:: Epoch 48, Iteration 150, Current loss 1.1554044216104502 Accuracy 60.73763159705498\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 48, Probability Accuracy 53.60328541243734\n",
      "Starting Training\n",
      "Training:: Epoch 49, Iteration 0, Current loss 1.4519628451895323 Accuracy 68.64434687156971\n",
      "Training:: Epoch 49, Iteration 10, Current loss 1.4537771387474199 Accuracy 63.66296079578701\n",
      "Training:: Epoch 49, Iteration 20, Current loss 1.2213985049883331 Accuracy 72.9688625630353\n",
      "Training:: Epoch 49, Iteration 30, Current loss 1.3897538053203402 Accuracy 66.81055155875299\n",
      "Training:: Epoch 49, Iteration 40, Current loss 1.604207181131561 Accuracy 65.1088755889469\n",
      "Training:: Epoch 49, Iteration 50, Current loss 1.233056221559489 Accuracy 74.294780152725\n",
      "Training:: Epoch 49, Iteration 60, Current loss 1.3290351742139301 Accuracy 60.007656701353845\n",
      "Training:: Epoch 49, Iteration 70, Current loss 1.299739352521973 Accuracy 57.91558738381838\n",
      "Training:: Epoch 49, Iteration 80, Current loss 1.592538951833486 Accuracy 59.72244548036221\n",
      "Training:: Epoch 49, Iteration 90, Current loss 1.2035886847667654 Accuracy 59.76405819897759\n",
      "Training:: Epoch 49, Iteration 100, Current loss 1.2588951162618414 Accuracy 62.77195111380775\n",
      "Training:: Epoch 49, Iteration 110, Current loss 1.1069303992019683 Accuracy 56.22111624600071\n",
      "Training:: Epoch 49, Iteration 120, Current loss 1.090687563054438 Accuracy 73.20694151870799\n",
      "Training:: Epoch 49, Iteration 130, Current loss 1.188948501995427 Accuracy 62.982336136482516\n",
      "Training:: Epoch 49, Iteration 140, Current loss 1.573921335116755 Accuracy 50.97383434979343\n",
      "Training:: Epoch 49, Iteration 150, Current loss 1.1773865617012675 Accuracy 65.41870415647922\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 49, Probability Accuracy 53.37873389402163\n",
      "Starting Training\n",
      "Training:: Epoch 50, Iteration 0, Current loss 1.4380018048234648 Accuracy 59.84952120383037\n",
      "Training:: Epoch 50, Iteration 10, Current loss 1.2835683380757712 Accuracy 67.63375714651421\n",
      "Training:: Epoch 50, Iteration 20, Current loss 1.2717683679497105 Accuracy 66.62180349932706\n",
      "Training:: Epoch 50, Iteration 30, Current loss 1.3019533654949842 Accuracy 66.24303232998885\n",
      "Training:: Epoch 50, Iteration 40, Current loss 1.3085766945015884 Accuracy 63.71901500326158\n",
      "Training:: Epoch 50, Iteration 50, Current loss 1.3932012550900623 Accuracy 68.18034731959875\n",
      "Training:: Epoch 50, Iteration 60, Current loss 1.4762556471165926 Accuracy 57.04855773578081\n",
      "Training:: Epoch 50, Iteration 70, Current loss 1.5831981309690557 Accuracy 72.72001277751158\n",
      "Training:: Epoch 50, Iteration 80, Current loss 1.2370894363383758 Accuracy 72.4096958174905\n",
      "Training:: Epoch 50, Iteration 90, Current loss 1.2611673755120791 Accuracy 66.51359219320186\n",
      "Training:: Epoch 50, Iteration 100, Current loss 1.3949344926668816 Accuracy 58.878434324426046\n",
      "Training:: Epoch 50, Iteration 110, Current loss 1.300204944110702 Accuracy 61.10202211897127\n",
      "Training:: Epoch 50, Iteration 120, Current loss 1.273757296789198 Accuracy 54.386096900391216\n",
      "Training:: Epoch 50, Iteration 130, Current loss 1.250508500083838 Accuracy 60.54539883566541\n",
      "Training:: Epoch 50, Iteration 140, Current loss 1.2785517745925827 Accuracy 68.07784400631718\n",
      "Training:: Epoch 50, Iteration 150, Current loss 1.6638257360349225 Accuracy 57.633561876315255\n",
      "Calculating Expectation\n",
      "Epoch 50 iter 0\n",
      "Epoch 50 iter 20\n",
      "Epoch 50 iter 40\n",
      "Epoch 50 iter 60\n",
      "Epoch 50 iter 80\n",
      "Epoch 50 iter 100\n",
      "Epoch 50 iter 120\n",
      "Epoch 50 iter 140\n",
      "Train Boundary avergage error = 302.566\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 50, Probability Accuracy 53.15925757136347\n",
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 1.4619872800403846 Accuracy 52.76205659625349\n",
      "Training:: Epoch 51, Iteration 10, Current loss 1.361648095975356 Accuracy 62.16654904728299\n",
      "Training:: Epoch 51, Iteration 20, Current loss 1.5151847741349427 Accuracy 54.388535031847134\n",
      "Training:: Epoch 51, Iteration 30, Current loss 1.630090579079659 Accuracy 62.85416045332538\n",
      "Training:: Epoch 51, Iteration 40, Current loss 1.4625325582266717 Accuracy 69.58564862454551\n",
      "Training:: Epoch 51, Iteration 50, Current loss 1.4364061174810905 Accuracy 56.738698609522665\n",
      "Training:: Epoch 51, Iteration 60, Current loss 1.418245523223142 Accuracy 62.04459341299913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 51, Iteration 70, Current loss 1.4754982789641145 Accuracy 57.957846130930214\n",
      "Training:: Epoch 51, Iteration 80, Current loss 1.438861078923034 Accuracy 74.34673366834171\n",
      "Training:: Epoch 51, Iteration 90, Current loss 1.3954897508777593 Accuracy 73.1957600360848\n",
      "Training:: Epoch 51, Iteration 100, Current loss 1.521165946425505 Accuracy 63.94140141110572\n",
      "Training:: Epoch 51, Iteration 110, Current loss 1.4577723675459644 Accuracy 70.94159713945173\n",
      "Training:: Epoch 51, Iteration 120, Current loss 1.215636334534329 Accuracy 71.34769035398698\n",
      "Training:: Epoch 51, Iteration 130, Current loss 1.6524516026085823 Accuracy 63.74384236453202\n",
      "Training:: Epoch 51, Iteration 140, Current loss 1.293335359138548 Accuracy 66.5706948875574\n",
      "Training:: Epoch 51, Iteration 150, Current loss 1.05488656014683 Accuracy 71.35678391959799\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 51, Probability Accuracy 51.22063636740274\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 1.568112876586656 Accuracy 56.50482002299461\n",
      "Training:: Epoch 52, Iteration 10, Current loss 1.5368112372107614 Accuracy 61.426533523537806\n",
      "Training:: Epoch 52, Iteration 20, Current loss 1.6477020526299482 Accuracy 64.11122912040166\n",
      "Training:: Epoch 52, Iteration 30, Current loss 1.9658275138638295 Accuracy 61.36963571913417\n",
      "Training:: Epoch 52, Iteration 40, Current loss 1.5315979561581146 Accuracy 54.722050010332715\n",
      "Training:: Epoch 52, Iteration 50, Current loss 1.3621799801147068 Accuracy 69.85248543448618\n",
      "Training:: Epoch 52, Iteration 60, Current loss 1.6498726158303658 Accuracy 54.085930918281385\n",
      "Training:: Epoch 52, Iteration 70, Current loss 1.7216289645408902 Accuracy 65.23966057640853\n",
      "Training:: Epoch 52, Iteration 80, Current loss 1.8887512211144195 Accuracy 57.515930220411576\n",
      "Training:: Epoch 52, Iteration 90, Current loss 1.352270145041857 Accuracy 69.46923076923076\n",
      "Training:: Epoch 52, Iteration 100, Current loss 1.311554966180431 Accuracy 61.187002267046445\n",
      "Training:: Epoch 52, Iteration 110, Current loss 1.4526792227872323 Accuracy 63.403517126432874\n",
      "Training:: Epoch 52, Iteration 120, Current loss 1.4097514790057915 Accuracy 78.60366070070476\n",
      "Training:: Epoch 52, Iteration 130, Current loss 1.452372300376971 Accuracy 74.60517340888819\n",
      "Training:: Epoch 52, Iteration 140, Current loss 1.3950874733364174 Accuracy 63.66873140516022\n",
      "Training:: Epoch 52, Iteration 150, Current loss 1.2600870130565114 Accuracy 66.49279462470162\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 52, Probability Accuracy 53.74259435721092\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 1.2010494965435514 Accuracy 69.04809220985692\n",
      "Training:: Epoch 53, Iteration 10, Current loss 1.2132611382584964 Accuracy 72.41048357327428\n",
      "Training:: Epoch 53, Iteration 20, Current loss 1.364997324728385 Accuracy 59.74935177182368\n",
      "Training:: Epoch 53, Iteration 30, Current loss 1.586058822081457 Accuracy 63.46034375647132\n",
      "Training:: Epoch 53, Iteration 40, Current loss 1.429316846737052 Accuracy 68.01878252998416\n",
      "Training:: Epoch 53, Iteration 50, Current loss 1.3641762832868716 Accuracy 66.01552188374426\n",
      "Training:: Epoch 53, Iteration 60, Current loss 1.2049959340133873 Accuracy 63.81013998353135\n",
      "Training:: Epoch 53, Iteration 70, Current loss 1.2795559452806806 Accuracy 59.23051519154558\n",
      "Training:: Epoch 53, Iteration 80, Current loss 1.4550654881769376 Accuracy 66.76684372379088\n",
      "Training:: Epoch 53, Iteration 90, Current loss 1.471550320364361 Accuracy 64.93624772313296\n",
      "Training:: Epoch 53, Iteration 100, Current loss 1.4979983361948532 Accuracy 61.261058629479685\n",
      "Training:: Epoch 53, Iteration 110, Current loss 1.3672851662904018 Accuracy 61.55872024970738\n",
      "Training:: Epoch 53, Iteration 120, Current loss 1.4411203243574193 Accuracy 57.57142857142857\n",
      "Training:: Epoch 53, Iteration 130, Current loss 1.5037329578437548 Accuracy 61.540495285097386\n",
      "Training:: Epoch 53, Iteration 140, Current loss 1.9413078049909267 Accuracy 45.920932969560795\n",
      "Training:: Epoch 53, Iteration 150, Current loss 1.3905442217629584 Accuracy 58.65093768905021\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 53, Probability Accuracy 53.528607532004806\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 1.3275728618585148 Accuracy 67.8425398861541\n",
      "Training:: Epoch 54, Iteration 10, Current loss 1.276321431086795 Accuracy 66.72401209467209\n",
      "Training:: Epoch 54, Iteration 20, Current loss 1.5139122685207758 Accuracy 58.09609110101721\n",
      "Training:: Epoch 54, Iteration 30, Current loss 1.3830118642017104 Accuracy 59.26228291524417\n",
      "Training:: Epoch 54, Iteration 40, Current loss 1.4563937455526719 Accuracy 61.72430830039526\n",
      "Training:: Epoch 54, Iteration 50, Current loss 1.4034393635275464 Accuracy 56.68830437882256\n",
      "Training:: Epoch 54, Iteration 60, Current loss 1.2253067964732514 Accuracy 61.34663341645885\n",
      "Training:: Epoch 54, Iteration 70, Current loss 1.4238652606319318 Accuracy 61.34844868735084\n",
      "Training:: Epoch 54, Iteration 80, Current loss 1.6513628913112084 Accuracy 55.19754507096279\n",
      "Training:: Epoch 54, Iteration 90, Current loss 1.3778358282137229 Accuracy 59.52712100139082\n",
      "Training:: Epoch 54, Iteration 100, Current loss 1.42195957952824 Accuracy 73.48052384150436\n",
      "Training:: Epoch 54, Iteration 110, Current loss 1.4963274912442721 Accuracy 54.99168654666377\n",
      "Training:: Epoch 54, Iteration 120, Current loss 1.4980030620002578 Accuracy 68.72712244656984\n",
      "Training:: Epoch 54, Iteration 130, Current loss 1.5173810999287451 Accuracy 61.82287188306105\n",
      "Training:: Epoch 54, Iteration 140, Current loss 1.4191794963016995 Accuracy 66.84307849263534\n",
      "Training:: Epoch 54, Iteration 150, Current loss 1.352502130741396 Accuracy 71.55028598665396\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 54, Probability Accuracy 51.03523635911671\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 1.3155251523350975 Accuracy 64.21167484997272\n",
      "Training:: Epoch 55, Iteration 10, Current loss 1.4206366720476686 Accuracy 71.23767444366615\n",
      "Training:: Epoch 55, Iteration 20, Current loss 1.3607939225428092 Accuracy 67.8897978825794\n",
      "Training:: Epoch 55, Iteration 30, Current loss 1.1247836986292437 Accuracy 72.30072185532177\n",
      "Training:: Epoch 55, Iteration 40, Current loss 1.280781326368278 Accuracy 61.843515541264736\n",
      "Training:: Epoch 55, Iteration 50, Current loss 1.5815374667448197 Accuracy 59.82012793717145\n",
      "Training:: Epoch 55, Iteration 60, Current loss 1.354693290991345 Accuracy 62.70692623493577\n",
      "Training:: Epoch 55, Iteration 70, Current loss 1.5145592872632776 Accuracy 65.67253975075204\n",
      "Training:: Epoch 55, Iteration 80, Current loss 1.0177876828006531 Accuracy 70.47712901371438\n",
      "Training:: Epoch 55, Iteration 90, Current loss 1.3761139017731139 Accuracy 62.2652698161692\n",
      "Training:: Epoch 55, Iteration 100, Current loss 1.4957202868131934 Accuracy 59.6332611740736\n",
      "Training:: Epoch 55, Iteration 110, Current loss 1.3400136989192561 Accuracy 63.735101206008885\n",
      "Training:: Epoch 55, Iteration 120, Current loss 1.5099063105225448 Accuracy 70.49568078979844\n",
      "Training:: Epoch 55, Iteration 130, Current loss 1.5259260111356034 Accuracy 57.90375651220181\n",
      "Training:: Epoch 55, Iteration 140, Current loss 1.5623061914253382 Accuracy 61.385746357353526\n",
      "Training:: Epoch 55, Iteration 150, Current loss 1.3900445088358508 Accuracy 64.48785374405209\n",
      "Calculating Expectation\n",
      "Epoch 55 iter 0\n",
      "Epoch 55 iter 20\n",
      "Epoch 55 iter 40\n",
      "Epoch 55 iter 60\n",
      "Epoch 55 iter 80\n",
      "Epoch 55 iter 100\n",
      "Epoch 55 iter 120\n",
      "Epoch 55 iter 140\n",
      "Train Boundary avergage error = 303.739\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 53.7098645233459\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 1.5515660772282054 Accuracy 65.85882955002376\n",
      "Training:: Epoch 56, Iteration 10, Current loss 1.3190332458257719 Accuracy 53.67224581563827\n",
      "Training:: Epoch 56, Iteration 20, Current loss 1.5300369036234163 Accuracy 59.63023382272974\n",
      "Training:: Epoch 56, Iteration 30, Current loss 1.3533506685591208 Accuracy 66.42372613976968\n",
      "Training:: Epoch 56, Iteration 40, Current loss 1.2331108351584872 Accuracy 63.4207650273224\n",
      "Training:: Epoch 56, Iteration 50, Current loss 1.448580807719633 Accuracy 64.22094508301404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 56, Iteration 60, Current loss 1.4936496192178308 Accuracy 72.73447951246098\n",
      "Training:: Epoch 56, Iteration 70, Current loss 1.4993396360078568 Accuracy 69.73164258330875\n",
      "Training:: Epoch 56, Iteration 80, Current loss 1.3970054289851066 Accuracy 61.41932141932142\n",
      "Training:: Epoch 56, Iteration 90, Current loss 1.329755921555375 Accuracy 68.71928990331273\n",
      "Training:: Epoch 56, Iteration 100, Current loss 1.1575411920140142 Accuracy 73.4373265238148\n",
      "Training:: Epoch 56, Iteration 110, Current loss 1.3329033552462883 Accuracy 61.29217591141753\n",
      "Training:: Epoch 56, Iteration 120, Current loss 1.2846153678225911 Accuracy 67.51760059527217\n",
      "Training:: Epoch 56, Iteration 130, Current loss 1.5487002781274537 Accuracy 63.066202090592334\n",
      "Training:: Epoch 56, Iteration 140, Current loss 1.2625483642235467 Accuracy 57.97618553707007\n",
      "Training:: Epoch 56, Iteration 150, Current loss 1.381593306719861 Accuracy 63.5500614670127\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 56, Probability Accuracy 53.35584372540084\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 1.592392169306398 Accuracy 58.96662513933513\n",
      "Training:: Epoch 57, Iteration 10, Current loss 1.6761898528703443 Accuracy 64.39047619047619\n",
      "Training:: Epoch 57, Iteration 20, Current loss 1.3375721969288819 Accuracy 63.540699473847106\n",
      "Training:: Epoch 57, Iteration 30, Current loss 1.9242445406945636 Accuracy 50.10809350087826\n",
      "Training:: Epoch 57, Iteration 40, Current loss 2.2050742883721224 Accuracy 69.623176928661\n",
      "Training:: Epoch 57, Iteration 50, Current loss 1.85686252700752 Accuracy 61.86767881683136\n",
      "Training:: Epoch 57, Iteration 60, Current loss 1.7279868832023668 Accuracy 59.71952352792093\n",
      "Training:: Epoch 57, Iteration 70, Current loss 4.441831305609668 Accuracy 46.143821822058996\n",
      "Training:: Epoch 57, Iteration 80, Current loss 3.8152521958511927 Accuracy 65.02234294225883\n",
      "Training:: Epoch 57, Iteration 90, Current loss 5.612044128021482 Accuracy 51.702925731432856\n",
      "Training:: Epoch 57, Iteration 100, Current loss 4.228347338430174 Accuracy 43.53668368718087\n",
      "Training:: Epoch 57, Iteration 110, Current loss 8.429630761091936 Accuracy 25.663924794359577\n",
      "Training:: Epoch 57, Iteration 120, Current loss 5.581350081160334 Accuracy 41.493399807376356\n",
      "Training:: Epoch 57, Iteration 130, Current loss 4.937504581918826 Accuracy 49.45286195286195\n",
      "Training:: Epoch 57, Iteration 140, Current loss 3.453007832479514 Accuracy 44.66406132571772\n",
      "Training:: Epoch 57, Iteration 150, Current loss 4.656277036454032 Accuracy 57.44076367975453\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 57, Probability Accuracy 45.57484360111033\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 2.9471094236826323 Accuracy 63.339883764686206\n",
      "Training:: Epoch 58, Iteration 10, Current loss 5.459969347746655 Accuracy 51.51593901593902\n",
      "Training:: Epoch 58, Iteration 20, Current loss 2.9618004253352197 Accuracy 64.13160457131143\n",
      "Training:: Epoch 58, Iteration 30, Current loss 2.8761608546975035 Accuracy 57.53730926163993\n",
      "Training:: Epoch 58, Iteration 40, Current loss 2.5997745445233775 Accuracy 64.8424543946932\n",
      "Training:: Epoch 58, Iteration 50, Current loss 2.185505643061172 Accuracy 59.14846760873555\n",
      "Training:: Epoch 58, Iteration 60, Current loss 3.1372605448073205 Accuracy 69.84757320497393\n",
      "Training:: Epoch 58, Iteration 70, Current loss 1.992075355280483 Accuracy 60.95362771260259\n",
      "Training:: Epoch 58, Iteration 80, Current loss 1.8188513734427285 Accuracy 57.18051512988083\n",
      "Training:: Epoch 58, Iteration 90, Current loss 2.5148215342816815 Accuracy 67.00867567246365\n",
      "Training:: Epoch 58, Iteration 100, Current loss 3.266341925483215 Accuracy 38.98563199364364\n",
      "Training:: Epoch 58, Iteration 110, Current loss 2.4588416740101353 Accuracy 73.89883384089727\n",
      "Training:: Epoch 58, Iteration 120, Current loss 2.3300523423098194 Accuracy 54.58630716362675\n",
      "Training:: Epoch 58, Iteration 130, Current loss 1.8985761487000594 Accuracy 67.14262268176596\n",
      "Training:: Epoch 58, Iteration 140, Current loss 2.1099327781969386 Accuracy 61.58353774870537\n",
      "Training:: Epoch 58, Iteration 150, Current loss 2.0027584767943805 Accuracy 56.54056874510827\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 58, Probability Accuracy 55.242573642126196\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 1.798296915906585 Accuracy 54.381815792931285\n",
      "Training:: Epoch 59, Iteration 10, Current loss 1.751953559724962 Accuracy 68.94184564087476\n",
      "Training:: Epoch 59, Iteration 20, Current loss 1.7307399986593086 Accuracy 66.11659386291323\n",
      "Training:: Epoch 59, Iteration 30, Current loss 2.2232003702882706 Accuracy 66.00648435297434\n",
      "Training:: Epoch 59, Iteration 40, Current loss 2.087577666811444 Accuracy 52.38095238095238\n",
      "Training:: Epoch 59, Iteration 50, Current loss 1.9082499350907467 Accuracy 63.40193089430894\n",
      "Training:: Epoch 59, Iteration 60, Current loss 1.8455511269020581 Accuracy 75.27480439687035\n",
      "Training:: Epoch 59, Iteration 70, Current loss 1.8350451701524761 Accuracy 72.26003434895999\n",
      "Training:: Epoch 59, Iteration 80, Current loss 1.720444965952177 Accuracy 65.5489567456772\n",
      "Training:: Epoch 59, Iteration 90, Current loss 1.6789745735798174 Accuracy 70.91898428053204\n",
      "Training:: Epoch 59, Iteration 100, Current loss 2.51830372310352 Accuracy 59.179589794897446\n",
      "Training:: Epoch 59, Iteration 110, Current loss 1.6090591579577134 Accuracy 69.97284365705418\n",
      "Training:: Epoch 59, Iteration 120, Current loss 1.6935488587693364 Accuracy 52.97938788257339\n",
      "Training:: Epoch 59, Iteration 130, Current loss 1.399947167652138 Accuracy 71.60358909467388\n",
      "Training:: Epoch 59, Iteration 140, Current loss 1.751337859672408 Accuracy 64.63040446304045\n",
      "Training:: Epoch 59, Iteration 150, Current loss 1.430141647497069 Accuracy 58.73537604456825\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 59, Probability Accuracy 55.47085387579235\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 1.3866279978164144 Accuracy 65.85711251150606\n",
      "Training:: Epoch 60, Iteration 10, Current loss 1.6156423146901844 Accuracy 62.95763060468943\n",
      "Training:: Epoch 60, Iteration 20, Current loss 1.306736488358118 Accuracy 68.80850388703792\n",
      "Training:: Epoch 60, Iteration 30, Current loss 1.4468542771489794 Accuracy 60.844062947067236\n",
      "Training:: Epoch 60, Iteration 40, Current loss 1.6633317001182522 Accuracy 65.0\n",
      "Training:: Epoch 60, Iteration 50, Current loss 1.4379750218917995 Accuracy 64.87326043737575\n",
      "Training:: Epoch 60, Iteration 60, Current loss 1.5999078254949033 Accuracy 59.80806766428107\n",
      "Training:: Epoch 60, Iteration 70, Current loss 1.4767358143196603 Accuracy 64.30850325981412\n",
      "Training:: Epoch 60, Iteration 80, Current loss 1.5827611770204177 Accuracy 62.40668100721245\n",
      "Training:: Epoch 60, Iteration 90, Current loss 1.598549687925796 Accuracy 60.271350091559846\n",
      "Training:: Epoch 60, Iteration 100, Current loss 1.4047691830154394 Accuracy 60.00774721930164\n",
      "Training:: Epoch 60, Iteration 110, Current loss 1.645436231732587 Accuracy 62.50954927425516\n",
      "Training:: Epoch 60, Iteration 120, Current loss 1.6377749150469554 Accuracy 58.648436943673005\n",
      "Training:: Epoch 60, Iteration 130, Current loss 1.4169120640820316 Accuracy 66.2606978275181\n",
      "Training:: Epoch 60, Iteration 140, Current loss 1.466868425447417 Accuracy 62.44541484716157\n",
      "Training:: Epoch 60, Iteration 150, Current loss 1.407883855285272 Accuracy 67.8192319700281\n",
      "Calculating Expectation\n",
      "Epoch 60 iter 0\n",
      "Epoch 60 iter 20\n",
      "Epoch 60 iter 40\n",
      "Epoch 60 iter 60\n",
      "Epoch 60 iter 80\n",
      "Epoch 60 iter 100\n",
      "Epoch 60 iter 120\n",
      "Epoch 60 iter 140\n",
      "Train Boundary avergage error = 296.264\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 55.30047230393172\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 1.777554757045034 Accuracy 62.467742903438754\n",
      "Training:: Epoch 61, Iteration 10, Current loss 1.6862669885414767 Accuracy 55.79814747985083\n",
      "Training:: Epoch 61, Iteration 20, Current loss 1.543257640868029 Accuracy 59.65446434893436\n",
      "Training:: Epoch 61, Iteration 30, Current loss 1.387869059512028 Accuracy 60.2636415488941\n",
      "Training:: Epoch 61, Iteration 40, Current loss 1.1708449372526255 Accuracy 64.38386897750856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 61, Iteration 50, Current loss 1.7230975114753446 Accuracy 55.31174549069734\n",
      "Training:: Epoch 61, Iteration 60, Current loss 1.5233012782378776 Accuracy 78.09562729873315\n",
      "Training:: Epoch 61, Iteration 70, Current loss 1.416407017666774 Accuracy 65.06977490242207\n",
      "Training:: Epoch 61, Iteration 80, Current loss 1.4485983337273924 Accuracy 66.66342128322461\n",
      "Training:: Epoch 61, Iteration 90, Current loss 1.5125014814280144 Accuracy 66.37786748055528\n",
      "Training:: Epoch 61, Iteration 100, Current loss 1.6414887574575452 Accuracy 61.65214461079285\n",
      "Training:: Epoch 61, Iteration 110, Current loss 1.3516127504605877 Accuracy 61.048546644346374\n",
      "Training:: Epoch 61, Iteration 120, Current loss 1.5260754936686634 Accuracy 67.35709326544782\n",
      "Training:: Epoch 61, Iteration 130, Current loss 1.6144277453800675 Accuracy 57.14637752587482\n",
      "Training:: Epoch 61, Iteration 140, Current loss 1.5650051333811934 Accuracy 57.317161215796695\n",
      "Training:: Epoch 61, Iteration 150, Current loss 1.3542035754928092 Accuracy 63.54784627289141\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 61, Probability Accuracy 55.40384057670796\n",
      "Starting Training\n",
      "Training:: Epoch 62, Iteration 0, Current loss 1.3993345464170326 Accuracy 65.18092780482083\n",
      "Training:: Epoch 62, Iteration 10, Current loss 1.4524185537714325 Accuracy 64.86116119725929\n",
      "Training:: Epoch 62, Iteration 20, Current loss 1.5378296190235097 Accuracy 52.30459620877694\n",
      "Training:: Epoch 62, Iteration 30, Current loss 1.4071382830279584 Accuracy 68.21686938548375\n",
      "Training:: Epoch 62, Iteration 40, Current loss 1.5134768857247523 Accuracy 61.27930262692897\n",
      "Training:: Epoch 62, Iteration 50, Current loss 1.5604956678510598 Accuracy 67.56199294128432\n",
      "Training:: Epoch 62, Iteration 60, Current loss 1.4989574515159065 Accuracy 65.8716015535755\n",
      "Training:: Epoch 62, Iteration 70, Current loss 1.4900785815737132 Accuracy 63.91533784558908\n",
      "Training:: Epoch 62, Iteration 80, Current loss 1.4745214416155101 Accuracy 61.10661467147377\n",
      "Training:: Epoch 62, Iteration 90, Current loss 1.5828887277565453 Accuracy 56.60664265706283\n",
      "Training:: Epoch 62, Iteration 100, Current loss 1.3745916215153262 Accuracy 63.65281547991926\n",
      "Training:: Epoch 62, Iteration 110, Current loss 1.4340985431311748 Accuracy 66.28847384651189\n",
      "Training:: Epoch 62, Iteration 120, Current loss 1.2880748071941661 Accuracy 51.349179010219316\n",
      "Training:: Epoch 62, Iteration 130, Current loss 1.2744353059431268 Accuracy 73.27142677876772\n",
      "Training:: Epoch 62, Iteration 140, Current loss 1.4626158908129838 Accuracy 63.44767487918878\n",
      "Training:: Epoch 62, Iteration 150, Current loss 1.5506186884985234 Accuracy 69.57610554605424\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 62, Probability Accuracy 55.41223018602146\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 1.2793981984581393 Accuracy 65.0511766249672\n",
      "Training:: Epoch 63, Iteration 10, Current loss 1.3344575499715325 Accuracy 68.21207798383261\n",
      "Training:: Epoch 63, Iteration 20, Current loss 1.353959882604565 Accuracy 60.3871625063678\n",
      "Training:: Epoch 63, Iteration 30, Current loss 1.5028210540613647 Accuracy 68.82513661202186\n",
      "Training:: Epoch 63, Iteration 40, Current loss 1.4528147772934847 Accuracy 58.84098634867534\n",
      "Training:: Epoch 63, Iteration 50, Current loss 1.4817355695235293 Accuracy 63.797814207650276\n",
      "Training:: Epoch 63, Iteration 60, Current loss 1.5241959889327286 Accuracy 63.59716479621973\n",
      "Training:: Epoch 63, Iteration 70, Current loss 1.1956580949192823 Accuracy 70.10226266205008\n",
      "Training:: Epoch 63, Iteration 80, Current loss 1.408397803705391 Accuracy 60.52317293986208\n",
      "Training:: Epoch 63, Iteration 90, Current loss 1.3054348658360428 Accuracy 61.44929649340235\n",
      "Training:: Epoch 63, Iteration 100, Current loss 1.4600450838769765 Accuracy 61.4122019200991\n",
      "Training:: Epoch 63, Iteration 110, Current loss 1.69944692530234 Accuracy 65.30943235168587\n",
      "Training:: Epoch 63, Iteration 120, Current loss 1.4739372632823662 Accuracy 66.69563058589871\n",
      "Training:: Epoch 63, Iteration 130, Current loss 1.4575376872012857 Accuracy 60.56555269922879\n",
      "Training:: Epoch 63, Iteration 140, Current loss 1.4193686504456067 Accuracy 68.42252994011976\n",
      "Training:: Epoch 63, Iteration 150, Current loss 1.6707656683049963 Accuracy 67.86758164951074\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 63, Probability Accuracy 55.67696896880308\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 1.6147623880523825 Accuracy 75.90412707417387\n",
      "Training:: Epoch 64, Iteration 10, Current loss 1.2350292154705238 Accuracy 64.59846974860156\n",
      "Training:: Epoch 64, Iteration 20, Current loss 1.3741138807939624 Accuracy 63.86458183904748\n",
      "Training:: Epoch 64, Iteration 30, Current loss 1.4222224623171504 Accuracy 67.15954274353876\n",
      "Training:: Epoch 64, Iteration 40, Current loss 1.3744676191332241 Accuracy 66.27381479840496\n",
      "Training:: Epoch 64, Iteration 50, Current loss 1.6945599905543314 Accuracy 56.02144187525553\n",
      "Training:: Epoch 64, Iteration 60, Current loss 1.19521236668081 Accuracy 71.88114285714286\n",
      "Training:: Epoch 64, Iteration 70, Current loss 1.2805291176515428 Accuracy 71.45739651792921\n",
      "Training:: Epoch 64, Iteration 80, Current loss 1.5024219066132078 Accuracy 66.88680585144155\n",
      "Training:: Epoch 64, Iteration 90, Current loss 1.5196944532643575 Accuracy 56.850715746421265\n",
      "Training:: Epoch 64, Iteration 100, Current loss 1.651252335488342 Accuracy 60.559006211180126\n",
      "Training:: Epoch 64, Iteration 110, Current loss 1.4647743218962772 Accuracy 66.1781530918006\n",
      "Training:: Epoch 64, Iteration 120, Current loss 1.4082432104157205 Accuracy 68.93483515918325\n",
      "Training:: Epoch 64, Iteration 130, Current loss 1.4964376964048773 Accuracy 58.96859360266918\n",
      "Training:: Epoch 64, Iteration 140, Current loss 1.413675355888783 Accuracy 66.63021868787277\n",
      "Training:: Epoch 64, Iteration 150, Current loss 1.470922347786387 Accuracy 67.12509712509713\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 64, Probability Accuracy 55.855222272859095\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 1.3017570280377684 Accuracy 63.593802789709926\n",
      "Training:: Epoch 65, Iteration 10, Current loss 1.4511111956387186 Accuracy 64.88681837185689\n",
      "Training:: Epoch 65, Iteration 20, Current loss 1.4405882629296123 Accuracy 49.74384104780149\n",
      "Training:: Epoch 65, Iteration 30, Current loss 1.321470454061469 Accuracy 71.09347233740078\n",
      "Training:: Epoch 65, Iteration 40, Current loss 1.3148504863775279 Accuracy 53.31558710440698\n",
      "Training:: Epoch 65, Iteration 50, Current loss 1.3742121091596102 Accuracy 67.90370158911409\n",
      "Training:: Epoch 65, Iteration 60, Current loss 1.291876096359997 Accuracy 62.4580017683466\n",
      "Training:: Epoch 65, Iteration 70, Current loss 1.6859795196113065 Accuracy 56.4361923236802\n",
      "Training:: Epoch 65, Iteration 80, Current loss 1.68440983573269 Accuracy 58.4077892325315\n",
      "Training:: Epoch 65, Iteration 90, Current loss 1.2700597717138247 Accuracy 68.82937603533959\n",
      "Training:: Epoch 65, Iteration 100, Current loss 1.8437150760232417 Accuracy 56.95513995844029\n",
      "Training:: Epoch 65, Iteration 110, Current loss 1.4054495581090225 Accuracy 65.90136835338414\n",
      "Training:: Epoch 65, Iteration 120, Current loss 1.4732035351693225 Accuracy 69.69126697523015\n",
      "Training:: Epoch 65, Iteration 130, Current loss 1.3774893172128175 Accuracy 58.266256669982816\n",
      "Training:: Epoch 65, Iteration 140, Current loss 1.560564180653649 Accuracy 52.91825933548956\n",
      "Training:: Epoch 65, Iteration 150, Current loss 1.2495813935306541 Accuracy 62.88238542241855\n",
      "Calculating Expectation\n",
      "Epoch 65 iter 0\n",
      "Epoch 65 iter 20\n",
      "Epoch 65 iter 40\n",
      "Epoch 65 iter 60\n",
      "Epoch 65 iter 80\n",
      "Epoch 65 iter 100\n",
      "Epoch 65 iter 120\n",
      "Epoch 65 iter 140\n",
      "Train Boundary avergage error = 301.101\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 55.53797075030037\n",
      "Starting Training\n",
      "Training:: Epoch 66, Iteration 0, Current loss 1.462405023005801 Accuracy 67.75212372032237\n",
      "Training:: Epoch 66, Iteration 10, Current loss 1.7468437635074943 Accuracy 57.33687841333717\n",
      "Training:: Epoch 66, Iteration 20, Current loss 1.6468730355845413 Accuracy 57.027814569536424\n",
      "Training:: Epoch 66, Iteration 30, Current loss 1.416254599748677 Accuracy 70.11494252873563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 66, Iteration 40, Current loss 1.2382077325094163 Accuracy 65.70276299982838\n",
      "Training:: Epoch 66, Iteration 50, Current loss 1.4076736898253008 Accuracy 58.13700177505245\n",
      "Training:: Epoch 66, Iteration 60, Current loss 1.4902532034189133 Accuracy 62.79683377308707\n",
      "Training:: Epoch 66, Iteration 70, Current loss 1.374709933877884 Accuracy 71.96296069172747\n",
      "Training:: Epoch 66, Iteration 80, Current loss 1.5395019351330337 Accuracy 50.08027113806636\n",
      "Training:: Epoch 66, Iteration 90, Current loss 1.4658467826875645 Accuracy 56.03530534351145\n",
      "Training:: Epoch 66, Iteration 100, Current loss 1.1075956534585698 Accuracy 66.44353441486848\n",
      "Training:: Epoch 66, Iteration 110, Current loss 1.418234542505004 Accuracy 59.693537641572284\n",
      "Training:: Epoch 66, Iteration 120, Current loss 1.4125878619670456 Accuracy 70.07865646258503\n",
      "Training:: Epoch 66, Iteration 130, Current loss 1.2954632684325802 Accuracy 71.97683262154098\n",
      "Training:: Epoch 66, Iteration 140, Current loss 1.5344474577276919 Accuracy 58.282772568486855\n",
      "Training:: Epoch 66, Iteration 150, Current loss 1.400463811423034 Accuracy 60.07253357511192\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 66, Probability Accuracy 55.112275759207854\n",
      "Starting Training\n",
      "Training:: Epoch 67, Iteration 0, Current loss 1.1762915212475624 Accuracy 52.074026802807914\n",
      "Training:: Epoch 67, Iteration 10, Current loss 1.3736526091870767 Accuracy 75.83913869537682\n",
      "Training:: Epoch 67, Iteration 20, Current loss 1.7160657864721878 Accuracy 62.416566832106795\n",
      "Training:: Epoch 67, Iteration 30, Current loss 1.3059511224203741 Accuracy 67.17448902346707\n",
      "Training:: Epoch 67, Iteration 40, Current loss 1.708493876148806 Accuracy 65.86821451929366\n",
      "Training:: Epoch 67, Iteration 50, Current loss 1.6550316357445918 Accuracy 58.35421888053467\n",
      "Training:: Epoch 67, Iteration 60, Current loss 1.4976727714096516 Accuracy 56.376211943432345\n",
      "Training:: Epoch 67, Iteration 70, Current loss 1.20782769462783 Accuracy 64.49307977945314\n",
      "Training:: Epoch 67, Iteration 80, Current loss 1.4334066051820964 Accuracy 67.52674047663727\n",
      "Training:: Epoch 67, Iteration 90, Current loss 1.5043811719447175 Accuracy 71.19389648773111\n",
      "Training:: Epoch 67, Iteration 100, Current loss 1.2875419630361997 Accuracy 74.98378939177798\n",
      "Training:: Epoch 67, Iteration 110, Current loss 1.3875201053917117 Accuracy 63.350402340892465\n",
      "Training:: Epoch 67, Iteration 120, Current loss 1.4961059542094173 Accuracy 48.16839803171132\n",
      "Training:: Epoch 67, Iteration 130, Current loss 1.3426718526237709 Accuracy 58.10694529809465\n",
      "Training:: Epoch 67, Iteration 140, Current loss 1.233151339294967 Accuracy 69.79241231209735\n",
      "Training:: Epoch 67, Iteration 150, Current loss 1.4104293497551852 Accuracy 56.32544771169578\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 67, Probability Accuracy 55.282657331068485\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 1.3913412459215884 Accuracy 52.87050544924054\n",
      "Training:: Epoch 68, Iteration 10, Current loss 1.278682899303649 Accuracy 67.81026523037929\n",
      "Training:: Epoch 68, Iteration 20, Current loss 1.3693672859156854 Accuracy 55.32724505327245\n",
      "Training:: Epoch 68, Iteration 30, Current loss 1.5217591277396956 Accuracy 64.51168326755622\n",
      "Training:: Epoch 68, Iteration 40, Current loss 1.3710793065075113 Accuracy 63.93244104525175\n",
      "Training:: Epoch 68, Iteration 50, Current loss 1.4251372518793985 Accuracy 65.71665391481766\n",
      "Training:: Epoch 68, Iteration 60, Current loss 1.6611634141455387 Accuracy 58.93866299104066\n",
      "Training:: Epoch 68, Iteration 70, Current loss 1.4743473675861596 Accuracy 66.6796267496112\n",
      "Training:: Epoch 68, Iteration 80, Current loss 1.451804082343422 Accuracy 56.74418604651163\n",
      "Training:: Epoch 68, Iteration 90, Current loss 1.521323558681613 Accuracy 58.61810691882783\n",
      "Training:: Epoch 68, Iteration 100, Current loss 1.4754596852394921 Accuracy 59.90876837303598\n",
      "Training:: Epoch 68, Iteration 110, Current loss 1.3432312892587435 Accuracy 70.4429710771596\n",
      "Training:: Epoch 68, Iteration 120, Current loss 1.421138053386499 Accuracy 75.96463022508038\n",
      "Training:: Epoch 68, Iteration 130, Current loss 1.5758989218483155 Accuracy 66.28658867663755\n",
      "Training:: Epoch 68, Iteration 140, Current loss 1.431731107998509 Accuracy 64.19878744966147\n",
      "Training:: Epoch 68, Iteration 150, Current loss 1.5014667375195627 Accuracy 58.48947823580283\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 68, Probability Accuracy 55.81876372374363\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 1.4334720496049287 Accuracy 58.15962545981496\n",
      "Training:: Epoch 69, Iteration 10, Current loss 1.3997302316411622 Accuracy 62.37214791502754\n",
      "Training:: Epoch 69, Iteration 20, Current loss 1.4829010695982898 Accuracy 66.88950276243094\n",
      "Training:: Epoch 69, Iteration 30, Current loss 1.4695814540124963 Accuracy 52.1387554201336\n",
      "Training:: Epoch 69, Iteration 40, Current loss 1.2829432533420504 Accuracy 59.05521703624599\n",
      "Training:: Epoch 69, Iteration 50, Current loss 1.359223686653654 Accuracy 69.82136069935386\n",
      "Training:: Epoch 69, Iteration 60, Current loss 1.2220136092445424 Accuracy 70.6929091399299\n",
      "Training:: Epoch 69, Iteration 70, Current loss 1.2311127179151644 Accuracy 70.55250899227549\n",
      "Training:: Epoch 69, Iteration 80, Current loss 1.4136319383735374 Accuracy 54.63420984118719\n",
      "Training:: Epoch 69, Iteration 90, Current loss 1.4447934607626154 Accuracy 56.30298435453856\n",
      "Training:: Epoch 69, Iteration 100, Current loss 1.6919853715078275 Accuracy 49.41066026911442\n",
      "Training:: Epoch 69, Iteration 110, Current loss 1.4619443897136284 Accuracy 57.008910759409495\n",
      "Training:: Epoch 69, Iteration 120, Current loss 1.3790295807000297 Accuracy 48.2920339264926\n",
      "Training:: Epoch 69, Iteration 130, Current loss 1.4133818227031811 Accuracy 61.17763646874226\n",
      "Training:: Epoch 69, Iteration 140, Current loss 1.431986234885943 Accuracy 59.943352501931166\n",
      "Training:: Epoch 69, Iteration 150, Current loss 1.379028134310178 Accuracy 64.76201230873468\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 69, Probability Accuracy 54.825578986618055\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 1.3955001802732352 Accuracy 62.73039392844235\n",
      "Training:: Epoch 70, Iteration 10, Current loss 1.4954805299472698 Accuracy 66.17074527252502\n",
      "Training:: Epoch 70, Iteration 20, Current loss 1.4990604337681082 Accuracy 68.46326416152552\n",
      "Training:: Epoch 70, Iteration 30, Current loss 1.388568136065866 Accuracy 60.11161307467446\n",
      "Training:: Epoch 70, Iteration 40, Current loss 1.6085229934448657 Accuracy 57.41738244285922\n",
      "Training:: Epoch 70, Iteration 50, Current loss 1.349451958155223 Accuracy 60.04177545691906\n",
      "Training:: Epoch 70, Iteration 60, Current loss 1.2826635841417704 Accuracy 67.36826165960024\n",
      "Training:: Epoch 70, Iteration 70, Current loss 1.9093100893877775 Accuracy 53.65531265887138\n",
      "Training:: Epoch 70, Iteration 80, Current loss 1.5236254535959624 Accuracy 68.73101293290513\n",
      "Training:: Epoch 70, Iteration 90, Current loss 1.2968780974615368 Accuracy 61.543235415804716\n",
      "Training:: Epoch 70, Iteration 100, Current loss 1.3488997219207417 Accuracy 62.07142857142857\n",
      "Training:: Epoch 70, Iteration 110, Current loss 1.4474379392946495 Accuracy 43.704518531054326\n",
      "Training:: Epoch 70, Iteration 120, Current loss 1.3060567438706383 Accuracy 57.797991282925906\n",
      "Training:: Epoch 70, Iteration 130, Current loss 1.297765273011373 Accuracy 54.37227023510826\n",
      "Training:: Epoch 70, Iteration 140, Current loss 1.2640875724132061 Accuracy 70.94048766026829\n",
      "Training:: Epoch 70, Iteration 150, Current loss 1.0603703627863745 Accuracy 62.53522519404756\n",
      "Calculating Expectation\n",
      "Epoch 70 iter 0\n",
      "Epoch 70 iter 20\n",
      "Epoch 70 iter 40\n",
      "Epoch 70 iter 60\n",
      "Epoch 70 iter 80\n",
      "Epoch 70 iter 100\n",
      "Epoch 70 iter 120\n",
      "Epoch 70 iter 140\n",
      "Train Boundary avergage error = 305.017\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 55.50379086050462\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 1.337778920592219 Accuracy 63.19720711212146\n",
      "Training:: Epoch 71, Iteration 10, Current loss 1.3935867753608568 Accuracy 65.32618972581153\n",
      "Training:: Epoch 71, Iteration 20, Current loss 1.2240174337060519 Accuracy 70.00435350457118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 71, Iteration 30, Current loss 1.2515707999808747 Accuracy 63.71251013239665\n",
      "Training:: Epoch 71, Iteration 40, Current loss 1.371655434006976 Accuracy 66.63201094391245\n",
      "Training:: Epoch 71, Iteration 50, Current loss 1.6821321220462198 Accuracy 56.90411283631622\n",
      "Training:: Epoch 71, Iteration 60, Current loss 1.4475517778594158 Accuracy 61.04277482269504\n",
      "Training:: Epoch 71, Iteration 70, Current loss 1.6482858240814457 Accuracy 61.304923824243765\n",
      "Training:: Epoch 71, Iteration 80, Current loss 1.5037849875686573 Accuracy 58.78277153558052\n",
      "Training:: Epoch 71, Iteration 90, Current loss 1.2708476099899602 Accuracy 62.03420207602121\n",
      "Training:: Epoch 71, Iteration 100, Current loss 1.389862243251306 Accuracy 63.76065788809848\n",
      "Training:: Epoch 71, Iteration 110, Current loss 1.3826176961553398 Accuracy 64.60393751733062\n",
      "Training:: Epoch 71, Iteration 120, Current loss 1.9497471027756221 Accuracy 49.0605749486653\n",
      "Training:: Epoch 71, Iteration 130, Current loss 1.5853714204761964 Accuracy 64.19171405361494\n",
      "Training:: Epoch 71, Iteration 140, Current loss 1.4625962822066199 Accuracy 62.23038634747571\n",
      "Training:: Epoch 71, Iteration 150, Current loss 1.4825198434777611 Accuracy 63.11137301119623\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 71, Probability Accuracy 55.17504246592369\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 1.3698906085956264 Accuracy 69.97441619944478\n",
      "Training:: Epoch 72, Iteration 10, Current loss 1.2470223181003037 Accuracy 59.90626723028855\n",
      "Training:: Epoch 72, Iteration 20, Current loss 1.4381091449221968 Accuracy 55.6224679732837\n",
      "Training:: Epoch 72, Iteration 30, Current loss 1.3866486431299925 Accuracy 59.53351818626651\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.9896572303969083 Accuracy 63.66750958479484\n",
      "Training:: Epoch 72, Iteration 50, Current loss 1.8592830425559808 Accuracy 53.39565217391304\n",
      "Training:: Epoch 72, Iteration 60, Current loss 1.4780102488324076 Accuracy 69.55581973414027\n",
      "Training:: Epoch 72, Iteration 70, Current loss 1.3625427059683328 Accuracy 63.88918317618392\n",
      "Training:: Epoch 72, Iteration 80, Current loss 1.1209693936622027 Accuracy 70.33976124885216\n",
      "Training:: Epoch 72, Iteration 90, Current loss 1.1809998210836712 Accuracy 58.480395302573314\n",
      "Training:: Epoch 72, Iteration 100, Current loss 1.2776606142444422 Accuracy 60.150570015057\n",
      "Training:: Epoch 72, Iteration 110, Current loss 1.7050060769124125 Accuracy 51.100555579117014\n",
      "Training:: Epoch 72, Iteration 120, Current loss 1.3304555893993364 Accuracy 59.575747635449126\n",
      "Training:: Epoch 72, Iteration 130, Current loss 1.5119196055646247 Accuracy 64.1343419711358\n",
      "Training:: Epoch 72, Iteration 140, Current loss 1.353482706770802 Accuracy 63.796733625448134\n",
      "Training:: Epoch 72, Iteration 150, Current loss 1.3740014072517857 Accuracy 72.85946531791907\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 72, Probability Accuracy 55.59214069685545\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 1.3237957531674454 Accuracy 59.2524120399792\n",
      "Training:: Epoch 73, Iteration 10, Current loss 1.1790258791954566 Accuracy 63.758723828514455\n",
      "Training:: Epoch 73, Iteration 20, Current loss 1.4981821971734395 Accuracy 68.70802579958774\n",
      "Training:: Epoch 73, Iteration 30, Current loss 1.221224856695707 Accuracy 64.85531370038413\n",
      "Training:: Epoch 73, Iteration 40, Current loss 1.4536870815378957 Accuracy 53.085718343867626\n",
      "Training:: Epoch 73, Iteration 50, Current loss 1.528154388845306 Accuracy 52.04902694610779\n",
      "Training:: Epoch 73, Iteration 60, Current loss 1.4203440884179113 Accuracy 65.51914919776235\n",
      "Training:: Epoch 73, Iteration 70, Current loss 1.411393930891981 Accuracy 68.75181844631946\n",
      "Training:: Epoch 73, Iteration 80, Current loss 1.3999424754367404 Accuracy 67.51726784343822\n",
      "Training:: Epoch 73, Iteration 90, Current loss 1.4801367519690398 Accuracy 67.85566498386163\n",
      "Training:: Epoch 73, Iteration 100, Current loss 1.2506896998689558 Accuracy 60.67227577193269\n",
      "Training:: Epoch 73, Iteration 110, Current loss 1.3002640788826236 Accuracy 70.94221372424049\n",
      "Training:: Epoch 73, Iteration 120, Current loss 1.161345328187716 Accuracy 73.69499928493111\n",
      "Training:: Epoch 73, Iteration 130, Current loss 1.4262111224128105 Accuracy 68.5989035678979\n",
      "Training:: Epoch 73, Iteration 140, Current loss 1.5013962999130621 Accuracy 58.80170654795029\n",
      "Training:: Epoch 73, Iteration 150, Current loss 1.1651886524925494 Accuracy 63.4504176483187\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 73, Probability Accuracy 55.154120230351744\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 1.3572816389018283 Accuracy 52.20105650712342\n",
      "Training:: Epoch 74, Iteration 10, Current loss 1.4023282229152194 Accuracy 70.60037433411719\n",
      "Training:: Epoch 74, Iteration 20, Current loss 1.181386972708729 Accuracy 70.19466624915192\n",
      "Training:: Epoch 74, Iteration 30, Current loss 1.458529422958009 Accuracy 63.94826090385677\n",
      "Training:: Epoch 74, Iteration 40, Current loss 1.3013804947801888 Accuracy 61.78798586572438\n",
      "Training:: Epoch 74, Iteration 50, Current loss 1.5762915040042444 Accuracy 57.95893160413772\n",
      "Training:: Epoch 74, Iteration 60, Current loss 1.6023714953776582 Accuracy 66.6641957005189\n",
      "Training:: Epoch 74, Iteration 70, Current loss 1.5473836195463146 Accuracy 52.562885619364025\n",
      "Training:: Epoch 74, Iteration 80, Current loss 1.4289176336028038 Accuracy 70.19753211801148\n",
      "Training:: Epoch 74, Iteration 90, Current loss 1.337821117590468 Accuracy 60.98609860986099\n",
      "Training:: Epoch 74, Iteration 100, Current loss 1.1935585595692437 Accuracy 69.93453700770775\n",
      "Training:: Epoch 74, Iteration 110, Current loss 1.5610905438588867 Accuracy 57.87078618046824\n",
      "Training:: Epoch 74, Iteration 120, Current loss 1.5571255441119634 Accuracy 62.16995165489029\n",
      "Training:: Epoch 74, Iteration 130, Current loss 1.3966798767771227 Accuracy 55.16655336505779\n",
      "Training:: Epoch 74, Iteration 140, Current loss 1.2783360450040544 Accuracy 62.461637861411724\n",
      "Training:: Epoch 74, Iteration 150, Current loss 1.321430744065204 Accuracy 56.18376376170388\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 74, Probability Accuracy 55.23345900484733\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 1.3578556203101826 Accuracy 63.43823391263504\n",
      "Training:: Epoch 75, Iteration 10, Current loss 1.5502069467370037 Accuracy 64.82008249182194\n",
      "Training:: Epoch 75, Iteration 20, Current loss 1.2782355029311026 Accuracy 70.6299977580151\n",
      "Training:: Epoch 75, Iteration 30, Current loss 1.2859679173608978 Accuracy 70.6725029844807\n",
      "Training:: Epoch 75, Iteration 40, Current loss 1.341725456633583 Accuracy 58.30258302583026\n",
      "Training:: Epoch 75, Iteration 50, Current loss 1.2025891643513176 Accuracy 71.65252483443709\n",
      "Training:: Epoch 75, Iteration 60, Current loss 1.5032827882982054 Accuracy 75.66131025957972\n",
      "Training:: Epoch 75, Iteration 70, Current loss 1.2415145923458244 Accuracy 64.91228070175438\n",
      "Training:: Epoch 75, Iteration 80, Current loss 1.351893380354622 Accuracy 68.71227827689472\n",
      "Training:: Epoch 75, Iteration 90, Current loss 1.1048423553664763 Accuracy 65.11985924785573\n",
      "Training:: Epoch 75, Iteration 100, Current loss 1.5092929585392363 Accuracy 59.19233756148071\n",
      "Training:: Epoch 75, Iteration 110, Current loss 1.3106863398539494 Accuracy 65.57594362659597\n",
      "Training:: Epoch 75, Iteration 120, Current loss 1.4008475206234778 Accuracy 59.12338398729871\n",
      "Training:: Epoch 75, Iteration 130, Current loss 1.375618924425533 Accuracy 57.187648456057005\n",
      "Training:: Epoch 75, Iteration 140, Current loss 1.5616187852228847 Accuracy 63.42246690260847\n",
      "Training:: Epoch 75, Iteration 150, Current loss 1.1938245871892335 Accuracy 69.8232973065706\n",
      "Calculating Expectation\n",
      "Epoch 75 iter 0\n",
      "Epoch 75 iter 20\n",
      "Epoch 75 iter 40\n",
      "Epoch 75 iter 60\n",
      "Epoch 75 iter 80\n",
      "Epoch 75 iter 100\n",
      "Epoch 75 iter 120\n",
      "Epoch 75 iter 140\n",
      "Train Boundary avergage error = 304.633\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 55.12377263123006\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 1.4429379126084363 Accuracy 55.399432680255956\n",
      "Training:: Epoch 76, Iteration 10, Current loss 1.353579893791036 Accuracy 71.63150198454147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 76, Iteration 20, Current loss 1.808550389669551 Accuracy 60.04411652848559\n",
      "Training:: Epoch 76, Iteration 30, Current loss 1.5967165778060457 Accuracy 61.38864697965323\n",
      "Training:: Epoch 76, Iteration 40, Current loss 1.5336160286617408 Accuracy 59.900831733845166\n",
      "Training:: Epoch 76, Iteration 50, Current loss 1.5858040725610922 Accuracy 63.164566991522065\n",
      "Training:: Epoch 76, Iteration 60, Current loss 1.7827004834738172 Accuracy 54.77292704365661\n",
      "Training:: Epoch 76, Iteration 70, Current loss 1.580141496744321 Accuracy 60.920703174402426\n",
      "Training:: Epoch 76, Iteration 80, Current loss 1.338232479226444 Accuracy 62.34278775262382\n",
      "Training:: Epoch 76, Iteration 90, Current loss 1.6173819388360842 Accuracy 63.79765395894428\n",
      "Training:: Epoch 76, Iteration 100, Current loss 1.5990633558807827 Accuracy 63.409655662051826\n",
      "Training:: Epoch 76, Iteration 110, Current loss 1.5861587375294062 Accuracy 59.39861766035786\n",
      "Training:: Epoch 76, Iteration 120, Current loss 1.2416859450454112 Accuracy 67.20965563808238\n",
      "Training:: Epoch 76, Iteration 130, Current loss 1.4096439125002835 Accuracy 63.439389259079476\n",
      "Training:: Epoch 76, Iteration 140, Current loss 1.4294703341491313 Accuracy 63.22096181498642\n",
      "Training:: Epoch 76, Iteration 150, Current loss 1.4296007852449046 Accuracy 63.066836641777456\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 76, Probability Accuracy 54.97700625595559\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 1.6779771967124937 Accuracy 59.63788915875469\n",
      "Training:: Epoch 77, Iteration 10, Current loss 1.193541355988213 Accuracy 68.20433601448889\n",
      "Training:: Epoch 77, Iteration 20, Current loss 1.3351606930050026 Accuracy 60.075051933257384\n",
      "Training:: Epoch 77, Iteration 30, Current loss 1.4937971347461112 Accuracy 64.3882156077278\n",
      "Training:: Epoch 77, Iteration 40, Current loss 1.5273089420114703 Accuracy 60.207876418422806\n",
      "Training:: Epoch 77, Iteration 50, Current loss 1.368108472297898 Accuracy 53.751215953307394\n",
      "Training:: Epoch 77, Iteration 60, Current loss 1.5709386128469145 Accuracy 55.937981979056744\n",
      "Training:: Epoch 77, Iteration 70, Current loss 1.2009200041662533 Accuracy 61.454177335794675\n",
      "Training:: Epoch 77, Iteration 80, Current loss 1.5192142740469743 Accuracy 62.340631929046566\n",
      "Training:: Epoch 77, Iteration 90, Current loss 1.886094229930098 Accuracy 64.70748151370161\n",
      "Training:: Epoch 77, Iteration 100, Current loss 1.4514242226513159 Accuracy 65.17179256208982\n",
      "Training:: Epoch 77, Iteration 110, Current loss 2.747841421226258 Accuracy 54.728578383641675\n",
      "Training:: Epoch 77, Iteration 120, Current loss 1.6453814767298232 Accuracy 58.36629652575895\n",
      "Training:: Epoch 77, Iteration 130, Current loss 1.7636330831160716 Accuracy 66.68947316977813\n",
      "Training:: Epoch 77, Iteration 140, Current loss 1.657961265788157 Accuracy 68.11895331471982\n",
      "Training:: Epoch 77, Iteration 150, Current loss 1.4717178939863738 Accuracy 72.52779774235826\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 77, Probability Accuracy 55.85294361353938\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 1.6579973439741902 Accuracy 57.17325227963526\n",
      "Training:: Epoch 78, Iteration 10, Current loss 1.50385234722873 Accuracy 61.08172029625941\n",
      "Training:: Epoch 78, Iteration 20, Current loss 1.5066906385143666 Accuracy 65.91079741702958\n",
      "Training:: Epoch 78, Iteration 30, Current loss 1.7881490217909697 Accuracy 65.707909744607\n",
      "Training:: Epoch 78, Iteration 40, Current loss 1.2850267502854562 Accuracy 56.919182083739045\n",
      "Training:: Epoch 78, Iteration 50, Current loss 1.6651993687763318 Accuracy 63.6083321913115\n",
      "Training:: Epoch 78, Iteration 60, Current loss 1.5733295633382363 Accuracy 64.39572976937495\n",
      "Training:: Epoch 78, Iteration 70, Current loss 1.70110797042137 Accuracy 50.10378057820608\n",
      "Training:: Epoch 78, Iteration 80, Current loss 1.1110539412888667 Accuracy 73.4819064430715\n",
      "Training:: Epoch 78, Iteration 90, Current loss 1.5822922532260093 Accuracy 60.80425452509797\n",
      "Training:: Epoch 78, Iteration 100, Current loss 1.378683179910551 Accuracy 71.926017772267\n",
      "Training:: Epoch 78, Iteration 110, Current loss 1.82189138932405 Accuracy 56.39168765743073\n",
      "Training:: Epoch 78, Iteration 120, Current loss 1.4414914676689212 Accuracy 75.55097837281153\n",
      "Training:: Epoch 78, Iteration 130, Current loss 1.4482313860590237 Accuracy 66.82242990654206\n",
      "Training:: Epoch 78, Iteration 140, Current loss 1.521291197337365 Accuracy 53.100936524453694\n",
      "Training:: Epoch 78, Iteration 150, Current loss 1.4743043001255591 Accuracy 60.69281561052528\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 78, Probability Accuracy 55.622902597671626\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 2.1026200679247333 Accuracy 46.13997113997114\n",
      "Training:: Epoch 79, Iteration 10, Current loss 1.586238481406606 Accuracy 69.3385214007782\n",
      "Training:: Epoch 79, Iteration 20, Current loss 1.492632916502569 Accuracy 69.44036286973743\n",
      "Training:: Epoch 79, Iteration 30, Current loss 1.422378254508772 Accuracy 68.89639164380671\n",
      "Training:: Epoch 79, Iteration 40, Current loss 1.5608591994228758 Accuracy 63.16044889293297\n",
      "Training:: Epoch 79, Iteration 50, Current loss 1.5945024632685043 Accuracy 62.351908731899954\n",
      "Training:: Epoch 79, Iteration 60, Current loss 1.3551691655701417 Accuracy 71.56167284277396\n",
      "Training:: Epoch 79, Iteration 70, Current loss 1.289642198750764 Accuracy 70.83620096352374\n",
      "Training:: Epoch 79, Iteration 80, Current loss 1.4792739034711448 Accuracy 61.2023045983259\n",
      "Training:: Epoch 79, Iteration 90, Current loss 1.5503667427041106 Accuracy 68.78271817639653\n",
      "Training:: Epoch 79, Iteration 100, Current loss 1.6242468088190267 Accuracy 61.54092926268513\n",
      "Training:: Epoch 79, Iteration 110, Current loss 1.2689962229836267 Accuracy 63.45630480798307\n",
      "Training:: Epoch 79, Iteration 120, Current loss 1.4863722931811867 Accuracy 67.2404489531621\n",
      "Training:: Epoch 79, Iteration 130, Current loss 1.4036285686728522 Accuracy 65.90932509015971\n",
      "Training:: Epoch 79, Iteration 140, Current loss 1.5014228059471357 Accuracy 55.87219816956212\n",
      "Training:: Epoch 79, Iteration 150, Current loss 1.4475515574506292 Accuracy 58.94634620053411\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 79, Probability Accuracy 54.79398848241289\n",
      "Starting Training\n",
      "Training:: Epoch 80, Iteration 0, Current loss 1.534731097166372 Accuracy 58.52945864102428\n",
      "Training:: Epoch 80, Iteration 10, Current loss 1.40182457692681 Accuracy 62.31670724112551\n",
      "Training:: Epoch 80, Iteration 20, Current loss 1.4089167529571942 Accuracy 68.88872716687287\n",
      "Training:: Epoch 80, Iteration 30, Current loss 1.5718394686881565 Accuracy 62.08127954934111\n",
      "Training:: Epoch 80, Iteration 40, Current loss 1.4128070843710034 Accuracy 71.00250626566417\n",
      "Training:: Epoch 80, Iteration 50, Current loss 1.8641941003363423 Accuracy 59.320918602355725\n",
      "Training:: Epoch 80, Iteration 60, Current loss 1.8096606542589413 Accuracy 65.51924755163385\n",
      "Training:: Epoch 80, Iteration 70, Current loss 1.2289588135555032 Accuracy 69.46225304199922\n",
      "Training:: Epoch 80, Iteration 80, Current loss 1.6471865662092493 Accuracy 69.03255735492577\n",
      "Training:: Epoch 80, Iteration 90, Current loss 1.4485026872184543 Accuracy 67.60172626387177\n",
      "Training:: Epoch 80, Iteration 100, Current loss 1.3795373884501432 Accuracy 56.67417981467568\n",
      "Training:: Epoch 80, Iteration 110, Current loss 1.509670087303883 Accuracy 70.91911965485228\n",
      "Training:: Epoch 80, Iteration 120, Current loss 1.5284972756189377 Accuracy 63.782926055220564\n",
      "Training:: Epoch 80, Iteration 130, Current loss 1.4799437799636077 Accuracy 65.64380264741276\n",
      "Training:: Epoch 80, Iteration 140, Current loss 1.5197255948789363 Accuracy 58.39856145546859\n",
      "Training:: Epoch 80, Iteration 150, Current loss 1.6280236669885153 Accuracy 53.44485585823856\n",
      "Calculating Expectation\n",
      "Epoch 80 iter 0\n",
      "Epoch 80 iter 20\n",
      "Epoch 80 iter 40\n",
      "Epoch 80 iter 60\n",
      "Epoch 80 iter 80\n",
      "Epoch 80 iter 100\n",
      "Epoch 80 iter 120\n",
      "Epoch 80 iter 140\n",
      "Train Boundary avergage error = 303.798\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 80, Probability Accuracy 54.589737747027385\n",
      "Starting Training\n",
      "Training:: Epoch 81, Iteration 0, Current loss 1.6174070636033633 Accuracy 63.09302553066615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 81, Iteration 10, Current loss 1.1629801563514075 Accuracy 62.18761656098471\n",
      "Training:: Epoch 81, Iteration 20, Current loss 1.3346428243860524 Accuracy 68.43265081259756\n",
      "Training:: Epoch 81, Iteration 30, Current loss 1.855269693156351 Accuracy 60.766569071067345\n",
      "Training:: Epoch 81, Iteration 40, Current loss 2.0579570489118724 Accuracy 45.59561036049325\n",
      "Training:: Epoch 81, Iteration 50, Current loss 1.3842321339858348 Accuracy 66.43310178193899\n",
      "Training:: Epoch 81, Iteration 60, Current loss 1.5061085621327026 Accuracy 65.62912365132345\n",
      "Training:: Epoch 81, Iteration 70, Current loss 1.5649958950127205 Accuracy 69.5739837398374\n",
      "Training:: Epoch 81, Iteration 80, Current loss 1.4182230188342884 Accuracy 57.25722241559345\n",
      "Training:: Epoch 81, Iteration 90, Current loss 1.4703700765844345 Accuracy 67.46501789781972\n",
      "Training:: Epoch 81, Iteration 100, Current loss 1.754672470482662 Accuracy 64.32218903555255\n",
      "Training:: Epoch 81, Iteration 110, Current loss 1.3532304969709774 Accuracy 71.01797075566469\n",
      "Training:: Epoch 81, Iteration 120, Current loss 1.5847937149225073 Accuracy 67.57580051005951\n",
      "Training:: Epoch 81, Iteration 130, Current loss 1.7361757205632762 Accuracy 54.98044437047516\n",
      "Training:: Epoch 81, Iteration 140, Current loss 1.4064631030189565 Accuracy 59.98494967547738\n",
      "Training:: Epoch 81, Iteration 150, Current loss 1.6055250521248015 Accuracy 60.89668615984405\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 81, Probability Accuracy 54.63624311223433\n",
      "Starting Training\n",
      "Training:: Epoch 82, Iteration 0, Current loss 1.5427336370599911 Accuracy 60.85089773614364\n",
      "Training:: Epoch 82, Iteration 10, Current loss 1.2356296928712565 Accuracy 55.914686869518675\n",
      "Training:: Epoch 82, Iteration 20, Current loss 1.6779596875505507 Accuracy 63.452083153627186\n",
      "Training:: Epoch 82, Iteration 30, Current loss 1.425598357453885 Accuracy 72.09592616408821\n",
      "Training:: Epoch 82, Iteration 40, Current loss 1.5127254476945882 Accuracy 65.4772477926963\n",
      "Training:: Epoch 82, Iteration 50, Current loss 1.7501389159465792 Accuracy 57.835078307519254\n",
      "Training:: Epoch 82, Iteration 60, Current loss 1.3625152413108337 Accuracy 61.78101554854844\n",
      "Training:: Epoch 82, Iteration 70, Current loss 1.431387485172704 Accuracy 65.15601187134034\n",
      "Training:: Epoch 82, Iteration 80, Current loss 1.506532873633108 Accuracy 61.60908560208035\n",
      "Training:: Epoch 82, Iteration 90, Current loss 1.385385084230902 Accuracy 65.49059542350545\n",
      "Training:: Epoch 82, Iteration 100, Current loss 1.4438735665440015 Accuracy 61.648774891420146\n",
      "Training:: Epoch 82, Iteration 110, Current loss 1.7211749160741612 Accuracy 68.86481957991742\n",
      "Training:: Epoch 82, Iteration 120, Current loss 1.6677221049674258 Accuracy 49.3026810163663\n",
      "Training:: Epoch 82, Iteration 130, Current loss 1.6471044428055224 Accuracy 59.300510783200906\n",
      "Training:: Epoch 82, Iteration 140, Current loss 1.5836408495907373 Accuracy 73.27466419638722\n",
      "Training:: Epoch 82, Iteration 150, Current loss 1.305550565165508 Accuracy 64.95598426671661\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 82, Probability Accuracy 53.96517794257778\n",
      "Starting Training\n",
      "Training:: Epoch 83, Iteration 0, Current loss 1.5874271818445251 Accuracy 63.09494144031896\n",
      "Training:: Epoch 83, Iteration 10, Current loss 1.5001486574240115 Accuracy 62.65612802498048\n",
      "Training:: Epoch 83, Iteration 20, Current loss 1.3638950770745817 Accuracy 55.35153250017454\n",
      "Training:: Epoch 83, Iteration 30, Current loss 1.3970925918250465 Accuracy 68.54547943955092\n",
      "Training:: Epoch 83, Iteration 40, Current loss 1.4843928460592428 Accuracy 68.57806887809001\n",
      "Training:: Epoch 83, Iteration 50, Current loss 1.2985522141384955 Accuracy 74.33612959213191\n",
      "Training:: Epoch 83, Iteration 60, Current loss 1.3164069185463514 Accuracy 72.5053659357771\n",
      "Training:: Epoch 83, Iteration 70, Current loss 1.6296662941573836 Accuracy 60.83127092735478\n",
      "Training:: Epoch 83, Iteration 80, Current loss 1.2322152298756501 Accuracy 66.44788146607746\n",
      "Training:: Epoch 83, Iteration 90, Current loss 1.5013506940790808 Accuracy 54.933979715506794\n",
      "Training:: Epoch 83, Iteration 100, Current loss 1.4320544508039592 Accuracy 49.7147277826129\n",
      "Training:: Epoch 83, Iteration 110, Current loss 1.6452024848736468 Accuracy 66.2930525933657\n",
      "Training:: Epoch 83, Iteration 120, Current loss 1.7077071762214815 Accuracy 58.24329034506797\n",
      "Training:: Epoch 83, Iteration 130, Current loss 1.3198428018932566 Accuracy 58.740790354989954\n",
      "Training:: Epoch 83, Iteration 140, Current loss 1.6379924821072496 Accuracy 59.60513945471639\n",
      "Training:: Epoch 83, Iteration 150, Current loss 1.2206517054866681 Accuracy 50.130036410194855\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 83, Probability Accuracy 46.73457761942247\n",
      "Starting Training\n",
      "Training:: Epoch 84, Iteration 0, Current loss 3.818456000395116 Accuracy 51.49527995964546\n",
      "Training:: Epoch 84, Iteration 10, Current loss 2.1537130095163186 Accuracy 50.85751548744702\n",
      "Training:: Epoch 84, Iteration 20, Current loss 3.6666262057534373 Accuracy 50.185303514377\n",
      "Training:: Epoch 84, Iteration 30, Current loss 2.8276799829393657 Accuracy 69.2946605141727\n",
      "Training:: Epoch 84, Iteration 40, Current loss 6.84097411790728 Accuracy 55.05793450881612\n",
      "Training:: Epoch 84, Iteration 50, Current loss 7.203866441738227 Accuracy 45.93610111471956\n",
      "Training:: Epoch 84, Iteration 60, Current loss 3.9291480670927887 Accuracy 55.11123431977268\n",
      "Training:: Epoch 84, Iteration 70, Current loss 5.2112871419923055 Accuracy 42.13557820115197\n",
      "Training:: Epoch 84, Iteration 80, Current loss 2.75862399723519 Accuracy 52.47918062601704\n",
      "Training:: Epoch 84, Iteration 90, Current loss 3.8144354131198104 Accuracy 52.012696580580005\n",
      "Training:: Epoch 84, Iteration 100, Current loss 3.934753486906681 Accuracy 60.00807645712747\n",
      "Training:: Epoch 84, Iteration 110, Current loss 5.1392292954775485 Accuracy 48.2903981264637\n",
      "Training:: Epoch 84, Iteration 120, Current loss 3.9519629838527974 Accuracy 53.98371104815864\n",
      "Training:: Epoch 84, Iteration 130, Current loss 3.6545644492911653 Accuracy 57.15813440044156\n",
      "Training:: Epoch 84, Iteration 140, Current loss 2.132705932707785 Accuracy 59.28930088837389\n",
      "Training:: Epoch 84, Iteration 150, Current loss 3.418554761052143 Accuracy 58.22811012212792\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 84, Probability Accuracy 52.632576542238056\n",
      "Starting Training\n",
      "Training:: Epoch 85, Iteration 0, Current loss 2.8011158928182174 Accuracy 58.196280126786675\n",
      "Training:: Epoch 85, Iteration 10, Current loss 2.102742356823901 Accuracy 63.124925426560075\n",
      "Training:: Epoch 85, Iteration 20, Current loss 2.6826916423593503 Accuracy 59.355140812094454\n",
      "Training:: Epoch 85, Iteration 30, Current loss 1.9004847100803635 Accuracy 63.133309378368665\n",
      "Training:: Epoch 85, Iteration 40, Current loss 2.7207934721298823 Accuracy 64.92369276957719\n",
      "Training:: Epoch 85, Iteration 50, Current loss 1.678458447823538 Accuracy 63.100557256890404\n",
      "Training:: Epoch 85, Iteration 60, Current loss 1.9290484266122763 Accuracy 65.39216235914111\n",
      "Training:: Epoch 85, Iteration 70, Current loss 1.714506277669332 Accuracy 63.48937406206129\n",
      "Training:: Epoch 85, Iteration 80, Current loss 2.172585999575566 Accuracy 58.16122584943371\n",
      "Training:: Epoch 85, Iteration 90, Current loss 1.5673891240311373 Accuracy 62.69552054717722\n",
      "Training:: Epoch 85, Iteration 100, Current loss 1.8604024369140866 Accuracy 65.23730987722192\n",
      "Training:: Epoch 85, Iteration 110, Current loss 1.4265601730233963 Accuracy 58.34256767452905\n",
      "Training:: Epoch 85, Iteration 120, Current loss 1.7780044651830644 Accuracy 57.346289160663424\n",
      "Training:: Epoch 85, Iteration 130, Current loss 1.786753805361573 Accuracy 70.3255168157976\n",
      "Training:: Epoch 85, Iteration 140, Current loss 1.3424190823893989 Accuracy 72.46151673492444\n",
      "Training:: Epoch 85, Iteration 150, Current loss 1.7403667376762169 Accuracy 51.33890132598588\n",
      "Calculating Expectation\n",
      "Epoch 85 iter 0\n",
      "Epoch 85 iter 20\n",
      "Epoch 85 iter 40\n",
      "Epoch 85 iter 60\n",
      "Epoch 85 iter 80\n",
      "Epoch 85 iter 100\n",
      "Epoch 85 iter 120\n",
      "Epoch 85 iter 140\n",
      "Train Boundary avergage error = 300.297\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 85, Probability Accuracy 53.66294485644446\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 86, Iteration 0, Current loss 1.6765314169062755 Accuracy 71.87411397788489\n",
      "Training:: Epoch 86, Iteration 10, Current loss 1.5384611911372383 Accuracy 68.5643436482926\n",
      "Training:: Epoch 86, Iteration 20, Current loss 1.8819612901414624 Accuracy 63.897978825794034\n",
      "Training:: Epoch 86, Iteration 30, Current loss 1.4216701314877365 Accuracy 58.88117159317289\n",
      "Training:: Epoch 86, Iteration 40, Current loss 1.3440762546493816 Accuracy 66.71924290220821\n",
      "Training:: Epoch 86, Iteration 50, Current loss 1.5055665510865965 Accuracy 64.4927536231884\n",
      "Training:: Epoch 86, Iteration 60, Current loss 1.917860044209297 Accuracy 68.03199043276777\n",
      "Training:: Epoch 86, Iteration 70, Current loss 1.460301774734901 Accuracy 71.86244576570786\n",
      "Training:: Epoch 86, Iteration 80, Current loss 1.6163152901429174 Accuracy 61.348990405269944\n",
      "Training:: Epoch 86, Iteration 90, Current loss 1.5759791832574006 Accuracy 57.18875911664202\n",
      "Training:: Epoch 86, Iteration 100, Current loss 1.6137277541012824 Accuracy 60.90129045334775\n",
      "Training:: Epoch 86, Iteration 110, Current loss 1.5015227481376434 Accuracy 57.16183307504981\n",
      "Training:: Epoch 86, Iteration 120, Current loss 1.1947099388949565 Accuracy 67.73323053199691\n",
      "Training:: Epoch 86, Iteration 130, Current loss 1.5082534484709853 Accuracy 57.9153413261561\n",
      "Training:: Epoch 86, Iteration 140, Current loss 1.6327707198930568 Accuracy 56.383680391972106\n",
      "Training:: Epoch 86, Iteration 150, Current loss 1.4530998426616777 Accuracy 57.40125222230811\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 86, Probability Accuracy 53.751916145337034\n",
      "Starting Training\n",
      "Training:: Epoch 87, Iteration 0, Current loss 1.6094503609842916 Accuracy 67.00677461996034\n",
      "Training:: Epoch 87, Iteration 10, Current loss 1.502086937321328 Accuracy 59.08214031881409\n",
      "Training:: Epoch 87, Iteration 20, Current loss 1.549446920606763 Accuracy 67.91268606976227\n",
      "Training:: Epoch 87, Iteration 30, Current loss 1.4448384520621087 Accuracy 63.53542261737137\n",
      "Training:: Epoch 87, Iteration 40, Current loss 1.5451685895178136 Accuracy 65.21956921021874\n",
      "Training:: Epoch 87, Iteration 50, Current loss 1.8372033058351864 Accuracy 68.18978467701552\n",
      "Training:: Epoch 87, Iteration 60, Current loss 1.53559927514825 Accuracy 66.29762760603882\n",
      "Training:: Epoch 87, Iteration 70, Current loss 1.50770471844096 Accuracy 56.847722882493535\n",
      "Training:: Epoch 87, Iteration 80, Current loss 1.3755425151129006 Accuracy 62.31661310605002\n",
      "Training:: Epoch 87, Iteration 90, Current loss 1.478413373917298 Accuracy 69.16402924806286\n",
      "Training:: Epoch 87, Iteration 100, Current loss 1.4205491888838977 Accuracy 58.05688622754491\n",
      "Training:: Epoch 87, Iteration 110, Current loss 1.2276298019216143 Accuracy 61.822229289235175\n",
      "Training:: Epoch 87, Iteration 120, Current loss 1.417484243851909 Accuracy 66.09363264624885\n",
      "Training:: Epoch 87, Iteration 130, Current loss 1.4976181397361579 Accuracy 59.969712266532056\n",
      "Training:: Epoch 87, Iteration 140, Current loss 1.643826139035364 Accuracy 70.13601105089789\n",
      "Training:: Epoch 87, Iteration 150, Current loss 1.6107431187422627 Accuracy 63.638565891472865\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 87, Probability Accuracy 53.59044205990803\n",
      "Starting Training\n",
      "Training:: Epoch 88, Iteration 0, Current loss 1.3364719139409367 Accuracy 68.53135833595967\n",
      "Training:: Epoch 88, Iteration 10, Current loss 1.4493261973136002 Accuracy 71.01797606529468\n",
      "Training:: Epoch 88, Iteration 20, Current loss 1.6087707541656122 Accuracy 67.61924484020156\n",
      "Training:: Epoch 88, Iteration 30, Current loss 1.3440559330287247 Accuracy 63.36888933825388\n",
      "Training:: Epoch 88, Iteration 40, Current loss 1.25420898821169 Accuracy 71.46206045650833\n",
      "Training:: Epoch 88, Iteration 50, Current loss 1.6948184892111855 Accuracy 74.38874948940888\n",
      "Training:: Epoch 88, Iteration 60, Current loss 1.3666738103401257 Accuracy 56.41717791411043\n",
      "Training:: Epoch 88, Iteration 70, Current loss 1.531134595318178 Accuracy 69.02301918789058\n",
      "Training:: Epoch 88, Iteration 80, Current loss 1.3676322696954109 Accuracy 71.80334434765194\n",
      "Training:: Epoch 88, Iteration 90, Current loss 1.6168713345816552 Accuracy 66.9383003492433\n",
      "Training:: Epoch 88, Iteration 100, Current loss 1.5096803148157054 Accuracy 58.779011099899094\n",
      "Training:: Epoch 88, Iteration 110, Current loss 1.3956734612964719 Accuracy 65.86310176053766\n",
      "Training:: Epoch 88, Iteration 120, Current loss 1.4912395871492203 Accuracy 50.74626865671642\n",
      "Training:: Epoch 88, Iteration 130, Current loss 1.414521488721219 Accuracy 57.33751962323391\n",
      "Training:: Epoch 88, Iteration 140, Current loss 1.3601765918239184 Accuracy 67.31083287969516\n",
      "Training:: Epoch 88, Iteration 150, Current loss 1.3145632076879976 Accuracy 66.018813113847\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 88, Probability Accuracy 53.594585076852965\n",
      "Starting Training\n",
      "Training:: Epoch 89, Iteration 0, Current loss 1.588347600813079 Accuracy 68.83397961944269\n",
      "Training:: Epoch 89, Iteration 10, Current loss 1.3597038554818697 Accuracy 70.57640076451061\n",
      "Training:: Epoch 89, Iteration 20, Current loss 1.2996626524763621 Accuracy 72.63157894736842\n",
      "Training:: Epoch 89, Iteration 30, Current loss 1.3968406048577109 Accuracy 62.76742330843426\n",
      "Training:: Epoch 89, Iteration 40, Current loss 1.4451013595849211 Accuracy 65.50137362637362\n",
      "Training:: Epoch 89, Iteration 50, Current loss 1.4518630421905825 Accuracy 74.10174518079161\n",
      "Training:: Epoch 89, Iteration 60, Current loss 1.4586945410516365 Accuracy 65.96244131455398\n",
      "Training:: Epoch 89, Iteration 70, Current loss 1.5926386338991945 Accuracy 65.68797888094778\n",
      "Training:: Epoch 89, Iteration 80, Current loss 1.3762167830566687 Accuracy 66.0295346893285\n",
      "Training:: Epoch 89, Iteration 90, Current loss 1.6518722217613055 Accuracy 73.62719462084422\n",
      "Training:: Epoch 89, Iteration 100, Current loss 1.6164424881876573 Accuracy 54.244174973017586\n",
      "Training:: Epoch 89, Iteration 110, Current loss 1.4910755852200175 Accuracy 55.699950649777925\n",
      "Training:: Epoch 89, Iteration 120, Current loss 1.4130749154758544 Accuracy 66.87258687258688\n",
      "Training:: Epoch 89, Iteration 130, Current loss 1.4307190572855941 Accuracy 61.52825683205094\n",
      "Training:: Epoch 89, Iteration 140, Current loss 1.6013272984232292 Accuracy 58.37282146012368\n",
      "Training:: Epoch 89, Iteration 150, Current loss 1.324169108443795 Accuracy 67.34966208944944\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 89, Probability Accuracy 53.54435099639557\n",
      "Starting Training\n",
      "Training:: Epoch 90, Iteration 0, Current loss 1.5685209727028129 Accuracy 68.03204548391265\n",
      "Training:: Epoch 90, Iteration 10, Current loss 1.4609631188526229 Accuracy 55.602419668128086\n",
      "Training:: Epoch 90, Iteration 20, Current loss 1.5836335876704004 Accuracy 65.81355381113846\n",
      "Training:: Epoch 90, Iteration 30, Current loss 1.3356039435752889 Accuracy 53.35149497191673\n",
      "Training:: Epoch 90, Iteration 40, Current loss 1.4088841598580488 Accuracy 60.79955580233204\n",
      "Training:: Epoch 90, Iteration 50, Current loss 1.6247138186512124 Accuracy 68.3457586137417\n",
      "Training:: Epoch 90, Iteration 60, Current loss 1.4078448897296845 Accuracy 73.19420665401879\n",
      "Training:: Epoch 90, Iteration 70, Current loss 1.4184742950428644 Accuracy 61.22994652406417\n",
      "Training:: Epoch 90, Iteration 80, Current loss 1.5269815437176193 Accuracy 62.682301209234154\n",
      "Training:: Epoch 90, Iteration 90, Current loss 1.5155795823755775 Accuracy 66.8229372285827\n",
      "Training:: Epoch 90, Iteration 100, Current loss 1.3834714224582436 Accuracy 60.89461901998176\n",
      "Training:: Epoch 90, Iteration 110, Current loss 1.3191098581009855 Accuracy 65.86481208021722\n",
      "Training:: Epoch 90, Iteration 120, Current loss 1.5018989365117263 Accuracy 68.374336710082\n",
      "Training:: Epoch 90, Iteration 130, Current loss 1.4457264452795269 Accuracy 62.75249392029381\n",
      "Training:: Epoch 90, Iteration 140, Current loss 1.4195053336986485 Accuracy 59.97122450028262\n",
      "Training:: Epoch 90, Iteration 150, Current loss 1.607084584798136 Accuracy 55.994431877501306\n",
      "Calculating Expectation\n",
      "Epoch 90 iter 0\n",
      "Epoch 90 iter 20\n",
      "Epoch 90 iter 40\n",
      "Epoch 90 iter 60\n",
      "Epoch 90 iter 80\n",
      "Epoch 90 iter 100\n",
      "Epoch 90 iter 120\n",
      "Epoch 90 iter 140\n",
      "Train Boundary avergage error = 300.711\n",
      "Calculating Validation Data Accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:: Epoch 90, Probability Accuracy 53.5735592658574\n",
      "Starting Training\n",
      "Training:: Epoch 91, Iteration 0, Current loss 1.4586029702415733 Accuracy 63.04373592509186\n",
      "Training:: Epoch 91, Iteration 10, Current loss 1.276841766120526 Accuracy 70.43877497782647\n",
      "Training:: Epoch 91, Iteration 20, Current loss 1.3757950726915564 Accuracy 60.37549947999343\n",
      "Training:: Epoch 91, Iteration 30, Current loss 1.2929358567587124 Accuracy 68.9336066082852\n",
      "Training:: Epoch 91, Iteration 40, Current loss 1.4876570899518797 Accuracy 68.77937253905753\n",
      "Training:: Epoch 91, Iteration 50, Current loss 1.744646564530509 Accuracy 50.37065196730659\n",
      "Training:: Epoch 91, Iteration 60, Current loss 1.3554900811240678 Accuracy 67.79814301217894\n",
      "Training:: Epoch 91, Iteration 70, Current loss 1.5655974392648018 Accuracy 59.761581093799016\n",
      "Training:: Epoch 91, Iteration 80, Current loss 1.5113465499375724 Accuracy 60.743729170321\n",
      "Training:: Epoch 91, Iteration 90, Current loss 1.3887440580451 Accuracy 67.34402852049911\n",
      "Training:: Epoch 91, Iteration 100, Current loss 1.4709074996059093 Accuracy 70.569965583816\n",
      "Training:: Epoch 91, Iteration 110, Current loss 1.5013157394925731 Accuracy 61.548643282594306\n",
      "Training:: Epoch 91, Iteration 120, Current loss 1.3465295487675129 Accuracy 72.2593291709017\n",
      "Training:: Epoch 91, Iteration 130, Current loss 1.330344630697503 Accuracy 68.79596250901226\n",
      "Training:: Epoch 91, Iteration 140, Current loss 1.14289872161696 Accuracy 60.5878382524391\n",
      "Training:: Epoch 91, Iteration 150, Current loss 1.511176162976445 Accuracy 62.96219789299732\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 91, Probability Accuracy 54.28864399055392\n",
      "Starting Training\n",
      "Training:: Epoch 92, Iteration 0, Current loss 1.6623231340666453 Accuracy 64.28933027604896\n",
      "Training:: Epoch 92, Iteration 10, Current loss 1.3439990346890034 Accuracy 56.324611863672914\n",
      "Training:: Epoch 92, Iteration 20, Current loss 1.3259604774201075 Accuracy 67.2978685925665\n",
      "Training:: Epoch 92, Iteration 30, Current loss 1.3062513198670156 Accuracy 64.93190972444216\n",
      "Training:: Epoch 92, Iteration 40, Current loss 1.2987520535270587 Accuracy 61.493716032608695\n",
      "Training:: Epoch 92, Iteration 50, Current loss 1.512041109875672 Accuracy 59.7632189424846\n",
      "Training:: Epoch 92, Iteration 60, Current loss 1.5347662695039361 Accuracy 61.61762925293619\n",
      "Training:: Epoch 92, Iteration 70, Current loss 1.2759004767519209 Accuracy 63.65871912755266\n",
      "Training:: Epoch 92, Iteration 80, Current loss 1.4956310032066882 Accuracy 58.13480186759248\n",
      "Training:: Epoch 92, Iteration 90, Current loss 1.4332376781295424 Accuracy 67.24771805273834\n",
      "Training:: Epoch 92, Iteration 100, Current loss 1.3813601570075063 Accuracy 64.48231822785925\n",
      "Training:: Epoch 92, Iteration 110, Current loss 1.4888971067379737 Accuracy 57.825405867274284\n",
      "Training:: Epoch 92, Iteration 120, Current loss 1.3980233322649178 Accuracy 70.0180342651037\n",
      "Training:: Epoch 92, Iteration 130, Current loss 1.1294794515533257 Accuracy 67.72748703441773\n",
      "Training:: Epoch 92, Iteration 140, Current loss 1.429039323340946 Accuracy 63.602850823298105\n",
      "Training:: Epoch 92, Iteration 150, Current loss 1.2401401932066634 Accuracy 67.32705636197201\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 92, Probability Accuracy 53.76786676057505\n",
      "Starting Training\n",
      "Training:: Epoch 93, Iteration 0, Current loss 1.3171206248850877 Accuracy 61.49795456523631\n",
      "Training:: Epoch 93, Iteration 10, Current loss 1.4432717021381463 Accuracy 61.95310265942236\n",
      "Training:: Epoch 93, Iteration 20, Current loss 1.4508909516610167 Accuracy 66.78981139412794\n",
      "Training:: Epoch 93, Iteration 30, Current loss 1.0322755617109363 Accuracy 64.63286099074833\n",
      "Training:: Epoch 93, Iteration 40, Current loss 1.2766360510333217 Accuracy 60.56671318487956\n",
      "Training:: Epoch 93, Iteration 50, Current loss 1.3402364925558503 Accuracy 66.17024183631644\n",
      "Training:: Epoch 93, Iteration 60, Current loss 1.3523948195380746 Accuracy 72.73666092943202\n",
      "Training:: Epoch 93, Iteration 70, Current loss 1.364787014083578 Accuracy 64.14864257749824\n",
      "Training:: Epoch 93, Iteration 80, Current loss 1.413401057932874 Accuracy 61.23946527172333\n",
      "Training:: Epoch 93, Iteration 90, Current loss 1.483773891587386 Accuracy 60.231403666497286\n",
      "Training:: Epoch 93, Iteration 100, Current loss 1.4608789427031286 Accuracy 76.09756097560975\n",
      "Training:: Epoch 93, Iteration 110, Current loss 1.572464776346129 Accuracy 53.16653635652854\n",
      "Training:: Epoch 93, Iteration 120, Current loss 1.3008982236379012 Accuracy 70.97601051593821\n",
      "Training:: Epoch 93, Iteration 130, Current loss 1.3146825007969432 Accuracy 69.86473501367433\n",
      "Training:: Epoch 93, Iteration 140, Current loss 1.3894377720782125 Accuracy 70.22422648127271\n",
      "Training:: Epoch 93, Iteration 150, Current loss 1.2912901517126731 Accuracy 63.46116283612214\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 93, Probability Accuracy 54.03415917471102\n",
      "Starting Training\n",
      "Training:: Epoch 94, Iteration 0, Current loss 1.5631466564007224 Accuracy 67.94171220400729\n",
      "Training:: Epoch 94, Iteration 10, Current loss 1.595779448821611 Accuracy 54.125728892084034\n",
      "Training:: Epoch 94, Iteration 20, Current loss 1.3028395581430146 Accuracy 61.706928590390234\n",
      "Training:: Epoch 94, Iteration 30, Current loss 1.5329137595712496 Accuracy 56.41508288299229\n",
      "Training:: Epoch 94, Iteration 40, Current loss 1.325763074008934 Accuracy 59.73420048070126\n",
      "Training:: Epoch 94, Iteration 50, Current loss 1.2199363028074335 Accuracy 62.97269228419574\n",
      "Training:: Epoch 94, Iteration 60, Current loss 1.2185546779084964 Accuracy 70.39786567683564\n",
      "Training:: Epoch 94, Iteration 70, Current loss 1.3335523291467173 Accuracy 62.79813845258872\n",
      "Training:: Epoch 94, Iteration 80, Current loss 1.4560082569919732 Accuracy 61.62505887894489\n",
      "Training:: Epoch 94, Iteration 90, Current loss 1.5203281649955795 Accuracy 60.21997490219237\n",
      "Training:: Epoch 94, Iteration 100, Current loss 1.6292698909918144 Accuracy 60.33471452402733\n",
      "Training:: Epoch 94, Iteration 110, Current loss 1.3133439401070013 Accuracy 63.22196859652763\n",
      "Training:: Epoch 94, Iteration 120, Current loss 1.2394622509007474 Accuracy 69.42348616753723\n",
      "Training:: Epoch 94, Iteration 130, Current loss 1.3199971155339938 Accuracy 68.69810787913019\n",
      "Training:: Epoch 94, Iteration 140, Current loss 1.3354230321817402 Accuracy 60.77618432678584\n",
      "Training:: Epoch 94, Iteration 150, Current loss 1.4739306472617004 Accuracy 59.91239763854504\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 94, Probability Accuracy 54.131727223764344\n",
      "Starting Training\n",
      "Training:: Epoch 95, Iteration 0, Current loss 1.2303364697256107 Accuracy 74.27450980392157\n",
      "Training:: Epoch 95, Iteration 10, Current loss 1.4846561456639877 Accuracy 67.6466566388623\n",
      "Training:: Epoch 95, Iteration 20, Current loss 1.5593233177289563 Accuracy 68.48786854125882\n",
      "Training:: Epoch 95, Iteration 30, Current loss 1.4427381168568523 Accuracy 72.04856265699135\n",
      "Training:: Epoch 95, Iteration 40, Current loss 1.4909107087137543 Accuracy 64.96688741721854\n",
      "Training:: Epoch 95, Iteration 50, Current loss 1.1433720484616603 Accuracy 67.04099261818753\n",
      "Training:: Epoch 95, Iteration 60, Current loss 1.5529698298259782 Accuracy 71.86534898085237\n",
      "Training:: Epoch 95, Iteration 70, Current loss 1.2438486902658352 Accuracy 58.96888346552776\n",
      "Training:: Epoch 95, Iteration 80, Current loss 1.2088431128206978 Accuracy 56.28449914034929\n",
      "Training:: Epoch 95, Iteration 90, Current loss 1.2995313735717957 Accuracy 62.79104928938615\n",
      "Training:: Epoch 95, Iteration 100, Current loss 1.4563285742102188 Accuracy 62.780269058295964\n",
      "Training:: Epoch 95, Iteration 110, Current loss 1.5257919819055716 Accuracy 46.76215563604434\n",
      "Training:: Epoch 95, Iteration 120, Current loss 1.4498756449328638 Accuracy 63.28547763666483\n",
      "Training:: Epoch 95, Iteration 130, Current loss 1.2716890954912392 Accuracy 57.42117067593122\n",
      "Training:: Epoch 95, Iteration 140, Current loss 1.392274417129982 Accuracy 57.71485345255837\n",
      "Training:: Epoch 95, Iteration 150, Current loss 1.4988454449088529 Accuracy 48.50837307308417\n",
      "Calculating Expectation\n",
      "Epoch 95 iter 0\n",
      "Epoch 95 iter 20\n",
      "Epoch 95 iter 40\n",
      "Epoch 95 iter 60\n",
      "Epoch 95 iter 80\n",
      "Epoch 95 iter 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 iter 120\n",
      "Epoch 95 iter 140\n",
      "Train Boundary avergage error = 301.082\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 95, Probability Accuracy 53.960206322243856\n",
      "Starting Training\n",
      "Training:: Epoch 96, Iteration 0, Current loss 1.1911894923971653 Accuracy 67.38725173872517\n",
      "Training:: Epoch 96, Iteration 10, Current loss 1.382753995154627 Accuracy 58.22444273635665\n",
      "Training:: Epoch 96, Iteration 20, Current loss 1.2372252650864755 Accuracy 64.1248206599713\n",
      "Training:: Epoch 96, Iteration 30, Current loss 1.2637789735392162 Accuracy 62.41968257455105\n",
      "Training:: Epoch 96, Iteration 40, Current loss 1.3326667470937592 Accuracy 59.28501247071272\n",
      "Training:: Epoch 96, Iteration 50, Current loss 1.3528790694952888 Accuracy 59.37553173387783\n",
      "Training:: Epoch 96, Iteration 60, Current loss 1.364871702597676 Accuracy 61.975374732334046\n",
      "Training:: Epoch 96, Iteration 70, Current loss 1.4859395469197436 Accuracy 61.7142232425932\n",
      "Training:: Epoch 96, Iteration 80, Current loss 1.4890291911207718 Accuracy 65.29457399717063\n",
      "Training:: Epoch 96, Iteration 90, Current loss 1.4636074159246406 Accuracy 56.492922938665465\n",
      "Training:: Epoch 96, Iteration 100, Current loss 1.4992395011850834 Accuracy 65.93688480582234\n",
      "Training:: Epoch 96, Iteration 110, Current loss 1.685067187534043 Accuracy 56.12517355548435\n",
      "Training:: Epoch 96, Iteration 120, Current loss 1.2011016201980713 Accuracy 73.32439678284182\n",
      "Training:: Epoch 96, Iteration 130, Current loss 1.489890895661376 Accuracy 63.48694316436252\n",
      "Training:: Epoch 96, Iteration 140, Current loss 1.4917376157598112 Accuracy 71.77995880285957\n",
      "Training:: Epoch 96, Iteration 150, Current loss 1.4424730578761755 Accuracy 55.84098263223932\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 96, Probability Accuracy 54.335770808302605\n",
      "Starting Training\n",
      "Training:: Epoch 97, Iteration 0, Current loss 1.5514523118467842 Accuracy 69.17123124553466\n",
      "Training:: Epoch 97, Iteration 10, Current loss 1.5837967549923675 Accuracy 63.177858949143015\n",
      "Training:: Epoch 97, Iteration 20, Current loss 1.3411259169509888 Accuracy 62.784597628790266\n",
      "Training:: Epoch 97, Iteration 30, Current loss 1.5748369948214438 Accuracy 66.2269129287599\n",
      "Training:: Epoch 97, Iteration 40, Current loss 1.4097990504483682 Accuracy 65.68033550792171\n",
      "Training:: Epoch 97, Iteration 50, Current loss 1.4651828763951338 Accuracy 67.55338123630433\n",
      "Training:: Epoch 97, Iteration 60, Current loss 1.2691225924258203 Accuracy 61.0977866100185\n",
      "Training:: Epoch 97, Iteration 70, Current loss 1.4833334450946611 Accuracy 61.807991785744846\n",
      "Training:: Epoch 97, Iteration 80, Current loss 1.5964678263699394 Accuracy 60.19455252918288\n",
      "Training:: Epoch 97, Iteration 90, Current loss 1.3232609856420843 Accuracy 54.33360588716271\n",
      "Training:: Epoch 97, Iteration 100, Current loss 1.52903010295489 Accuracy 67.28374564170679\n",
      "Training:: Epoch 97, Iteration 110, Current loss 1.2414972569615441 Accuracy 63.85129347267432\n",
      "Training:: Epoch 97, Iteration 120, Current loss 1.4698184513616623 Accuracy 58.53769645433149\n",
      "Training:: Epoch 97, Iteration 130, Current loss 1.474365361185296 Accuracy 61.3267191907402\n",
      "Training:: Epoch 97, Iteration 140, Current loss 1.5707414229568233 Accuracy 73.64300415674563\n",
      "Training:: Epoch 97, Iteration 150, Current loss 1.5426301923464085 Accuracy 56.0816931565511\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 97, Probability Accuracy 54.2510461117786\n",
      "Starting Training\n",
      "Training:: Epoch 98, Iteration 0, Current loss 1.533433087193428 Accuracy 66.61011721159778\n",
      "Training:: Epoch 98, Iteration 10, Current loss 1.4409346914949746 Accuracy 69.68361581920904\n",
      "Training:: Epoch 98, Iteration 20, Current loss 1.3135626575335033 Accuracy 58.406528374758615\n",
      "Training:: Epoch 98, Iteration 30, Current loss 1.3759937901785153 Accuracy 63.132487822003775\n",
      "Training:: Epoch 98, Iteration 40, Current loss 1.6579620282097678 Accuracy 54.44792132320072\n",
      "Training:: Epoch 98, Iteration 50, Current loss 1.5041849584561557 Accuracy 64.340906469842\n",
      "Training:: Epoch 98, Iteration 60, Current loss 1.5523874594082223 Accuracy 55.038022813688215\n",
      "Training:: Epoch 98, Iteration 70, Current loss 1.365998132827845 Accuracy 64.26428303537942\n",
      "Training:: Epoch 98, Iteration 80, Current loss 1.4555443768282337 Accuracy 64.05964834186513\n",
      "Training:: Epoch 98, Iteration 90, Current loss 1.4012544363903308 Accuracy 44.86052248719084\n",
      "Training:: Epoch 98, Iteration 100, Current loss 1.475461656119778 Accuracy 61.1577964519141\n",
      "Training:: Epoch 98, Iteration 110, Current loss 1.305235397157614 Accuracy 50.733819026501955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-c983d94f141d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mes_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initialize_epoch = 25\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0\n",
    "for epoch in range(26, 1000):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        psuedo_l = get_single_random(predictions[-1], item[4])\n",
    "        \n",
    "        loss = 0\n",
    "        for p in predictions:\n",
    "            if epoch <= initialize_epoch:\n",
    "                loss += ce_criterion(p, psuedo_l)\n",
    "                loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(p[:, :, 1:], dim=1), \n",
    "                                                                    F.log_softmax(p.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                            max=16) * src_mask_mse[:, :, 1:])\n",
    "            else:\n",
    "                prob = torch.softmax(p, dim=1)\n",
    "                prob = prob.permute(0, 2, 1)\n",
    "                es_loss, _ = get_estimated_loss(prob, item_1, item[4], item_2)\n",
    "                loss += es_loss\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(predictions[-1], dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "    if epoch == initialize_epoch:\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-initial-15-epochs.wt\")\n",
    "    # Calculating Expectation Step\n",
    "    model.eval()\n",
    "    if (epoch >= initialize_epoch) and (epoch % expectation_cal_gap == 0):\n",
    "        print(\"Calculating Expectation\")\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        for i, item in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                item_0 = item[0].to(device)\n",
    "                item_1 = item[1].to(device)\n",
    "                item_2 = item[2].to(device)\n",
    "                src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "                src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "                middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "                prob = torch.softmax(predictions[-1], dim=1)\n",
    "                prob = prob.permute(0, 2, 1)\n",
    "                for j in range(len(prob)):\n",
    "                    generate_posterior_loss_weights(prob[j].cpu(), item[4][j])\n",
    "                \n",
    "                if i % 20 == 0:\n",
    "                    print(f\"Epoch {epoch} iter {i}\")\n",
    "                    \n",
    "        get_boundary_err()\n",
    "\n",
    "    print(\"Calculating Validation Data Accuracy\")\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, item in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            item_0 = item[0].to(device)\n",
    "            item_1 = item[1].to(device)\n",
    "            item_2 = item[2].to(device)\n",
    "            src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "            src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "            middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "            pred = torch.argmax(predictions[-1], dim=1)\n",
    "            correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "            total += float(torch.sum(src_mask).item())\n",
    "    val_acc = correct * 100.0 / total\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-emmax-best-model-fulllikelihood.wt\")\n",
    "    torch.save(model.state_dict(), config.output_dir + \"ms-tcn-emmax-last-model-fulllikelihood.wt\")\n",
    "    print(f\"Validation:: Epoch {epoch}, Probability Accuracy {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(f'/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/'\n",
    "#                                  f'mstcnnew-full-supervised-split1/ms-tcn-best-model.wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.49465550814103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 84, 234, 1087, 1197, 1292]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vidid = list(posterior_table.keys())[np.random.randint(len(posterior_table))]\n",
    "\n",
    "for proba in posterior_table[vidid]:\n",
    "    plt.plot(proba)\n",
    "    \n",
    "for proba in prior_table[vidid]:\n",
    "    plt.plot(2*proba)\n",
    "    \n",
    "for bd in video_id_boundary_frames[vidid]:\n",
    "    plt.plot([bd, bd], [0, 2])\n",
    "print(video_id_boundary_frames[vidid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 69.60005698208626\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "\n",
    "print(f\"Validation:: Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_proba = prob[j].cpu()\n",
    "ele = item[4][j]\n",
    "frame_count = frame_id_dict[ele]\n",
    "label_list = [label_name_to_label_id_dict[item] for item in weak_labels[ele]]\n",
    "seqlen = len(label_list)\n",
    "label_seq_prob = framewise_proba[:frame_count,label_list]\n",
    "label_seq_logprob = torch.log(label_seq_prob + 1e-10)\n",
    "\n",
    "left_probsum = torch.cumsum(label_seq_prob, dim=1)\n",
    "right_probsum = torch.sum(label_seq_prob, dim=1, keepdim=True) - torch.cumsum(label_seq_prob, dim=1) + label_seq_prob\n",
    "\n",
    "left_logprobsum = torch.log(torch.clip(left_probsum, min=1e-20))\n",
    "right_logprobsum = torch.log(torch.clip(right_probsum, min=1e-20))\n",
    "\n",
    "left_logprobsum_left_idsum = torch.cumsum(left_logprobsum, dim=0)\n",
    "left_logprobsum_left_idsum -= left_logprobsum\n",
    "right_logprobsum_right_idsum = torch.sum(right_logprobsum, dim=0, keepdim=True) - torch.cumsum(right_logprobsum, dim=0)\n",
    "\n",
    "for k in range(seqlen):\n",
    "    left_logprobsum_left_idsum[:k,k] = -1000\n",
    "    remaining_labelcount = seqlen-k-1\n",
    "    if remaining_labelcount > 0:\n",
    "        right_logprobsum_right_idsum[-remaining_labelcount:,k] = -1000\n",
    "\n",
    "loglikelihood = label_seq_logprob + left_logprobsum_left_idsum + right_logprobsum_right_idsum\n",
    "posterior = loglikelihood.T + torch.log(torch.clip(torch.tensor(prior_table[ele]), min=1e-10))\n",
    "posterior = torch.softmax(posterior, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration done\n",
      "11 iteration done\n",
      "21 iteration done\n",
      "31 iteration done\n",
      "41 iteration done\n",
      "51 iteration done\n",
      "61 iteration done\n",
      "71 iteration done\n",
      "81 iteration done\n",
      "91 iteration done\n",
      "101 iteration done\n",
      "111 iteration done\n",
      "121 iteration done\n",
      "131 iteration done\n",
      "141 iteration done\n",
      "151 iteration done\n",
      "161 iteration done\n",
      "171 iteration done\n",
      "181 iteration done\n",
      "Train Boundary avergage error = 118.393\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        for j in range(len(prob)):\n",
    "            generate_posterior_loss_weights(prob[j].cpu(), item[4][j])\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f'{i+1} iteration done')\n",
    "get_boundary_err()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
