{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mstcn_model import *\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utility.adaptive_data_loader import BreakfastWithWeights, collate_fn_override_wtd\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 8, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 4, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-random-select4/split4/', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split4.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split4.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split4_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=4,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-random-select4/\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "config.output_dir = config.output_dir + f\"split{config.split}\"\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "config.output_dir = config.output_dir + \"/\"\n",
    "if not os.path.exists(os.path.join(config.output_dir, \"posterior_weights\")):\n",
    "    os.mkdir(os.path.join(config.output_dir, \"posterior_weights\"))\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1136\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 576\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = BreakfastWithWeights(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override_wtd(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "\n",
    "trainloder_expectation = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=20,\n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override_wtd(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.label_id_csv)\n",
    "label_id_to_label_name = {}\n",
    "label_name_to_label_id_dict = {}\n",
    "for i, ele in df.iterrows():\n",
    "    label_id_to_label_name[ele.label_id] = ele.label_name\n",
    "    label_name_to_label_id_dict[ele.label_name] = ele.label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_frames_dict = pickle.load(open(\"data/breakfast_len_assum_annotations.pkl\", 'rb'))\n",
    "# loaded_vidid_selected_frames\n",
    "boundary_frames_dict = pickle.load(open(\"data/breakfast_boundary_annotations.pkl\", \"rb\"))\n",
    "num_boundary = 0\n",
    "for key in boundary_frames_dict.keys():\n",
    "    num_boundary += len(boundary_frames_dict[key])\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frames_dict = pickle.load(open(\"data/breakfast_random4frame_selection.pkl\", \"rb\"))\n",
    "# print(selected_frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_var_actions = pickle.load(open(\"data/breakfast_meanvar_actions.pkl\", \"rb\"))\n",
    "mat_poisson = pickle.load(open(\"data/breakfast_possion_class_dict.pkl\", \"rb\"))\n",
    "\n",
    "def get_possion_prob(minlen, maxlen, cur_class):\n",
    "    prob = mat_poisson[label_id_to_label_name[cur_class]][minlen:maxlen]\n",
    "    return torch.tensor(prob)\n",
    "\n",
    "def get_poisson_logcdf(minlen, cur_class):\n",
    "    return np.log(np.sum(np.exp(mat_poisson[label_id_to_label_name[cur_class]][minlen:])) + 1e-20)\n",
    "\n",
    "def get_possion_prob_for_all_class(minlen, maxlen):\n",
    "    ele_list = []\n",
    "    for i in range(config.num_class):\n",
    "        prob = mat_poisson[label_id_to_label_name[i]][minlen:maxlen]\n",
    "        ele_list.append(torch.tensor(prob))\n",
    "    return torch.stack(ele_list, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, best_val_acc=None):\n",
    "    model.eval()\n",
    "    print(\"Calculating Validation Data Accuracy\")\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, item in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            item_0 = item[0].to(device)\n",
    "            item_1 = item[1].to(device)\n",
    "            item_2 = item[2].to(device)\n",
    "            src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "            src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "            middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "            pred = torch.argmax(predictions[-1], dim=1)\n",
    "            correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "            total += float(torch.sum(src_mask).item())\n",
    "    val_acc = correct * 100.0 / total\n",
    "    if best_val_acc is not None and val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-emmax-best-model.wt\")\n",
    "    torch.save(model.state_dict(), config.output_dir + \"ms-tcn-emmax-last-model.wt\")\n",
    "    print(f\"Validation:: Probability Accuracy {val_acc}\")\n",
    "    _ = model.train()\n",
    "    return val_acc, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vals_per_segment(selected_frames, cur_vid_feat, labels, first_ele_flag, last_ele_flag, vidid, gt_labels):\n",
    "    prob_each_segment = []\n",
    "    LOW_VAL = -10000000\n",
    "    num_frames = len(cur_vid_feat)\n",
    "    log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "    cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "    prev_boundary = 0\n",
    "    per_frame_weights = torch.zeros((num_frames, config.num_class))\n",
    "    start_time = time()\n",
    "    boundary_error = 0\n",
    "    current_boundary = 0\n",
    "    labels = [config.num_class-1] + labels if selected_frames[0] != 0 else labels\n",
    "    labels = labels + [config.num_class-1] if selected_frames[-1] != num_frames-1 else labels\n",
    "    selected_frames = [0] + selected_frames if selected_frames[0] != 0 else selected_frames\n",
    "    selected_frames = selected_frames + [num_frames-1] if selected_frames[-1] != num_frames-1 else selected_frames\n",
    "\n",
    "    for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "        next_ele = selected_frames[i + 1]\n",
    "        label_cur_ele = labels[i]\n",
    "        label_next_ele = labels[i + 1]\n",
    "        if cur_ele == next_ele-1:\n",
    "            per_frame_weights[cur_ele, label_cur_ele] = 1.0\n",
    "            if label_cur_ele != label_next_ele:\n",
    "                prev_boundary = cur_ele\n",
    "            continue\n",
    "        \n",
    "        seg_len = next_ele - cur_ele\n",
    "        mat_b1_b2_c_prob = LOW_VAL * torch.ones((seg_len, seg_len, config.num_class), dtype=cumsum_feat.dtype)\n",
    "        b1_prior = get_possion_prob(cur_ele-prev_boundary, next_ele-prev_boundary, label_cur_ele)\n",
    "        \n",
    "        # find dummy label where we will keep the diagonal (b1=b2) probabilities, later we will distribute among\n",
    "        # rest of the classes after the softmax by dividing by (num_class - 2)\n",
    "        dummy_label = 0\n",
    "        while True:\n",
    "            if dummy_label != label_cur_ele and dummy_label != label_next_ele:\n",
    "                break\n",
    "            else:\n",
    "                dummy_label += 1\n",
    "        \n",
    "        for b1 in range(cur_ele, next_ele - 1):\n",
    "\n",
    "            cur_boundary_len = b1 - prev_boundary\n",
    "            strt_index = cumsum_feat[cur_ele - 1, label_cur_ele] if cur_ele > 0 else 0\n",
    "            left_sum = (cumsum_feat[b1, label_cur_ele] - strt_index)\n",
    "            right_sum = cumsum_feat[next_ele-1, label_next_ele] - cumsum_feat[b1+1:next_ele, label_next_ele] # mid_seg_len\n",
    "            mid_sum = (cumsum_feat[b1+1:next_ele, :] - cumsum_feat[b1, :])  # mid_seg_len\n",
    "            b2_prior = get_possion_prob_for_all_class(1, next_ele-b1)  # mid_seg_len x num_class\n",
    "            \n",
    "            mat_b1_b2_c_prob[b1-cur_ele, b1+1-cur_ele:next_ele-cur_ele] = (left_sum + right_sum[:,None] + mid_sum) \\\n",
    "                                                                            + b1_prior[b1-cur_ele] + b2_prior\n",
    "            # when mid segment is absent but right and left is not the same\n",
    "            # we assign the probability to a dummy label for now and then later \n",
    "            # re-distribute among other classes after the softmax\n",
    "            if label_cur_ele != label_next_ele:\n",
    "                rightsum_wo_midseg = cumsum_feat[next_ele-1, label_next_ele] - cumsum_feat[b1, label_next_ele]\n",
    "                mat_b1_b2_c_prob[b1-cur_ele, b1-cur_ele, dummy_label] = left_sum + rightsum_wo_midseg + b1_prior[b1-cur_ele]\n",
    "        \n",
    "#         if vidid=='P33_cam01_P33_coffee' and cur_ele==346:\n",
    "#             import pdb\n",
    "#             pdb.set_trace()\n",
    "        # when mid segment is absent b1 can also be next_ele-1\n",
    "        b1 = next_ele - 1\n",
    "        if label_cur_ele != label_next_ele:\n",
    "            left_sum = (cumsum_feat[b1, label_cur_ele] - strt_index)\n",
    "            mat_b1_b2_c_prob[b1-cur_ele, b1-cur_ele, dummy_label] = left_sum + b1_prior[b1-cur_ele]\n",
    "        else:\n",
    "            # returns prob that the left class length >= seg len\n",
    "            b1_prior_ = get_poisson_logcdf(next_ele - prev_boundary, label_cur_ele) \n",
    "            mat_b1_b2_c_prob[b1-cur_ele, b1-cur_ele, dummy_label] = left_sum + b1_prior_\n",
    "        \n",
    "        mat_b1_b2_c_prob[:, :, label_cur_ele] = LOW_VAL\n",
    "        mat_b1_b2_c_prob[:, :, label_next_ele] = LOW_VAL\n",
    "        mat_b1_b2_c_prob = torch.softmax(mat_b1_b2_c_prob.flatten(), dim=0).reshape((seg_len, seg_len, config.num_class))\n",
    "        \n",
    "        # re-distribute the dummy class probability among the left-over classes\n",
    "        left_over_classes = config.num_class - 2 + (label_cur_ele==label_next_ele)\n",
    "        for b1 in range(cur_ele, next_ele):\n",
    "            assigned_prob = mat_b1_b2_c_prob[b1-cur_ele, b1-cur_ele, dummy_label]\n",
    "            mat_b1_b2_c_prob[b1-cur_ele, b1-cur_ele, :] = assigned_prob/left_over_classes\n",
    "            mat_b1_b2_c_prob[b1-cur_ele, b1-cur_ele, label_cur_ele] = 0\n",
    "            mat_b1_b2_c_prob[b1-cur_ele, b1-cur_ele, label_next_ele] = 0\n",
    "        \n",
    "        marginal_b1 = torch.sum(mat_b1_b2_c_prob, axis=(1,2))\n",
    "        mean_b1 = round(torch.sum(marginal_b1.squeeze() * torch.arange(cur_ele, next_ele, 1)).item())\n",
    "        cumm_b1_prob = torch.cumsum(marginal_b1, dim=0)\n",
    "        cumm_b1_c_prob = torch.cumsum(torch.sum(mat_b1_b2_c_prob, dim=1), dim=0)\n",
    "        cumm_b2_c_prob = torch.cumsum(torch.sum(mat_b1_b2_c_prob, dim=0), dim=0)\n",
    "\n",
    "        per_frame_weights[cur_ele, label_cur_ele] = 1.0\n",
    "        per_frame_weights[cur_ele+1:next_ele, :] = cumm_b1_c_prob[:-1] - cumm_b2_c_prob[:-1]\n",
    "        per_frame_weights[cur_ele+1:next_ele, label_cur_ele] = 1 - cumm_b1_prob[:-1]\n",
    "        per_frame_weights[cur_ele+1:next_ele, label_next_ele] = 0\n",
    "        remaining_probability = 1 - torch.sum(per_frame_weights[cur_ele+1:next_ele, :], dim=-1)\n",
    "        # we use \"+=\" in the next line because left and right label might be the same\n",
    "        # in that case using \"=\" would just overwrite the previous probability\n",
    "        per_frame_weights[cur_ele+1:next_ele, label_next_ele] += remaining_probability\n",
    "        \n",
    "        expected_boundary = round(torch.sum(torch.sum(mat_b1_b2_c_prob, axis=(0,2)).squeeze() * \\\n",
    "                            torch.arange(cur_ele, next_ele, 1)).item())\n",
    "        if not (label_cur_ele == label_next_ele and expected_boundary >= next_ele-2):\n",
    "            prev_boundary = expected_boundary\n",
    "        if expected_boundary == 0 and i > 0:\n",
    "            print(f'Estimated boundary has become zero! for {vidid} and cur_ele, next_ele {cur_ele, next_ele}')\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        # boundary_error += (boundary_frames_dict[vidid + '.txt'][current_boundary] - mean_b1)**2\n",
    "        # boundary_error += (boundary_frames_dict[vidid + '.txt'][current_boundary+1] - prev_boundary)**2\n",
    "        # current_boundary += 2\n",
    "        # prob_each_segment.append(mat_b1_b2_c_prob)\n",
    "        \n",
    "    posterior_prediction = torch.argmax(per_frame_weights, dim=1)\n",
    "    correct = torch.sum(posterior_prediction == gt_labels[:num_frames]).item()\n",
    "    \n",
    "    return (vidid, per_frame_weights, [correct, num_frames, boundary_error]) #, prob_each_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_acc_correct, posterior_acc_total = 0, 0\n",
    "posterior_boundary_total_mse = 0\n",
    "results = []\n",
    "\n",
    "# Step 2: Define callback function to collect the output in `results`\n",
    "def collect_result(result):\n",
    "    global posterior_acc_correct, posterior_acc_total, posterior_boundary_total_mse\n",
    "    fname = os.path.join(config.output_dir, 'posterior_weights', result[0] + '.wt')\n",
    "    torch.save(result[1], fname)\n",
    "    correct, total, boundary_err = result[2]\n",
    "    posterior_acc_correct += correct\n",
    "    posterior_acc_total += total\n",
    "    posterior_boundary_total_mse += boundary_err\n",
    "    # print(f'Dumped in file {fname} at time {time()}')\n",
    "    return\n",
    "\n",
    "def calculate_element_probb(data_feat, data_count, video_ids, gt_labels): # loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global posterior_acc_correct, posterior_acc_total, posterior_boundary_total_mse\n",
    "    pool = mp.Pool(20)\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "#         if cur_vidid!='P33_cam01_P33_coffee':\n",
    "#             continue\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num][:cur_vid_count].detach().cpu()\n",
    "        cur_gt_labels = gt_labels[iter_num].detach().cpu()\n",
    "        \n",
    "        cur_video_select_frames = selected_frames_dict[cur_vidid + \".txt\"]\n",
    "        selected_frames_indices_and_labels = cur_video_select_frames\n",
    "        selected_frames_indices = [ele[0] for ele in selected_frames_indices_and_labels]\n",
    "        selected_frames_labels = [label_name_to_label_id_dict[ele[1]] for ele in selected_frames_indices_and_labels]\n",
    "        with torch.no_grad():\n",
    "            # Multi-processing\n",
    "            pool.apply_async(prob_vals_per_segment,\n",
    "                             args=(selected_frames_indices, cur_vid_feat, selected_frames_labels,\n",
    "                                   cur_video_select_frames[1], cur_video_select_frames[2], cur_vidid, cur_gt_labels),\n",
    "                             callback=collect_result)\n",
    "#             results.append(prob_vals_per_segment(selected_frames_indices, cur_vid_feat, selected_frames_labels,\n",
    "#                                    cur_video_select_frames[1], cur_video_select_frames[2], cur_vidid, cur_gt_labels))\n",
    "    # Step 4: Close Pool and let all the processes complete\n",
    "    pool.close()\n",
    "    pool.join()  # postpones the execution of next line of code until all processes in the queue are done.\n",
    "    return results\n",
    "\n",
    "def perform_expectation(model, dataloader):\n",
    "    global posterior_acc_correct, posterior_acc_total, posterior_boundary_total_mse\n",
    "    posterior_acc_correct, posterior_acc_total, posterior_boundary_total_mse = 0, 0, 0\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    curtime = time()\n",
    "    print(f'Calculating expectation')\n",
    "\n",
    "    for i, item in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            item_0 = item[0].to(device) # features\n",
    "            item_1 = item[1].to(device) # count\n",
    "            item_2 = item[2].to(device) # gt frame-wise labels\n",
    "            item_4 = item[4] # video-ids\n",
    "            src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "            src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "            middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "            prob = torch.softmax(predictions[-1], dim=1)\n",
    "            prob = prob.permute(0, 2, 1)\n",
    "            \n",
    "            calculate_element_probb(prob, item_1, item_4, item_2)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"iter {i+1} of Expectation completed in a total of {(time() - curtime)/60.: .1f} minutes\")\n",
    "    _ = model.train()\n",
    "    print(f'Expectation step finished, '\n",
    "          f'posterior frame-wise accuracy {100*posterior_acc_correct/posterior_acc_total: .2f}%, '\n",
    "          f'boundary mse {(posterior_boundary_total_mse/num_boundary)**0.5: .2f}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = MultiStageModel(num_stages=4, num_layers=10, num_f_maps=64, dim=2048, num_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 54.8859796469258\n"
     ]
    }
   ],
   "source": [
    "loaded_file=torch.load(os.path.join(config.output_dir, \"ms-tcn-initial-30-epochs.wt\"))\n",
    "model.load_state_dict(loaded_file)\n",
    "# loaded_file=torch.load('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split1/ms-tcn-best-model.wt')\n",
    "# model.load_state_dict(loaded_file)\n",
    "_ = validate(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = next(iter(trainloader))\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     item_0 = item[0].to(device) # features\n",
    "#     item_1 = item[1].to(device) # count\n",
    "#     item_2 = item[2].to(device) # gt frame-wise labels\n",
    "#     item_4 = item[4] # video-ids\n",
    "#     src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "#     src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "#     middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "#     prob = torch.softmax(predictions[-1], dim=1)\n",
    "#     prob = prob.permute(0, 2, 1)\n",
    "\n",
    "#     res = calculate_element_probb(prob, item_1, item_4, item_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 2\n",
    "# vidid = res[idx][0]\n",
    "# mat = res[idx][1]\n",
    "# mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linspace(0, 5281, 4 + 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary_frames_dict[f'{vidid}.txt'], selected_frames_dict[f'{vidid}.txt'], weakly_labels[f'{vidid}.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(20, 5))\n",
    "# for i in range(48):\n",
    "#     plt.plot(mat[:,i])\n",
    "    \n",
    "# for bd in boundary_frames_dict[f'{vidid}.txt']:\n",
    "#     plt.plot([bd, bd], [0, 2])\n",
    "    \n",
    "# for bd in selected_frames_dict[f'{vidid}.txt']:\n",
    "#     plt.plot([bd[0], bd[0]], [0, 2], '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating Expectation Step\n",
    "# perform_expectation(model, trainloder_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(video_ids, len_frames, device):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((len(video_ids), len_frames), dtype=torch.long, device=device) * (-100)\n",
    "    for iter_num, cur_vidid in enumerate(video_ids):\n",
    "        selected_frames_indices_and_labels = selected_frames_dict[cur_vidid + \".txt\"]\n",
    "        selected_frames_indices = [ele[0] for ele in selected_frames_indices_and_labels]\n",
    "        selected_frames_labels = [label_name_to_label_id_dict[ele[1]] for ele in selected_frames_indices_and_labels]\n",
    "\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(selected_frames_indices))\n",
    "        frame_labels = torch.from_numpy(np.array(selected_frames_labels)).to(device)\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = frame_labels\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weakly_labels = pickle.load(open(\"data/breakfast_weaklysupervised_labels.pkl\", \"rb\"))\n",
    "prior_probs = pickle.load(open('data/breakfast_lengthmodel_multinomial_prior.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch 31: Iteration 20 with loss 0.7871538400650024\n",
      "Epoch 31: Iteration 40 with loss 1.4430299997329712\n",
      "Epoch 31: Iteration 60 with loss 1.254341959953308\n",
      "Epoch 31: Iteration 80 with loss 2.1584863662719727\n",
      "Epoch 31: Iteration 100 with loss 1.2598638534545898\n",
      "Epoch 31: Iteration 120 with loss 1.1599088907241821\n",
      "Epoch 31: Iteration 140 with loss 0.6828204393386841\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  6.9 minutes\n",
      "iter 20 of Expectation completed in a total of  13.0 minutes\n",
      "iter 30 of Expectation completed in a total of  18.2 minutes\n",
      "iter 40 of Expectation completed in a total of  25.0 minutes\n",
      "iter 50 of Expectation completed in a total of  29.8 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  76.04%, boundary mse  0.00\n",
      "Epoch 31 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 54.80117517107955\n",
      "Starting Training\n",
      "Epoch 32: Iteration 20 with loss 4.410118103027344\n",
      "Epoch 32: Iteration 40 with loss 2.175783157348633\n",
      "Epoch 32: Iteration 60 with loss 4.707254409790039\n",
      "Epoch 32: Iteration 80 with loss 3.0687198638916016\n",
      "Epoch 32: Iteration 100 with loss 1.8836946487426758\n",
      "Epoch 32: Iteration 120 with loss 1.5820960998535156\n",
      "Epoch 32: Iteration 140 with loss 2.9259893894195557\n",
      "Epoch 32 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.691886875077614\n",
      "Starting Training\n",
      "Epoch 33: Iteration 20 with loss 2.880369186401367\n",
      "Epoch 33: Iteration 40 with loss 0.976061999797821\n",
      "Epoch 33: Iteration 60 with loss 2.2054216861724854\n",
      "Epoch 33: Iteration 80 with loss 1.5670547485351562\n",
      "Epoch 33: Iteration 100 with loss 2.3839504718780518\n",
      "Epoch 33: Iteration 120 with loss 2.6137263774871826\n",
      "Epoch 33: Iteration 140 with loss 2.593898057937622\n",
      "Epoch 33 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.241400270590006\n",
      "Starting Training\n",
      "Epoch 34: Iteration 20 with loss 1.0785064697265625\n",
      "Epoch 34: Iteration 40 with loss 1.7978370189666748\n",
      "Epoch 34: Iteration 60 with loss 3.457850456237793\n",
      "Epoch 34: Iteration 80 with loss 2.367048740386963\n",
      "Epoch 34: Iteration 100 with loss 3.6211183071136475\n",
      "Epoch 34: Iteration 120 with loss 1.987981915473938\n",
      "Epoch 34: Iteration 140 with loss 1.5078024864196777\n",
      "Epoch 34 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 57.417450440852555\n",
      "Starting Training\n",
      "Epoch 35: Iteration 20 with loss 5.47739839553833\n",
      "Epoch 35: Iteration 40 with loss 2.549919605255127\n",
      "Epoch 35: Iteration 60 with loss 1.9296611547470093\n",
      "Epoch 35: Iteration 80 with loss 2.57340407371521\n",
      "Epoch 35: Iteration 100 with loss 1.8425852060317993\n",
      "Epoch 35: Iteration 120 with loss 2.3206567764282227\n",
      "Epoch 35: Iteration 140 with loss 2.393990993499756\n",
      "Epoch 35 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.84123425643305\n",
      "Starting Training\n",
      "Epoch 36: Iteration 20 with loss 2.5081400871276855\n",
      "Epoch 36: Iteration 40 with loss 2.0074923038482666\n",
      "Epoch 36: Iteration 60 with loss 2.972855567932129\n",
      "Epoch 36: Iteration 80 with loss 2.301439046859741\n",
      "Epoch 36: Iteration 100 with loss 1.4026232957839966\n",
      "Epoch 36: Iteration 120 with loss 3.172644853591919\n",
      "Epoch 36: Iteration 140 with loss 2.115877389907837\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.4 minutes\n",
      "iter 20 of Expectation completed in a total of  7.0 minutes\n",
      "iter 30 of Expectation completed in a total of  10.1 minutes\n",
      "iter 40 of Expectation completed in a total of  13.5 minutes\n",
      "iter 50 of Expectation completed in a total of  17.2 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.86%, boundary mse  0.00\n",
      "Epoch 36 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 56.22847208151687\n",
      "Starting Training\n",
      "Epoch 37: Iteration 20 with loss 1.6235344409942627\n",
      "Epoch 37: Iteration 40 with loss 1.9943408966064453\n",
      "Epoch 37: Iteration 60 with loss 1.9650895595550537\n",
      "Epoch 37: Iteration 80 with loss 1.6526012420654297\n",
      "Epoch 37: Iteration 100 with loss 4.104737281799316\n",
      "Epoch 37: Iteration 120 with loss 1.972475290298462\n",
      "Epoch 37: Iteration 140 with loss 2.302917003631592\n",
      "Epoch 37 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.71370074314211\n",
      "Starting Training\n",
      "Epoch 38: Iteration 20 with loss 2.7022910118103027\n",
      "Epoch 38: Iteration 40 with loss 2.2562320232391357\n",
      "Epoch 38: Iteration 60 with loss 2.22704815864563\n",
      "Epoch 38: Iteration 80 with loss 1.0264942646026611\n",
      "Epoch 38: Iteration 100 with loss 2.4711427688598633\n",
      "Epoch 38: Iteration 120 with loss 2.4221646785736084\n",
      "Epoch 38: Iteration 140 with loss 1.046651005744934\n",
      "Epoch 38 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 56.49963398453585\n",
      "Starting Training\n",
      "Epoch 39: Iteration 20 with loss 1.1404545307159424\n",
      "Epoch 39: Iteration 40 with loss 1.771697759628296\n",
      "Epoch 39: Iteration 60 with loss 1.3497378826141357\n",
      "Epoch 39: Iteration 80 with loss 1.3875818252563477\n",
      "Epoch 39: Iteration 100 with loss 0.9101555347442627\n",
      "Epoch 39: Iteration 120 with loss 3.167470932006836\n",
      "Epoch 39: Iteration 140 with loss 2.0217483043670654\n",
      "Epoch 39 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 57.01197393447016\n",
      "Starting Training\n",
      "Epoch 40: Iteration 20 with loss 2.0966129302978516\n",
      "Epoch 40: Iteration 40 with loss 3.703065872192383\n",
      "Epoch 40: Iteration 60 with loss 0.7942739725112915\n",
      "Epoch 40: Iteration 80 with loss 2.02299427986145\n",
      "Epoch 40: Iteration 100 with loss 2.2627484798431396\n",
      "Epoch 40: Iteration 120 with loss 1.7591602802276611\n",
      "Epoch 40: Iteration 140 with loss 1.122793436050415\n",
      "Epoch 40 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 55.708288943064986\n",
      "Starting Training\n",
      "Epoch 41: Iteration 20 with loss 0.6711766123771667\n",
      "Epoch 41: Iteration 40 with loss 0.6672397255897522\n",
      "Epoch 41: Iteration 60 with loss 0.7729765772819519\n",
      "Epoch 41: Iteration 80 with loss 1.232991337776184\n",
      "Epoch 41: Iteration 100 with loss 1.2427101135253906\n",
      "Epoch 41: Iteration 120 with loss 1.868177890777588\n",
      "Epoch 41: Iteration 140 with loss 1.2975702285766602\n",
      "Calculating expectation\n",
      "iter 20 of Expectation completed in a total of  6.8 minutes\n",
      "iter 30 of Expectation completed in a total of  9.9 minutes\n",
      "iter 40 of Expectation completed in a total of  13.9 minutes\n",
      "iter 50 of Expectation completed in a total of  17.6 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  74.08%, boundary mse  0.00\n",
      "Epoch 41 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 60.23340021830208\n",
      "Starting Training\n",
      "Epoch 42: Iteration 20 with loss 0.5754073858261108\n",
      "Epoch 42: Iteration 40 with loss 1.2936557531356812\n",
      "Epoch 42: Iteration 60 with loss 0.7072346806526184\n",
      "Epoch 42: Iteration 80 with loss 1.0506937503814697\n",
      "Epoch 42: Iteration 100 with loss 2.3872716426849365\n",
      "Epoch 42: Iteration 120 with loss 1.0515971183776855\n",
      "Epoch 42: Iteration 140 with loss 1.1578145027160645\n",
      "Epoch 42 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.09719998169923\n",
      "Starting Training\n",
      "Epoch 43: Iteration 20 with loss 4.59800386428833\n",
      "Epoch 43: Iteration 40 with loss 1.1675617694854736\n",
      "Epoch 43: Iteration 60 with loss 3.9449334144592285\n",
      "Epoch 43: Iteration 80 with loss 1.781217098236084\n",
      "Epoch 43: Iteration 100 with loss 1.0138704776763916\n",
      "Epoch 43: Iteration 120 with loss 1.3154784440994263\n",
      "Epoch 43: Iteration 140 with loss 4.176033973693848\n",
      "Epoch 43 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 56.32169164504343\n",
      "Starting Training\n",
      "Epoch 44: Iteration 20 with loss 4.406867027282715\n",
      "Epoch 44: Iteration 40 with loss 1.7260297536849976\n",
      "Epoch 44: Iteration 60 with loss 1.1451895236968994\n",
      "Epoch 44: Iteration 80 with loss 1.6196649074554443\n",
      "Epoch 44: Iteration 100 with loss 1.3973268270492554\n",
      "Epoch 44: Iteration 120 with loss 0.9305485486984253\n",
      "Epoch 44: Iteration 140 with loss 1.0919691324234009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 60.14026235465591\n",
      "Starting Training\n",
      "Epoch 45: Iteration 20 with loss 0.46913662552833557\n",
      "Epoch 45: Iteration 40 with loss 1.4388813972473145\n",
      "Epoch 45: Iteration 60 with loss 1.2819610834121704\n",
      "Epoch 45: Iteration 80 with loss 0.6182953119277954\n",
      "Epoch 45: Iteration 100 with loss 1.1402822732925415\n",
      "Epoch 45: Iteration 120 with loss 1.1565797328948975\n",
      "Epoch 45: Iteration 140 with loss 0.9091622233390808\n",
      "Epoch 45 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.31721122360277\n",
      "Starting Training\n",
      "Epoch 46: Iteration 20 with loss 1.112587571144104\n",
      "Epoch 46: Iteration 40 with loss 0.6386730670928955\n",
      "Epoch 46: Iteration 60 with loss 0.9089552164077759\n",
      "Epoch 46: Iteration 80 with loss 1.024571180343628\n",
      "Epoch 46: Iteration 100 with loss 1.2925093173980713\n",
      "Epoch 46: Iteration 120 with loss 1.5584391355514526\n",
      "Epoch 46: Iteration 140 with loss 0.8325422406196594\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.6 minutes\n",
      "iter 20 of Expectation completed in a total of  7.1 minutes\n",
      "iter 30 of Expectation completed in a total of  10.1 minutes\n",
      "iter 40 of Expectation completed in a total of  13.4 minutes\n",
      "iter 50 of Expectation completed in a total of  17.0 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.76%, boundary mse  0.00\n",
      "Epoch 46 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.4059372937078\n",
      "Starting Training\n",
      "Epoch 47: Iteration 20 with loss 0.6452604532241821\n",
      "Epoch 47: Iteration 40 with loss 0.48733314871788025\n",
      "Epoch 47: Iteration 60 with loss 1.0657622814178467\n",
      "Epoch 47: Iteration 80 with loss 0.6254553198814392\n",
      "Epoch 47: Iteration 100 with loss 1.1172688007354736\n",
      "Epoch 47: Iteration 120 with loss 1.4067745208740234\n",
      "Epoch 47: Iteration 140 with loss 2.1814117431640625\n",
      "Epoch 47 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 53.877803122896225\n",
      "Starting Training\n",
      "Epoch 48: Iteration 20 with loss 0.7999083399772644\n",
      "Epoch 48: Iteration 40 with loss 0.8710823655128479\n",
      "Epoch 48: Iteration 60 with loss 1.1914253234863281\n",
      "Epoch 48: Iteration 80 with loss 1.0387617349624634\n",
      "Epoch 48: Iteration 100 with loss 0.547325849533081\n",
      "Epoch 48: Iteration 120 with loss 0.9333226680755615\n",
      "Epoch 48: Iteration 140 with loss 0.6625423431396484\n",
      "Epoch 48 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 57.839266923313225\n",
      "Starting Training\n",
      "Epoch 49: Iteration 20 with loss 0.5934705138206482\n",
      "Epoch 49: Iteration 40 with loss 0.8555140495300293\n",
      "Epoch 49: Iteration 60 with loss 0.8661719560623169\n",
      "Epoch 49: Iteration 80 with loss 0.5825374722480774\n",
      "Epoch 49: Iteration 100 with loss 0.5864043831825256\n",
      "Epoch 49: Iteration 120 with loss 1.5593777894973755\n",
      "Epoch 49: Iteration 140 with loss 0.967216968536377\n",
      "Epoch 49 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.385927359002345\n",
      "Starting Training\n",
      "Epoch 50: Iteration 20 with loss 1.2650947570800781\n",
      "Epoch 50: Iteration 40 with loss 0.9374107718467712\n",
      "Epoch 50: Iteration 60 with loss 0.9192180037498474\n",
      "Epoch 50: Iteration 80 with loss 1.0766241550445557\n",
      "Epoch 50: Iteration 100 with loss 0.8342427015304565\n",
      "Epoch 50: Iteration 120 with loss 0.7124541401863098\n",
      "Epoch 50: Iteration 140 with loss 0.8066366910934448\n",
      "Epoch 50 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.33624729573396\n",
      "Starting Training\n",
      "Epoch 51: Iteration 20 with loss 0.5384577512741089\n",
      "Epoch 51: Iteration 40 with loss 1.2559734582901\n",
      "Epoch 51: Iteration 60 with loss 0.5887861251831055\n",
      "Epoch 51: Iteration 80 with loss 0.6803677678108215\n",
      "Epoch 51: Iteration 100 with loss 0.7167161703109741\n",
      "Epoch 51: Iteration 120 with loss 0.9921558499336243\n",
      "Epoch 51: Iteration 140 with loss 0.4486826956272125\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.8 minutes\n",
      "iter 20 of Expectation completed in a total of  7.4 minutes\n",
      "iter 30 of Expectation completed in a total of  10.6 minutes\n",
      "iter 40 of Expectation completed in a total of  14.4 minutes\n",
      "iter 50 of Expectation completed in a total of  18.0 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.94%, boundary mse  0.00\n",
      "Epoch 51 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.77392009098099\n",
      "Starting Training\n",
      "Epoch 52: Iteration 20 with loss 0.5826801657676697\n",
      "Epoch 52: Iteration 40 with loss 0.33420220017433167\n",
      "Epoch 52: Iteration 60 with loss 0.8487327098846436\n",
      "Epoch 52: Iteration 80 with loss 0.5941927433013916\n",
      "Epoch 52: Iteration 100 with loss 1.8753786087036133\n",
      "Epoch 52: Iteration 120 with loss 1.4197553396224976\n",
      "Epoch 52: Iteration 140 with loss 1.2586767673492432\n",
      "Epoch 52 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.45748991823476\n",
      "Starting Training\n",
      "Epoch 53: Iteration 20 with loss 0.7687598466873169\n",
      "Epoch 53: Iteration 40 with loss 1.4956943988800049\n",
      "Epoch 53: Iteration 60 with loss 0.6507435441017151\n",
      "Epoch 53: Iteration 80 with loss 1.2914685010910034\n",
      "Epoch 53: Iteration 100 with loss 0.5493499040603638\n",
      "Epoch 53: Iteration 120 with loss 0.5853528380393982\n",
      "Epoch 53: Iteration 140 with loss 0.5239444375038147\n",
      "Epoch 53 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.65431146608801\n",
      "Starting Training\n",
      "Epoch 54: Iteration 20 with loss 0.46003788709640503\n",
      "Epoch 54: Iteration 40 with loss 0.4812687337398529\n",
      "Epoch 54: Iteration 60 with loss 0.542789101600647\n",
      "Epoch 54: Iteration 80 with loss 0.7000042200088501\n",
      "Epoch 54: Iteration 100 with loss 0.5728731751441956\n",
      "Epoch 54: Iteration 120 with loss 0.6407343149185181\n",
      "Epoch 54: Iteration 140 with loss 0.6473011374473572\n",
      "Epoch 54 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.68650121896221\n",
      "Starting Training\n",
      "Epoch 55: Iteration 20 with loss 0.5372892618179321\n",
      "Epoch 55: Iteration 40 with loss 0.5536332130432129\n",
      "Epoch 55: Iteration 60 with loss 0.47865280508995056\n",
      "Epoch 55: Iteration 80 with loss 0.45133668184280396\n",
      "Epoch 55: Iteration 100 with loss 0.5976294279098511\n",
      "Epoch 55: Iteration 120 with loss 0.6196407079696655\n",
      "Epoch 55: Iteration 140 with loss 0.5212391018867493\n",
      "Epoch 55 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.94352904267348\n",
      "Starting Training\n",
      "Epoch 56: Iteration 20 with loss 0.9380460977554321\n",
      "Epoch 56: Iteration 40 with loss 0.5191447734832764\n",
      "Epoch 56: Iteration 60 with loss 0.48868077993392944\n",
      "Epoch 56: Iteration 80 with loss 0.5067108273506165\n",
      "Epoch 56: Iteration 100 with loss 0.33113381266593933\n",
      "Epoch 56: Iteration 120 with loss 0.4872245192527771\n",
      "Epoch 56: Iteration 140 with loss 0.7854945063591003\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.2 minutes\n",
      "iter 20 of Expectation completed in a total of  7.0 minutes\n",
      "iter 30 of Expectation completed in a total of  10.1 minutes\n",
      "iter 40 of Expectation completed in a total of  13.5 minutes\n",
      "iter 50 of Expectation completed in a total of  17.1 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.99%, boundary mse  0.00\n",
      "Epoch 56 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.50716998150315\n",
      "Starting Training\n",
      "Epoch 57: Iteration 20 with loss 0.32485783100128174\n",
      "Epoch 57: Iteration 40 with loss 0.3164607286453247\n",
      "Epoch 57: Iteration 60 with loss 0.7449271082878113\n",
      "Epoch 57: Iteration 80 with loss 0.5089163184165955\n",
      "Epoch 57: Iteration 100 with loss 0.385734885931015\n",
      "Epoch 57: Iteration 120 with loss 0.6039175987243652\n",
      "Epoch 57: Iteration 140 with loss 0.5876513123512268\n",
      "Epoch 57 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.54556892528709\n",
      "Starting Training\n",
      "Epoch 58: Iteration 20 with loss 0.3603348433971405\n",
      "Epoch 58: Iteration 40 with loss 0.5062838792800903\n",
      "Epoch 58: Iteration 60 with loss 0.5960281491279602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Iteration 80 with loss 0.5493444204330444\n",
      "Epoch 58: Iteration 100 with loss 0.36516082286834717\n",
      "Epoch 58: Iteration 120 with loss 0.5092846155166626\n",
      "Epoch 58: Iteration 140 with loss 0.3883707523345947\n",
      "Epoch 58 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.68437702207204\n",
      "Starting Training\n",
      "Epoch 59: Iteration 20 with loss 0.4433003067970276\n",
      "Epoch 59: Iteration 40 with loss 0.4741995930671692\n",
      "Epoch 59: Iteration 60 with loss 0.4756790101528168\n",
      "Epoch 59: Iteration 80 with loss 0.3847227990627289\n",
      "Epoch 59: Iteration 100 with loss 0.2968137860298157\n",
      "Epoch 59: Iteration 120 with loss 0.50529545545578\n",
      "Epoch 59: Iteration 140 with loss 0.5757080316543579\n",
      "Epoch 59 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.472686095987555\n",
      "Starting Training\n",
      "Epoch 60: Iteration 20 with loss 0.9714802503585815\n",
      "Epoch 60: Iteration 40 with loss 0.9681916236877441\n",
      "Epoch 60: Iteration 60 with loss 1.2373591661453247\n",
      "Epoch 60: Iteration 80 with loss 1.6613173484802246\n",
      "Epoch 60: Iteration 100 with loss 0.9238123893737793\n",
      "Epoch 60: Iteration 120 with loss 2.822798252105713\n",
      "Epoch 60: Iteration 140 with loss 1.567662000656128\n",
      "Epoch 60 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 57.6623866822659\n",
      "Starting Training\n",
      "Epoch 61: Iteration 20 with loss 1.1374211311340332\n",
      "Epoch 61: Iteration 40 with loss 0.8807047009468079\n",
      "Epoch 61: Iteration 60 with loss 0.7454316020011902\n",
      "Epoch 61: Iteration 80 with loss 0.7597212195396423\n",
      "Epoch 61: Iteration 100 with loss 1.2582405805587769\n",
      "Epoch 61: Iteration 120 with loss 1.144989013671875\n",
      "Epoch 61: Iteration 140 with loss 1.0188277959823608\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.6 minutes\n",
      "iter 20 of Expectation completed in a total of  6.6 minutes\n",
      "iter 30 of Expectation completed in a total of  9.7 minutes\n",
      "iter 40 of Expectation completed in a total of  13.1 minutes\n",
      "iter 50 of Expectation completed in a total of  17.0 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.24%, boundary mse  0.00\n",
      "Epoch 61 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.814436695664675\n",
      "Starting Training\n",
      "Epoch 62: Iteration 20 with loss 1.0154571533203125\n",
      "Epoch 62: Iteration 40 with loss 2.0919125080108643\n",
      "Epoch 62: Iteration 60 with loss 0.9242662191390991\n",
      "Epoch 62: Iteration 80 with loss 0.8381543755531311\n",
      "Epoch 62: Iteration 100 with loss 1.8936357498168945\n",
      "Epoch 62: Iteration 120 with loss 1.4331694841384888\n",
      "Epoch 62: Iteration 140 with loss 0.9815927147865295\n",
      "Epoch 62 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.72783482244982\n",
      "Starting Training\n",
      "Epoch 63: Iteration 20 with loss 0.478843629360199\n",
      "Epoch 63: Iteration 40 with loss 0.5673356056213379\n",
      "Epoch 63: Iteration 60 with loss 0.35200998187065125\n",
      "Epoch 63: Iteration 80 with loss 0.7638602256774902\n",
      "Epoch 63: Iteration 100 with loss 0.5857522487640381\n",
      "Epoch 63: Iteration 120 with loss 0.6125156283378601\n",
      "Epoch 63: Iteration 140 with loss 0.7039364576339722\n",
      "Epoch 63 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.453084660684056\n",
      "Starting Training\n",
      "Epoch 64: Iteration 20 with loss 0.5569213628768921\n",
      "Epoch 64: Iteration 40 with loss 0.5076429843902588\n",
      "Epoch 64: Iteration 60 with loss 3.661581516265869\n",
      "Epoch 64: Iteration 80 with loss 6.328402042388916\n",
      "Epoch 64: Iteration 100 with loss 2.7969117164611816\n",
      "Epoch 64: Iteration 120 with loss 0.6588190197944641\n",
      "Epoch 64: Iteration 140 with loss 0.8184488415718079\n",
      "Epoch 64 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.49049666991287\n",
      "Starting Training\n",
      "Epoch 65: Iteration 20 with loss 0.7383138537406921\n",
      "Epoch 65: Iteration 40 with loss 0.9383184909820557\n",
      "Epoch 65: Iteration 60 with loss 0.46703752875328064\n",
      "Epoch 65: Iteration 80 with loss 0.5511688590049744\n",
      "Epoch 65: Iteration 100 with loss 0.5525293946266174\n",
      "Epoch 65: Iteration 120 with loss 0.7503206133842468\n",
      "Epoch 65: Iteration 140 with loss 0.8014605045318604\n",
      "Epoch 65 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 60.10553990548958\n",
      "Starting Training\n",
      "Epoch 66: Iteration 20 with loss 0.7651920914649963\n",
      "Epoch 66: Iteration 40 with loss 0.6780132055282593\n",
      "Epoch 66: Iteration 60 with loss 1.4268043041229248\n",
      "Epoch 66: Iteration 80 with loss 0.6306439638137817\n",
      "Epoch 66: Iteration 100 with loss 0.9269206523895264\n",
      "Epoch 66: Iteration 120 with loss 0.4166504740715027\n",
      "Epoch 66: Iteration 140 with loss 0.38443490862846375\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.9 minutes\n",
      "iter 20 of Expectation completed in a total of  7.5 minutes\n",
      "iter 30 of Expectation completed in a total of  10.4 minutes\n",
      "iter 40 of Expectation completed in a total of  13.9 minutes\n",
      "iter 50 of Expectation completed in a total of  18.4 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.46%, boundary mse  0.00\n",
      "Epoch 66 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.50594448329728\n",
      "Starting Training\n",
      "Epoch 67: Iteration 20 with loss 0.6898436546325684\n",
      "Epoch 67: Iteration 40 with loss 1.5057146549224854\n",
      "Epoch 67: Iteration 60 with loss 0.5352431535720825\n",
      "Epoch 67: Iteration 80 with loss 0.5250951051712036\n",
      "Epoch 67: Iteration 100 with loss 0.4966610074043274\n",
      "Epoch 67: Iteration 120 with loss 0.6465538144111633\n",
      "Epoch 67: Iteration 140 with loss 0.45146703720092773\n",
      "Epoch 67 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.88445349316009\n",
      "Starting Training\n",
      "Epoch 68: Iteration 20 with loss 0.48940715193748474\n",
      "Epoch 68: Iteration 40 with loss 0.6550366878509521\n",
      "Epoch 68: Iteration 60 with loss 0.719693124294281\n",
      "Epoch 68: Iteration 80 with loss 1.0999596118927002\n",
      "Epoch 68: Iteration 100 with loss 0.7040674090385437\n",
      "Epoch 68: Iteration 120 with loss 0.8661971092224121\n",
      "Epoch 68: Iteration 140 with loss 0.6214423179626465\n",
      "Epoch 68 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.72375636442068\n",
      "Starting Training\n",
      "Epoch 69: Iteration 20 with loss 0.6243414282798767\n",
      "Epoch 69: Iteration 40 with loss 0.597625732421875\n",
      "Epoch 69: Iteration 60 with loss 0.43762150406837463\n",
      "Epoch 69: Iteration 80 with loss 0.5830962061882019\n",
      "Epoch 69: Iteration 100 with loss 0.900938868522644\n",
      "Epoch 69: Iteration 120 with loss 0.5319178700447083\n",
      "Epoch 69: Iteration 140 with loss 0.6820100545883179\n",
      "Epoch 69 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.587719527578614\n",
      "Starting Training\n",
      "Epoch 70: Iteration 20 with loss 0.7776327729225159\n",
      "Epoch 70: Iteration 40 with loss 0.7870799899101257\n",
      "Epoch 70: Iteration 60 with loss 0.3909938633441925\n",
      "Epoch 70: Iteration 80 with loss 0.6383018493652344\n",
      "Epoch 70: Iteration 100 with loss 0.37011390924453735\n",
      "Epoch 70: Iteration 120 with loss 0.47082212567329407\n",
      "Epoch 70: Iteration 140 with loss 0.4412737190723419\n",
      "Epoch 70 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 58.41222818449794\n",
      "Starting Training\n",
      "Epoch 71: Iteration 20 with loss 0.4803416132926941\n",
      "Epoch 71: Iteration 40 with loss 0.8392750024795532\n",
      "Epoch 71: Iteration 60 with loss 0.33285465836524963\n",
      "Epoch 71: Iteration 80 with loss 0.4608932137489319\n",
      "Epoch 71: Iteration 100 with loss 0.3338592052459717\n",
      "Epoch 71: Iteration 120 with loss 0.79714035987854\n",
      "Epoch 71: Iteration 140 with loss 0.4547588527202606\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  4.4 minutes\n",
      "iter 20 of Expectation completed in a total of  7.7 minutes\n",
      "iter 30 of Expectation completed in a total of  11.2 minutes\n",
      "iter 40 of Expectation completed in a total of  14.2 minutes\n",
      "iter 50 of Expectation completed in a total of  17.8 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.50%, boundary mse  0.00\n",
      "Epoch 71 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.348917313185055\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: Iteration 20 with loss 0.5793716311454773\n",
      "Epoch 72: Iteration 40 with loss 0.3403429090976715\n",
      "Epoch 72: Iteration 60 with loss 0.4525173604488373\n",
      "Epoch 72: Iteration 80 with loss 0.2579137086868286\n",
      "Epoch 72: Iteration 100 with loss 0.3536054193973541\n",
      "Epoch 72: Iteration 120 with loss 0.3154594898223877\n",
      "Epoch 72: Iteration 140 with loss 0.3199898600578308\n",
      "Epoch 72 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.38331296282982\n",
      "Starting Training\n",
      "Epoch 73: Iteration 20 with loss 0.31046271324157715\n",
      "Epoch 73: Iteration 40 with loss 0.38239508867263794\n",
      "Epoch 73: Iteration 60 with loss 0.5022700428962708\n",
      "Epoch 73: Iteration 80 with loss 0.6177935600280762\n",
      "Epoch 73: Iteration 100 with loss 0.26858988404273987\n",
      "Epoch 73: Iteration 120 with loss 0.42259636521339417\n",
      "Epoch 73: Iteration 140 with loss 0.49236851930618286\n",
      "Epoch 73 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.24172707011157\n",
      "Starting Training\n",
      "Epoch 74: Iteration 20 with loss 0.3212699592113495\n",
      "Epoch 74: Iteration 40 with loss 0.342072457075119\n",
      "Epoch 74: Iteration 60 with loss 0.37703660130500793\n",
      "Epoch 74: Iteration 80 with loss 0.6963136792182922\n",
      "Epoch 74: Iteration 100 with loss 0.5260063409805298\n",
      "Epoch 74: Iteration 120 with loss 1.5215585231781006\n",
      "Epoch 74: Iteration 140 with loss 1.0790272951126099\n",
      "Epoch 74 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.24867155994484\n",
      "Starting Training\n",
      "Epoch 75: Iteration 20 with loss 0.4530569314956665\n",
      "Epoch 75: Iteration 40 with loss 0.4621501564979553\n",
      "Epoch 75: Iteration 60 with loss 0.3921186923980713\n",
      "Epoch 75: Iteration 80 with loss 0.8472391963005066\n",
      "Epoch 75: Iteration 100 with loss 0.6905876994132996\n",
      "Epoch 75: Iteration 120 with loss 0.4090534746646881\n",
      "Epoch 75: Iteration 140 with loss 0.5921695232391357\n",
      "Epoch 75 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.36599258818685\n",
      "Starting Training\n",
      "Epoch 76: Iteration 20 with loss 0.3223955035209656\n",
      "Epoch 76: Iteration 40 with loss 0.418995201587677\n",
      "Epoch 76: Iteration 60 with loss 0.6125161647796631\n",
      "Epoch 76: Iteration 80 with loss 0.5899949073791504\n",
      "Epoch 76: Iteration 100 with loss 0.42877069115638733\n",
      "Epoch 76: Iteration 120 with loss 0.5193303227424622\n",
      "Epoch 76: Iteration 140 with loss 0.4862038493156433\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  4.1 minutes\n",
      "iter 20 of Expectation completed in a total of  7.7 minutes\n",
      "iter 30 of Expectation completed in a total of  11.4 minutes\n",
      "iter 40 of Expectation completed in a total of  15.4 minutes\n",
      "iter 50 of Expectation completed in a total of  18.9 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.61%, boundary mse  0.00\n",
      "Epoch 76 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.7454068327244\n",
      "Starting Training\n",
      "Epoch 77: Iteration 20 with loss 0.2694506049156189\n",
      "Epoch 77: Iteration 40 with loss 0.5411343574523926\n",
      "Epoch 77: Iteration 60 with loss 0.6162344813346863\n",
      "Epoch 77: Iteration 80 with loss 0.5391284227371216\n",
      "Epoch 77: Iteration 100 with loss 0.3990425169467926\n",
      "Epoch 77: Iteration 120 with loss 0.874821662902832\n",
      "Epoch 77: Iteration 140 with loss 3.119016170501709\n",
      "Epoch 77 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 44.02667664494539\n",
      "Starting Training\n",
      "Epoch 78: Iteration 20 with loss 1.3946380615234375\n",
      "Epoch 78: Iteration 40 with loss 0.7523216009140015\n",
      "Epoch 78: Iteration 60 with loss 0.7154168486595154\n",
      "Epoch 78: Iteration 80 with loss 1.2034941911697388\n",
      "Epoch 78: Iteration 100 with loss 0.7113590836524963\n",
      "Epoch 78: Iteration 120 with loss 0.8767144680023193\n",
      "Epoch 78: Iteration 140 with loss 0.3772451877593994\n",
      "Epoch 78 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.51917986392068\n",
      "Starting Training\n",
      "Epoch 79: Iteration 20 with loss 0.42480897903442383\n",
      "Epoch 79: Iteration 40 with loss 0.6989574432373047\n",
      "Epoch 79: Iteration 60 with loss 0.43115556240081787\n",
      "Epoch 79: Iteration 80 with loss 0.37759947776794434\n",
      "Epoch 79: Iteration 100 with loss 0.8517749905586243\n",
      "Epoch 79: Iteration 120 with loss 0.5746884942054749\n",
      "Epoch 79: Iteration 140 with loss 0.2975323498249054\n",
      "Epoch 79 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.24303426819783\n",
      "Starting Training\n",
      "Epoch 80: Iteration 20 with loss 0.4975564479827881\n",
      "Epoch 80: Iteration 40 with loss 0.3667713701725006\n",
      "Epoch 80: Iteration 60 with loss 0.25478991866111755\n",
      "Epoch 80: Iteration 80 with loss 0.3023483455181122\n",
      "Epoch 80: Iteration 100 with loss 0.35670679807662964\n",
      "Epoch 80: Iteration 120 with loss 0.3316842019557953\n",
      "Epoch 80: Iteration 140 with loss 0.3868104815483093\n",
      "Epoch 80 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.14319701435957\n",
      "Starting Training\n",
      "Epoch 81: Iteration 20 with loss 0.28016212582588196\n",
      "Epoch 81: Iteration 40 with loss 0.3003172278404236\n",
      "Epoch 81: Iteration 60 with loss 0.4250010550022125\n",
      "Epoch 81: Iteration 80 with loss 0.24688735604286194\n",
      "Epoch 81: Iteration 100 with loss 0.2683265507221222\n",
      "Epoch 81: Iteration 120 with loss 0.3669623136520386\n",
      "Epoch 81: Iteration 140 with loss 0.2632231116294861\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.3 minutes\n",
      "iter 20 of Expectation completed in a total of  7.3 minutes\n",
      "iter 30 of Expectation completed in a total of  11.4 minutes\n",
      "iter 40 of Expectation completed in a total of  15.4 minutes\n",
      "iter 50 of Expectation completed in a total of  18.5 minutes\n",
      "Expectation step finished, posterior frame-wise accuracy  73.64%, boundary mse  0.00\n",
      "Epoch 81 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.18600775168465\n",
      "Starting Training\n",
      "Epoch 82: Iteration 20 with loss 0.2502361834049225\n",
      "Epoch 82: Iteration 40 with loss 0.2681722342967987\n",
      "Epoch 82: Iteration 60 with loss 0.33013197779655457\n",
      "Epoch 82: Iteration 80 with loss 0.35763007402420044\n",
      "Epoch 82: Iteration 100 with loss 0.5198850035667419\n",
      "Epoch 82: Iteration 120 with loss 0.6552531719207764\n",
      "Epoch 82: Iteration 140 with loss 0.835109293460846\n",
      "Epoch 82 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.437071484127344\n",
      "Starting Training\n",
      "Epoch 83: Iteration 20 with loss 0.31328919529914856\n",
      "Epoch 83: Iteration 40 with loss 0.3329683244228363\n",
      "Epoch 83: Iteration 60 with loss 0.5416736006736755\n",
      "Epoch 83: Iteration 80 with loss 0.4752567708492279\n",
      "Epoch 83: Iteration 100 with loss 0.6828792095184326\n",
      "Epoch 83: Iteration 120 with loss 0.2625810205936432\n",
      "Epoch 83: Iteration 140 with loss 0.26742154359817505\n",
      "Epoch 83 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.32244655193825\n",
      "Starting Training\n",
      "Epoch 84: Iteration 20 with loss 0.3061428666114807\n",
      "Epoch 84: Iteration 40 with loss 0.22858931124210358\n",
      "Epoch 84: Iteration 60 with loss 0.2904359996318817\n",
      "Epoch 84: Iteration 80 with loss 0.21169112622737885\n",
      "Epoch 84: Iteration 100 with loss 0.33526554703712463\n",
      "Epoch 84: Iteration 120 with loss 0.4100022315979004\n",
      "Epoch 84: Iteration 140 with loss 0.40298226475715637\n",
      "Epoch 84 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.33249563722639\n",
      "Starting Training\n",
      "Epoch 85: Iteration 20 with loss 0.27597957849502563\n",
      "Epoch 85: Iteration 40 with loss 0.20517699420452118\n",
      "Epoch 85: Iteration 60 with loss 0.29183119535446167\n",
      "Epoch 85: Iteration 80 with loss 0.6739033460617065\n",
      "Epoch 85: Iteration 100 with loss 0.50660240650177\n",
      "Epoch 85: Iteration 120 with loss 0.29173365235328674\n",
      "Epoch 85: Iteration 140 with loss 0.2980833053588867\n",
      "Epoch 85 finished, starting validation\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 59.011660206929456\n",
      "Starting Training\n",
      "Epoch 86: Iteration 20 with loss 0.1598014384508133\n",
      "Epoch 86: Iteration 40 with loss 0.2513369023799896\n",
      "Epoch 86: Iteration 60 with loss 0.22427034378051758\n",
      "Epoch 86: Iteration 80 with loss 0.35051679611206055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: Iteration 100 with loss 0.2475200593471527\n",
      "Epoch 86: Iteration 120 with loss 0.24375499784946442\n",
      "Epoch 86: Iteration 140 with loss 0.23198260366916656\n",
      "Calculating expectation\n",
      "iter 10 of Expectation completed in a total of  3.6 minutes\n",
      "iter 20 of Expectation completed in a total of  7.1 minutes\n",
      "iter 30 of Expectation completed in a total of  11.9 minutes\n",
      "iter 40 of Expectation completed in a total of  16.3 minutes\n"
     ]
    }
   ],
   "source": [
    "initialize_epoch = 30\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0\n",
    "for epoch in range(30, 150):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)  # features\n",
    "        item_1 = item[1].to(device)  # count\n",
    "        item_2 = item[2].to(device)  # target\n",
    "        weights = item[5].to(device)  # posterior weight\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        boundary_target_tensor = get_single_random(item[4], item_2.shape[1], item_2.device)\n",
    "        \n",
    "        loss = 0\n",
    "        for p in predictions:\n",
    "            if epoch <= initialize_epoch:\n",
    "                loss += ce_criterion(p, boundary_target_tensor)\n",
    "                loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(p[:, :, 1:], dim=1), \n",
    "                                                                    F.log_softmax(p.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                            max=16) * src_mask_mse[:, :, 1:])\n",
    "            else:\n",
    "                prob = torch.softmax(p, dim=1)\n",
    "                prob = prob.permute(0, 2, 1)\n",
    "                total_count = torch.sum(src_mask)\n",
    "                weighted_loss_sum = -torch.sum(torch.sum(torch.log(prob + 1e-8) * weights, dim=-1) * src_mask)\n",
    "                loss += weighted_loss_sum/total_count\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%20 == 0:\n",
    "            print(f'Epoch {epoch+1}: Iteration {i+1} with loss {loss.item()}')\n",
    "\n",
    "    if (epoch >= initialize_epoch) and ((epoch % (3 * expectation_cal_gap)) == 0):\n",
    "        torch.save(model.state_dict(), config.output_dir + f\"ms-tcn-initial-{epoch}-epochs.wt\")\n",
    "\n",
    "    if epoch >= initialize_epoch and (epoch % expectation_cal_gap == 0):\n",
    "        perform_expectation(model, trainloder_expectation)\n",
    "    \n",
    "    print(f'Epoch {epoch+1} finished, starting validation')\n",
    "    val_acc, best_val_acc = validate(model, testloader, best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:: Epoch 105, Probability Accuracy 61.02425300046298\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation:: Epoch {epoch}, Probability Accuracy {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:: Epoch 105, Probability Accuracy 63.70656599831428\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation:: Epoch {epoch}, Probability Accuracy {best_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\n",
    "\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-speed/final-em-maximized.wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split1/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(config.output_dir + \"ms-tcn-emmax-best-model.wt\"))\n",
    "# model.load_state_dict(torch.load(config.output_dir + \"ms-tcn-initial-15-epochs.wt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
