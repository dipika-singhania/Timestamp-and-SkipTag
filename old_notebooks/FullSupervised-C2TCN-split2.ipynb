{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import C2F_TCN\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 12, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 2, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/fullsupervised-c2ftcn-split2/', 'project_name': 'breakfast-split-2', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split2.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split2.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split2_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=12,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=2,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/fullsupervised-c2ftcn-split2/\",\n",
    "    project_name=\"breakfast-split-2\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1261\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 451\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", allow_pickle=True).item()\n",
    "# loaded_vidid_selected_frames\n",
    "video_id_boundary_frames = pickle.load(open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"rb\"))\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_var_actions = pickle.load(open(\"dump_dir/mean_var_actions.pkl\", \"rb\"))\n",
    "def get_possion_prob(cur_len, cur_class):\n",
    "    mean_class, std_class = loaded_mean_var_actions[cur_class]\n",
    "    mean_class = mean_class * 10\n",
    "    prob = cur_len * torch.log(torch.tensor(mean_class, device=device) + 1e-5)\n",
    "    prob = prob - mean_class\n",
    "    factorials = torch.cumsum(torch.log(torch.arange(1, torch.max(cur_len)+1, 1).type(torch.float).to(device)), \n",
    "                              dim=0)[min(cur_len)-1:]\n",
    "    prob = prob - factorials\n",
    "    return prob\n",
    "\n",
    "# get_possion_prob(torch.arange(10, 21), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_video_each_segment_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vals_per_segment(selected_frames, cur_vid_feat, labels):\n",
    "    prob_each_segment = []\n",
    "    \n",
    "    log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "    cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "    cur_boundary = 0\n",
    "    for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "\n",
    "        next_ele = selected_frames[i + 1]\n",
    "        label_cur_ele = labels[cur_ele]\n",
    "        label_next_ele = labels[next_ele]\n",
    "\n",
    "        indices = torch.arange(cur_ele, next_ele).to(cumsum_feat.device)\n",
    "        strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "        end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "        left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "        right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "\n",
    "        cur_boundary_len = indices - cur_boundary\n",
    "        len_prob = get_possion_prob(cur_boundary_len, label_cur_ele.item())\n",
    "\n",
    "        prob = torch.softmax((left_sum + right_sum + len_prob), dim=0)\n",
    "#         prob = torch.softmax((left_sum + right_sum), dim=0)\n",
    "\n",
    "        cur_boundary = round(np.sum(np.arange(cur_ele, next_ele, 1) * prob.detach().cpu().numpy()).item())\n",
    "        prob_each_segment.append(prob)\n",
    "    \n",
    "    return prob_each_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frames = torch.tensor([2, 10, 17, 21])\n",
    "cur_vid_feat = torch.randn((27, 48))\n",
    "labels = torch.tensor([47, 47, 47, 47, 47, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10])\n",
    "# print(len(labels))\n",
    "# probs_all_segs = prob_vals_per_segment_new(selected_frames, cur_vid_feat, labels)\n",
    "# print(probs_all_segs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_element_probb(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_video = prob_vals_per_segment(selected_frames, cur_vid_feat, labels)\n",
    "        prob_video_each_segment_dict[cur_vidid] = prob_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_loss(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    loss_arr = []\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_each_video = prob_video_each_segment_dict[cur_vidid]\n",
    "        \n",
    "        log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "        cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "        \n",
    "        \n",
    "        current_vid_prob_arr = []\n",
    "        # Start segment log probability\n",
    "        start_ele_idx = selected_frames[0]\n",
    "        if start_ele_idx > 0:\n",
    "            start_ele_label_idx = labels[selected_frames[0]]\n",
    "            first_segment_prob = (cumsum_feat[start_ele_idx - 1, :])[start_ele_label_idx]\n",
    "            current_vid_prob_arr.append(first_segment_prob.unsqueeze(0))\n",
    "        \n",
    "        for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "            next_ele = selected_frames[i + 1]\n",
    "            label_cur_ele = labels[cur_ele]\n",
    "            label_next_ele = labels[next_ele]\n",
    "\n",
    "            indices = torch.arange(cur_ele, next_ele)\n",
    "            strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "            end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "            left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "            right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "            current_vid_prob_arr.append((left_sum + right_sum)  * (prob_each_video[i]))\n",
    "        \n",
    "        # End segment log probability\n",
    "        end_index = cur_vid_count.item() - 1\n",
    "        last_ele_label_idx = labels[selected_frames[-1]]\n",
    "        last_segment_sum_prob = (cumsum_feat[end_index, :] - cumsum_feat[selected_frames[-1] - 1, :])[last_ele_label_idx]\n",
    "        current_vid_prob_arr.append(last_segment_sum_prob.unsqueeze(0))\n",
    "            \n",
    "        loss_arr.append(torch.cat(current_vid_prob_arr))\n",
    "    return -torch.mean(torch.cat(loss_arr)), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_estimated_boundaries():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames, video_id_boundary_frames\n",
    "    estimated_boundary_dict = {}\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        selected_ele_list = loaded_vidid_selected_frames[ele + \".txt\"]\n",
    "        boundary_list = []\n",
    "        for i, segment in enumerate(estimated_boundary_probs):\n",
    "            estimated_boundary = np.sum(np.arange(selected_ele_list[i], selected_ele_list[i + 1], 1) \\\n",
    "                                        * segment.detach().cpu().numpy())\n",
    "            estimated_boundary = round(estimated_boundary.item())\n",
    "            \n",
    "            if (estimated_boundary < selected_ele_list[i]) or (estimated_boundary > selected_ele_list[i + 1]):\n",
    "                print(\"Estimated value wrong\")\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            boundary_list.append(estimated_boundary)\n",
    "\n",
    "        estimated_boundary_dict[ele] = boundary_list\n",
    "    return estimated_boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_boundary_err():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    err_list = []\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    estimated_boundar_dict = get_estimated_boundaries()\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if ele + \".txt\" not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        estimated_boundary_list = estimated_boundar_dict[ele]\n",
    "        \n",
    "        estimated = np.array(estimated_boundary_list) #.detach().cpu().numpy()\n",
    "        actual = np.array(video_id_boundary_frames[ele][:-1])\n",
    "        if len(actual) != len(estimated):\n",
    "            print(ele)\n",
    "            continue\n",
    "        \n",
    "        mse_err = (actual - estimated)**2\n",
    "        err_list.append(mse_err)\n",
    "        \n",
    "        estimated_labels = []\n",
    "        actual_labels = []\n",
    "        start_v1 = 0\n",
    "        start_v2 = 0\n",
    "        for i, v1 in enumerate(estimated):\n",
    "            estimated_labels.extend([i] * (v1 - start_v1))\n",
    "            start_v1 = v1\n",
    "            v2 = actual[i]\n",
    "            actual_labels.extend([i] * (v2 - start_v2))\n",
    "            start_v2 = v2\n",
    "            \n",
    "        last_ele = video_id_boundary_frames[ele][-1]\n",
    "        estimated_labels.extend([i+1] * (last_ele - start_v1))\n",
    "        actual_labels.extend([i+1] * (last_ele - start_v2))\n",
    "        \n",
    "        correct += np.sum(np.array(actual_labels) == np.array(estimated_labels))\n",
    "        total += len(actual_labels)\n",
    "        \n",
    "    print(f\"Train Boundary avergage error = {np.sqrt(np.mean(np.concatenate(err_list))):.3f}\")\n",
    "    print(f\"Train From boundary avergage accuracy = {correct * 100.0 / total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "        \n",
    "        weights = [1, 1, 1, 1, 1, 1]\n",
    "        vidlen = outp[0].shape[-1]\n",
    "        ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "        for i, outp_ele in enumerate(outp[1]):\n",
    "            upped_logit = F.upsample(outp_ele, size=vidlen, mode='nearest')\n",
    "            ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i+1] / sum(weights)\n",
    "        \n",
    "        return ensemble_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = C2F_TCN(n_channels=2048, n_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(labels_all, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((labels_all.shape[0], labels_all.shape[1]), dtype=torch.long, device=labels_all.device) * (-100)\n",
    "    for iter_num, labels in enumerate(labels_all):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(loaded_vidid_selected_frames[cur_vidid + \".txt\"]))\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = labels[frame_idx_tensor]\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dipika16/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 0, Iteration 0, Current loss 3.917290210723877 Accuracy 4.048159134531495\n",
      "Training:: Epoch 0, Iteration 10, Current loss 3.7912206649780273 Accuracy 2.8741328047571852\n",
      "Training:: Epoch 0, Iteration 20, Current loss 3.5595016479492188 Accuracy 11.242704142347142\n",
      "Training:: Epoch 0, Iteration 30, Current loss 3.296705961227417 Accuracy 24.961967152038252\n",
      "Training:: Epoch 0, Iteration 40, Current loss 2.9616641998291016 Accuracy 32.17869087434305\n",
      "Training:: Epoch 0, Iteration 50, Current loss 3.2367687225341797 Accuracy 20.64052702595444\n",
      "Training:: Epoch 0, Iteration 60, Current loss 3.174970865249634 Accuracy 17.247565351102\n",
      "Training:: Epoch 0, Iteration 70, Current loss 2.829922676086426 Accuracy 26.706960872205673\n",
      "Training:: Epoch 0, Iteration 80, Current loss 2.74573016166687 Accuracy 25.99086860312036\n",
      "Training:: Epoch 0, Iteration 90, Current loss 2.7673542499542236 Accuracy 40.68290314750384\n",
      "Training:: Epoch 0, Iteration 100, Current loss 2.8317387104034424 Accuracy 26.01172047766475\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 0, Probability Accuracy 27.121846128350665\n",
      "Starting Training\n",
      "Training:: Epoch 1, Iteration 0, Current loss 2.645176649093628 Accuracy 30.63706727906779\n",
      "Training:: Epoch 1, Iteration 10, Current loss 2.64924955368042 Accuracy 36.60388669630814\n",
      "Training:: Epoch 1, Iteration 20, Current loss 2.5789456367492676 Accuracy 35.20414356816251\n",
      "Training:: Epoch 1, Iteration 30, Current loss 2.503445863723755 Accuracy 29.02445670686014\n",
      "Training:: Epoch 1, Iteration 40, Current loss 2.2665352821350098 Accuracy 43.06061501099406\n",
      "Training:: Epoch 1, Iteration 50, Current loss 2.5707764625549316 Accuracy 37.19254269152436\n",
      "Training:: Epoch 1, Iteration 60, Current loss 2.331449031829834 Accuracy 38.42552859483499\n",
      "Training:: Epoch 1, Iteration 70, Current loss 2.4940037727355957 Accuracy 32.23542116630669\n",
      "Training:: Epoch 1, Iteration 80, Current loss 2.1545677185058594 Accuracy 42.26304967831797\n",
      "Training:: Epoch 1, Iteration 90, Current loss 2.6472339630126953 Accuracy 31.277881303292112\n",
      "Training:: Epoch 1, Iteration 100, Current loss 2.622053861618042 Accuracy 33.90479030013914\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 1, Probability Accuracy 29.53846791233376\n",
      "Starting Training\n",
      "Training:: Epoch 2, Iteration 0, Current loss 2.8358144760131836 Accuracy 25.96670599030017\n",
      "Training:: Epoch 2, Iteration 10, Current loss 2.749058246612549 Accuracy 27.22914669223394\n",
      "Training:: Epoch 2, Iteration 20, Current loss 2.8943300247192383 Accuracy 23.579450418160096\n",
      "Training:: Epoch 2, Iteration 30, Current loss 1.9226385354995728 Accuracy 46.14539713050045\n",
      "Training:: Epoch 2, Iteration 40, Current loss 1.907837152481079 Accuracy 51.73847945433372\n",
      "Training:: Epoch 2, Iteration 50, Current loss 1.8290338516235352 Accuracy 53.41925971120602\n",
      "Training:: Epoch 2, Iteration 60, Current loss 2.1855628490448 Accuracy 38.23439878234399\n",
      "Training:: Epoch 2, Iteration 70, Current loss 2.184931993484497 Accuracy 32.28838844157436\n",
      "Training:: Epoch 2, Iteration 80, Current loss 2.4155433177948 Accuracy 38.53722531977698\n",
      "Training:: Epoch 2, Iteration 90, Current loss 2.404913902282715 Accuracy 35.019714555450655\n",
      "Training:: Epoch 2, Iteration 100, Current loss 1.8901283740997314 Accuracy 52.10269265678351\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 2, Probability Accuracy 35.118801010896135\n",
      "Starting Training\n",
      "Training:: Epoch 3, Iteration 0, Current loss 2.13183331489563 Accuracy 53.251837352734796\n",
      "Training:: Epoch 3, Iteration 10, Current loss 2.1069703102111816 Accuracy 41.00890796754423\n",
      "Training:: Epoch 3, Iteration 20, Current loss 2.0685110092163086 Accuracy 43.81775414195132\n",
      "Training:: Epoch 3, Iteration 30, Current loss 1.9970362186431885 Accuracy 53.04528171448324\n",
      "Training:: Epoch 3, Iteration 40, Current loss 1.9218248128890991 Accuracy 45.92418651459242\n",
      "Training:: Epoch 3, Iteration 50, Current loss 1.976952075958252 Accuracy 46.925344208018736\n",
      "Training:: Epoch 3, Iteration 60, Current loss 1.7213497161865234 Accuracy 49.698028087026124\n",
      "Training:: Epoch 3, Iteration 70, Current loss 2.133042335510254 Accuracy 46.81732740943267\n",
      "Training:: Epoch 3, Iteration 80, Current loss 2.075361967086792 Accuracy 43.82087257351528\n",
      "Training:: Epoch 3, Iteration 90, Current loss 1.6278483867645264 Accuracy 60.45316979929914\n",
      "Training:: Epoch 3, Iteration 100, Current loss 1.7842962741851807 Accuracy 56.217655629804554\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 3, Probability Accuracy 36.83131706508679\n",
      "Starting Training\n",
      "Training:: Epoch 4, Iteration 0, Current loss 1.4345771074295044 Accuracy 62.759916983200846\n",
      "Training:: Epoch 4, Iteration 10, Current loss 1.5235258340835571 Accuracy 58.440111420612816\n",
      "Training:: Epoch 4, Iteration 20, Current loss 2.0585711002349854 Accuracy 40.60161974546857\n",
      "Training:: Epoch 4, Iteration 30, Current loss 1.976488709449768 Accuracy 46.46142958244869\n",
      "Training:: Epoch 4, Iteration 40, Current loss 2.1093504428863525 Accuracy 44.592574878938244\n",
      "Training:: Epoch 4, Iteration 50, Current loss 2.197659730911255 Accuracy 32.401729559748425\n",
      "Training:: Epoch 4, Iteration 60, Current loss 1.4722621440887451 Accuracy 62.4891058044274\n",
      "Training:: Epoch 4, Iteration 70, Current loss 1.8831173181533813 Accuracy 39.85017357938973\n",
      "Training:: Epoch 4, Iteration 80, Current loss 1.996819019317627 Accuracy 38.320110817277424\n",
      "Training:: Epoch 4, Iteration 90, Current loss 1.9697149991989136 Accuracy 44.81295639048668\n",
      "Training:: Epoch 4, Iteration 100, Current loss 1.7729339599609375 Accuracy 51.65617805065234\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 4, Probability Accuracy 38.24201433483863\n",
      "Starting Training\n",
      "Training:: Epoch 5, Iteration 0, Current loss 1.2693450450897217 Accuracy 72.33073575549034\n",
      "Training:: Epoch 5, Iteration 10, Current loss 2.02665638923645 Accuracy 49.28444532175286\n",
      "Training:: Epoch 5, Iteration 20, Current loss 1.8046250343322754 Accuracy 52.845998266397\n",
      "Training:: Epoch 5, Iteration 30, Current loss 1.8090325593948364 Accuracy 57.362173827741806\n",
      "Training:: Epoch 5, Iteration 40, Current loss 1.4349135160446167 Accuracy 63.56143021572049\n",
      "Training:: Epoch 5, Iteration 50, Current loss 1.6362590789794922 Accuracy 57.56722234958897\n",
      "Training:: Epoch 5, Iteration 60, Current loss 1.5396901369094849 Accuracy 52.67341040462428\n",
      "Training:: Epoch 5, Iteration 70, Current loss 1.4834493398666382 Accuracy 52.574826820023524\n",
      "Training:: Epoch 5, Iteration 80, Current loss 1.2883644104003906 Accuracy 64.45479004806676\n",
      "Training:: Epoch 5, Iteration 90, Current loss 1.4302693605422974 Accuracy 53.163087637840974\n",
      "Training:: Epoch 5, Iteration 100, Current loss 1.604110598564148 Accuracy 53.56188578415833\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 5, Probability Accuracy 41.11892530140448\n",
      "Starting Training\n",
      "Training:: Epoch 6, Iteration 0, Current loss 1.8228517770767212 Accuracy 52.673430216616396\n",
      "Training:: Epoch 6, Iteration 10, Current loss 1.445318579673767 Accuracy 54.51179166540201\n",
      "Training:: Epoch 6, Iteration 20, Current loss 1.7904750108718872 Accuracy 53.39342832218668\n",
      "Training:: Epoch 6, Iteration 30, Current loss 1.5865286588668823 Accuracy 58.52777054962847\n",
      "Training:: Epoch 6, Iteration 40, Current loss 1.4699498414993286 Accuracy 58.70197459054033\n",
      "Training:: Epoch 6, Iteration 50, Current loss 1.552904725074768 Accuracy 61.43538338380147\n",
      "Training:: Epoch 6, Iteration 60, Current loss 1.4122039079666138 Accuracy 62.15778222191573\n",
      "Training:: Epoch 6, Iteration 70, Current loss 1.5284855365753174 Accuracy 60.988010320230686\n",
      "Training:: Epoch 6, Iteration 80, Current loss 1.2908251285552979 Accuracy 62.700734672648075\n",
      "Training:: Epoch 6, Iteration 90, Current loss 1.6965384483337402 Accuracy 58.39537726414046\n",
      "Training:: Epoch 6, Iteration 100, Current loss 1.1803979873657227 Accuracy 68.4092379104364\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 6, Probability Accuracy 41.52131582218171\n",
      "Starting Training\n",
      "Training:: Epoch 7, Iteration 0, Current loss 1.4034054279327393 Accuracy 63.011954665424625\n",
      "Training:: Epoch 7, Iteration 10, Current loss 1.492993950843811 Accuracy 62.69698456840613\n",
      "Training:: Epoch 7, Iteration 20, Current loss 1.0677255392074585 Accuracy 72.68795873834557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 7, Iteration 30, Current loss 1.4511733055114746 Accuracy 64.80456079420726\n",
      "Training:: Epoch 7, Iteration 40, Current loss 1.467482089996338 Accuracy 60.68353306702175\n",
      "Training:: Epoch 7, Iteration 50, Current loss 1.6432359218597412 Accuracy 54.825038657579746\n",
      "Training:: Epoch 7, Iteration 60, Current loss 1.4701894521713257 Accuracy 57.94500500654702\n",
      "Training:: Epoch 7, Iteration 70, Current loss 1.6034448146820068 Accuracy 56.561217565515236\n",
      "Training:: Epoch 7, Iteration 80, Current loss 1.3515931367874146 Accuracy 64.35549201810817\n",
      "Training:: Epoch 7, Iteration 90, Current loss 1.4517549276351929 Accuracy 58.19922113264205\n",
      "Training:: Epoch 7, Iteration 100, Current loss 1.2805202007293701 Accuracy 65.34252775633004\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 7, Probability Accuracy 42.75904213448233\n",
      "Starting Training\n",
      "Training:: Epoch 8, Iteration 0, Current loss 1.2728251218795776 Accuracy 67.4015699742426\n",
      "Training:: Epoch 8, Iteration 10, Current loss 1.0920534133911133 Accuracy 67.9962976022567\n",
      "Training:: Epoch 8, Iteration 20, Current loss 1.2617383003234863 Accuracy 68.91723277415777\n",
      "Training:: Epoch 8, Iteration 30, Current loss 1.4683526754379272 Accuracy 64.31779798052392\n",
      "Training:: Epoch 8, Iteration 40, Current loss 1.0510884523391724 Accuracy 69.94523451991874\n",
      "Training:: Epoch 8, Iteration 50, Current loss 1.0065053701400757 Accuracy 70.39258451472192\n",
      "Training:: Epoch 8, Iteration 60, Current loss 1.5665123462677002 Accuracy 52.01655758796218\n",
      "Training:: Epoch 8, Iteration 70, Current loss 1.372528076171875 Accuracy 64.6249195106246\n",
      "Training:: Epoch 8, Iteration 80, Current loss 1.3590823411941528 Accuracy 57.61254049223666\n",
      "Training:: Epoch 8, Iteration 90, Current loss 1.0877586603164673 Accuracy 69.16374854093714\n",
      "Training:: Epoch 8, Iteration 100, Current loss 1.0559855699539185 Accuracy 70.65182374628628\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 8, Probability Accuracy 40.39441521315822\n",
      "Starting Training\n",
      "Training:: Epoch 9, Iteration 0, Current loss 1.4552416801452637 Accuracy 62.1812427728075\n",
      "Training:: Epoch 9, Iteration 10, Current loss 1.5622544288635254 Accuracy 64.05697962865518\n",
      "Training:: Epoch 9, Iteration 20, Current loss 0.9475409388542175 Accuracy 77.21831512793655\n",
      "Training:: Epoch 9, Iteration 30, Current loss 1.2393596172332764 Accuracy 65.82035595105673\n",
      "Training:: Epoch 9, Iteration 40, Current loss 1.0772910118103027 Accuracy 68.82652605237287\n",
      "Training:: Epoch 9, Iteration 50, Current loss 1.137728214263916 Accuracy 71.4509237527833\n",
      "Training:: Epoch 9, Iteration 60, Current loss 1.2661850452423096 Accuracy 67.69945124525115\n",
      "Training:: Epoch 9, Iteration 70, Current loss 1.1268333196640015 Accuracy 65.646434580044\n",
      "Training:: Epoch 9, Iteration 80, Current loss 1.019916296005249 Accuracy 74.78018975709105\n",
      "Training:: Epoch 9, Iteration 90, Current loss 1.283075213432312 Accuracy 67.62393054856568\n",
      "Training:: Epoch 9, Iteration 100, Current loss 2.0229883193969727 Accuracy 51.19019138755981\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 9, Probability Accuracy 41.32338318763724\n",
      "Starting Training\n",
      "Training:: Epoch 10, Iteration 0, Current loss 1.6368697881698608 Accuracy 59.93704787674637\n",
      "Training:: Epoch 10, Iteration 10, Current loss 0.980434775352478 Accuracy 67.05459835432104\n",
      "Training:: Epoch 10, Iteration 20, Current loss 1.0250698328018188 Accuracy 74.27454739869474\n",
      "Training:: Epoch 10, Iteration 30, Current loss 1.1438474655151367 Accuracy 70.69521410579345\n",
      "Training:: Epoch 10, Iteration 40, Current loss 0.9332042336463928 Accuracy 76.16365611296617\n",
      "Training:: Epoch 10, Iteration 50, Current loss 1.013514518737793 Accuracy 69.1540979773179\n",
      "Training:: Epoch 10, Iteration 60, Current loss 1.206027626991272 Accuracy 68.32186527589079\n",
      "Training:: Epoch 10, Iteration 70, Current loss 1.3500893115997314 Accuracy 62.04145267791636\n",
      "Training:: Epoch 10, Iteration 80, Current loss 1.0542198419570923 Accuracy 72.01260469358985\n",
      "Training:: Epoch 10, Iteration 90, Current loss 0.9174673557281494 Accuracy 74.30156998529621\n",
      "Training:: Epoch 10, Iteration 100, Current loss 1.0663572549819946 Accuracy 72.17330292931065\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 10, Probability Accuracy 43.27650495090525\n",
      "Starting Training\n",
      "Training:: Epoch 11, Iteration 0, Current loss 1.5558091402053833 Accuracy 57.57029792162667\n",
      "Training:: Epoch 11, Iteration 10, Current loss 0.8977592587471008 Accuracy 79.29592376919005\n",
      "Training:: Epoch 11, Iteration 20, Current loss 1.2587811946868896 Accuracy 66.4748637189582\n",
      "Training:: Epoch 11, Iteration 30, Current loss 1.0664583444595337 Accuracy 72.38392857142857\n",
      "Training:: Epoch 11, Iteration 40, Current loss 0.8200480341911316 Accuracy 76.91215810449309\n",
      "Training:: Epoch 11, Iteration 50, Current loss 1.2635308504104614 Accuracy 63.87795275590551\n",
      "Training:: Epoch 11, Iteration 60, Current loss 0.9013428688049316 Accuracy 77.25372354324536\n",
      "Training:: Epoch 11, Iteration 70, Current loss 1.0057592391967773 Accuracy 73.39197361187466\n",
      "Training:: Epoch 11, Iteration 80, Current loss 1.0579577684402466 Accuracy 74.13245633930596\n",
      "Training:: Epoch 11, Iteration 90, Current loss 1.0434203147888184 Accuracy 70.78729111805676\n",
      "Training:: Epoch 11, Iteration 100, Current loss 1.0255136489868164 Accuracy 73.52238575942052\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 11, Probability Accuracy 45.045573186394336\n",
      "Starting Training\n",
      "Training:: Epoch 12, Iteration 0, Current loss 0.9891934394836426 Accuracy 73.93523407250969\n",
      "Training:: Epoch 12, Iteration 10, Current loss 0.8931292295455933 Accuracy 76.56362875936276\n",
      "Training:: Epoch 12, Iteration 20, Current loss 0.9468315243721008 Accuracy 72.57872340425531\n",
      "Training:: Epoch 12, Iteration 30, Current loss 0.6605786681175232 Accuracy 82.26460210538593\n",
      "Training:: Epoch 12, Iteration 40, Current loss 1.015714406967163 Accuracy 71.17383380129057\n",
      "Training:: Epoch 12, Iteration 50, Current loss 1.070386528968811 Accuracy 64.81903735334356\n",
      "Training:: Epoch 12, Iteration 60, Current loss 0.6860541701316833 Accuracy 83.21801702068775\n",
      "Training:: Epoch 12, Iteration 70, Current loss 0.9309589862823486 Accuracy 75.27669374960016\n",
      "Training:: Epoch 12, Iteration 80, Current loss 1.0699788331985474 Accuracy 74.43545141528418\n",
      "Training:: Epoch 12, Iteration 90, Current loss 0.996110737323761 Accuracy 73.40821887338042\n",
      "Training:: Epoch 12, Iteration 100, Current loss 0.9851728677749634 Accuracy 73.18294227637372\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 12, Probability Accuracy 43.579048763309444\n",
      "Starting Training\n",
      "Training:: Epoch 13, Iteration 0, Current loss 1.1188517808914185 Accuracy 61.62385863496026\n",
      "Training:: Epoch 13, Iteration 10, Current loss 1.1308842897415161 Accuracy 70.37847878727568\n",
      "Training:: Epoch 13, Iteration 20, Current loss 1.1398930549621582 Accuracy 66.29157175398633\n",
      "Training:: Epoch 13, Iteration 30, Current loss 1.5940624475479126 Accuracy 60.0350935828877\n",
      "Training:: Epoch 13, Iteration 40, Current loss 0.8422555327415466 Accuracy 74.04827669028461\n",
      "Training:: Epoch 13, Iteration 50, Current loss 0.9963986873626709 Accuracy 75.86533341222413\n",
      "Training:: Epoch 13, Iteration 60, Current loss 0.9586611986160278 Accuracy 69.69779102859466\n",
      "Training:: Epoch 13, Iteration 70, Current loss 0.8724751472473145 Accuracy 81.17225293711127\n",
      "Training:: Epoch 13, Iteration 80, Current loss 0.9541691541671753 Accuracy 71.44102012954949\n",
      "Training:: Epoch 13, Iteration 90, Current loss 0.7840948104858398 Accuracy 78.95760770654869\n",
      "Training:: Epoch 13, Iteration 100, Current loss 0.8994473814964294 Accuracy 75.29388324367405\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 13, Probability Accuracy 40.788416124621946\n",
      "Starting Training\n",
      "Training:: Epoch 14, Iteration 0, Current loss 0.8105296492576599 Accuracy 75.79668791167765\n",
      "Training:: Epoch 14, Iteration 10, Current loss 0.9528743624687195 Accuracy 72.40405221118255\n",
      "Training:: Epoch 14, Iteration 20, Current loss 0.5951364040374756 Accuracy 85.15993707393812\n",
      "Training:: Epoch 14, Iteration 30, Current loss 0.709319531917572 Accuracy 82.06851761649618\n",
      "Training:: Epoch 14, Iteration 40, Current loss 0.7970880270004272 Accuracy 77.35604285236023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 14, Iteration 50, Current loss 0.8983643651008606 Accuracy 72.77789150460593\n",
      "Training:: Epoch 14, Iteration 60, Current loss 1.020982265472412 Accuracy 71.6130453944469\n",
      "Training:: Epoch 14, Iteration 70, Current loss 0.8560483455657959 Accuracy 74.67153284671532\n",
      "Training:: Epoch 14, Iteration 80, Current loss 0.8679929375648499 Accuracy 73.9433049089652\n",
      "Training:: Epoch 14, Iteration 90, Current loss 0.8920577168464661 Accuracy 76.52412379833046\n",
      "Training:: Epoch 14, Iteration 100, Current loss 1.1127408742904663 Accuracy 70.83452007974935\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 14, Probability Accuracy 44.61428512242615\n",
      "Starting Training\n",
      "Training:: Epoch 15, Iteration 0, Current loss 0.6831477284431458 Accuracy 77.03475897582607\n",
      "Training:: Epoch 15, Iteration 10, Current loss 0.8670005202293396 Accuracy 75.46602894285013\n",
      "Training:: Epoch 15, Iteration 20, Current loss 0.7496782541275024 Accuracy 79.57327237224365\n",
      "Training:: Epoch 15, Iteration 30, Current loss 1.2252871990203857 Accuracy 69.18114797261717\n",
      "Training:: Epoch 15, Iteration 40, Current loss 0.9957873225212097 Accuracy 74.16319969742814\n",
      "Training:: Epoch 15, Iteration 50, Current loss 0.719854474067688 Accuracy 81.34643183762574\n",
      "Training:: Epoch 15, Iteration 60, Current loss 0.7005797624588013 Accuracy 81.59499609069586\n",
      "Training:: Epoch 15, Iteration 70, Current loss 1.1359127759933472 Accuracy 68.66717042548244\n",
      "Training:: Epoch 15, Iteration 80, Current loss 0.8200059533119202 Accuracy 76.91624951184009\n",
      "Training:: Epoch 15, Iteration 90, Current loss 0.49173054099082947 Accuracy 85.84861672365558\n",
      "Training:: Epoch 15, Iteration 100, Current loss 1.0493899583816528 Accuracy 72.38035119935728\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 44.166321415254586\n",
      "Starting Training\n",
      "Training:: Epoch 16, Iteration 0, Current loss 0.6254920959472656 Accuracy 80.31326075972625\n",
      "Training:: Epoch 16, Iteration 10, Current loss 0.5817804932594299 Accuracy 86.51655716596784\n",
      "Training:: Epoch 16, Iteration 20, Current loss 1.0687336921691895 Accuracy 73.93000780358965\n",
      "Training:: Epoch 16, Iteration 30, Current loss 0.9860811829566956 Accuracy 76.6958612399107\n",
      "Training:: Epoch 16, Iteration 40, Current loss 0.8827459812164307 Accuracy 73.89922832501135\n",
      "Training:: Epoch 16, Iteration 50, Current loss 0.750018298625946 Accuracy 74.73064040999152\n",
      "Training:: Epoch 16, Iteration 60, Current loss 0.892331600189209 Accuracy 67.19451156368658\n",
      "Training:: Epoch 16, Iteration 70, Current loss 0.6549399495124817 Accuracy 80.32035928143712\n",
      "Training:: Epoch 16, Iteration 80, Current loss 0.5772960782051086 Accuracy 85.4277505852309\n",
      "Training:: Epoch 16, Iteration 90, Current loss 0.7048112154006958 Accuracy 82.32296890672016\n",
      "Training:: Epoch 16, Iteration 100, Current loss 0.9722503423690796 Accuracy 69.55439382623848\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 16, Probability Accuracy 45.47976136222397\n",
      "Starting Training\n",
      "Training:: Epoch 17, Iteration 0, Current loss 0.5094630718231201 Accuracy 89.04323617976252\n",
      "Training:: Epoch 17, Iteration 10, Current loss 0.8537495136260986 Accuracy 78.02025822718775\n",
      "Training:: Epoch 17, Iteration 20, Current loss 0.5572289228439331 Accuracy 86.38821247265673\n",
      "Training:: Epoch 17, Iteration 30, Current loss 0.4356082081794739 Accuracy 88.1945216649987\n",
      "Training:: Epoch 17, Iteration 40, Current loss 0.6457971334457397 Accuracy 84.931992065741\n",
      "Training:: Epoch 17, Iteration 50, Current loss 0.3863615393638611 Accuracy 89.21451104100946\n",
      "Training:: Epoch 17, Iteration 60, Current loss 0.6798368096351624 Accuracy 78.77734614598242\n",
      "Training:: Epoch 17, Iteration 70, Current loss 0.9456138014793396 Accuracy 70.24384525205159\n",
      "Training:: Epoch 17, Iteration 80, Current loss 0.5880216360092163 Accuracy 81.93622227484411\n",
      "Training:: Epoch 17, Iteration 90, Current loss 0.6370481848716736 Accuracy 82.55242621792607\n",
      "Training:: Epoch 17, Iteration 100, Current loss 0.6585344672203064 Accuracy 81.22655983634503\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 17, Probability Accuracy 45.20166134979492\n",
      "Starting Training\n",
      "Training:: Epoch 18, Iteration 0, Current loss 0.6594420671463013 Accuracy 81.28542042735401\n",
      "Training:: Epoch 18, Iteration 10, Current loss 0.5657574534416199 Accuracy 86.13619907777905\n",
      "Training:: Epoch 18, Iteration 20, Current loss 0.6789134740829468 Accuracy 81.09127714390873\n",
      "Training:: Epoch 18, Iteration 30, Current loss 0.7318769097328186 Accuracy 80.15352185498946\n",
      "Training:: Epoch 18, Iteration 40, Current loss 0.6183800101280212 Accuracy 83.03568852600905\n",
      "Training:: Epoch 18, Iteration 50, Current loss 0.5621617436408997 Accuracy 85.00556935995202\n",
      "Training:: Epoch 18, Iteration 60, Current loss 1.0587232112884521 Accuracy 73.67351382739348\n",
      "Training:: Epoch 18, Iteration 70, Current loss 0.6997054815292358 Accuracy 80.47104461069549\n",
      "Training:: Epoch 18, Iteration 80, Current loss 0.5276249051094055 Accuracy 87.12969384722085\n",
      "Training:: Epoch 18, Iteration 90, Current loss 0.5633620023727417 Accuracy 83.63699847638395\n",
      "Training:: Epoch 18, Iteration 100, Current loss 0.6541370749473572 Accuracy 81.37910288120864\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 18, Probability Accuracy 40.357645937771885\n",
      "Starting Training\n",
      "Training:: Epoch 19, Iteration 0, Current loss 0.5655158162117004 Accuracy 86.05017319198069\n",
      "Training:: Epoch 19, Iteration 10, Current loss 0.7515963912010193 Accuracy 75.36279323513367\n",
      "Training:: Epoch 19, Iteration 20, Current loss 1.2884758710861206 Accuracy 68.49019050326983\n",
      "Training:: Epoch 19, Iteration 30, Current loss 0.6792007684707642 Accuracy 81.3628983154049\n",
      "Training:: Epoch 19, Iteration 40, Current loss 0.5460801720619202 Accuracy 84.79428813798954\n",
      "Training:: Epoch 19, Iteration 50, Current loss 0.6833360195159912 Accuracy 80.62649757465958\n",
      "Training:: Epoch 19, Iteration 60, Current loss 0.7758298516273499 Accuracy 76.56629885917337\n",
      "Training:: Epoch 19, Iteration 70, Current loss 0.5918326377868652 Accuracy 82.35269657782028\n",
      "Training:: Epoch 19, Iteration 80, Current loss 0.5136394500732422 Accuracy 84.60698689956332\n",
      "Training:: Epoch 19, Iteration 90, Current loss 0.7541932463645935 Accuracy 78.84384851041158\n",
      "Training:: Epoch 19, Iteration 100, Current loss 0.7789487242698669 Accuracy 79.2760045809889\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 19, Probability Accuracy 42.416207482288605\n",
      "Starting Training\n",
      "Training:: Epoch 20, Iteration 0, Current loss 0.7408232688903809 Accuracy 77.7585642847128\n",
      "Training:: Epoch 20, Iteration 10, Current loss 0.6776108741760254 Accuracy 81.3287654472042\n",
      "Training:: Epoch 20, Iteration 20, Current loss 0.4870639741420746 Accuracy 85.47264533059501\n",
      "Training:: Epoch 20, Iteration 30, Current loss 0.5193763375282288 Accuracy 85.56975083320108\n",
      "Training:: Epoch 20, Iteration 40, Current loss 0.5850290060043335 Accuracy 83.36695689105967\n",
      "Training:: Epoch 20, Iteration 50, Current loss 0.6315128207206726 Accuracy 81.02962962962962\n",
      "Training:: Epoch 20, Iteration 60, Current loss 0.6104000806808472 Accuracy 84.87130448641327\n",
      "Training:: Epoch 20, Iteration 70, Current loss 0.4477769732475281 Accuracy 87.46068259706117\n",
      "Training:: Epoch 20, Iteration 80, Current loss 0.49063098430633545 Accuracy 84.28036747192922\n",
      "Training:: Epoch 20, Iteration 90, Current loss 0.5654363632202148 Accuracy 84.44403364451428\n",
      "Training:: Epoch 20, Iteration 100, Current loss 1.0368117094039917 Accuracy 71.50221795947728\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 20, Probability Accuracy 46.115714463272155\n",
      "Starting Training\n",
      "Training:: Epoch 21, Iteration 0, Current loss 0.44994640350341797 Accuracy 91.0930516050386\n",
      "Training:: Epoch 21, Iteration 10, Current loss 0.6777008771896362 Accuracy 84.61292386436341\n",
      "Training:: Epoch 21, Iteration 20, Current loss 0.9859347939491272 Accuracy 75.10221237247335\n",
      "Training:: Epoch 21, Iteration 30, Current loss 0.6290914416313171 Accuracy 75.77193978035184\n",
      "Training:: Epoch 21, Iteration 40, Current loss 0.5925187468528748 Accuracy 82.80469020150262\n",
      "Training:: Epoch 21, Iteration 50, Current loss 0.5251839756965637 Accuracy 86.88009485697347\n",
      "Training:: Epoch 21, Iteration 60, Current loss 0.5722801685333252 Accuracy 82.70927869491383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 21, Iteration 70, Current loss 0.47582772374153137 Accuracy 87.31190978587983\n",
      "Training:: Epoch 21, Iteration 80, Current loss 0.9939087629318237 Accuracy 73.59525899912204\n",
      "Training:: Epoch 21, Iteration 90, Current loss 0.8717333078384399 Accuracy 73.96014307613694\n",
      "Training:: Epoch 21, Iteration 100, Current loss 0.4713751971721649 Accuracy 87.8361780526026\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 21, Probability Accuracy 45.891991548245436\n",
      "Starting Training\n",
      "Training:: Epoch 22, Iteration 0, Current loss 0.5726208686828613 Accuracy 83.60723575313312\n",
      "Training:: Epoch 22, Iteration 10, Current loss 0.8087828159332275 Accuracy 79.31085404873247\n",
      "Training:: Epoch 22, Iteration 20, Current loss 0.5961409211158752 Accuracy 82.3606941495204\n",
      "Training:: Epoch 22, Iteration 30, Current loss 0.6765402555465698 Accuracy 80.22388059701493\n",
      "Training:: Epoch 22, Iteration 40, Current loss 0.5400852560997009 Accuracy 83.97334179625516\n",
      "Training:: Epoch 22, Iteration 50, Current loss 0.5391867160797119 Accuracy 85.27745925848708\n",
      "Training:: Epoch 22, Iteration 60, Current loss 0.40676337480545044 Accuracy 89.48674322983128\n",
      "Training:: Epoch 22, Iteration 70, Current loss 0.5375411510467529 Accuracy 87.00902335456476\n",
      "Training:: Epoch 22, Iteration 80, Current loss 0.46398860216140747 Accuracy 83.7523379909781\n",
      "Training:: Epoch 22, Iteration 90, Current loss 0.6608003377914429 Accuracy 84.1538311561708\n",
      "Training:: Epoch 22, Iteration 100, Current loss 0.9647805690765381 Accuracy 75.83928099392017\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 22, Probability Accuracy 45.48597588764138\n",
      "Starting Training\n",
      "Training:: Epoch 23, Iteration 0, Current loss 0.40865379571914673 Accuracy 89.85971943887776\n",
      "Training:: Epoch 23, Iteration 10, Current loss 0.4992937743663788 Accuracy 85.5477953550993\n",
      "Training:: Epoch 23, Iteration 20, Current loss 0.6669543981552124 Accuracy 83.75032362840398\n",
      "Training:: Epoch 23, Iteration 30, Current loss 0.4561748504638672 Accuracy 86.97896322749847\n",
      "Training:: Epoch 23, Iteration 40, Current loss 0.4044765830039978 Accuracy 87.71643444295458\n",
      "Training:: Epoch 23, Iteration 50, Current loss 0.3729941248893738 Accuracy 89.56115174520878\n",
      "Training:: Epoch 23, Iteration 60, Current loss 0.8100347518920898 Accuracy 71.38752973919289\n",
      "Training:: Epoch 23, Iteration 70, Current loss 0.4657408595085144 Accuracy 88.2270283479961\n",
      "Training:: Epoch 23, Iteration 80, Current loss 0.5454874038696289 Accuracy 84.87334224695552\n",
      "Training:: Epoch 23, Iteration 90, Current loss 0.7758801579475403 Accuracy 80.37354085603113\n",
      "Training:: Epoch 23, Iteration 100, Current loss 0.7663724422454834 Accuracy 80.08289458944775\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 23, Probability Accuracy 46.59060778058582\n",
      "Starting Training\n",
      "Training:: Epoch 24, Iteration 0, Current loss 0.4052927792072296 Accuracy 88.2215812556798\n",
      "Training:: Epoch 24, Iteration 10, Current loss 0.38021352887153625 Accuracy 87.37243650353754\n",
      "Training:: Epoch 24, Iteration 20, Current loss 0.39978450536727905 Accuracy 88.09790994529386\n",
      "Training:: Epoch 24, Iteration 30, Current loss 0.8547275066375732 Accuracy 78.40443297403978\n",
      "Training:: Epoch 24, Iteration 40, Current loss 0.44884589314460754 Accuracy 87.74637228126748\n",
      "Training:: Epoch 24, Iteration 50, Current loss 0.39706090092658997 Accuracy 91.20112796280772\n",
      "Training:: Epoch 24, Iteration 60, Current loss 0.41424667835235596 Accuracy 90.21565119126095\n",
      "Training:: Epoch 24, Iteration 70, Current loss 0.3483394384384155 Accuracy 91.14558368556364\n",
      "Training:: Epoch 24, Iteration 80, Current loss 0.5875152349472046 Accuracy 88.05365241725369\n",
      "Training:: Epoch 24, Iteration 90, Current loss 0.5947650074958801 Accuracy 82.52261701848695\n",
      "Training:: Epoch 24, Iteration 100, Current loss 0.3717912435531616 Accuracy 90.51060554225111\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 24, Probability Accuracy 45.86464763640883\n",
      "Starting Training\n",
      "Training:: Epoch 25, Iteration 0, Current loss 0.23658114671707153 Accuracy 93.35133310833615\n",
      "Training:: Epoch 25, Iteration 10, Current loss 0.598630428314209 Accuracy 84.82558995241132\n",
      "Training:: Epoch 25, Iteration 20, Current loss 0.37072622776031494 Accuracy 89.82906820930157\n",
      "Training:: Epoch 25, Iteration 30, Current loss 0.32723161578178406 Accuracy 91.29817631993087\n",
      "Training:: Epoch 25, Iteration 40, Current loss 0.41413071751594543 Accuracy 87.5385477992711\n",
      "Training:: Epoch 25, Iteration 50, Current loss 0.39483779668807983 Accuracy 86.58935932503756\n",
      "Training:: Epoch 25, Iteration 60, Current loss 0.3543616533279419 Accuracy 90.41939672091549\n",
      "Training:: Epoch 25, Iteration 70, Current loss 0.44540974497795105 Accuracy 88.5128970722402\n",
      "Training:: Epoch 25, Iteration 80, Current loss 0.4803929328918457 Accuracy 85.20541657103512\n",
      "Training:: Epoch 25, Iteration 90, Current loss 0.9556500911712646 Accuracy 76.95616550594019\n",
      "Training:: Epoch 25, Iteration 100, Current loss 0.5594109892845154 Accuracy 86.31146917990424\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 25, Probability Accuracy 44.14312052036293\n",
      "Starting Training\n",
      "Training:: Epoch 26, Iteration 0, Current loss 0.361216276884079 Accuracy 91.2796442687747\n",
      "Training:: Epoch 26, Iteration 10, Current loss 0.5397812724113464 Accuracy 84.74112577799056\n",
      "Training:: Epoch 26, Iteration 20, Current loss 0.4074174761772156 Accuracy 88.37579617834395\n",
      "Training:: Epoch 26, Iteration 30, Current loss 0.396778404712677 Accuracy 90.21205314653368\n",
      "Training:: Epoch 26, Iteration 40, Current loss 0.3645254373550415 Accuracy 89.93257501672757\n",
      "Training:: Epoch 26, Iteration 50, Current loss 0.4763999581336975 Accuracy 84.16476997126973\n",
      "Training:: Epoch 26, Iteration 60, Current loss 0.3071562349796295 Accuracy 90.79592107005695\n",
      "Training:: Epoch 26, Iteration 70, Current loss 0.38032811880111694 Accuracy 88.34546029737425\n",
      "Training:: Epoch 26, Iteration 80, Current loss 0.3406713604927063 Accuracy 90.80720092915215\n",
      "Training:: Epoch 26, Iteration 90, Current loss 0.7789268493652344 Accuracy 75.66301527817815\n",
      "Training:: Epoch 26, Iteration 100, Current loss 0.4614085555076599 Accuracy 87.01827791235411\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 26, Probability Accuracy 45.203836433691016\n",
      "Starting Training\n",
      "Training:: Epoch 27, Iteration 0, Current loss 0.27466100454330444 Accuracy 92.45105489812904\n",
      "Training:: Epoch 27, Iteration 10, Current loss 0.8207199573516846 Accuracy 81.00399915807199\n",
      "Training:: Epoch 27, Iteration 20, Current loss 0.2699381709098816 Accuracy 92.73629040768229\n",
      "Training:: Epoch 27, Iteration 30, Current loss 0.40190190076828003 Accuracy 87.85287571746515\n",
      "Training:: Epoch 27, Iteration 40, Current loss 0.5241643190383911 Accuracy 85.66196418882257\n",
      "Training:: Epoch 27, Iteration 50, Current loss 0.4418320059776306 Accuracy 88.29437752520485\n",
      "Training:: Epoch 27, Iteration 60, Current loss 0.7501636743545532 Accuracy 77.59826618067186\n",
      "Training:: Epoch 27, Iteration 70, Current loss 0.5703896284103394 Accuracy 84.67616904226057\n",
      "Training:: Epoch 27, Iteration 80, Current loss 0.43645423650741577 Accuracy 86.06841844323252\n",
      "Training:: Epoch 27, Iteration 90, Current loss 0.5064332485198975 Accuracy 83.05666003976143\n",
      "Training:: Epoch 27, Iteration 100, Current loss 0.37881821393966675 Accuracy 89.17824074074075\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 27, Probability Accuracy 46.246323072461365\n",
      "Starting Training\n",
      "Training:: Epoch 28, Iteration 0, Current loss 0.3089803159236908 Accuracy 91.74557798820797\n",
      "Training:: Epoch 28, Iteration 10, Current loss 0.31494656205177307 Accuracy 91.77650429799426\n",
      "Training:: Epoch 28, Iteration 20, Current loss 0.4032585024833679 Accuracy 88.15627703656482\n",
      "Training:: Epoch 28, Iteration 30, Current loss 0.37426358461380005 Accuracy 89.66255070654839\n",
      "Training:: Epoch 28, Iteration 40, Current loss 0.40077701210975647 Accuracy 90.00654117305037\n",
      "Training:: Epoch 28, Iteration 50, Current loss 0.33304694294929504 Accuracy 91.82866882395079\n",
      "Training:: Epoch 28, Iteration 60, Current loss 0.34282490611076355 Accuracy 91.904047976012\n",
      "Training:: Epoch 28, Iteration 70, Current loss 0.3203032612800598 Accuracy 92.32067760469111\n",
      "Training:: Epoch 28, Iteration 80, Current loss 0.42267000675201416 Accuracy 85.6980488728926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 28, Iteration 90, Current loss 0.474159836769104 Accuracy 88.62680335275606\n",
      "Training:: Epoch 28, Iteration 100, Current loss 0.3970343768596649 Accuracy 90.91652358760527\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 28, Probability Accuracy 42.70280067945478\n",
      "Starting Training\n",
      "Training:: Epoch 29, Iteration 0, Current loss 0.46678298711776733 Accuracy 86.40503919880365\n",
      "Training:: Epoch 29, Iteration 10, Current loss 0.5901486873626709 Accuracy 81.72786778701082\n",
      "Training:: Epoch 29, Iteration 20, Current loss 0.4024909734725952 Accuracy 90.19958576539258\n",
      "Training:: Epoch 29, Iteration 30, Current loss 0.6046680808067322 Accuracy 82.91369269168455\n",
      "Training:: Epoch 29, Iteration 40, Current loss 0.4233800768852234 Accuracy 86.69393784444065\n",
      "Training:: Epoch 29, Iteration 50, Current loss 0.3564354181289673 Accuracy 89.75579211020664\n",
      "Training:: Epoch 29, Iteration 60, Current loss 0.37330904603004456 Accuracy 89.22599933562175\n",
      "Training:: Epoch 29, Iteration 70, Current loss 0.656891942024231 Accuracy 81.45851377100294\n",
      "Training:: Epoch 29, Iteration 80, Current loss 0.4686511158943176 Accuracy 86.15343203230148\n",
      "Training:: Epoch 29, Iteration 90, Current loss 0.6555518507957458 Accuracy 80.58648476945197\n",
      "Training:: Epoch 29, Iteration 100, Current loss 0.5386385321617126 Accuracy 85.69481812781719\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 29, Probability Accuracy 45.96625512698347\n",
      "Starting Training\n",
      "Training:: Epoch 30, Iteration 0, Current loss 0.4459967315196991 Accuracy 87.70816521164991\n",
      "Training:: Epoch 30, Iteration 10, Current loss 0.4042890965938568 Accuracy 86.70890607101947\n",
      "Training:: Epoch 30, Iteration 20, Current loss 0.3451036810874939 Accuracy 90.0138140011586\n",
      "Training:: Epoch 30, Iteration 30, Current loss 0.45168501138687134 Accuracy 87.62052532417157\n",
      "Training:: Epoch 30, Iteration 40, Current loss 0.35448095202445984 Accuracy 90.54360649786732\n",
      "Training:: Epoch 30, Iteration 50, Current loss 0.3055824935436249 Accuracy 91.43069290222417\n",
      "Training:: Epoch 30, Iteration 60, Current loss 0.30195382237434387 Accuracy 90.29735767660426\n",
      "Training:: Epoch 30, Iteration 70, Current loss 0.24920734763145447 Accuracy 91.7851275091078\n",
      "Training:: Epoch 30, Iteration 80, Current loss 0.5098828673362732 Accuracy 87.60839403399237\n",
      "Training:: Epoch 30, Iteration 90, Current loss 0.46285927295684814 Accuracy 86.49226719315564\n",
      "Training:: Epoch 30, Iteration 100, Current loss 0.603992760181427 Accuracy 85.8027147720741\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 30, Probability Accuracy 45.58820483075776\n",
      "Starting Training\n",
      "Training:: Epoch 31, Iteration 0, Current loss 0.5311582088470459 Accuracy 87.47371485878813\n",
      "Training:: Epoch 31, Iteration 10, Current loss 0.4276472330093384 Accuracy 88.51895630855442\n",
      "Training:: Epoch 31, Iteration 20, Current loss 0.4302251636981964 Accuracy 89.3559018594621\n",
      "Training:: Epoch 31, Iteration 30, Current loss 0.3803858458995819 Accuracy 90.95694047819026\n",
      "Training:: Epoch 31, Iteration 40, Current loss 0.1562620848417282 Accuracy 95.78188709584794\n",
      "Training:: Epoch 31, Iteration 50, Current loss 0.2072685956954956 Accuracy 93.80749869814268\n",
      "Training:: Epoch 31, Iteration 60, Current loss 0.39460012316703796 Accuracy 91.8238289775589\n",
      "Training:: Epoch 31, Iteration 70, Current loss 0.265909880399704 Accuracy 94.03288463275815\n",
      "Training:: Epoch 31, Iteration 80, Current loss 0.5713621377944946 Accuracy 82.87418655097613\n",
      "Training:: Epoch 31, Iteration 90, Current loss 0.2339928299188614 Accuracy 93.02801160412803\n",
      "Training:: Epoch 31, Iteration 100, Current loss 0.4057098627090454 Accuracy 87.88320541954295\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 31, Probability Accuracy 42.687782243029375\n",
      "Starting Training\n",
      "Training:: Epoch 32, Iteration 0, Current loss 0.34711822867393494 Accuracy 91.15131578947368\n",
      "Training:: Epoch 32, Iteration 10, Current loss 0.2694518268108368 Accuracy 92.36901056061444\n",
      "Training:: Epoch 32, Iteration 20, Current loss 0.2485552579164505 Accuracy 93.94455252918289\n",
      "Training:: Epoch 32, Iteration 30, Current loss 0.7644697427749634 Accuracy 80.98728043609934\n",
      "Training:: Epoch 32, Iteration 40, Current loss 0.5529624223709106 Accuracy 85.70425595934606\n",
      "Training:: Epoch 32, Iteration 50, Current loss 0.3793128430843353 Accuracy 88.51197761846477\n",
      "Training:: Epoch 32, Iteration 60, Current loss 0.5631031394004822 Accuracy 83.2773000581884\n",
      "Training:: Epoch 32, Iteration 70, Current loss 0.2610413134098053 Accuracy 93.01873023595232\n",
      "Training:: Epoch 32, Iteration 80, Current loss 0.5437288284301758 Accuracy 86.84384941675503\n",
      "Training:: Epoch 32, Iteration 90, Current loss 0.5120315551757812 Accuracy 83.44056891296987\n",
      "Training:: Epoch 32, Iteration 100, Current loss 0.3884303569793701 Accuracy 87.3046875\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 32, Probability Accuracy 43.72239714960434\n",
      "Starting Training\n",
      "Training:: Epoch 33, Iteration 0, Current loss 0.21600638329982758 Accuracy 94.05658010287291\n",
      "Training:: Epoch 33, Iteration 10, Current loss 0.32412612438201904 Accuracy 89.51180563476674\n",
      "Training:: Epoch 33, Iteration 20, Current loss 0.2563451826572418 Accuracy 93.78590316403756\n",
      "Training:: Epoch 33, Iteration 30, Current loss 0.24270056188106537 Accuracy 93.45253445657468\n",
      "Training:: Epoch 33, Iteration 40, Current loss 0.238128662109375 Accuracy 93.22321989432066\n",
      "Training:: Epoch 33, Iteration 50, Current loss 0.22616712749004364 Accuracy 93.67102106105038\n",
      "Training:: Epoch 33, Iteration 60, Current loss 0.22064407169818878 Accuracy 94.66445532138667\n",
      "Training:: Epoch 33, Iteration 70, Current loss 0.4388924241065979 Accuracy 88.9018487198927\n",
      "Training:: Epoch 33, Iteration 80, Current loss 0.5469819903373718 Accuracy 86.51908544855648\n",
      "Training:: Epoch 33, Iteration 90, Current loss 0.31011852622032166 Accuracy 89.71696533244013\n",
      "Training:: Epoch 33, Iteration 100, Current loss 0.4466676115989685 Accuracy 86.53786226238704\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 33, Probability Accuracy 43.52788250403944\n",
      "Starting Training\n",
      "Training:: Epoch 34, Iteration 0, Current loss 0.4018900990486145 Accuracy 88.18300898203593\n",
      "Training:: Epoch 34, Iteration 10, Current loss 0.21504563093185425 Accuracy 92.97411071353399\n",
      "Training:: Epoch 34, Iteration 20, Current loss 0.4441855847835541 Accuracy 88.32833772705933\n",
      "Training:: Epoch 34, Iteration 30, Current loss 0.6711555123329163 Accuracy 80.48623202424712\n",
      "Training:: Epoch 34, Iteration 40, Current loss 0.2638448178768158 Accuracy 92.36102614299115\n",
      "Training:: Epoch 34, Iteration 50, Current loss 0.24548178911209106 Accuracy 93.41402973694244\n",
      "Training:: Epoch 34, Iteration 60, Current loss 0.4405871331691742 Accuracy 88.19769162474105\n",
      "Training:: Epoch 34, Iteration 70, Current loss 0.24293041229248047 Accuracy 93.66125290023201\n",
      "Training:: Epoch 34, Iteration 80, Current loss 0.3418494462966919 Accuracy 88.76963192044146\n",
      "Training:: Epoch 34, Iteration 90, Current loss 0.5324957966804504 Accuracy 83.4809015073906\n",
      "Training:: Epoch 34, Iteration 100, Current loss 0.38292986154556274 Accuracy 90.44546111244883\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 34, Probability Accuracy 46.81660935493226\n",
      "Starting Training\n",
      "Training:: Epoch 35, Iteration 0, Current loss 0.21221554279327393 Accuracy 93.44567834122854\n",
      "Training:: Epoch 35, Iteration 10, Current loss 0.5836910605430603 Accuracy 82.18782436030507\n",
      "Training:: Epoch 35, Iteration 20, Current loss 0.3184443414211273 Accuracy 91.75647570429193\n",
      "Training:: Epoch 35, Iteration 30, Current loss 0.4152434468269348 Accuracy 88.4469247209689\n",
      "Validation:: Epoch 36, Probability Accuracy 44.73826490450346\n",
      "Starting Training\n",
      "Training:: Epoch 37, Iteration 0, Current loss 0.19532819092273712 Accuracy 94.70424473708906\n",
      "Training:: Epoch 37, Iteration 10, Current loss 0.2270926684141159 Accuracy 92.32635296635867\n",
      "Training:: Epoch 37, Iteration 20, Current loss 0.30058106780052185 Accuracy 92.91759130402076\n",
      "Training:: Epoch 37, Iteration 30, Current loss 0.25245335698127747 Accuracy 92.88540709061114\n",
      "Training:: Epoch 37, Iteration 40, Current loss 0.33906468749046326 Accuracy 90.45029025678573\n",
      "Training:: Epoch 37, Iteration 50, Current loss 0.24676549434661865 Accuracy 94.54838572822536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 37, Iteration 60, Current loss 0.16153424978256226 Accuracy 95.25893268408238\n",
      "Training:: Epoch 37, Iteration 70, Current loss 0.36529454588890076 Accuracy 89.95357948980026\n",
      "Training:: Epoch 37, Iteration 80, Current loss 0.3481544256210327 Accuracy 91.20784670610243\n",
      "Training:: Epoch 37, Iteration 90, Current loss 0.25349345803260803 Accuracy 91.78124709022626\n",
      "Training:: Epoch 37, Iteration 100, Current loss 0.20361006259918213 Accuracy 94.35985701045942\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 37, Probability Accuracy 43.17769399676845\n",
      "Starting Training\n",
      "Training:: Epoch 38, Iteration 0, Current loss 0.23664182424545288 Accuracy 93.68384521950374\n",
      "Training:: Epoch 38, Iteration 10, Current loss 0.4301421642303467 Accuracy 89.98899024850581\n",
      "Training:: Epoch 38, Iteration 20, Current loss 0.4278266131877899 Accuracy 87.47083757762955\n",
      "Training:: Epoch 38, Iteration 30, Current loss 0.2754284739494324 Accuracy 90.63172492946296\n",
      "Training:: Epoch 38, Iteration 40, Current loss 0.280886173248291 Accuracy 92.4516968665532\n",
      "Training:: Epoch 38, Iteration 50, Current loss 0.340351402759552 Accuracy 88.60368389780155\n",
      "Training:: Epoch 38, Iteration 60, Current loss 0.3457639515399933 Accuracy 87.05957041582901\n",
      "Training:: Epoch 38, Iteration 70, Current loss 0.2191498875617981 Accuracy 93.35755870745908\n",
      "Training:: Epoch 38, Iteration 80, Current loss 0.29052218794822693 Accuracy 91.29113663284751\n",
      "Training:: Epoch 38, Iteration 90, Current loss 0.30949798226356506 Accuracy 91.81491294473884\n",
      "Training:: Epoch 38, Iteration 100, Current loss 0.21126148104667664 Accuracy 94.06031363088059\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 38, Probability Accuracy 46.49003604424742\n",
      "Starting Training\n",
      "Training:: Epoch 39, Iteration 0, Current loss 0.27224260568618774 Accuracy 92.05463704905839\n",
      "Training:: Epoch 39, Iteration 10, Current loss 0.5058897733688354 Accuracy 84.98872209691218\n",
      "Training:: Epoch 39, Iteration 20, Current loss 0.5608037114143372 Accuracy 82.91563713378045\n",
      "Training:: Epoch 39, Iteration 30, Current loss 0.19893574714660645 Accuracy 94.82685810810811\n",
      "Training:: Epoch 39, Iteration 40, Current loss 0.3450069725513458 Accuracy 90.76998152602043\n",
      "Training:: Epoch 39, Iteration 50, Current loss 0.335693895816803 Accuracy 89.65005467895641\n",
      "Training:: Epoch 39, Iteration 60, Current loss 0.20418982207775116 Accuracy 94.81080315741036\n",
      "Training:: Epoch 39, Iteration 70, Current loss 0.24612689018249512 Accuracy 95.00435919790759\n",
      "Training:: Epoch 39, Iteration 80, Current loss 0.33139872550964355 Accuracy 93.21834574676598\n",
      "Training:: Epoch 39, Iteration 90, Current loss 0.1918904036283493 Accuracy 94.88013283979659\n",
      "Training:: Epoch 39, Iteration 100, Current loss 0.22108976542949677 Accuracy 92.51513483764447\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 39, Probability Accuracy 47.18678791896259\n",
      "Starting Training\n",
      "Training:: Epoch 40, Iteration 0, Current loss 0.32635191082954407 Accuracy 91.47789075431544\n",
      "Training:: Epoch 40, Iteration 10, Current loss 0.32551389932632446 Accuracy 90.49777462250498\n",
      "Training:: Epoch 40, Iteration 20, Current loss 0.12071673572063446 Accuracy 96.84138766853508\n",
      "Training:: Epoch 40, Iteration 30, Current loss 0.32860615849494934 Accuracy 90.67893123083662\n",
      "Training:: Epoch 40, Iteration 40, Current loss 0.26140135526657104 Accuracy 92.47966556095788\n",
      "Training:: Epoch 40, Iteration 50, Current loss 0.3763420581817627 Accuracy 90.62925770802846\n",
      "Training:: Epoch 40, Iteration 60, Current loss 0.3682538568973541 Accuracy 89.65929092280614\n",
      "Training:: Epoch 40, Iteration 70, Current loss 0.3929280936717987 Accuracy 88.29650748396294\n",
      "Training:: Epoch 40, Iteration 80, Current loss 0.2680257260799408 Accuracy 92.99313271174138\n",
      "Training:: Epoch 40, Iteration 90, Current loss 0.21371248364448547 Accuracy 94.15937149270482\n",
      "Training:: Epoch 40, Iteration 100, Current loss 0.19206726551055908 Accuracy 94.436073646687\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 40, Probability Accuracy 46.910034387040646\n",
      "Starting Training\n",
      "Training:: Epoch 41, Iteration 0, Current loss 0.23167094588279724 Accuracy 93.15028157090802\n",
      "Training:: Epoch 41, Iteration 10, Current loss 0.36569225788116455 Accuracy 89.68243510343919\n",
      "Training:: Epoch 41, Iteration 20, Current loss 0.22391143441200256 Accuracy 93.29565117083862\n",
      "Training:: Epoch 41, Iteration 30, Current loss 0.17642298340797424 Accuracy 95.38930194134655\n",
      "Training:: Epoch 41, Iteration 40, Current loss 0.2440761923789978 Accuracy 92.39150283036095\n",
      "Training:: Epoch 41, Iteration 50, Current loss 0.2244921326637268 Accuracy 94.27933930397595\n",
      "Training:: Epoch 41, Iteration 60, Current loss 0.2650229334831238 Accuracy 92.88339330832248\n",
      "Training:: Epoch 41, Iteration 70, Current loss 0.2459298074245453 Accuracy 91.93621679679431\n",
      "Training:: Epoch 41, Iteration 80, Current loss 0.2822871804237366 Accuracy 90.6897486541962\n",
      "Training:: Epoch 41, Iteration 90, Current loss 0.36725032329559326 Accuracy 87.98173173173173\n",
      "Training:: Epoch 41, Iteration 100, Current loss 0.20585504174232483 Accuracy 93.6231590667544\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 41, Probability Accuracy 45.55765008078883\n",
      "Starting Training\n",
      "Training:: Epoch 42, Iteration 0, Current loss 0.19910195469856262 Accuracy 94.1402399646511\n",
      "Training:: Epoch 42, Iteration 10, Current loss 0.3247945010662079 Accuracy 91.14052596561264\n",
      "Training:: Epoch 42, Iteration 20, Current loss 0.16948562860488892 Accuracy 95.20422425842523\n",
      "Training:: Epoch 42, Iteration 30, Current loss 0.32523924112319946 Accuracy 92.62445468259469\n",
      "Training:: Epoch 42, Iteration 40, Current loss 0.216734379529953 Accuracy 93.06439876296162\n",
      "Training:: Epoch 42, Iteration 50, Current loss 0.11929439008235931 Accuracy 96.4867791953932\n",
      "Training:: Epoch 42, Iteration 60, Current loss 0.16263265907764435 Accuracy 95.82468947769229\n",
      "Training:: Epoch 42, Iteration 70, Current loss 0.14298996329307556 Accuracy 96.0458704098258\n",
      "Training:: Epoch 42, Iteration 80, Current loss 0.2581058442592621 Accuracy 93.65943050465182\n",
      "Training:: Epoch 42, Iteration 90, Current loss 0.18099112808704376 Accuracy 94.38096792622216\n",
      "Training:: Epoch 42, Iteration 100, Current loss 0.29941627383232117 Accuracy 90.45846329275246\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 42, Probability Accuracy 45.69550896963169\n",
      "Starting Training\n",
      "Training:: Epoch 43, Iteration 0, Current loss 0.1803818792104721 Accuracy 95.63139931740615\n",
      "Training:: Epoch 43, Iteration 10, Current loss 0.15979796648025513 Accuracy 95.4452268473126\n",
      "Training:: Epoch 43, Iteration 20, Current loss 0.1820058524608612 Accuracy 94.40527065527066\n",
      "Training:: Epoch 43, Iteration 30, Current loss 0.46857547760009766 Accuracy 85.79027060604884\n",
      "Training:: Epoch 43, Iteration 40, Current loss 0.18628394603729248 Accuracy 93.86822871883061\n",
      "Training:: Epoch 43, Iteration 50, Current loss 0.1992524117231369 Accuracy 94.41872807146858\n",
      "Training:: Epoch 43, Iteration 60, Current loss 0.16268718242645264 Accuracy 94.95880737544135\n",
      "Training:: Epoch 43, Iteration 70, Current loss 0.1859559714794159 Accuracy 95.10517291849621\n",
      "Training:: Epoch 43, Iteration 80, Current loss 0.27107304334640503 Accuracy 91.12966933754656\n",
      "Training:: Epoch 43, Iteration 90, Current loss 0.1812814325094223 Accuracy 95.58542650185592\n",
      "Training:: Epoch 43, Iteration 100, Current loss 0.1999068260192871 Accuracy 93.77367424242425\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 43, Probability Accuracy 46.136429547996855\n",
      "Starting Training\n",
      "Training:: Epoch 44, Iteration 0, Current loss 0.13379314541816711 Accuracy 96.27667475367629\n",
      "Training:: Epoch 44, Iteration 10, Current loss 0.2373514324426651 Accuracy 92.92779259359281\n",
      "Training:: Epoch 44, Iteration 20, Current loss 0.25982052087783813 Accuracy 93.73058336209873\n",
      "Training:: Epoch 44, Iteration 30, Current loss 0.1627414971590042 Accuracy 95.76373386703615\n",
      "Training:: Epoch 44, Iteration 40, Current loss 0.31808343529701233 Accuracy 91.24285284381583\n",
      "Training:: Epoch 44, Iteration 50, Current loss 0.35096147656440735 Accuracy 91.15108540873558\n",
      "Training:: Epoch 44, Iteration 60, Current loss 0.20693455636501312 Accuracy 92.94850770744506\n",
      "Training:: Epoch 44, Iteration 70, Current loss 0.3695128858089447 Accuracy 89.7199298200013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 44, Iteration 80, Current loss 0.18038742244243622 Accuracy 94.48499845344881\n",
      "Training:: Epoch 44, Iteration 90, Current loss 0.2256098985671997 Accuracy 93.34759866856871\n",
      "Training:: Epoch 44, Iteration 100, Current loss 0.2967267632484436 Accuracy 93.41424424719196\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 44, Probability Accuracy 46.02094295065667\n",
      "Starting Training\n",
      "Training:: Epoch 45, Iteration 0, Current loss 0.09738133102655411 Accuracy 97.16278263882873\n",
      "Training:: Epoch 45, Iteration 10, Current loss 0.21051670610904694 Accuracy 95.31295572536895\n",
      "Training:: Epoch 45, Iteration 20, Current loss 0.4208747446537018 Accuracy 88.91078466499462\n",
      "Training:: Epoch 45, Iteration 30, Current loss 0.2727905511856079 Accuracy 90.9875876956287\n",
      "Training:: Epoch 45, Iteration 40, Current loss 0.3014087975025177 Accuracy 91.44242019053986\n",
      "Training:: Epoch 45, Iteration 50, Current loss 0.43143928050994873 Accuracy 86.94024478041757\n",
      "Training:: Epoch 45, Iteration 60, Current loss 0.14439034461975098 Accuracy 96.53998242530756\n",
      "Training:: Epoch 45, Iteration 70, Current loss 0.256115585565567 Accuracy 93.66369147861232\n",
      "Training:: Epoch 45, Iteration 80, Current loss 0.30240142345428467 Accuracy 90.89518596039285\n",
      "Training:: Epoch 45, Iteration 90, Current loss 0.18400408327579498 Accuracy 95.09836723768737\n",
      "Training:: Epoch 45, Iteration 100, Current loss 0.1942879855632782 Accuracy 94.20098224404987\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 45, Probability Accuracy 44.82723619339603\n",
      "Starting Training\n",
      "Training:: Epoch 46, Iteration 0, Current loss 0.24814219772815704 Accuracy 93.09647086829733\n",
      "Training:: Epoch 46, Iteration 10, Current loss 0.20894919335842133 Accuracy 95.16422341807386\n",
      "Training:: Epoch 46, Iteration 20, Current loss 0.3513668477535248 Accuracy 89.98266468548786\n",
      "Training:: Epoch 46, Iteration 30, Current loss 0.4351814389228821 Accuracy 91.05866177818515\n",
      "Training:: Epoch 46, Iteration 40, Current loss 0.22240982949733734 Accuracy 93.9875642065423\n",
      "Training:: Epoch 46, Iteration 50, Current loss 0.26329275965690613 Accuracy 92.93478260869566\n",
      "Training:: Epoch 46, Iteration 60, Current loss 0.25588756799697876 Accuracy 92.38390329737587\n",
      "Training:: Epoch 46, Iteration 70, Current loss 0.17595334351062775 Accuracy 95.58300355758084\n",
      "Training:: Epoch 46, Iteration 80, Current loss 0.09825576096773148 Accuracy 96.8730964467005\n",
      "Training:: Epoch 46, Iteration 90, Current loss 0.16169892251491547 Accuracy 95.43715459827122\n",
      "Training:: Epoch 46, Iteration 100, Current loss 0.21566173434257507 Accuracy 92.83966379402064\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 46, Probability Accuracy 47.004702324232504\n",
      "Starting Training\n",
      "Training:: Epoch 47, Iteration 0, Current loss 0.2111903727054596 Accuracy 94.33101946372669\n",
      "Training:: Epoch 47, Iteration 10, Current loss 0.17812150716781616 Accuracy 95.08493285092291\n",
      "Training:: Epoch 47, Iteration 20, Current loss 0.22227218747138977 Accuracy 93.34976332981262\n",
      "Training:: Epoch 47, Iteration 30, Current loss 0.3522575795650482 Accuracy 89.54227420819313\n",
      "Training:: Epoch 47, Iteration 40, Current loss 0.20164908468723297 Accuracy 93.56778882298889\n",
      "Training:: Epoch 47, Iteration 50, Current loss 0.2775874733924866 Accuracy 90.14421252371916\n",
      "Training:: Epoch 47, Iteration 60, Current loss 0.26236575841903687 Accuracy 93.04861426624262\n",
      "Training:: Epoch 47, Iteration 70, Current loss 0.233985036611557 Accuracy 93.74595730918499\n",
      "Training:: Epoch 47, Iteration 80, Current loss 0.24162280559539795 Accuracy 92.43471894574148\n",
      "Training:: Epoch 47, Iteration 90, Current loss 0.3333747684955597 Accuracy 88.23294136469083\n",
      "Training:: Epoch 47, Iteration 100, Current loss 0.15651170909404755 Accuracy 95.28676355680575\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 47, Probability Accuracy 46.63110577122261\n",
      "Starting Training\n",
      "Training:: Epoch 48, Iteration 0, Current loss 0.22513242065906525 Accuracy 93.5079365079365\n",
      "Training:: Epoch 48, Iteration 10, Current loss 0.13897822797298431 Accuracy 95.957061775109\n",
      "Training:: Epoch 48, Iteration 20, Current loss 0.2515825629234314 Accuracy 92.18277577294451\n",
      "Training:: Epoch 48, Iteration 30, Current loss 0.18736568093299866 Accuracy 94.1919191919192\n",
      "Training:: Epoch 48, Iteration 40, Current loss 0.24564746022224426 Accuracy 92.642408230255\n",
      "Training:: Epoch 48, Iteration 50, Current loss 0.16674008965492249 Accuracy 95.50301344459898\n",
      "Training:: Epoch 48, Iteration 60, Current loss 0.3981803357601166 Accuracy 87.17156105100463\n",
      "Training:: Epoch 48, Iteration 70, Current loss 0.3420875370502472 Accuracy 89.99875683739432\n",
      "Training:: Epoch 48, Iteration 80, Current loss 0.19343362748622894 Accuracy 94.9369316515107\n",
      "Training:: Epoch 48, Iteration 90, Current loss 0.19334468245506287 Accuracy 94.07009175795302\n",
      "Training:: Epoch 48, Iteration 100, Current loss 0.13534662127494812 Accuracy 96.60380731075163\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 48, Probability Accuracy 45.164892074408584\n",
      "Starting Training\n",
      "Training:: Epoch 49, Iteration 0, Current loss 0.12409680336713791 Accuracy 96.88380357994193\n",
      "Training:: Epoch 49, Iteration 10, Current loss 0.1354980170726776 Accuracy 96.45245170876672\n",
      "Training:: Epoch 49, Iteration 20, Current loss 0.17968343198299408 Accuracy 95.50632911392405\n",
      "Training:: Epoch 49, Iteration 30, Current loss 0.23211397230625153 Accuracy 92.13100011241428\n",
      "Training:: Epoch 49, Iteration 40, Current loss 0.25998786091804504 Accuracy 92.78598795840175\n",
      "Training:: Epoch 49, Iteration 50, Current loss 0.19981059432029724 Accuracy 93.94406392694064\n",
      "Training:: Epoch 49, Iteration 60, Current loss 0.11957700550556183 Accuracy 96.52148636209486\n",
      "Training:: Epoch 49, Iteration 70, Current loss 0.23171283304691315 Accuracy 92.04149887379701\n",
      "Training:: Epoch 49, Iteration 80, Current loss 0.12910105288028717 Accuracy 96.44585987261146\n",
      "Training:: Epoch 49, Iteration 90, Current loss 0.19537967443466187 Accuracy 94.71749889743715\n",
      "Training:: Epoch 49, Iteration 100, Current loss 0.3747817575931549 Accuracy 89.53455032017585\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 49, Probability Accuracy 46.73043460247752\n",
      "Starting Training\n",
      "Training:: Epoch 50, Iteration 0, Current loss 0.1810552477836609 Accuracy 95.19299499642602\n",
      "Training:: Epoch 50, Iteration 10, Current loss 0.1384851634502411 Accuracy 96.20859827275315\n",
      "Training:: Epoch 50, Iteration 20, Current loss 0.1738814115524292 Accuracy 95.67089491446526\n",
      "Training:: Epoch 50, Iteration 30, Current loss 0.18827801942825317 Accuracy 94.46765836536855\n",
      "Training:: Epoch 50, Iteration 40, Current loss 0.15756472945213318 Accuracy 95.36150292200838\n",
      "Training:: Epoch 50, Iteration 50, Current loss 0.19884423911571503 Accuracy 93.92412284872111\n",
      "Training:: Epoch 50, Iteration 60, Current loss 0.1692219227552414 Accuracy 95.04422476586889\n",
      "Training:: Epoch 50, Iteration 70, Current loss 0.2318813055753708 Accuracy 92.72220501703313\n",
      "Training:: Epoch 50, Iteration 80, Current loss 0.14368771016597748 Accuracy 96.58652755726922\n",
      "Training:: Epoch 50, Iteration 90, Current loss 0.13486698269844055 Accuracy 95.74979114452799\n",
      "Training:: Epoch 50, Iteration 100, Current loss 0.1739778369665146 Accuracy 95.14676226753305\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 50, Probability Accuracy 45.99981356423748\n",
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 0.2008548527956009 Accuracy 93.38015619267257\n",
      "Training:: Epoch 51, Iteration 10, Current loss 0.14155428111553192 Accuracy 95.56358570246854\n",
      "Training:: Epoch 51, Iteration 20, Current loss 0.2730841636657715 Accuracy 92.60280162675102\n",
      "Training:: Epoch 51, Iteration 30, Current loss 0.25634169578552246 Accuracy 92.67769287134931\n",
      "Training:: Epoch 51, Iteration 40, Current loss 0.14007870852947235 Accuracy 96.69927463898317\n",
      "Training:: Epoch 51, Iteration 50, Current loss 0.25553080439567566 Accuracy 90.83820662768031\n",
      "Training:: Epoch 51, Iteration 60, Current loss 0.16157789528369904 Accuracy 95.49788135593221\n",
      "Training:: Epoch 51, Iteration 70, Current loss 0.1737242043018341 Accuracy 94.63239211507725\n",
      "Training:: Epoch 51, Iteration 80, Current loss 0.11920791119337082 Accuracy 96.37830601686024\n",
      "Training:: Epoch 51, Iteration 90, Current loss 0.2465333640575409 Accuracy 94.05276059297303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 51, Iteration 100, Current loss 0.10155852884054184 Accuracy 97.3789816904941\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 51, Probability Accuracy 46.372270787587524\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 0.12050569802522659 Accuracy 96.96459936174949\n",
      "Training:: Epoch 52, Iteration 10, Current loss 0.3830178678035736 Accuracy 90.79160906413416\n",
      "Training:: Epoch 52, Iteration 20, Current loss 0.4585490822792053 Accuracy 86.71426292278238\n",
      "Training:: Epoch 52, Iteration 30, Current loss 0.2225940078496933 Accuracy 93.68005047989107\n",
      "Training:: Epoch 52, Iteration 40, Current loss 0.2238883376121521 Accuracy 93.43374446552815\n",
      "Training:: Epoch 52, Iteration 50, Current loss 0.21984615921974182 Accuracy 92.88229042115053\n",
      "Training:: Epoch 52, Iteration 60, Current loss 0.16489490866661072 Accuracy 95.3785135368167\n",
      "Training:: Epoch 52, Iteration 70, Current loss 0.15397495031356812 Accuracy 95.05035291338118\n",
      "Training:: Epoch 52, Iteration 80, Current loss 0.2774980664253235 Accuracy 92.26519337016575\n",
      "Training:: Epoch 52, Iteration 90, Current loss 0.20065002143383026 Accuracy 94.33143140659885\n",
      "Training:: Epoch 52, Iteration 100, Current loss 0.14317519962787628 Accuracy 95.92394196317713\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 52, Probability Accuracy 47.385445581472425\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 0.14462319016456604 Accuracy 96.316120906801\n",
      "Training:: Epoch 53, Iteration 10, Current loss 0.2326052188873291 Accuracy 93.14058786365415\n",
      "Training:: Epoch 53, Iteration 20, Current loss 0.16009534895420074 Accuracy 95.44492355938847\n",
      "Training:: Epoch 53, Iteration 30, Current loss 0.549809455871582 Accuracy 85.46662691144525\n",
      "Training:: Epoch 53, Iteration 40, Current loss 0.139133021235466 Accuracy 96.02921510132703\n",
      "Training:: Epoch 53, Iteration 50, Current loss 0.14593134820461273 Accuracy 96.06234842085185\n",
      "Training:: Epoch 53, Iteration 60, Current loss 0.21385319530963898 Accuracy 93.63636363636364\n",
      "Training:: Epoch 53, Iteration 70, Current loss 0.1489560455083847 Accuracy 97.17592721404525\n",
      "Training:: Epoch 53, Iteration 80, Current loss 0.1772238165140152 Accuracy 94.52006980802793\n",
      "Training:: Epoch 53, Iteration 90, Current loss 0.09951968491077423 Accuracy 96.92199144507651\n",
      "Training:: Epoch 53, Iteration 100, Current loss 0.11121176928281784 Accuracy 96.52264254721017\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 53, Probability Accuracy 48.02512739777106\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 0.13270804286003113 Accuracy 95.79353416605437\n",
      "Training:: Epoch 54, Iteration 10, Current loss 0.2105659693479538 Accuracy 94.31542603477331\n",
      "Training:: Epoch 54, Iteration 20, Current loss 0.14376766979694366 Accuracy 95.63702865104938\n",
      "Training:: Epoch 54, Iteration 30, Current loss 0.2755540609359741 Accuracy 92.4678137408254\n",
      "Training:: Epoch 54, Iteration 40, Current loss 0.35436588525772095 Accuracy 88.81655714548916\n",
      "Training:: Epoch 54, Iteration 50, Current loss 0.11972497403621674 Accuracy 96.6842122908774\n",
      "Training:: Epoch 54, Iteration 60, Current loss 0.12706272304058075 Accuracy 96.34693620769198\n",
      "Training:: Epoch 54, Iteration 70, Current loss 0.14650949835777283 Accuracy 96.75903957483155\n",
      "Training:: Epoch 54, Iteration 80, Current loss 0.19481341540813446 Accuracy 93.89119643399859\n",
      "Training:: Epoch 54, Iteration 90, Current loss 0.18058790266513824 Accuracy 94.42301888057949\n",
      "Training:: Epoch 54, Iteration 100, Current loss 0.1671816110610962 Accuracy 94.83480425435619\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 54, Probability Accuracy 45.42528068939802\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 0.29173189401626587 Accuracy 91.5016679220919\n",
      "Training:: Epoch 55, Iteration 10, Current loss 0.12024694681167603 Accuracy 96.44335649231434\n",
      "Training:: Epoch 55, Iteration 20, Current loss 0.10068544745445251 Accuracy 97.1477184841454\n",
      "Training:: Epoch 55, Iteration 30, Current loss 0.14531774818897247 Accuracy 95.58765688132442\n",
      "Training:: Epoch 55, Iteration 40, Current loss 0.10651715844869614 Accuracy 97.11488358061597\n",
      "Training:: Epoch 55, Iteration 50, Current loss 0.07512173801660538 Accuracy 98.35469910998611\n",
      "Training:: Epoch 55, Iteration 60, Current loss 0.1601000428199768 Accuracy 96.06272119543846\n",
      "Training:: Epoch 55, Iteration 70, Current loss 0.14846418797969818 Accuracy 95.54014433919711\n",
      "Training:: Epoch 55, Iteration 80, Current loss 0.17442508041858673 Accuracy 94.11349395463772\n",
      "Training:: Epoch 55, Iteration 90, Current loss 0.08348715305328369 Accuracy 97.42731133616465\n",
      "Training:: Epoch 55, Iteration 100, Current loss 0.059356532990932465 Accuracy 98.4042389328733\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 48.043149521481546\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 0.09513777494430542 Accuracy 97.39002617144087\n",
      "Training:: Epoch 56, Iteration 10, Current loss 0.1534530222415924 Accuracy 95.03442520342166\n",
      "Training:: Epoch 56, Iteration 20, Current loss 0.10707703232765198 Accuracy 96.89090468441644\n",
      "Training:: Epoch 56, Iteration 30, Current loss 0.17087146639823914 Accuracy 94.61793507595034\n",
      "Training:: Epoch 56, Iteration 40, Current loss 0.19470490515232086 Accuracy 95.11060222178912\n",
      "Training:: Epoch 56, Iteration 50, Current loss 0.10018632560968399 Accuracy 97.48206131767776\n",
      "Training:: Epoch 56, Iteration 60, Current loss 0.10473767668008804 Accuracy 97.15343011670936\n",
      "Training:: Epoch 56, Iteration 70, Current loss 0.12282535433769226 Accuracy 96.08887536143662\n",
      "Training:: Epoch 56, Iteration 80, Current loss 0.14647158980369568 Accuracy 95.72863397226004\n",
      "Training:: Epoch 56, Iteration 90, Current loss 0.1258191019296646 Accuracy 95.9335861563466\n",
      "Training:: Epoch 56, Iteration 100, Current loss 0.14844107627868652 Accuracy 95.3283847022753\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 56, Probability Accuracy 46.46952811036997\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 0.08490070700645447 Accuracy 97.60107043868871\n",
      "Training:: Epoch 57, Iteration 10, Current loss 0.12465248256921768 Accuracy 97.40899971338493\n",
      "Training:: Epoch 57, Iteration 20, Current loss 0.10904613882303238 Accuracy 96.67885435709933\n",
      "Training:: Epoch 57, Iteration 30, Current loss 0.20558781921863556 Accuracy 94.29946950415504\n",
      "Training:: Epoch 57, Iteration 40, Current loss 0.20534354448318481 Accuracy 92.76672694394213\n",
      "Training:: Epoch 57, Iteration 50, Current loss 0.14143291115760803 Accuracy 95.69715494944745\n",
      "Training:: Epoch 57, Iteration 60, Current loss 0.14786100387573242 Accuracy 94.42958590795905\n",
      "Training:: Epoch 57, Iteration 70, Current loss 0.151347815990448 Accuracy 95.04684902523802\n",
      "Training:: Epoch 57, Iteration 80, Current loss 0.33415094017982483 Accuracy 88.98590613581361\n",
      "Training:: Epoch 57, Iteration 90, Current loss 0.1306704431772232 Accuracy 95.8335897593698\n",
      "Training:: Epoch 57, Iteration 100, Current loss 0.18392793834209442 Accuracy 93.41093437269566\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 57, Probability Accuracy 46.4061399511124\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 0.24524737894535065 Accuracy 92.68398806563899\n",
      "Training:: Epoch 58, Iteration 10, Current loss 0.17565611004829407 Accuracy 94.51679476231142\n",
      "Training:: Epoch 58, Iteration 20, Current loss 0.1620243787765503 Accuracy 94.35389988358557\n",
      "Training:: Epoch 58, Iteration 30, Current loss 0.17028063535690308 Accuracy 94.5104249142254\n",
      "Training:: Epoch 58, Iteration 40, Current loss 0.1071406900882721 Accuracy 96.8616629874908\n",
      "Training:: Epoch 58, Iteration 50, Current loss 0.24943099915981293 Accuracy 92.91212917761922\n",
      "Training:: Epoch 58, Iteration 60, Current loss 0.1277938187122345 Accuracy 96.16532572021801\n",
      "Training:: Epoch 58, Iteration 70, Current loss 0.2540876567363739 Accuracy 93.06888332009046\n",
      "Training:: Epoch 58, Iteration 80, Current loss 0.1118544340133667 Accuracy 96.29863744907993\n",
      "Training:: Epoch 58, Iteration 90, Current loss 0.13927790522575378 Accuracy 95.67761566669267\n",
      "Training:: Epoch 58, Iteration 100, Current loss 0.23548151552677155 Accuracy 92.72700612192529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 58, Probability Accuracy 43.860152463023574\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 0.1261090189218521 Accuracy 96.84429404329921\n",
      "Training:: Epoch 59, Iteration 10, Current loss 0.14732412993907928 Accuracy 95.70015220700152\n",
      "Training:: Epoch 59, Iteration 20, Current loss 0.21843372285366058 Accuracy 94.30062211519166\n",
      "Training:: Epoch 59, Iteration 30, Current loss 0.14537246525287628 Accuracy 95.52131291302578\n",
      "Training:: Epoch 59, Iteration 40, Current loss 0.11564552038908005 Accuracy 96.87232394079028\n",
      "Training:: Epoch 59, Iteration 50, Current loss 0.2178802490234375 Accuracy 94.50619712768051\n",
      "Training:: Epoch 59, Iteration 60, Current loss 0.17366184294223785 Accuracy 94.87689393939394\n",
      "Training:: Epoch 59, Iteration 70, Current loss 0.1805352419614792 Accuracy 94.65128118106227\n",
      "Training:: Epoch 59, Iteration 80, Current loss 0.17190557718276978 Accuracy 95.7657467874447\n",
      "Training:: Epoch 59, Iteration 90, Current loss 0.12892182171344757 Accuracy 96.27056672760511\n",
      "Training:: Epoch 59, Iteration 100, Current loss 0.37200573086738586 Accuracy 88.9865971253563\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 59, Probability Accuracy 46.082984629407136\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 0.10409503430128098 Accuracy 96.94341991681708\n",
      "Training:: Epoch 60, Iteration 10, Current loss 0.12351956218481064 Accuracy 96.56781458485982\n",
      "Training:: Epoch 60, Iteration 20, Current loss 0.13252632319927216 Accuracy 95.80772607550483\n",
      "Training:: Epoch 60, Iteration 30, Current loss 0.08077885955572128 Accuracy 97.53694581280789\n",
      "Training:: Epoch 60, Iteration 40, Current loss 0.16487394273281097 Accuracy 95.0697122515574\n",
      "Training:: Epoch 60, Iteration 50, Current loss 0.12538324296474457 Accuracy 96.577243293247\n",
      "Training:: Epoch 60, Iteration 60, Current loss 0.08877301216125488 Accuracy 97.32961931290622\n",
      "Training:: Epoch 60, Iteration 70, Current loss 0.15417322516441345 Accuracy 95.45802041642308\n",
      "Training:: Epoch 60, Iteration 80, Current loss 0.2518885135650635 Accuracy 92.92992306092742\n",
      "Training:: Epoch 60, Iteration 90, Current loss 0.24033920466899872 Accuracy 92.32219827586206\n",
      "Training:: Epoch 60, Iteration 100, Current loss 0.08409339189529419 Accuracy 97.91858842451335\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 46.93251025396694\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 0.4186207056045532 Accuracy 87.6420886268312\n",
      "Training:: Epoch 61, Iteration 10, Current loss 0.1636873483657837 Accuracy 95.1784213341484\n",
      "Training:: Epoch 61, Iteration 20, Current loss 0.15862445533275604 Accuracy 95.1950494448826\n",
      "Training:: Epoch 61, Iteration 30, Current loss 0.15039634704589844 Accuracy 96.2846189674322\n",
      "Training:: Epoch 61, Iteration 40, Current loss 0.21431167423725128 Accuracy 92.30036816046719\n",
      "Training:: Epoch 61, Iteration 50, Current loss 0.186243936419487 Accuracy 94.8339096720109\n",
      "Training:: Epoch 61, Iteration 60, Current loss 0.13723796606063843 Accuracy 95.81937558622538\n",
      "Training:: Epoch 61, Iteration 70, Current loss 0.13036496937274933 Accuracy 96.30425269523013\n",
      "Training:: Epoch 61, Iteration 80, Current loss 0.29591554403305054 Accuracy 89.13481696500564\n",
      "Training:: Epoch 61, Iteration 90, Current loss 0.10553760826587677 Accuracy 97.11544227886057\n",
      "Training:: Epoch 61, Iteration 100, Current loss 0.22448910772800446 Accuracy 92.90527150217994\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 61, Probability Accuracy 42.79415420309069\n",
      "Starting Training\n",
      "Training:: Epoch 62, Iteration 0, Current loss 0.3006693124771118 Accuracy 92.33353329334133\n",
      "Training:: Epoch 62, Iteration 10, Current loss 0.22761768102645874 Accuracy 92.145468790807\n",
      "Training:: Epoch 62, Iteration 20, Current loss 0.17988714575767517 Accuracy 95.03775620280474\n",
      "Training:: Epoch 62, Iteration 30, Current loss 0.2218349575996399 Accuracy 93.94228324434275\n",
      "Training:: Epoch 62, Iteration 40, Current loss 0.1734314113855362 Accuracy 94.58893088350858\n",
      "Training:: Epoch 62, Iteration 50, Current loss 0.15370844304561615 Accuracy 94.98786234470941\n",
      "Training:: Epoch 62, Iteration 60, Current loss 0.23639923334121704 Accuracy 92.20102740956281\n",
      "Training:: Epoch 62, Iteration 70, Current loss 0.17142035067081451 Accuracy 94.04336577553283\n",
      "Training:: Epoch 62, Iteration 80, Current loss 0.19180624186992645 Accuracy 94.45934554371982\n",
      "Training:: Epoch 62, Iteration 90, Current loss 0.07859458029270172 Accuracy 97.95991839673587\n",
      "Training:: Epoch 62, Iteration 100, Current loss 0.13980969786643982 Accuracy 96.13019776876268\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 62, Probability Accuracy 45.044641007581724\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 0.14385823905467987 Accuracy 95.71305934113103\n",
      "Training:: Epoch 63, Iteration 10, Current loss 0.1364266574382782 Accuracy 95.98684210526316\n",
      "Training:: Epoch 63, Iteration 20, Current loss 0.22631336748600006 Accuracy 92.8024083196497\n",
      "Training:: Epoch 63, Iteration 30, Current loss 0.09628162533044815 Accuracy 96.88603653210801\n",
      "Training:: Epoch 63, Iteration 40, Current loss 0.16272544860839844 Accuracy 94.96546852927327\n",
      "Training:: Epoch 63, Iteration 50, Current loss 0.11109409481287003 Accuracy 96.61460149222509\n",
      "Training:: Epoch 63, Iteration 60, Current loss 0.17038919031620026 Accuracy 95.27498063516654\n",
      "Training:: Epoch 63, Iteration 70, Current loss 0.1357228308916092 Accuracy 95.27305551376561\n",
      "Training:: Epoch 63, Iteration 80, Current loss 0.10554856806993484 Accuracy 96.83247923661425\n",
      "Training:: Epoch 63, Iteration 90, Current loss 0.09115567058324814 Accuracy 97.63284515154707\n",
      "Training:: Epoch 63, Iteration 100, Current loss 0.13465197384357452 Accuracy 95.37908470331624\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 63, Probability Accuracy 48.124663379873226\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 0.1098824217915535 Accuracy 96.4737041241014\n",
      "Training:: Epoch 64, Iteration 10, Current loss 0.1557975560426712 Accuracy 95.16638109838578\n",
      "Training:: Epoch 64, Iteration 20, Current loss 0.09445416927337646 Accuracy 96.8971045930397\n",
      "Training:: Epoch 64, Iteration 30, Current loss 0.0974484458565712 Accuracy 97.24179491933988\n",
      "Training:: Epoch 64, Iteration 40, Current loss 0.12773990631103516 Accuracy 96.53105774955348\n",
      "Training:: Epoch 64, Iteration 50, Current loss 0.11663070321083069 Accuracy 96.99634862676615\n",
      "Training:: Epoch 64, Iteration 60, Current loss 0.09261320531368256 Accuracy 97.46754287546081\n",
      "Training:: Epoch 64, Iteration 70, Current loss 0.08970491588115692 Accuracy 97.34250366270552\n",
      "Training:: Epoch 64, Iteration 80, Current loss 0.09937074780464172 Accuracy 97.18430034129693\n",
      "Training:: Epoch 64, Iteration 90, Current loss 0.07508429884910583 Accuracy 97.581674543761\n",
      "Training:: Epoch 64, Iteration 100, Current loss 0.24979542195796967 Accuracy 94.22603446906521\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 64, Probability Accuracy 46.85006421676265\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 0.06575968861579895 Accuracy 98.13472208039757\n",
      "Training:: Epoch 65, Iteration 10, Current loss 0.18093882501125336 Accuracy 94.22139830508475\n",
      "Training:: Epoch 65, Iteration 20, Current loss 0.09152925759553909 Accuracy 97.15767145887344\n",
      "Training:: Epoch 65, Iteration 30, Current loss 0.1116240993142128 Accuracy 96.48931776367664\n",
      "Training:: Epoch 65, Iteration 40, Current loss 0.1159207820892334 Accuracy 96.87995769434161\n",
      "Training:: Epoch 65, Iteration 50, Current loss 0.15497927367687225 Accuracy 95.82483062607135\n",
      "Training:: Epoch 65, Iteration 60, Current loss 0.2147965282201767 Accuracy 93.99708103306047\n",
      "Training:: Epoch 65, Iteration 70, Current loss 0.2107299268245697 Accuracy 94.75156859420044\n",
      "Training:: Epoch 65, Iteration 80, Current loss 0.1364399492740631 Accuracy 95.60782905369007\n",
      "Training:: Epoch 65, Iteration 90, Current loss 0.1097145676612854 Accuracy 96.83730200356656\n",
      "Training:: Epoch 65, Iteration 100, Current loss 0.1555485874414444 Accuracy 95.35175475279055\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 45.81410282968057\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 66, Iteration 0, Current loss 0.11172472685575485 Accuracy 96.67385832434375\n",
      "Training:: Epoch 66, Iteration 10, Current loss 0.15622110664844513 Accuracy 94.98178874949413\n",
      "Training:: Epoch 66, Iteration 20, Current loss 0.11208449304103851 Accuracy 96.57155427063039\n",
      "Training:: Epoch 66, Iteration 30, Current loss 0.10167761147022247 Accuracy 96.93243243243244\n",
      "Training:: Epoch 66, Iteration 40, Current loss 0.10675836354494095 Accuracy 96.24168080386272\n",
      "Training:: Epoch 66, Iteration 50, Current loss 0.09170003235340118 Accuracy 97.31285988483685\n",
      "Training:: Epoch 66, Iteration 60, Current loss 0.06969950348138809 Accuracy 98.36774307393682\n",
      "Training:: Epoch 66, Iteration 70, Current loss 0.09589774906635284 Accuracy 97.10240963855422\n",
      "Training:: Epoch 66, Iteration 80, Current loss 0.10366518795490265 Accuracy 96.77512444989539\n",
      "Training:: Epoch 66, Iteration 90, Current loss 0.07478057593107224 Accuracy 97.61086963129578\n",
      "Training:: Epoch 66, Iteration 100, Current loss 0.11162137985229492 Accuracy 96.59032556228819\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 66, Probability Accuracy 46.91666321415254\n",
      "Starting Training\n",
      "Training:: Epoch 67, Iteration 0, Current loss 0.11471377313137054 Accuracy 97.56232278970431\n",
      "Training:: Epoch 67, Iteration 10, Current loss 0.520512580871582 Accuracy 86.17648516077887\n",
      "Training:: Epoch 67, Iteration 20, Current loss 0.10545527935028076 Accuracy 96.69129457115322\n",
      "Training:: Epoch 67, Iteration 30, Current loss 0.17191068828105927 Accuracy 96.22744439781788\n",
      "Training:: Epoch 67, Iteration 40, Current loss 0.15680460631847382 Accuracy 94.74349590791137\n",
      "Training:: Epoch 67, Iteration 50, Current loss 0.18425051867961884 Accuracy 94.06340989970883\n",
      "Training:: Epoch 67, Iteration 60, Current loss 0.11308272927999496 Accuracy 97.07937977320064\n",
      "Training:: Epoch 67, Iteration 70, Current loss 0.10593770444393158 Accuracy 96.31741593701715\n",
      "Training:: Epoch 67, Iteration 80, Current loss 0.18866892158985138 Accuracy 94.29043015461313\n",
      "Training:: Epoch 67, Iteration 90, Current loss 0.1153111606836319 Accuracy 96.25175508210732\n",
      "Training:: Epoch 67, Iteration 100, Current loss 0.3276882469654083 Accuracy 89.57473360852453\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 67, Probability Accuracy 46.661142644073415\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 0.20744961500167847 Accuracy 94.49305370024045\n",
      "Training:: Epoch 68, Iteration 10, Current loss 0.1216520145535469 Accuracy 96.44464925029244\n",
      "Training:: Epoch 68, Iteration 20, Current loss 0.09293587505817413 Accuracy 96.9249345944858\n",
      "Training:: Epoch 68, Iteration 30, Current loss 0.09764911979436874 Accuracy 97.61608079748164\n",
      "Training:: Epoch 68, Iteration 40, Current loss 0.12674596905708313 Accuracy 96.57707076247942\n",
      "Training:: Epoch 68, Iteration 50, Current loss 0.10372664034366608 Accuracy 96.83098591549296\n",
      "Training:: Epoch 68, Iteration 60, Current loss 0.13143323361873627 Accuracy 95.6488424278912\n",
      "Training:: Epoch 68, Iteration 70, Current loss 0.11137375980615616 Accuracy 96.97405250018912\n",
      "Training:: Epoch 68, Iteration 80, Current loss 0.14272645115852356 Accuracy 95.20797155379145\n",
      "Training:: Epoch 68, Iteration 90, Current loss 0.13728651404380798 Accuracy 95.51276540875097\n",
      "Training:: Epoch 68, Iteration 100, Current loss 0.12659896910190582 Accuracy 96.41835542484186\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 68, Probability Accuracy 45.21595475825496\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 0.07476445287466049 Accuracy 97.74562446202403\n",
      "Training:: Epoch 69, Iteration 10, Current loss 0.17659227550029755 Accuracy 94.26914637968865\n",
      "Training:: Epoch 69, Iteration 20, Current loss 0.17113719880580902 Accuracy 95.02392789109325\n",
      "Training:: Epoch 69, Iteration 30, Current loss 0.23024149239063263 Accuracy 93.70670051298323\n",
      "Training:: Epoch 69, Iteration 40, Current loss 0.21920064091682434 Accuracy 93.88754343476887\n",
      "Training:: Epoch 69, Iteration 50, Current loss 0.1323479264974594 Accuracy 95.85146641438033\n",
      "Training:: Epoch 69, Iteration 60, Current loss 0.09491374343633652 Accuracy 96.9473450344368\n",
      "Training:: Epoch 69, Iteration 70, Current loss 0.18044252693653107 Accuracy 95.0998693298488\n",
      "Training:: Epoch 69, Iteration 80, Current loss 0.17243511974811554 Accuracy 95.00106723585913\n",
      "Training:: Epoch 69, Iteration 90, Current loss 0.13148747384548187 Accuracy 96.40999767134984\n",
      "Training:: Epoch 69, Iteration 100, Current loss 0.1029805988073349 Accuracy 97.2464952536619\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 69, Probability Accuracy 44.41738824211791\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 0.1487809270620346 Accuracy 95.90594984571254\n",
      "Training:: Epoch 70, Iteration 10, Current loss 0.152832493185997 Accuracy 94.99048826886494\n",
      "Training:: Epoch 70, Iteration 20, Current loss 0.1259925812482834 Accuracy 96.1080492617541\n",
      "Training:: Epoch 70, Iteration 30, Current loss 0.12537050247192383 Accuracy 96.23754743833017\n",
      "Training:: Epoch 70, Iteration 40, Current loss 0.2598932683467865 Accuracy 92.86533470151505\n",
      "Training:: Epoch 70, Iteration 50, Current loss 0.08077126741409302 Accuracy 97.6138381940333\n",
      "Training:: Epoch 70, Iteration 60, Current loss 0.12444493919610977 Accuracy 96.39649388594307\n",
      "Training:: Epoch 70, Iteration 70, Current loss 0.07450085133314133 Accuracy 97.50325839035517\n",
      "Training:: Epoch 70, Iteration 80, Current loss 0.15398666262626648 Accuracy 95.2378114197163\n",
      "Training:: Epoch 70, Iteration 90, Current loss 0.12545929849147797 Accuracy 96.58771765586968\n",
      "Training:: Epoch 70, Iteration 100, Current loss 0.10350033640861511 Accuracy 97.13710925410089\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 44.09454364668351\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 0.08863013982772827 Accuracy 97.21842536905413\n",
      "Training:: Epoch 71, Iteration 10, Current loss 0.10184802860021591 Accuracy 96.96233292831106\n",
      "Training:: Epoch 71, Iteration 20, Current loss 0.14063973724842072 Accuracy 95.23657033063806\n",
      "Training:: Epoch 71, Iteration 30, Current loss 0.21054889261722565 Accuracy 93.77248201438849\n",
      "Training:: Epoch 71, Iteration 40, Current loss 0.1020469143986702 Accuracy 96.65906997042137\n",
      "Training:: Epoch 71, Iteration 50, Current loss 0.09216208755970001 Accuracy 97.59756524549543\n",
      "Training:: Epoch 71, Iteration 60, Current loss 0.0558198057115078 Accuracy 98.08112472689275\n",
      "Training:: Epoch 71, Iteration 70, Current loss 0.06192566081881523 Accuracy 98.20017243856876\n",
      "Training:: Epoch 71, Iteration 80, Current loss 0.12111064046621323 Accuracy 96.95898161244696\n",
      "Training:: Epoch 71, Iteration 90, Current loss 0.08905552327632904 Accuracy 97.78333680772705\n",
      "Training:: Epoch 71, Iteration 100, Current loss 0.06158449128270149 Accuracy 98.11172921726866\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 71, Probability Accuracy 48.11523801632349\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 0.09551085531711578 Accuracy 97.3259423503326\n",
      "Training:: Epoch 72, Iteration 10, Current loss 0.061911918222904205 Accuracy 98.00752457002457\n",
      "Training:: Epoch 72, Iteration 20, Current loss 0.11012666672468185 Accuracy 96.45167853509665\n",
      "Training:: Epoch 72, Iteration 30, Current loss 0.1338072270154953 Accuracy 96.15047021943573\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.06747622787952423 Accuracy 97.59735462764273\n",
      "Training:: Epoch 72, Iteration 50, Current loss 0.07402852177619934 Accuracy 97.69092881739705\n",
      "Training:: Epoch 72, Iteration 60, Current loss 0.46180975437164307 Accuracy 91.14553924336533\n",
      "Training:: Epoch 72, Iteration 70, Current loss 0.10883940011262894 Accuracy 96.37307818122342\n",
      "Training:: Epoch 72, Iteration 80, Current loss 0.06792934238910675 Accuracy 98.08191252779837\n",
      "Training:: Epoch 72, Iteration 90, Current loss 0.08955160528421402 Accuracy 97.34317343173431\n",
      "Training:: Epoch 72, Iteration 100, Current loss 0.0762067586183548 Accuracy 97.73520230152415\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 72, Probability Accuracy 47.49554625678419\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 0.06279827654361725 Accuracy 98.3800122797903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 73, Iteration 10, Current loss 0.0836850181221962 Accuracy 97.34345351043643\n",
      "Training:: Epoch 73, Iteration 20, Current loss 0.12545163929462433 Accuracy 97.10453409014123\n",
      "Training:: Epoch 73, Iteration 30, Current loss 0.14180706441402435 Accuracy 95.96464258262874\n",
      "Training:: Epoch 73, Iteration 40, Current loss 0.15697476267814636 Accuracy 95.83620096352374\n",
      "Training:: Epoch 73, Iteration 50, Current loss 0.17238417267799377 Accuracy 95.48451123555681\n",
      "Training:: Epoch 73, Iteration 60, Current loss 0.12938615679740906 Accuracy 96.84033858727379\n",
      "Training:: Epoch 73, Iteration 70, Current loss 0.052557893097400665 Accuracy 98.42006154411428\n",
      "Training:: Epoch 73, Iteration 80, Current loss 0.05376144498586655 Accuracy 98.35452880828755\n",
      "Training:: Epoch 73, Iteration 90, Current loss 0.0667339637875557 Accuracy 98.02634619828981\n",
      "Training:: Epoch 73, Iteration 100, Current loss 0.15709291398525238 Accuracy 95.83748361730014\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 73, Probability Accuracy 47.95966773004102\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 0.3044877052307129 Accuracy 90.52315278526851\n",
      "Training:: Epoch 74, Iteration 10, Current loss 0.16849219799041748 Accuracy 95.63323005422153\n",
      "Training:: Epoch 74, Iteration 20, Current loss 0.06993738561868668 Accuracy 97.79449012196548\n",
      "Training:: Epoch 74, Iteration 30, Current loss 0.1289995163679123 Accuracy 95.67499475120722\n",
      "Training:: Epoch 74, Iteration 40, Current loss 0.08825565874576569 Accuracy 98.1480628368729\n",
      "Training:: Epoch 74, Iteration 50, Current loss 0.06168469041585922 Accuracy 98.48450004599393\n",
      "Training:: Epoch 74, Iteration 60, Current loss 0.09089157730340958 Accuracy 97.14417584953557\n",
      "Training:: Epoch 74, Iteration 70, Current loss 0.05364033952355385 Accuracy 98.81229561805101\n",
      "Training:: Epoch 74, Iteration 80, Current loss 0.04975859820842743 Accuracy 98.71177370030581\n",
      "Training:: Epoch 74, Iteration 90, Current loss 0.07464495301246643 Accuracy 97.98923467178123\n",
      "Training:: Epoch 74, Iteration 100, Current loss 0.06641244888305664 Accuracy 98.10486891385767\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 74, Probability Accuracy 46.558913700957035\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 0.12099814414978027 Accuracy 97.2757610842471\n",
      "Training:: Epoch 75, Iteration 10, Current loss 0.13860376179218292 Accuracy 97.18432637405745\n",
      "Training:: Epoch 75, Iteration 20, Current loss 0.10156576335430145 Accuracy 97.03703703703704\n",
      "Training:: Epoch 75, Iteration 30, Current loss 0.08966407924890518 Accuracy 97.25559952081896\n",
      "Training:: Epoch 75, Iteration 40, Current loss 0.06981741636991501 Accuracy 97.82860993598143\n",
      "Training:: Epoch 75, Iteration 50, Current loss 0.08894634246826172 Accuracy 97.27965179542981\n",
      "Training:: Epoch 75, Iteration 60, Current loss 0.11885339766740799 Accuracy 96.3575042158516\n",
      "Training:: Epoch 75, Iteration 70, Current loss 0.08573375642299652 Accuracy 97.39831287062557\n",
      "Training:: Epoch 75, Iteration 80, Current loss 0.06962597370147705 Accuracy 97.85140437192501\n",
      "Training:: Epoch 75, Iteration 90, Current loss 0.22147397696971893 Accuracy 94.89840348330914\n",
      "Training:: Epoch 75, Iteration 100, Current loss 0.13496294617652893 Accuracy 95.85901309164149\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 48.18670505862369\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 0.0691874623298645 Accuracy 98.31791924815181\n",
      "Training:: Epoch 76, Iteration 10, Current loss 0.41717830300331116 Accuracy 90.46478198370868\n",
      "Training:: Epoch 76, Iteration 20, Current loss 0.11506495624780655 Accuracy 97.03141587701019\n",
      "Training:: Epoch 76, Iteration 30, Current loss 0.6474944949150085 Accuracy 81.46915665044205\n",
      "Training:: Epoch 76, Iteration 40, Current loss 0.12622614204883575 Accuracy 96.26672927917352\n",
      "Training:: Epoch 76, Iteration 50, Current loss 0.06273514032363892 Accuracy 98.22748425951274\n",
      "Training:: Epoch 76, Iteration 60, Current loss 0.29377317428588867 Accuracy 91.92\n",
      "Training:: Epoch 76, Iteration 70, Current loss 0.09399417787790298 Accuracy 97.02641182209055\n",
      "Training:: Epoch 76, Iteration 80, Current loss 0.10508646816015244 Accuracy 96.86596746221818\n",
      "Training:: Epoch 76, Iteration 90, Current loss 0.08444561064243317 Accuracy 97.37966062564931\n",
      "Training:: Epoch 76, Iteration 100, Current loss 0.10994748026132584 Accuracy 96.52931076772137\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 76, Probability Accuracy 45.441438455483286\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 0.1401490569114685 Accuracy 96.02728731942216\n",
      "Training:: Epoch 77, Iteration 10, Current loss 0.15427690744400024 Accuracy 95.09803921568627\n",
      "Training:: Epoch 77, Iteration 20, Current loss 0.17311696708202362 Accuracy 94.42749644091926\n",
      "Training:: Epoch 77, Iteration 30, Current loss 0.16856132447719574 Accuracy 95.16255556524013\n",
      "Training:: Epoch 77, Iteration 40, Current loss 0.14370377361774445 Accuracy 96.19998588666996\n",
      "Training:: Epoch 77, Iteration 50, Current loss 0.09404566138982773 Accuracy 97.85700370153906\n",
      "Training:: Epoch 77, Iteration 60, Current loss 0.08097036182880402 Accuracy 97.58477677482313\n",
      "Training:: Epoch 77, Iteration 70, Current loss 0.1925627589225769 Accuracy 93.40528562048588\n",
      "Training:: Epoch 77, Iteration 80, Current loss 0.12013204395771027 Accuracy 96.45502003156489\n",
      "Training:: Epoch 77, Iteration 90, Current loss 0.1514621376991272 Accuracy 95.73783975772984\n",
      "Training:: Epoch 77, Iteration 100, Current loss 0.12450599670410156 Accuracy 96.84826775021386\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 77, Probability Accuracy 45.44609934954634\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 0.08828488737344742 Accuracy 96.91652337032984\n",
      "Training:: Epoch 78, Iteration 10, Current loss 0.06318134069442749 Accuracy 97.93191740800314\n",
      "Training:: Epoch 78, Iteration 20, Current loss 0.11822580546140671 Accuracy 96.58076807379926\n",
      "Training:: Epoch 78, Iteration 30, Current loss 0.11813482642173767 Accuracy 96.22411693057248\n",
      "Training:: Epoch 78, Iteration 40, Current loss 0.10700244456529617 Accuracy 96.29436325678496\n",
      "Training:: Epoch 78, Iteration 50, Current loss 0.06266971677541733 Accuracy 97.92156493933979\n",
      "Training:: Epoch 78, Iteration 60, Current loss 0.06484607607126236 Accuracy 97.84256325941372\n",
      "Training:: Epoch 78, Iteration 70, Current loss 0.26710817217826843 Accuracy 94.01261754553029\n",
      "Training:: Epoch 78, Iteration 80, Current loss 0.07704909890890121 Accuracy 97.54269326033112\n",
      "Training:: Epoch 78, Iteration 90, Current loss 0.0674068033695221 Accuracy 97.81101553472847\n",
      "Training:: Epoch 78, Iteration 100, Current loss 0.12356402724981308 Accuracy 95.56540118689537\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 78, Probability Accuracy 47.23836847992708\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 0.11036313325166702 Accuracy 96.875\n",
      "Training:: Epoch 79, Iteration 10, Current loss 0.11028151959180832 Accuracy 96.15706597568268\n",
      "Training:: Epoch 79, Iteration 20, Current loss 0.28644585609436035 Accuracy 91.70034642032333\n",
      "Training:: Epoch 79, Iteration 30, Current loss 0.07713652402162552 Accuracy 97.54664577695485\n",
      "Training:: Epoch 79, Iteration 40, Current loss 0.07680033147335052 Accuracy 97.65376782077394\n",
      "Training:: Epoch 79, Iteration 50, Current loss 0.05420265346765518 Accuracy 98.4106980326262\n",
      "Training:: Epoch 79, Iteration 60, Current loss 0.06715039908885956 Accuracy 97.73031323522628\n",
      "Training:: Epoch 79, Iteration 70, Current loss 0.08301522582769394 Accuracy 97.38995939936844\n",
      "Training:: Epoch 79, Iteration 80, Current loss 0.21863989531993866 Accuracy 93.95639974098856\n",
      "Training:: Epoch 79, Iteration 90, Current loss 0.09037971496582031 Accuracy 97.0686816417735\n",
      "Training:: Epoch 79, Iteration 100, Current loss 0.10952556133270264 Accuracy 96.70398529534631\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 79, Probability Accuracy 46.7082694618221\n",
      "Starting Training\n",
      "Training:: Epoch 80, Iteration 0, Current loss 0.05293527990579605 Accuracy 98.45303867403315\n",
      "Training:: Epoch 80, Iteration 10, Current loss 0.12867411971092224 Accuracy 95.86050187709938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 80, Iteration 20, Current loss 0.08621279895305634 Accuracy 97.17320991874622\n",
      "Training:: Epoch 80, Iteration 30, Current loss 0.08457599580287933 Accuracy 97.5570697396929\n",
      "Training:: Epoch 80, Iteration 40, Current loss 0.0686841607093811 Accuracy 97.73033610156963\n",
      "Training:: Epoch 80, Iteration 50, Current loss 0.1792612224817276 Accuracy 94.30829807240245\n",
      "Training:: Epoch 80, Iteration 60, Current loss 0.1690819263458252 Accuracy 94.87505102272212\n",
      "Training:: Epoch 80, Iteration 70, Current loss 0.0905556008219719 Accuracy 97.59899346450914\n",
      "Training:: Epoch 80, Iteration 80, Current loss 0.09063340723514557 Accuracy 97.38204623350106\n",
      "Training:: Epoch 80, Iteration 90, Current loss 0.09881400316953659 Accuracy 96.39007859933167\n",
      "Training:: Epoch 80, Iteration 100, Current loss 0.2449478656053543 Accuracy 93.81204853574351\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 80, Probability Accuracy 45.777229978870615\n",
      "Starting Training\n",
      "Training:: Epoch 81, Iteration 0, Current loss 0.1434207409620285 Accuracy 96.44291785527159\n",
      "Training:: Epoch 81, Iteration 10, Current loss 0.09796837717294693 Accuracy 96.93864370290635\n",
      "Training:: Epoch 81, Iteration 20, Current loss 0.05831672251224518 Accuracy 98.31019696198851\n",
      "Training:: Epoch 81, Iteration 30, Current loss 0.09371674805879593 Accuracy 97.43796884016157\n",
      "Training:: Epoch 81, Iteration 40, Current loss 0.20117627084255219 Accuracy 94.63696369636963\n",
      "Training:: Epoch 81, Iteration 50, Current loss 0.1495296061038971 Accuracy 95.9622501850481\n",
      "Training:: Epoch 81, Iteration 60, Current loss 0.12608085572719574 Accuracy 96.71133477675698\n",
      "Training:: Epoch 81, Iteration 70, Current loss 0.09824787825345993 Accuracy 97.33928276317964\n",
      "Training:: Epoch 81, Iteration 80, Current loss 0.1042318344116211 Accuracy 97.3932500466157\n",
      "Training:: Epoch 81, Iteration 90, Current loss 0.08356965333223343 Accuracy 97.90051431641974\n",
      "Training:: Epoch 81, Iteration 100, Current loss 0.13639406859874725 Accuracy 95.9779756527724\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 81, Probability Accuracy 47.37892032978415\n",
      "Starting Training\n",
      "Training:: Epoch 82, Iteration 0, Current loss 0.06760308891534805 Accuracy 97.78298038381044\n",
      "Training:: Epoch 82, Iteration 10, Current loss 0.1472763866186142 Accuracy 95.44571662236761\n",
      "Training:: Epoch 82, Iteration 20, Current loss 0.1275593340396881 Accuracy 96.35602225139071\n",
      "Training:: Epoch 82, Iteration 30, Current loss 0.23231737315654755 Accuracy 92.4595954190303\n",
      "Training:: Epoch 82, Iteration 40, Current loss 0.18011215329170227 Accuracy 94.50412755852086\n",
      "Training:: Epoch 82, Iteration 50, Current loss 0.08209878206253052 Accuracy 97.39671760045275\n",
      "Training:: Epoch 82, Iteration 60, Current loss 0.07684897631406784 Accuracy 97.64327424035805\n",
      "Training:: Epoch 82, Iteration 70, Current loss 0.0742979347705841 Accuracy 97.35973597359735\n",
      "Training:: Epoch 82, Iteration 80, Current loss 0.1335745006799698 Accuracy 95.98953792502179\n",
      "Training:: Epoch 82, Iteration 90, Current loss 0.0881824865937233 Accuracy 97.30163980908274\n",
      "Training:: Epoch 82, Iteration 100, Current loss 0.1462830901145935 Accuracy 95.37072714262123\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 82, Probability Accuracy 48.585988316692216\n",
      "Starting Training\n",
      "Training:: Epoch 83, Iteration 0, Current loss 0.0639989897608757 Accuracy 98.08511553853437\n",
      "Training:: Epoch 83, Iteration 10, Current loss 0.1581629365682602 Accuracy 96.17947236664742\n",
      "Training:: Epoch 83, Iteration 20, Current loss 0.06529687345027924 Accuracy 98.32648780914096\n",
      "Training:: Epoch 83, Iteration 30, Current loss 0.11078738421201706 Accuracy 96.00453001132503\n",
      "Training:: Epoch 83, Iteration 40, Current loss 0.08333975821733475 Accuracy 97.37981229649492\n",
      "Training:: Epoch 83, Iteration 50, Current loss 0.1253148466348648 Accuracy 95.84363483696413\n",
      "Training:: Epoch 83, Iteration 60, Current loss 0.14978773891925812 Accuracy 95.9203036053131\n",
      "Training:: Epoch 83, Iteration 70, Current loss 0.08571232855319977 Accuracy 97.50257997936016\n",
      "Training:: Epoch 83, Iteration 80, Current loss 0.055407483130693436 Accuracy 98.24478037480324\n",
      "Training:: Epoch 83, Iteration 90, Current loss 0.1905163675546646 Accuracy 93.89305859866673\n",
      "Training:: Epoch 83, Iteration 100, Current loss 0.16079634428024292 Accuracy 94.85129945521122\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 83, Probability Accuracy 45.36572482081452\n",
      "Starting Training\n",
      "Training:: Epoch 84, Iteration 0, Current loss 0.130349263548851 Accuracy 96.5896001316439\n",
      "Training:: Epoch 84, Iteration 10, Current loss 0.33362877368927 Accuracy 90.5969591226321\n",
      "Training:: Epoch 84, Iteration 20, Current loss 0.2380557358264923 Accuracy 92.97818366311517\n",
      "Training:: Epoch 84, Iteration 30, Current loss 0.23546016216278076 Accuracy 92.80171533591995\n",
      "Training:: Epoch 84, Iteration 40, Current loss 0.2298583984375 Accuracy 92.36443917936568\n",
      "Training:: Epoch 84, Iteration 50, Current loss 0.13649417459964752 Accuracy 95.75285865282983\n",
      "Training:: Epoch 84, Iteration 60, Current loss 0.20098638534545898 Accuracy 92.55501991196815\n",
      "Training:: Epoch 84, Iteration 70, Current loss 0.13251635432243347 Accuracy 95.56635388739946\n",
      "Training:: Epoch 84, Iteration 80, Current loss 0.32055771350860596 Accuracy 92.05928470096319\n",
      "Training:: Epoch 84, Iteration 90, Current loss 0.15574488043785095 Accuracy 95.28267179005029\n",
      "Training:: Epoch 84, Iteration 100, Current loss 0.10887051373720169 Accuracy 96.08208955223881\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 84, Probability Accuracy 45.456767618179555\n",
      "Starting Training\n",
      "Training:: Epoch 85, Iteration 0, Current loss 0.10206425189971924 Accuracy 97.26418748789463\n",
      "Training:: Epoch 85, Iteration 10, Current loss 0.13740262389183044 Accuracy 95.73770491803279\n",
      "Training:: Epoch 85, Iteration 20, Current loss 0.13995994627475739 Accuracy 96.40375260597638\n",
      "Training:: Epoch 85, Iteration 30, Current loss 0.11385031044483185 Accuracy 97.36290708812261\n",
      "Training:: Epoch 85, Iteration 40, Current loss 0.06296966224908829 Accuracy 98.28892568618596\n",
      "Training:: Epoch 85, Iteration 50, Current loss 0.15857452154159546 Accuracy 95.25403103133556\n",
      "Training:: Epoch 85, Iteration 60, Current loss 0.08080810308456421 Accuracy 97.73742073468162\n",
      "Training:: Epoch 85, Iteration 70, Current loss 0.05891214311122894 Accuracy 97.9002863245921\n",
      "Training:: Epoch 85, Iteration 80, Current loss 0.12293052673339844 Accuracy 96.16668953844746\n",
      "Training:: Epoch 85, Iteration 90, Current loss 0.11196711659431458 Accuracy 96.72513380319008\n",
      "Training:: Epoch 85, Iteration 100, Current loss 0.07211660593748093 Accuracy 97.98152202144632\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 85, Probability Accuracy 45.39555454281808\n",
      "Starting Training\n",
      "Training:: Epoch 86, Iteration 0, Current loss 0.07797736674547195 Accuracy 97.75892428365616\n",
      "Training:: Epoch 86, Iteration 10, Current loss 0.27117106318473816 Accuracy 90.85872576177286\n",
      "Training:: Epoch 86, Iteration 20, Current loss 0.08567948639392853 Accuracy 97.3880339521082\n",
      "Training:: Epoch 86, Iteration 30, Current loss 0.11527036130428314 Accuracy 96.77271477033098\n",
      "Training:: Epoch 86, Iteration 40, Current loss 0.12845690548419952 Accuracy 96.9647419519673\n",
      "Training:: Epoch 86, Iteration 50, Current loss 0.09777761250734329 Accuracy 96.76865557759805\n",
      "Training:: Epoch 86, Iteration 60, Current loss 0.09482397884130478 Accuracy 97.03287890938252\n",
      "Training:: Epoch 86, Iteration 70, Current loss 0.17685377597808838 Accuracy 94.60283407666685\n",
      "Training:: Epoch 86, Iteration 80, Current loss 0.06084265932440758 Accuracy 98.22891695588393\n",
      "Training:: Epoch 86, Iteration 90, Current loss 0.06993652880191803 Accuracy 97.95353081109265\n",
      "Training:: Epoch 86, Iteration 100, Current loss 0.10924134403467178 Accuracy 96.86509841574652\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 86, Probability Accuracy 47.358826697601195\n",
      "Starting Training\n",
      "Training:: Epoch 87, Iteration 0, Current loss 0.06693664938211441 Accuracy 97.7811692885654\n",
      "Training:: Epoch 87, Iteration 10, Current loss 0.07705767452716827 Accuracy 97.52665245202559\n",
      "Training:: Epoch 87, Iteration 20, Current loss 0.11806665360927582 Accuracy 96.66505265706331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 87, Iteration 30, Current loss 0.0803704783320427 Accuracy 97.58886038842067\n",
      "Training:: Epoch 87, Iteration 40, Current loss 0.05735111981630325 Accuracy 98.16865815992263\n",
      "Training:: Epoch 87, Iteration 50, Current loss 0.1975138932466507 Accuracy 96.52685676392574\n",
      "Training:: Epoch 87, Iteration 60, Current loss 0.06146660074591637 Accuracy 98.29658910306044\n",
      "Training:: Epoch 87, Iteration 70, Current loss 0.13723962008953094 Accuracy 95.72384137601529\n",
      "Training:: Epoch 87, Iteration 80, Current loss 0.16961883008480072 Accuracy 94.26884489034893\n",
      "Training:: Epoch 87, Iteration 90, Current loss 0.05963023751974106 Accuracy 98.15529976378838\n",
      "Training:: Epoch 87, Iteration 100, Current loss 0.13484929502010345 Accuracy 96.11458798986739\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 87, Probability Accuracy 48.134503045117455\n",
      "Starting Training\n",
      "Training:: Epoch 88, Iteration 0, Current loss 0.10328249633312225 Accuracy 97.01593760596812\n",
      "Training:: Epoch 88, Iteration 10, Current loss 0.08084028214216232 Accuracy 97.39149988756465\n",
      "Training:: Epoch 88, Iteration 20, Current loss 0.08391998708248138 Accuracy 98.10814893134959\n",
      "Training:: Epoch 88, Iteration 30, Current loss 0.1271331012248993 Accuracy 96.41000910893212\n",
      "Training:: Epoch 88, Iteration 40, Current loss 0.11397452652454376 Accuracy 96.47321102258607\n",
      "Training:: Epoch 88, Iteration 50, Current loss 0.11984249949455261 Accuracy 96.02670650854421\n",
      "Training:: Epoch 88, Iteration 60, Current loss 0.14182928204536438 Accuracy 95.55215979479902\n",
      "Training:: Epoch 88, Iteration 70, Current loss 0.2532399892807007 Accuracy 92.6267078007933\n",
      "Training:: Epoch 88, Iteration 80, Current loss 0.19153465330600739 Accuracy 94.72570761599066\n",
      "Training:: Epoch 88, Iteration 90, Current loss 0.09735438972711563 Accuracy 97.22054119477825\n",
      "Training:: Epoch 88, Iteration 100, Current loss 0.11796925961971283 Accuracy 96.21178001301107\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 88, Probability Accuracy 46.51613705100054\n",
      "Starting Training\n",
      "Training:: Epoch 89, Iteration 0, Current loss 0.07041793316602707 Accuracy 98.18530928805392\n",
      "Training:: Epoch 89, Iteration 10, Current loss 0.11362241953611374 Accuracy 96.22478768577494\n",
      "Training:: Epoch 89, Iteration 20, Current loss 0.13251468539237976 Accuracy 96.60034769171335\n",
      "Training:: Epoch 89, Iteration 30, Current loss 0.07960900664329529 Accuracy 97.77111739625987\n",
      "Training:: Epoch 89, Iteration 40, Current loss 0.0692843645811081 Accuracy 98.02641341445319\n",
      "Training:: Epoch 89, Iteration 50, Current loss 0.05208714306354523 Accuracy 98.4078670100679\n",
      "Training:: Epoch 89, Iteration 60, Current loss 0.06111789494752884 Accuracy 98.14238680641556\n",
      "Training:: Epoch 89, Iteration 70, Current loss 0.07612298429012299 Accuracy 97.50343455398172\n",
      "Training:: Epoch 89, Iteration 80, Current loss 0.062447626143693924 Accuracy 98.14478652338077\n",
      "Training:: Epoch 89, Iteration 90, Current loss 0.08314833790063858 Accuracy 97.55426475084072\n",
      "Training:: Epoch 89, Iteration 100, Current loss 0.15387122333049774 Accuracy 95.32675340212353\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 89, Probability Accuracy 47.570534863487595\n",
      "Starting Training\n",
      "Training:: Epoch 90, Iteration 0, Current loss 0.058828920125961304 Accuracy 98.6691159931019\n",
      "Training:: Epoch 90, Iteration 10, Current loss 0.18504106998443604 Accuracy 95.07505559673832\n",
      "Training:: Epoch 90, Iteration 20, Current loss 0.10975885391235352 Accuracy 97.47968085899635\n",
      "Training:: Epoch 90, Iteration 30, Current loss 0.06108745560050011 Accuracy 98.38318162115301\n",
      "Training:: Epoch 90, Iteration 40, Current loss 0.07491392642259598 Accuracy 97.44269190325973\n",
      "Training:: Epoch 90, Iteration 50, Current loss 0.08395035564899445 Accuracy 97.39472648563557\n",
      "Training:: Epoch 90, Iteration 60, Current loss 0.10907923430204391 Accuracy 96.56174736317581\n",
      "Training:: Epoch 90, Iteration 70, Current loss 0.06123557686805725 Accuracy 98.1120802963317\n",
      "Training:: Epoch 90, Iteration 80, Current loss 0.04006901755928993 Accuracy 98.74281734285216\n",
      "Training:: Epoch 90, Iteration 90, Current loss 0.07389785349369049 Accuracy 97.73085790559398\n",
      "Training:: Epoch 90, Iteration 100, Current loss 0.05892277508974075 Accuracy 98.4353191801954\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 90, Probability Accuracy 46.3549736918424\n",
      "Starting Training\n",
      "Training:: Epoch 91, Iteration 0, Current loss 0.09985566139221191 Accuracy 97.14419875352174\n",
      "Training:: Epoch 91, Iteration 10, Current loss 0.07792668789625168 Accuracy 97.89605286658332\n",
      "Training:: Epoch 91, Iteration 20, Current loss 0.0657125934958458 Accuracy 98.0782858810288\n",
      "Training:: Epoch 91, Iteration 30, Current loss 0.07253602147102356 Accuracy 97.84899247390143\n",
      "Training:: Epoch 91, Iteration 40, Current loss 0.08386637270450592 Accuracy 97.75464380485813\n",
      "Training:: Epoch 91, Iteration 50, Current loss 0.09485676884651184 Accuracy 97.15703971119133\n",
      "Training:: Epoch 91, Iteration 60, Current loss 0.10341424494981766 Accuracy 97.12742980561555\n",
      "Training:: Epoch 91, Iteration 70, Current loss 0.18311461806297302 Accuracy 94.41230717639168\n",
      "Training:: Epoch 91, Iteration 80, Current loss 0.1093851700425148 Accuracy 96.66758849557522\n",
      "Training:: Epoch 91, Iteration 90, Current loss 0.38987037539482117 Accuracy 88.90373554871319\n",
      "Training:: Epoch 91, Iteration 100, Current loss 0.05360843986272812 Accuracy 98.2863901419298\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 91, Probability Accuracy 48.27432986700916\n",
      "Starting Training\n",
      "Training:: Epoch 92, Iteration 0, Current loss 0.05481281504034996 Accuracy 98.3569319300371\n",
      "Training:: Epoch 92, Iteration 10, Current loss 0.09770258516073227 Accuracy 96.94365784591349\n",
      "Training:: Epoch 92, Iteration 20, Current loss 0.10534126311540604 Accuracy 97.4668942924376\n",
      "Training:: Epoch 92, Iteration 30, Current loss 0.07785337418317795 Accuracy 97.88698498267232\n",
      "Training:: Epoch 92, Iteration 40, Current loss 0.08548291027545929 Accuracy 97.16186252771618\n",
      "Training:: Epoch 92, Iteration 50, Current loss 0.058013007044792175 Accuracy 98.15867318110811\n",
      "Training:: Epoch 92, Iteration 60, Current loss 0.0898365005850792 Accuracy 97.31985559566787\n",
      "Training:: Epoch 92, Iteration 70, Current loss 0.044806066900491714 Accuracy 98.73802816901409\n",
      "Training:: Epoch 92, Iteration 80, Current loss 0.06417834758758545 Accuracy 97.8546171630627\n",
      "Training:: Epoch 92, Iteration 90, Current loss 0.07522603124380112 Accuracy 98.20272440047651\n",
      "Training:: Epoch 92, Iteration 100, Current loss 0.09102524071931839 Accuracy 97.5538647514755\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 92, Probability Accuracy 48.98578945187886\n",
      "Starting Training\n",
      "Training:: Epoch 93, Iteration 0, Current loss 0.1709105670452118 Accuracy 94.26627793974733\n",
      "Training:: Epoch 93, Iteration 10, Current loss 0.07873328030109406 Accuracy 97.40202553940995\n",
      "Training:: Epoch 93, Iteration 20, Current loss 0.10757298767566681 Accuracy 96.63246610480029\n",
      "Training:: Epoch 93, Iteration 30, Current loss 0.09591429680585861 Accuracy 96.3828726527447\n",
      "Training:: Epoch 93, Iteration 40, Current loss 0.09564422816038132 Accuracy 97.63706358688438\n",
      "Training:: Epoch 93, Iteration 50, Current loss 0.07689263671636581 Accuracy 97.67194297880218\n",
      "Training:: Epoch 93, Iteration 60, Current loss 0.13075606524944305 Accuracy 96.56717645124158\n",
      "Training:: Epoch 93, Iteration 70, Current loss 0.03336767852306366 Accuracy 98.93287435456111\n",
      "Training:: Epoch 93, Iteration 80, Current loss 0.14827117323875427 Accuracy 95.6830985915493\n",
      "Training:: Epoch 93, Iteration 90, Current loss 0.1491585224866867 Accuracy 95.62496444216875\n",
      "Training:: Epoch 93, Iteration 100, Current loss 0.07207175344228745 Accuracy 97.66014033499322\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 93, Probability Accuracy 45.88753780502962\n",
      "Starting Training\n",
      "Training:: Epoch 94, Iteration 0, Current loss 0.0589185506105423 Accuracy 98.28053158657177\n",
      "Training:: Epoch 94, Iteration 10, Current loss 0.07486821711063385 Accuracy 97.81680647094068\n",
      "Training:: Epoch 94, Iteration 20, Current loss 0.09778273850679398 Accuracy 97.12629281036395\n",
      "Training:: Epoch 94, Iteration 30, Current loss 0.10178064554929733 Accuracy 96.79460363940598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 94, Iteration 40, Current loss 0.09667320549488068 Accuracy 97.16556981820551\n",
      "Training:: Epoch 94, Iteration 50, Current loss 0.06441584974527359 Accuracy 97.97818447355209\n",
      "Training:: Epoch 94, Iteration 60, Current loss 0.0951591208577156 Accuracy 97.29799948038452\n",
      "Training:: Epoch 94, Iteration 70, Current loss 0.056947752833366394 Accuracy 98.39382940108892\n",
      "Training:: Epoch 94, Iteration 80, Current loss 0.09029396623373032 Accuracy 97.1423768700622\n",
      "Training:: Epoch 94, Iteration 90, Current loss 0.14822782576084137 Accuracy 96.14903216416825\n",
      "Training:: Epoch 94, Iteration 100, Current loss 0.054342467337846756 Accuracy 98.5958485958486\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 94, Probability Accuracy 49.16083191780255\n",
      "Starting Training\n",
      "Training:: Epoch 95, Iteration 0, Current loss 0.08340480178594589 Accuracy 98.08546484913462\n",
      "Training:: Epoch 95, Iteration 10, Current loss 0.06796960532665253 Accuracy 97.85770960063475\n",
      "Training:: Epoch 95, Iteration 20, Current loss 0.07678902894258499 Accuracy 97.66638584667228\n",
      "Training:: Epoch 95, Iteration 30, Current loss 0.05797838419675827 Accuracy 98.09633027522936\n",
      "Training:: Epoch 95, Iteration 40, Current loss 0.07317549735307693 Accuracy 98.57960950534337\n",
      "Training:: Epoch 95, Iteration 50, Current loss 0.07644236832857132 Accuracy 97.491257412194\n",
      "Training:: Epoch 95, Iteration 60, Current loss 0.06369558721780777 Accuracy 98.05176842521242\n",
      "Training:: Epoch 95, Iteration 70, Current loss 0.05166418105363846 Accuracy 98.57551822853792\n",
      "Training:: Epoch 95, Iteration 80, Current loss 0.06201605498790741 Accuracy 98.49470884500029\n",
      "Training:: Epoch 95, Iteration 90, Current loss 0.046098992228507996 Accuracy 98.55973095481795\n",
      "Training:: Epoch 95, Iteration 100, Current loss 0.07668623328208923 Accuracy 97.75696474378094\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 95, Probability Accuracy 48.34196461863529\n",
      "Starting Training\n",
      "Training:: Epoch 96, Iteration 0, Current loss 0.04714939743280411 Accuracy 98.63507425329551\n",
      "Training:: Epoch 96, Iteration 10, Current loss 0.08044090121984482 Accuracy 97.56660039761431\n",
      "Training:: Epoch 96, Iteration 20, Current loss 0.06504562497138977 Accuracy 98.06746987951807\n",
      "Training:: Epoch 96, Iteration 30, Current loss 0.056692492216825485 Accuracy 98.22296366142018\n",
      "Training:: Epoch 96, Iteration 40, Current loss 0.08658530563116074 Accuracy 96.888561369033\n",
      "Training:: Epoch 96, Iteration 50, Current loss 0.09720858931541443 Accuracy 97.39311106061098\n",
      "Training:: Epoch 96, Iteration 60, Current loss 0.05988083407282829 Accuracy 98.13303969167626\n",
      "Training:: Epoch 96, Iteration 70, Current loss 0.04072602093219757 Accuracy 98.78083809402227\n",
      "Training:: Epoch 96, Iteration 80, Current loss 0.05180026590824127 Accuracy 98.32246405280114\n",
      "Training:: Epoch 96, Iteration 90, Current loss 0.05074059218168259 Accuracy 98.58048566529766\n",
      "Training:: Epoch 96, Iteration 100, Current loss 0.06359484791755676 Accuracy 98.12033160961997\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 96, Probability Accuracy 48.778224302937396\n",
      "Starting Training\n",
      "Training:: Epoch 97, Iteration 0, Current loss 0.06692289561033249 Accuracy 98.37703397689909\n",
      "Training:: Epoch 97, Iteration 10, Current loss 0.08431614190340042 Accuracy 98.01059574008\n",
      "Training:: Epoch 97, Iteration 20, Current loss 0.05801679566502571 Accuracy 98.26642335766424\n",
      "Training:: Epoch 97, Iteration 30, Current loss 0.04737525060772896 Accuracy 98.71234337509998\n",
      "Training:: Epoch 97, Iteration 40, Current loss 0.07639780640602112 Accuracy 97.85958904109589\n",
      "Training:: Epoch 97, Iteration 50, Current loss 0.12054729461669922 Accuracy 96.87395581956562\n",
      "Training:: Epoch 97, Iteration 60, Current loss 0.055608734488487244 Accuracy 98.03138841800185\n",
      "Training:: Epoch 97, Iteration 70, Current loss 0.07616326957941055 Accuracy 97.6152546463915\n",
      "Training:: Epoch 97, Iteration 80, Current loss 0.03173234686255455 Accuracy 98.95117540687161\n",
      "Training:: Epoch 97, Iteration 90, Current loss 0.07138250023126602 Accuracy 97.87835926449787\n",
      "Training:: Epoch 97, Iteration 100, Current loss 0.03588496148586273 Accuracy 98.99307740717433\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 97, Probability Accuracy 47.750756100592454\n",
      "Starting Training\n",
      "Training:: Epoch 98, Iteration 0, Current loss 0.031140919774770737 Accuracy 99.06659605454786\n",
      "Training:: Epoch 98, Iteration 10, Current loss 0.06871557235717773 Accuracy 97.9356084194794\n",
      "Training:: Epoch 98, Iteration 20, Current loss 0.051236461848020554 Accuracy 98.47638834783362\n",
      "Training:: Epoch 98, Iteration 30, Current loss 0.0484221987426281 Accuracy 98.61019504381419\n",
      "Training:: Epoch 98, Iteration 40, Current loss 0.058390840888023376 Accuracy 97.90325481028592\n",
      "Training:: Epoch 98, Iteration 50, Current loss 0.09091401100158691 Accuracy 97.26394849785407\n",
      "Training:: Epoch 98, Iteration 60, Current loss 0.07112929224967957 Accuracy 98.02453493059956\n",
      "Training:: Epoch 98, Iteration 70, Current loss 0.05922473594546318 Accuracy 98.28679245283018\n",
      "Training:: Epoch 98, Iteration 80, Current loss 0.03871050104498863 Accuracy 98.85359894756624\n",
      "Training:: Epoch 98, Iteration 90, Current loss 0.06993640959262848 Accuracy 97.94748954957494\n",
      "Training:: Epoch 98, Iteration 100, Current loss 0.0437253974378109 Accuracy 98.907925712791\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 98, Probability Accuracy 47.87338940216266\n",
      "Starting Training\n",
      "Training:: Epoch 99, Iteration 0, Current loss 0.07076110690832138 Accuracy 98.35785306159175\n",
      "Training:: Epoch 99, Iteration 10, Current loss 0.08368488401174545 Accuracy 97.18266093531514\n",
      "Training:: Epoch 99, Iteration 20, Current loss 0.07603251934051514 Accuracy 97.24775406345745\n",
      "Training:: Epoch 99, Iteration 30, Current loss 0.07123236358165741 Accuracy 97.76353470612301\n",
      "Training:: Epoch 99, Iteration 40, Current loss 0.0797475203871727 Accuracy 97.415\n",
      "Training:: Epoch 99, Iteration 50, Current loss 0.09110953658819199 Accuracy 97.23894939141576\n",
      "Training:: Epoch 99, Iteration 60, Current loss 0.04643351957201958 Accuracy 98.47429961024685\n",
      "Training:: Epoch 99, Iteration 70, Current loss 0.03816333785653114 Accuracy 98.89914140154619\n",
      "Training:: Epoch 99, Iteration 80, Current loss 0.0866449773311615 Accuracy 97.41903195824695\n",
      "Training:: Epoch 99, Iteration 90, Current loss 0.07501981407403946 Accuracy 97.47502672416971\n",
      "Training:: Epoch 99, Iteration 100, Current loss 0.04229729250073433 Accuracy 98.72348076231572\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 99, Probability Accuracy 48.04853544350996\n",
      "Starting Training\n",
      "Training:: Epoch 100, Iteration 0, Current loss 0.04342180863022804 Accuracy 98.56975189016568\n",
      "Training:: Epoch 100, Iteration 10, Current loss 0.07222329080104828 Accuracy 97.77959067853723\n",
      "Training:: Epoch 100, Iteration 20, Current loss 0.14782847464084625 Accuracy 95.99320058280719\n",
      "Training:: Epoch 100, Iteration 30, Current loss 0.1505773365497589 Accuracy 95.01813526776189\n",
      "Training:: Epoch 100, Iteration 40, Current loss 0.1058930978178978 Accuracy 96.82179939914616\n",
      "Training:: Epoch 100, Iteration 50, Current loss 0.08056503534317017 Accuracy 97.65468227424749\n",
      "Training:: Epoch 100, Iteration 60, Current loss 0.08148150891065598 Accuracy 97.45118416014877\n",
      "Training:: Epoch 100, Iteration 70, Current loss 0.06981161236763 Accuracy 98.25825449533237\n",
      "Training:: Epoch 100, Iteration 80, Current loss 0.11791647225618362 Accuracy 96.39330773450357\n",
      "Training:: Epoch 100, Iteration 90, Current loss 0.1402258425951004 Accuracy 95.51874678993323\n",
      "Training:: Epoch 100, Iteration 100, Current loss 0.09647279977798462 Accuracy 97.08785564339631\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 100, Probability Accuracy 48.45952272444794\n",
      "Starting Training\n",
      "Training:: Epoch 101, Iteration 0, Current loss 0.04576806351542473 Accuracy 98.58240413380473\n",
      "Training:: Epoch 101, Iteration 10, Current loss 0.07090884447097778 Accuracy 98.04152902312411\n",
      "Training:: Epoch 101, Iteration 20, Current loss 0.13161487877368927 Accuracy 96.0422319216704\n",
      "Training:: Epoch 101, Iteration 30, Current loss 0.05837714672088623 Accuracy 98.22611163670766\n",
      "Training:: Epoch 101, Iteration 40, Current loss 0.06670954823493958 Accuracy 97.82360647424248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 101, Iteration 50, Current loss 0.07645203173160553 Accuracy 97.75904385871306\n",
      "Training:: Epoch 101, Iteration 60, Current loss 0.06277737021446228 Accuracy 98.1376172874609\n",
      "Training:: Epoch 101, Iteration 70, Current loss 0.08859521150588989 Accuracy 97.35039107500151\n",
      "Training:: Epoch 101, Iteration 80, Current loss 0.133977010846138 Accuracy 95.8122785618865\n",
      "Training:: Epoch 101, Iteration 90, Current loss 0.0811394453048706 Accuracy 97.0574063876652\n",
      "Training:: Epoch 101, Iteration 100, Current loss 0.07627381384372711 Accuracy 97.11389725637059\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 101, Probability Accuracy 45.92917512532626\n",
      "Starting Training\n",
      "Training:: Epoch 102, Iteration 0, Current loss 0.08626117557287216 Accuracy 97.35033974869984\n",
      "Training:: Epoch 102, Iteration 10, Current loss 0.09095043689012527 Accuracy 96.99115986793056\n",
      "Training:: Epoch 102, Iteration 20, Current loss 0.05928526073694229 Accuracy 98.20220466480579\n",
      "Training:: Epoch 102, Iteration 30, Current loss 0.10631118714809418 Accuracy 96.33135208031567\n",
      "Training:: Epoch 102, Iteration 40, Current loss 0.07375006377696991 Accuracy 97.59440318834763\n",
      "Training:: Epoch 102, Iteration 50, Current loss 0.14860422909259796 Accuracy 95.38645299357985\n",
      "Training:: Epoch 102, Iteration 60, Current loss 0.08411096036434174 Accuracy 97.07036769554432\n",
      "Training:: Epoch 102, Iteration 70, Current loss 0.10034690797328949 Accuracy 97.43744781319282\n",
      "Training:: Epoch 102, Iteration 80, Current loss 0.04832062870264053 Accuracy 98.35526315789474\n",
      "Training:: Epoch 102, Iteration 90, Current loss 0.050754666328430176 Accuracy 98.52582421412141\n",
      "Training:: Epoch 102, Iteration 100, Current loss 0.06012840196490288 Accuracy 98.03521499282098\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 102, Probability Accuracy 47.20853875792352\n",
      "Starting Training\n",
      "Training:: Epoch 103, Iteration 0, Current loss 0.06068338826298714 Accuracy 98.07259877931256\n",
      "Training:: Epoch 103, Iteration 10, Current loss 0.06213238462805748 Accuracy 98.25266245775141\n",
      "Training:: Epoch 103, Iteration 20, Current loss 0.08992587774991989 Accuracy 97.42822966507177\n",
      "Training:: Epoch 103, Iteration 30, Current loss 0.053108569234609604 Accuracy 98.69044302033993\n",
      "Training:: Epoch 103, Iteration 40, Current loss 0.13717223703861237 Accuracy 96.25412046748576\n",
      "Training:: Epoch 103, Iteration 50, Current loss 0.05139896646142006 Accuracy 98.54069867492686\n",
      "Training:: Epoch 103, Iteration 60, Current loss 0.07396293431520462 Accuracy 97.6530953031545\n",
      "Training:: Epoch 103, Iteration 70, Current loss 0.07271583378314972 Accuracy 98.00545325514511\n",
      "Training:: Epoch 103, Iteration 80, Current loss 0.08544566482305527 Accuracy 97.0202622169249\n",
      "Training:: Epoch 103, Iteration 90, Current loss 0.06509936600923538 Accuracy 97.71275118066488\n",
      "Training:: Epoch 103, Iteration 100, Current loss 0.0659254863858223 Accuracy 98.21729839163199\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 103, Probability Accuracy 47.058250818245845\n",
      "Starting Training\n",
      "Training:: Epoch 104, Iteration 0, Current loss 0.14197343587875366 Accuracy 95.65929949428732\n",
      "Training:: Epoch 104, Iteration 10, Current loss 0.11929096281528473 Accuracy 96.00639545212293\n",
      "Training:: Epoch 104, Iteration 20, Current loss 0.07969614118337631 Accuracy 97.37685905307681\n",
      "Training:: Epoch 104, Iteration 30, Current loss 0.07864462584257126 Accuracy 97.59911456707438\n",
      "Training:: Epoch 104, Iteration 40, Current loss 0.0769072026014328 Accuracy 97.9753600869644\n",
      "Training:: Epoch 104, Iteration 50, Current loss 0.09416814893484116 Accuracy 97.19799195357282\n",
      "Training:: Epoch 104, Iteration 60, Current loss 0.10620270669460297 Accuracy 96.96180990039541\n",
      "Training:: Epoch 104, Iteration 70, Current loss 0.12456178665161133 Accuracy 96.18516086671043\n",
      "Training:: Epoch 104, Iteration 80, Current loss 0.11779651045799255 Accuracy 95.71772682665716\n",
      "Training:: Epoch 104, Iteration 90, Current loss 0.22382062673568726 Accuracy 93.63849836845476\n",
      "Training:: Epoch 104, Iteration 100, Current loss 0.08678080886602402 Accuracy 97.67863999720738\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 104, Probability Accuracy 45.83139992542569\n",
      "Starting Training\n",
      "Training:: Epoch 105, Iteration 0, Current loss 0.09076737612485886 Accuracy 97.6172217353199\n",
      "Training:: Epoch 105, Iteration 10, Current loss 0.2748176157474518 Accuracy 91.4089524497909\n",
      "Training:: Epoch 105, Iteration 20, Current loss 0.29057055711746216 Accuracy 91.79732814429518\n",
      "Training:: Epoch 105, Iteration 30, Current loss 0.21419240534305573 Accuracy 93.61988202637058\n",
      "Training:: Epoch 105, Iteration 40, Current loss 0.322046160697937 Accuracy 90.85315832649712\n",
      "Training:: Epoch 105, Iteration 50, Current loss 0.26371756196022034 Accuracy 92.62059791360359\n",
      "Training:: Epoch 105, Iteration 60, Current loss 0.11001627147197723 Accuracy 97.04040741359158\n",
      "Training:: Epoch 105, Iteration 70, Current loss 0.21892902255058289 Accuracy 93.1457089611169\n",
      "Training:: Epoch 105, Iteration 80, Current loss 0.16373509168624878 Accuracy 94.72055130526194\n",
      "Training:: Epoch 105, Iteration 90, Current loss 0.2644423842430115 Accuracy 91.07910032853171\n",
      "Training:: Epoch 105, Iteration 100, Current loss 0.12453654408454895 Accuracy 96.11614573700759\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 105, Probability Accuracy 44.985188714421845\n",
      "Starting Training\n",
      "Training:: Epoch 106, Iteration 0, Current loss 0.11950788646936417 Accuracy 96.11096927921028\n",
      "Training:: Epoch 106, Iteration 10, Current loss 0.35652247071266174 Accuracy 90.06257940055522\n",
      "Training:: Epoch 106, Iteration 20, Current loss 0.11627338081598282 Accuracy 96.05271204060418\n",
      "Training:: Epoch 106, Iteration 30, Current loss 0.07339020073413849 Accuracy 97.6716366898791\n",
      "Training:: Epoch 106, Iteration 40, Current loss 0.11273900419473648 Accuracy 96.6084668739536\n",
      "Training:: Epoch 106, Iteration 50, Current loss 0.04569588229060173 Accuracy 98.64370376262828\n",
      "Training:: Epoch 106, Iteration 60, Current loss 0.1652480810880661 Accuracy 94.47277005826983\n",
      "Training:: Epoch 106, Iteration 70, Current loss 0.11943775415420532 Accuracy 96.32379142415964\n",
      "Training:: Epoch 106, Iteration 80, Current loss 0.0753365010023117 Accuracy 97.7246343162294\n",
      "Training:: Epoch 106, Iteration 90, Current loss 0.11821756511926651 Accuracy 96.18275731743165\n",
      "Training:: Epoch 106, Iteration 100, Current loss 0.12822966277599335 Accuracy 96.03874991705925\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 106, Probability Accuracy 46.28723536479264\n",
      "Starting Training\n",
      "Training:: Epoch 107, Iteration 0, Current loss 0.13077107071876526 Accuracy 95.9458784762095\n",
      "Training:: Epoch 107, Iteration 10, Current loss 0.04463091865181923 Accuracy 98.69901547116737\n",
      "Training:: Epoch 107, Iteration 20, Current loss 0.1366247534751892 Accuracy 95.34254718383117\n",
      "Training:: Epoch 107, Iteration 30, Current loss 0.0887887105345726 Accuracy 97.10144927536231\n",
      "Training:: Epoch 107, Iteration 40, Current loss 0.0759865790605545 Accuracy 97.93545459701662\n",
      "Training:: Epoch 107, Iteration 50, Current loss 0.1113460436463356 Accuracy 95.88561538764111\n",
      "Training:: Epoch 107, Iteration 60, Current loss 0.07316556572914124 Accuracy 97.91384536015993\n",
      "Training:: Epoch 107, Iteration 70, Current loss 0.05200199782848358 Accuracy 98.76122964626614\n",
      "Training:: Epoch 107, Iteration 80, Current loss 0.04926615580916405 Accuracy 98.45468715208195\n",
      "Training:: Epoch 107, Iteration 90, Current loss 0.06378158926963806 Accuracy 98.29663644570039\n",
      "Training:: Epoch 107, Iteration 100, Current loss 0.15285012125968933 Accuracy 95.71245877364206\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 107, Probability Accuracy 48.42710361685379\n",
      "Starting Training\n",
      "Training:: Epoch 108, Iteration 0, Current loss 0.0673941969871521 Accuracy 97.65256472161349\n",
      "Training:: Epoch 108, Iteration 10, Current loss 0.0742448940873146 Accuracy 97.79496462070101\n",
      "Training:: Epoch 108, Iteration 20, Current loss 0.13357461988925934 Accuracy 95.98390989541431\n",
      "Training:: Epoch 108, Iteration 30, Current loss 0.07174169272184372 Accuracy 97.99866577718478\n",
      "Training:: Epoch 108, Iteration 40, Current loss 0.1547488123178482 Accuracy 95.3495260663507\n",
      "Training:: Epoch 108, Iteration 50, Current loss 0.07402241975069046 Accuracy 97.544087831376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 108, Iteration 60, Current loss 0.08748374134302139 Accuracy 97.21701899919722\n",
      "Training:: Epoch 108, Iteration 70, Current loss 0.1217871829867363 Accuracy 96.71236989591674\n",
      "Training:: Epoch 108, Iteration 80, Current loss 0.0596158467233181 Accuracy 98.07961289834802\n",
      "Training:: Epoch 108, Iteration 90, Current loss 0.19287365674972534 Accuracy 94.25635892604805\n",
      "Training:: Epoch 108, Iteration 100, Current loss 0.040193162858486176 Accuracy 98.92837893535399\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 108, Probability Accuracy 46.83939594812943\n",
      "Starting Training\n",
      "Training:: Epoch 109, Iteration 0, Current loss 0.054539356380701065 Accuracy 98.24370418753175\n",
      "Training:: Epoch 109, Iteration 10, Current loss 0.06415300816297531 Accuracy 98.20879831796863\n",
      "Training:: Epoch 109, Iteration 20, Current loss 0.08098548650741577 Accuracy 97.19186729940354\n",
      "Training:: Epoch 109, Iteration 30, Current loss 0.07691514492034912 Accuracy 97.97371149911717\n",
      "Training:: Epoch 109, Iteration 40, Current loss 0.06864424794912338 Accuracy 97.74763865342697\n",
      "Training:: Epoch 109, Iteration 50, Current loss 0.07234381139278412 Accuracy 97.89151712887438\n",
      "Training:: Epoch 109, Iteration 60, Current loss 0.05610773712396622 Accuracy 98.1074481074481\n",
      "Training:: Epoch 109, Iteration 70, Current loss 0.04046131670475006 Accuracy 98.7046780281368\n",
      "Training:: Epoch 109, Iteration 80, Current loss 0.07516752928495407 Accuracy 97.77530589543937\n",
      "Training:: Epoch 109, Iteration 90, Current loss 0.07567525655031204 Accuracy 97.55102040816327\n",
      "Training:: Epoch 109, Iteration 100, Current loss 0.13130870461463928 Accuracy 96.38138721585389\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 109, Probability Accuracy 47.19590255624146\n",
      "Starting Training\n",
      "Training:: Epoch 110, Iteration 0, Current loss 0.07065907120704651 Accuracy 97.91524653487843\n",
      "Training:: Epoch 110, Iteration 10, Current loss 0.06222407519817352 Accuracy 98.36095707852088\n",
      "Training:: Epoch 110, Iteration 20, Current loss 0.051791027188301086 Accuracy 98.25303603424248\n",
      "Training:: Epoch 110, Iteration 30, Current loss 0.05758361518383026 Accuracy 98.31928655429243\n",
      "Training:: Epoch 110, Iteration 40, Current loss 0.06257424503564835 Accuracy 97.81138914058688\n",
      "Training:: Epoch 110, Iteration 50, Current loss 0.1288144290447235 Accuracy 96.89825360668185\n",
      "Training:: Epoch 110, Iteration 60, Current loss 0.06563475728034973 Accuracy 97.81463414634146\n",
      "Training:: Epoch 110, Iteration 70, Current loss 0.042815912514925 Accuracy 98.73398130378258\n",
      "Training:: Epoch 110, Iteration 80, Current loss 0.04011957719922066 Accuracy 98.75034493633461\n",
      "Training:: Epoch 110, Iteration 90, Current loss 0.056588053703308105 Accuracy 97.95534262982291\n",
      "Training:: Epoch 110, Iteration 100, Current loss 0.06848664581775665 Accuracy 98.2029598308668\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 110, Probability Accuracy 47.721029954012515\n",
      "Starting Training\n",
      "Training:: Epoch 111, Iteration 0, Current loss 0.0403381884098053 Accuracy 98.84590652181271\n",
      "Training:: Epoch 111, Iteration 10, Current loss 0.0828605368733406 Accuracy 97.43258749282846\n",
      "Training:: Epoch 111, Iteration 20, Current loss 0.05287545546889305 Accuracy 98.64984899626931\n",
      "Training:: Epoch 111, Iteration 30, Current loss 0.052462708204984665 Accuracy 98.40924154909573\n",
      "Training:: Epoch 111, Iteration 40, Current loss 0.06247346103191376 Accuracy 97.89344309152233\n",
      "Training:: Epoch 111, Iteration 50, Current loss 0.09385564923286438 Accuracy 97.21551124032595\n",
      "Training:: Epoch 111, Iteration 60, Current loss 0.0741809532046318 Accuracy 97.70820544301208\n",
      "Training:: Epoch 111, Iteration 70, Current loss 0.05574536323547363 Accuracy 98.54527296937417\n",
      "Training:: Epoch 111, Iteration 80, Current loss 0.06055537611246109 Accuracy 98.19123641304348\n",
      "Training:: Epoch 111, Iteration 90, Current loss 0.0708162933588028 Accuracy 97.81790137458604\n",
      "Training:: Epoch 111, Iteration 100, Current loss 0.0794002041220665 Accuracy 97.38463326746484\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 111, Probability Accuracy 46.861664664208476\n",
      "Starting Training\n",
      "Training:: Epoch 112, Iteration 0, Current loss 0.03981243446469307 Accuracy 98.71318980450384\n",
      "Training:: Epoch 112, Iteration 10, Current loss 0.04379889369010925 Accuracy 98.77615844544096\n",
      "Training:: Epoch 112, Iteration 20, Current loss 0.06907910853624344 Accuracy 98.33182503770739\n",
      "Training:: Epoch 112, Iteration 30, Current loss 0.13914623856544495 Accuracy 96.19652406417113\n",
      "Training:: Epoch 112, Iteration 40, Current loss 0.09022977203130722 Accuracy 97.38095238095238\n",
      "Training:: Epoch 112, Iteration 50, Current loss 0.08803001791238785 Accuracy 97.73310023310023\n",
      "Training:: Epoch 112, Iteration 60, Current loss 0.08817922323942184 Accuracy 97.40048005072234\n",
      "Training:: Epoch 112, Iteration 70, Current loss 0.05089668184518814 Accuracy 98.34642414220752\n",
      "Training:: Epoch 112, Iteration 80, Current loss 0.046578746289014816 Accuracy 98.6236870699022\n",
      "Training:: Epoch 112, Iteration 90, Current loss 0.09430612623691559 Accuracy 97.73494933439301\n",
      "Training:: Epoch 112, Iteration 100, Current loss 0.057077448815107346 Accuracy 98.09817405738677\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 112, Probability Accuracy 48.61467870903592\n",
      "Starting Training\n",
      "Training:: Epoch 113, Iteration 0, Current loss 0.055716101080179214 Accuracy 98.25288048798855\n",
      "Training:: Epoch 113, Iteration 10, Current loss 0.08105529099702835 Accuracy 97.59730696402272\n",
      "Training:: Epoch 113, Iteration 20, Current loss 0.07038436830043793 Accuracy 98.17299967268724\n",
      "Training:: Epoch 113, Iteration 30, Current loss 0.0560370497405529 Accuracy 98.33639039372095\n",
      "Training:: Epoch 113, Iteration 40, Current loss 0.07159256935119629 Accuracy 97.83296918892407\n",
      "Training:: Epoch 113, Iteration 50, Current loss 0.09347954392433167 Accuracy 97.17008883920693\n",
      "Training:: Epoch 113, Iteration 60, Current loss 0.07710504531860352 Accuracy 97.4433615997975\n",
      "Training:: Epoch 113, Iteration 70, Current loss 0.041918329894542694 Accuracy 98.75446622366181\n",
      "Training:: Epoch 113, Iteration 80, Current loss 0.045237112790346146 Accuracy 98.37617554858934\n",
      "Training:: Epoch 113, Iteration 90, Current loss 0.05139654129743576 Accuracy 98.47356168939142\n",
      "Training:: Epoch 113, Iteration 100, Current loss 0.047493353486061096 Accuracy 98.53753323788095\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 113, Probability Accuracy 47.01371338608775\n",
      "Starting Training\n",
      "Training:: Epoch 114, Iteration 0, Current loss 0.06707273423671722 Accuracy 98.4433110190203\n",
      "Training:: Epoch 114, Iteration 10, Current loss 0.10221026837825775 Accuracy 97.39140629579694\n",
      "Training:: Epoch 114, Iteration 20, Current loss 0.11353614926338196 Accuracy 96.78322633520011\n",
      "Training:: Epoch 114, Iteration 30, Current loss 0.0879872739315033 Accuracy 97.23102833366356\n",
      "Training:: Epoch 114, Iteration 40, Current loss 0.10519061237573624 Accuracy 96.71364317841079\n",
      "Training:: Epoch 114, Iteration 50, Current loss 0.0542023591697216 Accuracy 98.34155701754386\n",
      "Training:: Epoch 114, Iteration 60, Current loss 0.10146433115005493 Accuracy 96.90318105242295\n",
      "Training:: Epoch 114, Iteration 70, Current loss 0.054994698613882065 Accuracy 98.2307925443249\n",
      "Training:: Epoch 114, Iteration 80, Current loss 0.07774320244789124 Accuracy 97.41429066143368\n",
      "Training:: Epoch 114, Iteration 90, Current loss 0.11050886660814285 Accuracy 96.56930213590253\n",
      "Training:: Epoch 114, Iteration 100, Current loss 0.09463709592819214 Accuracy 97.13280431202276\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 114, Probability Accuracy 47.196213282512325\n",
      "Starting Training\n",
      "Training:: Epoch 115, Iteration 0, Current loss 0.03811005502939224 Accuracy 98.73281707362258\n",
      "Training:: Epoch 115, Iteration 10, Current loss 0.05052032694220543 Accuracy 98.50690833457139\n",
      "Training:: Epoch 115, Iteration 20, Current loss 0.062008798122406006 Accuracy 97.89995493465526\n",
      "Training:: Epoch 115, Iteration 30, Current loss 0.08201414346694946 Accuracy 98.19587628865979\n",
      "Training:: Epoch 115, Iteration 40, Current loss 0.04956480860710144 Accuracy 98.3313591182984\n",
      "Training:: Epoch 115, Iteration 50, Current loss 0.05636641010642052 Accuracy 98.24530772588389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 115, Iteration 60, Current loss 0.044454630464315414 Accuracy 98.7125152507545\n",
      "Training:: Epoch 115, Iteration 70, Current loss 0.048470329493284225 Accuracy 98.43210105888909\n",
      "Training:: Epoch 115, Iteration 80, Current loss 0.04409634321928024 Accuracy 98.7066637007849\n",
      "Training:: Epoch 115, Iteration 90, Current loss 0.05740375444293022 Accuracy 98.16395614099639\n",
      "Training:: Epoch 115, Iteration 100, Current loss 0.04949800297617912 Accuracy 98.38088964393441\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 115, Probability Accuracy 46.337780171520905\n",
      "Starting Training\n",
      "Training:: Epoch 116, Iteration 0, Current loss 0.06466753780841827 Accuracy 97.73994959302566\n",
      "Training:: Epoch 116, Iteration 10, Current loss 0.05779729783535004 Accuracy 98.30467675378267\n",
      "Training:: Epoch 116, Iteration 20, Current loss 0.0436863899230957 Accuracy 98.72639928499609\n",
      "Training:: Epoch 116, Iteration 30, Current loss 0.05540362000465393 Accuracy 98.16355595532569\n",
      "Training:: Epoch 116, Iteration 40, Current loss 0.06704872846603394 Accuracy 97.9204741911583\n",
      "Training:: Epoch 116, Iteration 50, Current loss 0.045864615589380264 Accuracy 98.63613360323886\n",
      "Training:: Epoch 116, Iteration 60, Current loss 0.06723794341087341 Accuracy 97.41942170173076\n",
      "Training:: Epoch 116, Iteration 70, Current loss 0.09143365174531937 Accuracy 97.4033988417355\n",
      "Training:: Epoch 116, Iteration 80, Current loss 0.09004952013492584 Accuracy 97.1500853450378\n",
      "Training:: Epoch 116, Iteration 90, Current loss 0.16487079858779907 Accuracy 94.83884670525143\n",
      "Training:: Epoch 116, Iteration 100, Current loss 0.09162093698978424 Accuracy 97.41225942099304\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 116, Probability Accuracy 46.297696482578615\n",
      "Starting Training\n",
      "Training:: Epoch 117, Iteration 0, Current loss 0.14627565443515778 Accuracy 94.94411609807479\n",
      "Training:: Epoch 117, Iteration 10, Current loss 0.15530230104923248 Accuracy 94.84086513100053\n",
      "Training:: Epoch 117, Iteration 20, Current loss 0.08978695422410965 Accuracy 97.41666162502268\n",
      "Training:: Epoch 117, Iteration 30, Current loss 0.07310567796230316 Accuracy 97.75922671353251\n",
      "Training:: Epoch 117, Iteration 40, Current loss 0.10790440440177917 Accuracy 96.75267394420554\n",
      "Training:: Epoch 117, Iteration 50, Current loss 0.056777555495500565 Accuracy 98.09828845961366\n",
      "Training:: Epoch 117, Iteration 60, Current loss 0.2009633481502533 Accuracy 92.50893796004206\n",
      "Training:: Epoch 117, Iteration 70, Current loss 0.11182121932506561 Accuracy 96.39491819417749\n",
      "Training:: Epoch 117, Iteration 80, Current loss 0.07885868102312088 Accuracy 97.36320092892738\n",
      "Training:: Epoch 117, Iteration 90, Current loss 0.12549670040607452 Accuracy 95.69351669941061\n",
      "Training:: Epoch 117, Iteration 100, Current loss 0.06654896587133408 Accuracy 97.47056488953565\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 117, Probability Accuracy 45.46132493681899\n",
      "Starting Training\n",
      "Training:: Epoch 118, Iteration 0, Current loss 0.07029200345277786 Accuracy 97.84231101182921\n",
      "Training:: Epoch 118, Iteration 10, Current loss 0.11152570694684982 Accuracy 97.08061813442562\n",
      "Training:: Epoch 118, Iteration 20, Current loss 0.06044215336441994 Accuracy 97.99063994302574\n",
      "Training:: Epoch 118, Iteration 30, Current loss 0.05615620315074921 Accuracy 98.46864365580943\n",
      "Training:: Epoch 118, Iteration 40, Current loss 0.0747927576303482 Accuracy 98.24305960007766\n",
      "Training:: Epoch 118, Iteration 50, Current loss 0.06498973071575165 Accuracy 97.94139127259449\n",
      "Training:: Epoch 118, Iteration 60, Current loss 0.05047549679875374 Accuracy 98.37055565481508\n",
      "Training:: Epoch 118, Iteration 70, Current loss 0.034942105412483215 Accuracy 99.05914367599803\n",
      "Training:: Epoch 118, Iteration 80, Current loss 0.11058696359395981 Accuracy 96.93408277976495\n",
      "Training:: Epoch 118, Iteration 90, Current loss 0.04907391965389252 Accuracy 98.62985043405502\n",
      "Training:: Epoch 118, Iteration 100, Current loss 0.07306500524282455 Accuracy 97.90848016225124\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 118, Probability Accuracy 48.116895223101466\n",
      "Starting Training\n",
      "Training:: Epoch 119, Iteration 0, Current loss 0.05614451318979263 Accuracy 98.21780089988751\n",
      "Training:: Epoch 119, Iteration 10, Current loss 0.06474098563194275 Accuracy 97.58212609671496\n",
      "Training:: Epoch 119, Iteration 20, Current loss 0.03498473018407822 Accuracy 99.20268569030634\n",
      "Training:: Epoch 119, Iteration 30, Current loss 0.06938230991363525 Accuracy 98.03739216880993\n",
      "Training:: Epoch 119, Iteration 40, Current loss 0.0649157464504242 Accuracy 98.27940084719857\n",
      "Training:: Epoch 119, Iteration 50, Current loss 0.03630433231592178 Accuracy 98.89625895462987\n",
      "Training:: Epoch 119, Iteration 60, Current loss 0.03403686732053757 Accuracy 98.99326067060487\n",
      "Training:: Epoch 119, Iteration 70, Current loss 0.06158572807908058 Accuracy 98.0300232040536\n",
      "Training:: Epoch 119, Iteration 80, Current loss 0.055332232266664505 Accuracy 98.63850968048897\n",
      "Training:: Epoch 119, Iteration 90, Current loss 0.07207021862268448 Accuracy 97.23407816553687\n",
      "Training:: Epoch 119, Iteration 100, Current loss 0.05597201734781265 Accuracy 98.08006854116364\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 119, Probability Accuracy 47.86375688776567\n",
      "Starting Training\n",
      "Training:: Epoch 120, Iteration 0, Current loss 0.07486511021852493 Accuracy 98.24643881340792\n",
      "Training:: Epoch 120, Iteration 10, Current loss 0.06496988981962204 Accuracy 97.94865495639357\n",
      "Training:: Epoch 120, Iteration 20, Current loss 0.04702980816364288 Accuracy 98.73060648801129\n",
      "Training:: Epoch 120, Iteration 30, Current loss 0.08212100714445114 Accuracy 97.85879941750714\n",
      "Training:: Epoch 120, Iteration 40, Current loss 0.04883231967687607 Accuracy 98.77747486795025\n",
      "Training:: Epoch 120, Iteration 50, Current loss 0.0474776029586792 Accuracy 98.32094687586017\n",
      "Training:: Epoch 120, Iteration 60, Current loss 0.061580877751111984 Accuracy 98.6475603321392\n",
      "Training:: Epoch 120, Iteration 70, Current loss 0.10072030872106552 Accuracy 98.15213009004167\n",
      "Training:: Epoch 120, Iteration 80, Current loss 0.05536540225148201 Accuracy 98.58979206049149\n",
      "Training:: Epoch 120, Iteration 90, Current loss 0.04253092408180237 Accuracy 98.81233551521696\n",
      "Training:: Epoch 120, Iteration 100, Current loss 0.0447688102722168 Accuracy 98.55800539585077\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 120, Probability Accuracy 48.065832539255084\n",
      "Starting Training\n",
      "Training:: Epoch 121, Iteration 0, Current loss 0.03427257016301155 Accuracy 98.8913043478261\n",
      "Training:: Epoch 121, Iteration 10, Current loss 0.06272869557142258 Accuracy 98.09712284974881\n",
      "Training:: Epoch 121, Iteration 20, Current loss 0.04677215963602066 Accuracy 98.59396433470508\n",
      "Training:: Epoch 121, Iteration 30, Current loss 0.046535324305295944 Accuracy 98.64341874770848\n",
      "Training:: Epoch 121, Iteration 40, Current loss 0.03326473757624626 Accuracy 99.0617131455176\n",
      "Training:: Epoch 121, Iteration 50, Current loss 0.06560397893190384 Accuracy 98.3588785046729\n",
      "Training:: Epoch 121, Iteration 60, Current loss 0.06703902781009674 Accuracy 97.85351466491889\n",
      "Training:: Epoch 121, Iteration 70, Current loss 0.07161524891853333 Accuracy 97.88689500070281\n",
      "Training:: Epoch 121, Iteration 80, Current loss 0.0393621064722538 Accuracy 98.87045042532422\n",
      "Training:: Epoch 121, Iteration 90, Current loss 0.04858822003006935 Accuracy 98.58884373845585\n",
      "Training:: Epoch 121, Iteration 100, Current loss 0.045813918113708496 Accuracy 98.74054231871798\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 121, Probability Accuracy 47.90301197331897\n",
      "Starting Training\n",
      "Training:: Epoch 122, Iteration 0, Current loss 0.05167335271835327 Accuracy 98.32789770668323\n",
      "Training:: Epoch 122, Iteration 10, Current loss 0.06281759589910507 Accuracy 97.95893236462149\n",
      "Training:: Epoch 122, Iteration 20, Current loss 0.09163829684257507 Accuracy 97.21274893688687\n",
      "Training:: Epoch 122, Iteration 30, Current loss 0.14880122244358063 Accuracy 96.37018466862045\n",
      "Training:: Epoch 122, Iteration 40, Current loss 0.08450641483068466 Accuracy 97.29807970664865\n",
      "Training:: Epoch 122, Iteration 50, Current loss 0.05636292323470116 Accuracy 98.1418689814187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 122, Iteration 60, Current loss 0.06337814778089523 Accuracy 97.91155732679337\n",
      "Training:: Epoch 122, Iteration 70, Current loss 0.05516152083873749 Accuracy 98.33017796289285\n",
      "Training:: Epoch 122, Iteration 80, Current loss 0.039543893188238144 Accuracy 98.67188050447403\n",
      "Training:: Epoch 122, Iteration 90, Current loss 0.0645243376493454 Accuracy 97.67949151904924\n",
      "Training:: Epoch 122, Iteration 100, Current loss 0.04328259453177452 Accuracy 98.49170962797827\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 122, Probability Accuracy 48.091726395160954\n",
      "Starting Training\n",
      "Training:: Epoch 123, Iteration 0, Current loss 0.03486746549606323 Accuracy 99.03745743473326\n",
      "Training:: Epoch 123, Iteration 10, Current loss 0.0987078920006752 Accuracy 96.74145299145299\n",
      "Training:: Epoch 123, Iteration 20, Current loss 0.04696516692638397 Accuracy 98.63866763215061\n",
      "Training:: Epoch 123, Iteration 30, Current loss 0.0513247512280941 Accuracy 98.30734556828112\n",
      "Training:: Epoch 123, Iteration 40, Current loss 0.04830072447657585 Accuracy 98.62886244552176\n",
      "Training:: Epoch 123, Iteration 50, Current loss 0.03293508291244507 Accuracy 98.73643849941179\n",
      "Training:: Epoch 123, Iteration 60, Current loss 0.06450124830007553 Accuracy 97.73882692161773\n",
      "Training:: Epoch 123, Iteration 70, Current loss 0.06908204406499863 Accuracy 98.20859872611464\n",
      "Training:: Epoch 123, Iteration 80, Current loss 0.04862966388463974 Accuracy 98.4358283838123\n",
      "Training:: Epoch 123, Iteration 90, Current loss 0.04162002354860306 Accuracy 98.56431981709368\n",
      "Training:: Epoch 123, Iteration 100, Current loss 0.028964856639504433 Accuracy 99.03390077113342\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 123, Probability Accuracy 48.04304594605792\n",
      "Starting Training\n",
      "Training:: Epoch 124, Iteration 0, Current loss 0.03815888240933418 Accuracy 98.78339598237743\n",
      "Training:: Epoch 124, Iteration 10, Current loss 0.037845972925424576 Accuracy 98.94090126217834\n",
      "Training:: Epoch 124, Iteration 20, Current loss 0.0391964353621006 Accuracy 98.8616631294617\n",
      "Training:: Epoch 124, Iteration 30, Current loss 0.05792838707566261 Accuracy 98.14978891958097\n",
      "Training:: Epoch 124, Iteration 40, Current loss 0.06526368111371994 Accuracy 97.6806982014562\n",
      "Training:: Epoch 124, Iteration 50, Current loss 0.04975752532482147 Accuracy 98.48882228050456\n",
      "Training:: Epoch 124, Iteration 60, Current loss 0.07608562707901001 Accuracy 97.3080878349767\n",
      "Training:: Epoch 124, Iteration 70, Current loss 0.03578926995396614 Accuracy 98.75465216146578\n",
      "Training:: Epoch 124, Iteration 80, Current loss 0.04375786334276199 Accuracy 98.65277427667142\n",
      "Training:: Epoch 124, Iteration 90, Current loss 0.07354304194450378 Accuracy 97.84302154027736\n",
      "Training:: Epoch 124, Iteration 100, Current loss 0.044184595346450806 Accuracy 98.42407762190223\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 124, Probability Accuracy 48.32704975763351\n",
      "Starting Training\n",
      "Training:: Epoch 125, Iteration 0, Current loss 0.037506960332393646 Accuracy 98.76972348070613\n",
      "Training:: Epoch 125, Iteration 10, Current loss 0.05417535826563835 Accuracy 98.13841704346409\n",
      "Training:: Epoch 125, Iteration 20, Current loss 0.060531746596097946 Accuracy 98.21311305429786\n",
      "Training:: Epoch 125, Iteration 30, Current loss 0.23106224834918976 Accuracy 95.2079445481205\n",
      "Training:: Epoch 125, Iteration 40, Current loss 0.03389522433280945 Accuracy 99.05987304855036\n",
      "Training:: Epoch 125, Iteration 50, Current loss 0.043188296258449554 Accuracy 98.75408206730087\n",
      "Training:: Epoch 125, Iteration 60, Current loss 0.05585275962948799 Accuracy 98.10841521288684\n",
      "Training:: Epoch 125, Iteration 70, Current loss 0.04109007865190506 Accuracy 98.89477319825006\n",
      "Training:: Epoch 125, Iteration 80, Current loss 0.03820129111409187 Accuracy 99.13156399642887\n",
      "Training:: Epoch 125, Iteration 90, Current loss 0.05042619630694389 Accuracy 98.53845685402838\n",
      "Training:: Epoch 125, Iteration 100, Current loss 0.04598062485456467 Accuracy 98.3822274725996\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 125, Probability Accuracy 47.910262252972615\n",
      "Starting Training\n",
      "Training:: Epoch 126, Iteration 0, Current loss 0.10659102350473404 Accuracy 97.40731525255039\n",
      "Training:: Epoch 126, Iteration 10, Current loss 0.06044667959213257 Accuracy 98.08891076115485\n",
      "Training:: Epoch 126, Iteration 20, Current loss 0.07514785975217819 Accuracy 97.5572658412679\n",
      "Training:: Epoch 126, Iteration 30, Current loss 0.07199384272098541 Accuracy 97.60802741758825\n",
      "Training:: Epoch 126, Iteration 40, Current loss 0.08585845679044724 Accuracy 96.94573957699845\n",
      "Training:: Epoch 126, Iteration 50, Current loss 0.08055770397186279 Accuracy 97.00558659217877\n",
      "Training:: Epoch 126, Iteration 60, Current loss 0.3369526267051697 Accuracy 89.10711165472654\n",
      "Training:: Epoch 126, Iteration 70, Current loss 0.057301558554172516 Accuracy 98.23103534362852\n",
      "Training:: Epoch 126, Iteration 80, Current loss 0.26822659373283386 Accuracy 91.72203370498161\n",
      "Training:: Epoch 126, Iteration 90, Current loss 0.041566863656044006 Accuracy 98.73296852620368\n",
      "Training:: Epoch 126, Iteration 100, Current loss 0.04843710735440254 Accuracy 98.40187175318238\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 126, Probability Accuracy 47.20025272403364\n",
      "Starting Training\n",
      "Training:: Epoch 127, Iteration 0, Current loss 0.04787116497755051 Accuracy 98.63421721758083\n",
      "Training:: Epoch 127, Iteration 10, Current loss 0.11561252921819687 Accuracy 96.26466046847794\n",
      "Training:: Epoch 127, Iteration 20, Current loss 0.17515449225902557 Accuracy 95.15880136371793\n",
      "Training:: Epoch 127, Iteration 30, Current loss 0.26411938667297363 Accuracy 93.13523042054898\n",
      "Training:: Epoch 127, Iteration 40, Current loss 0.29261714220046997 Accuracy 90.88868401010141\n",
      "Training:: Epoch 127, Iteration 50, Current loss 0.1397814303636551 Accuracy 95.70403280929597\n",
      "Training:: Epoch 127, Iteration 60, Current loss 0.32014864683151245 Accuracy 90.80574819475227\n",
      "Training:: Epoch 127, Iteration 70, Current loss 0.1885298192501068 Accuracy 93.88562849733862\n",
      "Training:: Epoch 127, Iteration 80, Current loss 0.1886088103055954 Accuracy 93.46206358494246\n",
      "Training:: Epoch 127, Iteration 90, Current loss 0.20666296780109406 Accuracy 93.44761904761904\n",
      "Training:: Epoch 127, Iteration 100, Current loss 0.08586425334215164 Accuracy 97.31521256110206\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 127, Probability Accuracy 45.108961345651906\n",
      "Starting Training\n",
      "Training:: Epoch 128, Iteration 0, Current loss 0.07016299664974213 Accuracy 97.80958666776668\n",
      "Training:: Epoch 128, Iteration 10, Current loss 0.1369255781173706 Accuracy 95.54765069189274\n",
      "Training:: Epoch 128, Iteration 20, Current loss 0.2071281373500824 Accuracy 93.30294079876718\n",
      "Training:: Epoch 128, Iteration 30, Current loss 0.10007347166538239 Accuracy 97.60454492210378\n",
      "Training:: Epoch 128, Iteration 40, Current loss 0.08038641512393951 Accuracy 97.14370163370593\n",
      "Training:: Epoch 128, Iteration 50, Current loss 0.05338360369205475 Accuracy 98.37520744085523\n",
      "Training:: Epoch 128, Iteration 60, Current loss 0.05756332352757454 Accuracy 98.44622632679167\n",
      "Training:: Epoch 128, Iteration 70, Current loss 0.09772293269634247 Accuracy 96.96531791907515\n",
      "Training:: Epoch 128, Iteration 80, Current loss 0.08583276718854904 Accuracy 96.92868534973798\n",
      "Training:: Epoch 128, Iteration 90, Current loss 0.12541697919368744 Accuracy 96.54200230149597\n",
      "Training:: Epoch 128, Iteration 100, Current loss 0.07065471261739731 Accuracy 97.92832878809627\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 128, Probability Accuracy 50.0066288271119\n",
      "Starting Training\n",
      "Training:: Epoch 129, Iteration 0, Current loss 0.06711215525865555 Accuracy 98.12244897959184\n",
      "Training:: Epoch 129, Iteration 10, Current loss 0.04074198007583618 Accuracy 98.73601053324556\n",
      "Training:: Epoch 129, Iteration 20, Current loss 0.07500448822975159 Accuracy 97.53151857040093\n",
      "Training:: Epoch 129, Iteration 30, Current loss 0.052909258753061295 Accuracy 98.61730345956317\n",
      "Training:: Epoch 129, Iteration 40, Current loss 0.05679052323102951 Accuracy 98.31298557158712\n",
      "Training:: Epoch 129, Iteration 50, Current loss 0.05899113789200783 Accuracy 98.12913385826772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 129, Iteration 60, Current loss 0.0927131175994873 Accuracy 96.87850168080679\n",
      "Training:: Epoch 129, Iteration 70, Current loss 0.06938320398330688 Accuracy 97.43784994400896\n",
      "Training:: Epoch 129, Iteration 80, Current loss 0.054368756711483 Accuracy 98.11571125265392\n",
      "Training:: Epoch 129, Iteration 90, Current loss 0.04923488199710846 Accuracy 98.47535581066889\n",
      "Training:: Epoch 129, Iteration 100, Current loss 0.06303814798593521 Accuracy 98.16400866312266\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 129, Probability Accuracy 46.78305091767825\n",
      "Starting Training\n",
      "Training:: Epoch 130, Iteration 0, Current loss 0.09036558866500854 Accuracy 97.18651330626973\n",
      "Training:: Epoch 130, Iteration 10, Current loss 0.46008387207984924 Accuracy 85.74463710506477\n",
      "Training:: Epoch 130, Iteration 20, Current loss 0.26688966155052185 Accuracy 91.02129540932872\n",
      "Training:: Epoch 130, Iteration 30, Current loss 0.12519420683383942 Accuracy 95.4416467726679\n",
      "Training:: Epoch 130, Iteration 40, Current loss 0.16826072335243225 Accuracy 93.96163314598913\n",
      "Training:: Epoch 130, Iteration 50, Current loss 0.08585181832313538 Accuracy 97.47089947089947\n",
      "Training:: Epoch 130, Iteration 60, Current loss 0.1251947581768036 Accuracy 96.11353973291814\n",
      "Training:: Epoch 130, Iteration 70, Current loss 0.057546768337488174 Accuracy 98.31363004172462\n",
      "Training:: Epoch 130, Iteration 80, Current loss 0.06439850479364395 Accuracy 98.13322938322938\n",
      "Training:: Epoch 130, Iteration 90, Current loss 0.09136469662189484 Accuracy 96.94974393606843\n",
      "Training:: Epoch 130, Iteration 100, Current loss 0.06783454120159149 Accuracy 97.85823559408465\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 130, Probability Accuracy 47.64966648713593\n",
      "Starting Training\n",
      "Training:: Epoch 131, Iteration 0, Current loss 0.04922802746295929 Accuracy 98.38388793502848\n",
      "Training:: Epoch 131, Iteration 10, Current loss 0.13388991355895996 Accuracy 96.18807235543474\n",
      "Training:: Epoch 131, Iteration 20, Current loss 0.07936598360538483 Accuracy 97.96602654695597\n",
      "Training:: Epoch 131, Iteration 30, Current loss 0.06825898587703705 Accuracy 97.62811359669999\n",
      "Training:: Epoch 131, Iteration 40, Current loss 0.07839974015951157 Accuracy 97.60352049202058\n",
      "Training:: Epoch 131, Iteration 50, Current loss 0.05083668977022171 Accuracy 98.46257434631354\n",
      "Training:: Epoch 131, Iteration 60, Current loss 0.037736453115940094 Accuracy 98.81577262834465\n",
      "Training:: Epoch 131, Iteration 70, Current loss 0.0550960935652256 Accuracy 98.40478046245778\n",
      "Training:: Epoch 131, Iteration 80, Current loss 0.032159071415662766 Accuracy 99.03977545518336\n",
      "Training:: Epoch 131, Iteration 90, Current loss 0.06254338473081589 Accuracy 97.66805135951661\n",
      "Training:: Epoch 131, Iteration 100, Current loss 0.34991127252578735 Accuracy 93.29067410035479\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 131, Probability Accuracy 47.57405642789079\n",
      "Starting Training\n",
      "Training:: Epoch 132, Iteration 0, Current loss 0.05475525185465813 Accuracy 98.26237219795517\n",
      "Training:: Epoch 132, Iteration 10, Current loss 0.0399504117667675 Accuracy 99.09816852727344\n",
      "Training:: Epoch 132, Iteration 20, Current loss 0.057044267654418945 Accuracy 98.27634529147981\n",
      "Training:: Epoch 132, Iteration 30, Current loss 0.054145678877830505 Accuracy 98.45621835247623\n",
      "Training:: Epoch 132, Iteration 40, Current loss 0.05350387468934059 Accuracy 98.18211775043936\n",
      "Training:: Epoch 132, Iteration 50, Current loss 0.04266366735100746 Accuracy 98.61906630051\n",
      "Training:: Epoch 132, Iteration 60, Current loss 0.03576485440135002 Accuracy 98.81073288995184\n",
      "Training:: Epoch 132, Iteration 70, Current loss 0.08083294332027435 Accuracy 97.1459127063699\n",
      "Training:: Epoch 132, Iteration 80, Current loss 0.03666701167821884 Accuracy 98.7998647734956\n",
      "Training:: Epoch 132, Iteration 90, Current loss 0.04841875284910202 Accuracy 98.60547847741017\n",
      "Training:: Epoch 132, Iteration 100, Current loss 0.04183606430888176 Accuracy 98.64094695309075\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 132, Probability Accuracy 48.40317769399677\n",
      "Starting Training\n",
      "Training:: Epoch 133, Iteration 0, Current loss 0.030146004632115364 Accuracy 99.17659886453359\n",
      "Training:: Epoch 133, Iteration 10, Current loss 0.17857176065444946 Accuracy 95.26516689677577\n",
      "Training:: Epoch 133, Iteration 20, Current loss 0.10077763348817825 Accuracy 97.27582292849036\n",
      "Training:: Epoch 133, Iteration 30, Current loss 0.08699113875627518 Accuracy 97.43881616391576\n",
      "Training:: Epoch 133, Iteration 40, Current loss 0.04446765035390854 Accuracy 98.57092941998603\n",
      "Training:: Epoch 133, Iteration 50, Current loss 0.04934435337781906 Accuracy 98.26064779393012\n",
      "Training:: Epoch 133, Iteration 60, Current loss 0.03610336408019066 Accuracy 98.85288966725044\n",
      "Training:: Epoch 133, Iteration 70, Current loss 0.07801111787557602 Accuracy 97.38505454174738\n",
      "Training:: Epoch 133, Iteration 80, Current loss 0.059279993176460266 Accuracy 98.492290607782\n",
      "Training:: Epoch 133, Iteration 90, Current loss 0.048467423766851425 Accuracy 98.58701599665261\n",
      "Training:: Epoch 133, Iteration 100, Current loss 0.05336155369877815 Accuracy 98.30816931787544\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 133, Probability Accuracy 47.84159174711024\n",
      "Starting Training\n",
      "Training:: Epoch 134, Iteration 0, Current loss 0.04761161655187607 Accuracy 98.24982841455045\n",
      "Training:: Epoch 134, Iteration 10, Current loss 0.09963008761405945 Accuracy 96.93592097211295\n",
      "Training:: Epoch 134, Iteration 20, Current loss 0.057629071176052094 Accuracy 98.34478192501862\n",
      "Training:: Epoch 134, Iteration 30, Current loss 0.046920210123062134 Accuracy 98.58106164711899\n",
      "Training:: Epoch 134, Iteration 40, Current loss 0.04893374815583229 Accuracy 98.6113089303518\n",
      "Training:: Epoch 134, Iteration 50, Current loss 0.06119716167449951 Accuracy 98.33193592515777\n",
      "Training:: Epoch 134, Iteration 60, Current loss 0.06782304495573044 Accuracy 98.24021869126943\n",
      "Training:: Epoch 134, Iteration 70, Current loss 0.06538370251655579 Accuracy 97.97969278833637\n",
      "Training:: Epoch 134, Iteration 80, Current loss 0.03440554440021515 Accuracy 98.8471470732907\n",
      "Training:: Epoch 134, Iteration 90, Current loss 0.0785166546702385 Accuracy 97.93637145313843\n",
      "Training:: Epoch 134, Iteration 100, Current loss 0.056384291499853134 Accuracy 98.10822701275846\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 134, Probability Accuracy 45.014396983883664\n",
      "Starting Training\n",
      "Training:: Epoch 135, Iteration 0, Current loss 0.07694174349308014 Accuracy 97.74990453250301\n",
      "Training:: Epoch 135, Iteration 10, Current loss 0.08466748893260956 Accuracy 97.31267919038042\n",
      "Training:: Epoch 135, Iteration 20, Current loss 0.0978562980890274 Accuracy 96.80947012401353\n",
      "Training:: Epoch 135, Iteration 30, Current loss 0.04675040766596794 Accuracy 98.60794610125829\n",
      "Training:: Epoch 135, Iteration 40, Current loss 0.07719140499830246 Accuracy 97.28923901745172\n",
      "Training:: Epoch 135, Iteration 50, Current loss 0.06395728141069412 Accuracy 98.14786192684183\n",
      "Training:: Epoch 135, Iteration 60, Current loss 0.08893930166959763 Accuracy 97.0552932703849\n",
      "Training:: Epoch 135, Iteration 70, Current loss 0.06831733882427216 Accuracy 98.24577802704556\n",
      "Training:: Epoch 135, Iteration 80, Current loss 0.03816120699048042 Accuracy 98.68506307885811\n",
      "Training:: Epoch 135, Iteration 90, Current loss 0.07123737037181854 Accuracy 97.68733908487285\n",
      "Training:: Epoch 135, Iteration 100, Current loss 0.06808789074420929 Accuracy 97.65751211631664\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 135, Probability Accuracy 47.36473049674773\n",
      "Starting Training\n",
      "Training:: Epoch 136, Iteration 0, Current loss 0.04154464229941368 Accuracy 98.6912433188471\n",
      "Training:: Epoch 136, Iteration 10, Current loss 0.08882778882980347 Accuracy 97.10757000390389\n",
      "Training:: Epoch 136, Iteration 20, Current loss 0.06555409729480743 Accuracy 97.88697048785384\n",
      "Training:: Epoch 136, Iteration 30, Current loss 0.04747474938631058 Accuracy 98.49530714604956\n",
      "Training:: Epoch 136, Iteration 40, Current loss 0.038289718329906464 Accuracy 98.77718506360279\n",
      "Training:: Epoch 136, Iteration 50, Current loss 0.04702481999993324 Accuracy 98.67859049652964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 136, Iteration 60, Current loss 0.03899724781513214 Accuracy 98.74176332031367\n",
      "Training:: Epoch 136, Iteration 70, Current loss 0.10553823411464691 Accuracy 97.11678552258262\n",
      "Training:: Epoch 136, Iteration 80, Current loss 0.030045349150896072 Accuracy 99.0494530257713\n",
      "Training:: Epoch 136, Iteration 90, Current loss 0.02951117791235447 Accuracy 99.06810035842294\n",
      "Training:: Epoch 136, Iteration 100, Current loss 0.02771734818816185 Accuracy 99.13457377758546\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 136, Probability Accuracy 49.20433359572441\n",
      "Starting Training\n",
      "Training:: Epoch 137, Iteration 0, Current loss 0.03726259991526604 Accuracy 98.63787710624558\n",
      "Training:: Epoch 137, Iteration 10, Current loss 0.06463370472192764 Accuracy 98.40856363710367\n",
      "Training:: Epoch 137, Iteration 20, Current loss 0.0724853128194809 Accuracy 97.97918356312518\n",
      "Training:: Epoch 137, Iteration 30, Current loss 0.06400709599256516 Accuracy 98.36624520528484\n",
      "Training:: Epoch 137, Iteration 40, Current loss 0.045826688408851624 Accuracy 98.60816230242982\n",
      "Training:: Epoch 137, Iteration 50, Current loss 0.05637875944375992 Accuracy 98.29570412017921\n",
      "Training:: Epoch 137, Iteration 60, Current loss 0.07929322123527527 Accuracy 97.88870182463485\n",
      "Training:: Epoch 137, Iteration 70, Current loss 0.0632394552230835 Accuracy 98.2463310421698\n",
      "Training:: Epoch 137, Iteration 80, Current loss 0.04711319878697395 Accuracy 98.5261102977062\n",
      "Training:: Epoch 137, Iteration 90, Current loss 0.04511294513940811 Accuracy 98.6171964036976\n",
      "Training:: Epoch 137, Iteration 100, Current loss 0.04280780255794525 Accuracy 98.60489602526981\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 137, Probability Accuracy 47.588142685503584\n",
      "Starting Training\n",
      "Training:: Epoch 138, Iteration 0, Current loss 0.03655678406357765 Accuracy 98.83322669276224\n",
      "Training:: Epoch 138, Iteration 10, Current loss 0.08830631524324417 Accuracy 97.95728010446787\n",
      "Training:: Epoch 138, Iteration 20, Current loss 0.04639055207371712 Accuracy 98.4000799960002\n",
      "Training:: Epoch 138, Iteration 30, Current loss 0.10567183792591095 Accuracy 97.0158511371468\n",
      "Training:: Epoch 138, Iteration 40, Current loss 0.04292311146855354 Accuracy 98.649725959143\n",
      "Training:: Epoch 138, Iteration 50, Current loss 0.06420258432626724 Accuracy 98.23059037032836\n",
      "Training:: Epoch 138, Iteration 60, Current loss 0.0387665331363678 Accuracy 98.82418812989921\n",
      "Training:: Epoch 138, Iteration 70, Current loss 0.048306286334991455 Accuracy 98.19511619676773\n",
      "Training:: Epoch 138, Iteration 80, Current loss 0.060899972915649414 Accuracy 98.2713668345298\n",
      "Training:: Epoch 138, Iteration 90, Current loss 0.037709008902311325 Accuracy 98.86247266889929\n",
      "Training:: Epoch 138, Iteration 100, Current loss 0.03671920299530029 Accuracy 98.88479179782354\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 138, Probability Accuracy 49.08283962381406\n",
      "Starting Training\n",
      "Training:: Epoch 139, Iteration 0, Current loss 0.04281966760754585 Accuracy 98.59588382381227\n",
      "Training:: Epoch 139, Iteration 10, Current loss 0.042731400579214096 Accuracy 98.80648286796442\n",
      "Training:: Epoch 139, Iteration 20, Current loss 0.0660325139760971 Accuracy 97.80023781212842\n",
      "Training:: Epoch 139, Iteration 30, Current loss 0.06795454025268555 Accuracy 98.09643530573763\n",
      "Training:: Epoch 139, Iteration 40, Current loss 0.039932508021593094 Accuracy 98.69843505956574\n",
      "Training:: Epoch 139, Iteration 50, Current loss 0.06838423013687134 Accuracy 97.72929652715939\n",
      "Training:: Epoch 139, Iteration 60, Current loss 0.03570359945297241 Accuracy 98.90783373929442\n",
      "Training:: Epoch 139, Iteration 70, Current loss 0.03453097864985466 Accuracy 98.89876975533336\n",
      "Training:: Epoch 139, Iteration 80, Current loss 0.038748059421777725 Accuracy 98.66863905325444\n",
      "Training:: Epoch 139, Iteration 90, Current loss 0.041444845497608185 Accuracy 98.65104530553018\n",
      "Training:: Epoch 139, Iteration 100, Current loss 0.0776536762714386 Accuracy 97.69394380387328\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 139, Probability Accuracy 48.07422214856859\n",
      "Starting Training\n",
      "Training:: Epoch 140, Iteration 0, Current loss 0.03784005716443062 Accuracy 98.94842101867681\n",
      "Training:: Epoch 140, Iteration 10, Current loss 0.039526596665382385 Accuracy 98.90499078457592\n",
      "Training:: Epoch 140, Iteration 20, Current loss 0.03341355919837952 Accuracy 99.03823360086695\n",
      "Training:: Epoch 140, Iteration 30, Current loss 0.04360412433743477 Accuracy 98.56939453522273\n",
      "Training:: Epoch 140, Iteration 40, Current loss 0.032511163502931595 Accuracy 99.10047131854236\n",
      "Training:: Epoch 140, Iteration 50, Current loss 0.034671928733587265 Accuracy 98.96338028169014\n",
      "Training:: Epoch 140, Iteration 60, Current loss 0.03948867321014404 Accuracy 98.65771812080537\n",
      "Training:: Epoch 140, Iteration 70, Current loss 0.038465701043605804 Accuracy 99.15456384552674\n",
      "Training:: Epoch 140, Iteration 80, Current loss 0.0339316800236702 Accuracy 98.88481084133258\n",
      "Training:: Epoch 140, Iteration 90, Current loss 0.06562604010105133 Accuracy 98.07368343953709\n",
      "Training:: Epoch 140, Iteration 100, Current loss 0.028993649408221245 Accuracy 99.13562843151803\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 140, Probability Accuracy 48.12652773749845\n",
      "Starting Training\n",
      "Training:: Epoch 141, Iteration 0, Current loss 0.039957597851753235 Accuracy 98.77199550730063\n",
      "Training:: Epoch 141, Iteration 10, Current loss 0.04694485664367676 Accuracy 98.51042701092354\n",
      "Training:: Epoch 141, Iteration 20, Current loss 0.04113754630088806 Accuracy 98.60862182812942\n",
      "Training:: Epoch 141, Iteration 30, Current loss 0.07678505033254623 Accuracy 97.55557282760765\n",
      "Training:: Epoch 141, Iteration 40, Current loss 0.05116872489452362 Accuracy 98.26010764801602\n",
      "Training:: Epoch 141, Iteration 50, Current loss 0.032653696835041046 Accuracy 99.01126822801591\n",
      "Training:: Epoch 141, Iteration 60, Current loss 0.08175821602344513 Accuracy 97.38185944719137\n",
      "Training:: Epoch 141, Iteration 70, Current loss 0.028952214866876602 Accuracy 99.09053516749691\n",
      "Training:: Epoch 141, Iteration 80, Current loss 0.03048001602292061 Accuracy 99.03277807630306\n",
      "Training:: Epoch 141, Iteration 90, Current loss 0.056383922696113586 Accuracy 98.30788961979232\n",
      "Training:: Epoch 141, Iteration 100, Current loss 0.02770012803375721 Accuracy 99.28916948652147\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 141, Probability Accuracy 46.2627915648175\n",
      "Starting Training\n",
      "Training:: Epoch 142, Iteration 0, Current loss 0.04032032936811447 Accuracy 98.65219518704684\n",
      "Training:: Epoch 142, Iteration 10, Current loss 0.8331788182258606 Accuracy 78.881504799658\n",
      "Training:: Epoch 142, Iteration 20, Current loss 0.5175830125808716 Accuracy 84.479427549195\n",
      "Training:: Epoch 142, Iteration 30, Current loss 0.6121079325675964 Accuracy 81.5322276020741\n",
      "Training:: Epoch 142, Iteration 40, Current loss 0.5439988970756531 Accuracy 85.06698920729438\n",
      "Training:: Epoch 142, Iteration 50, Current loss 0.3523649573326111 Accuracy 90.52453468697124\n",
      "Training:: Epoch 142, Iteration 60, Current loss 0.199552521109581 Accuracy 93.11186825667235\n",
      "Training:: Epoch 142, Iteration 70, Current loss 0.20214706659317017 Accuracy 93.42720970537262\n",
      "Training:: Epoch 142, Iteration 80, Current loss 0.3564053177833557 Accuracy 88.19959541469993\n",
      "Training:: Epoch 142, Iteration 90, Current loss 0.21046216785907745 Accuracy 92.76818332779807\n",
      "Training:: Epoch 142, Iteration 100, Current loss 0.2386227697134018 Accuracy 91.63350023537467\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 142, Probability Accuracy 45.486597340183124\n",
      "Starting Training\n",
      "Training:: Epoch 143, Iteration 0, Current loss 0.11724364757537842 Accuracy 96.18549854176894\n",
      "Training:: Epoch 143, Iteration 10, Current loss 0.21743075549602509 Accuracy 93.93529287375766\n",
      "Training:: Epoch 143, Iteration 20, Current loss 0.11458872258663177 Accuracy 96.33846153846154\n",
      "Training:: Epoch 143, Iteration 30, Current loss 0.13918854296207428 Accuracy 95.47608440322385\n",
      "Training:: Epoch 143, Iteration 40, Current loss 0.15343818068504333 Accuracy 95.18275372604684\n",
      "Training:: Epoch 143, Iteration 50, Current loss 0.12856823205947876 Accuracy 96.00615466952101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 143, Iteration 60, Current loss 0.1209813803434372 Accuracy 96.84160222434616\n",
      "Training:: Epoch 143, Iteration 70, Current loss 0.16852699220180511 Accuracy 94.98513450585665\n",
      "Training:: Epoch 143, Iteration 80, Current loss 0.2820633351802826 Accuracy 91.46586345381526\n",
      "Training:: Epoch 143, Iteration 90, Current loss 0.2170143574476242 Accuracy 92.47291145733153\n",
      "Training:: Epoch 143, Iteration 100, Current loss 0.09344017505645752 Accuracy 96.87213611188588\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 143, Probability Accuracy 47.64935576086506\n",
      "Starting Training\n",
      "Training:: Epoch 144, Iteration 0, Current loss 0.09911312162876129 Accuracy 96.71292653265613\n",
      "Training:: Epoch 144, Iteration 10, Current loss 0.08896800875663757 Accuracy 97.22222222222223\n",
      "Training:: Epoch 144, Iteration 20, Current loss 0.10096815228462219 Accuracy 96.89314294680685\n",
      "Training:: Epoch 144, Iteration 30, Current loss 0.1397220343351364 Accuracy 96.08461756259354\n",
      "Training:: Epoch 144, Iteration 40, Current loss 0.12902644276618958 Accuracy 95.50640471189094\n",
      "Training:: Epoch 144, Iteration 50, Current loss 0.08153875917196274 Accuracy 97.44306356662901\n",
      "Training:: Epoch 144, Iteration 60, Current loss 0.09669778496026993 Accuracy 96.87097051491446\n",
      "Training:: Epoch 144, Iteration 70, Current loss 0.09930645674467087 Accuracy 97.1704306722689\n",
      "Training:: Epoch 144, Iteration 80, Current loss 0.08606979995965958 Accuracy 96.59873313136877\n",
      "Training:: Epoch 144, Iteration 90, Current loss 0.15557187795639038 Accuracy 95.00854700854701\n",
      "Training:: Epoch 144, Iteration 100, Current loss 0.07052575051784515 Accuracy 97.66855423818794\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 144, Probability Accuracy 48.63549736918424\n",
      "Starting Training\n",
      "Training:: Epoch 145, Iteration 0, Current loss 0.14704443514347076 Accuracy 95.8509462183781\n",
      "Training:: Epoch 145, Iteration 10, Current loss 0.1380794793367386 Accuracy 95.58012264005802\n",
      "Training:: Epoch 145, Iteration 20, Current loss 0.05266831815242767 Accuracy 98.32356865247404\n",
      "Training:: Epoch 145, Iteration 30, Current loss 0.044326942414045334 Accuracy 98.6496689510977\n",
      "Training:: Epoch 145, Iteration 40, Current loss 0.08540111780166626 Accuracy 97.1976401179941\n",
      "Training:: Epoch 145, Iteration 50, Current loss 0.11840653419494629 Accuracy 96.23415557776258\n",
      "Training:: Epoch 145, Iteration 60, Current loss 0.07338228076696396 Accuracy 97.58359200031433\n",
      "Training:: Epoch 145, Iteration 70, Current loss 0.07117082923650742 Accuracy 97.69161193912463\n",
      "Training:: Epoch 145, Iteration 80, Current loss 0.04689491540193558 Accuracy 98.57972457275594\n",
      "Training:: Epoch 145, Iteration 90, Current loss 0.040539857000112534 Accuracy 98.58685411986376\n",
      "Training:: Epoch 145, Iteration 100, Current loss 0.03827793896198273 Accuracy 98.68455669560642\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 145, Probability Accuracy 48.959274143431244\n",
      "Starting Training\n",
      "Training:: Epoch 146, Iteration 0, Current loss 0.05146526172757149 Accuracy 98.45842034148139\n",
      "Training:: Epoch 146, Iteration 10, Current loss 0.06224828585982323 Accuracy 97.71624898949071\n",
      "Training:: Epoch 146, Iteration 20, Current loss 0.12062858790159225 Accuracy 96.55764365739067\n",
      "Training:: Epoch 146, Iteration 30, Current loss 0.08470726758241653 Accuracy 97.44794311833418\n",
      "Training:: Epoch 146, Iteration 40, Current loss 0.07019501179456711 Accuracy 97.68521503643046\n",
      "Training:: Epoch 146, Iteration 50, Current loss 0.06489963084459305 Accuracy 98.20145717633872\n",
      "Training:: Epoch 146, Iteration 60, Current loss 0.10026952624320984 Accuracy 96.83899192803007\n",
      "Training:: Epoch 146, Iteration 70, Current loss 0.07091085612773895 Accuracy 97.83512008642899\n",
      "Training:: Epoch 146, Iteration 80, Current loss 0.05492103099822998 Accuracy 98.4112245950686\n",
      "Training:: Epoch 146, Iteration 90, Current loss 0.05010131746530533 Accuracy 98.23022010707912\n",
      "Training:: Epoch 146, Iteration 100, Current loss 0.03500908240675926 Accuracy 98.82396800650716\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 146, Probability Accuracy 49.40703069975556\n",
      "Starting Training\n",
      "Training:: Epoch 147, Iteration 0, Current loss 0.03304808586835861 Accuracy 98.93519205101587\n",
      "Training:: Epoch 147, Iteration 10, Current loss 0.06805958598852158 Accuracy 97.48922413793103\n",
      "Training:: Epoch 147, Iteration 20, Current loss 0.06854626536369324 Accuracy 97.79603831084023\n",
      "Training:: Epoch 147, Iteration 30, Current loss 0.04637032374739647 Accuracy 98.59582709745577\n",
      "Training:: Epoch 147, Iteration 40, Current loss 0.05242606997489929 Accuracy 98.28447701532312\n",
      "Training:: Epoch 147, Iteration 50, Current loss 0.04012028127908707 Accuracy 98.74638715743288\n",
      "Training:: Epoch 147, Iteration 60, Current loss 0.03401976823806763 Accuracy 98.66597516351169\n",
      "Training:: Epoch 147, Iteration 70, Current loss 0.059378258883953094 Accuracy 98.4014983230977\n",
      "Training:: Epoch 147, Iteration 80, Current loss 0.034837037324905396 Accuracy 98.77127514335123\n",
      "Training:: Epoch 147, Iteration 90, Current loss 0.03151623159646988 Accuracy 99.00579632259799\n",
      "Training:: Epoch 147, Iteration 100, Current loss 0.03485589474439621 Accuracy 98.85834943812182\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 147, Probability Accuracy 48.919294029912585\n",
      "Starting Training\n",
      "Training:: Epoch 148, Iteration 0, Current loss 0.06394433975219727 Accuracy 98.23508853681267\n",
      "Training:: Epoch 148, Iteration 10, Current loss 0.03170968219637871 Accuracy 99.03488161937466\n",
      "Training:: Epoch 148, Iteration 20, Current loss 0.04257810860872269 Accuracy 98.59521531100478\n",
      "Training:: Epoch 148, Iteration 30, Current loss 0.028529953211545944 Accuracy 99.04663365604907\n",
      "Training:: Epoch 148, Iteration 40, Current loss 0.037256352603435516 Accuracy 98.946474086661\n",
      "Training:: Epoch 148, Iteration 50, Current loss 0.04002160578966141 Accuracy 98.70921761165664\n",
      "Training:: Epoch 148, Iteration 60, Current loss 0.038420308381319046 Accuracy 98.75029036004646\n",
      "Training:: Epoch 148, Iteration 70, Current loss 0.0351196825504303 Accuracy 98.94062303218877\n",
      "Training:: Epoch 148, Iteration 80, Current loss 0.09017667174339294 Accuracy 97.01825127334465\n",
      "Training:: Epoch 148, Iteration 90, Current loss 0.09252937138080597 Accuracy 96.79745914113677\n",
      "Training:: Epoch 148, Iteration 100, Current loss 0.04281488060951233 Accuracy 98.53682026966958\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 148, Probability Accuracy 49.42080623109749\n",
      "Starting Training\n",
      "Training:: Epoch 149, Iteration 0, Current loss 0.04126308113336563 Accuracy 98.83951907997908\n",
      "Training:: Epoch 149, Iteration 10, Current loss 0.06111375242471695 Accuracy 98.16733067729083\n",
      "Training:: Epoch 149, Iteration 20, Current loss 0.032221850007772446 Accuracy 98.95774396235967\n",
      "Training:: Epoch 149, Iteration 30, Current loss 0.049779973924160004 Accuracy 98.61496993572466\n",
      "Training:: Epoch 149, Iteration 40, Current loss 0.04024457931518555 Accuracy 98.82847319495734\n",
      "Training:: Epoch 149, Iteration 50, Current loss 0.06605967879295349 Accuracy 97.96713645515361\n",
      "Training:: Epoch 149, Iteration 60, Current loss 0.035252489149570465 Accuracy 98.72617565367302\n",
      "Training:: Epoch 149, Iteration 70, Current loss 0.03408253565430641 Accuracy 98.87832393231265\n",
      "Training:: Epoch 149, Iteration 80, Current loss 0.03447449207305908 Accuracy 98.84287731766207\n",
      "Training:: Epoch 149, Iteration 90, Current loss 0.04155847430229187 Accuracy 98.55260004792716\n",
      "Training:: Epoch 149, Iteration 100, Current loss 0.06264583766460419 Accuracy 97.96407185628742\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 149, Probability Accuracy 48.917947549405476\n",
      "Starting Training\n",
      "Training:: Epoch 150, Iteration 0, Current loss 0.03391022980213165 Accuracy 98.94766646217734\n",
      "Training:: Epoch 150, Iteration 10, Current loss 0.04760832339525223 Accuracy 98.49436278007707\n",
      "Training:: Epoch 150, Iteration 20, Current loss 0.04233444109559059 Accuracy 98.52422775885762\n",
      "Training:: Epoch 150, Iteration 30, Current loss 0.05053258314728737 Accuracy 98.59624318319531\n",
      "Training:: Epoch 150, Iteration 40, Current loss 0.03490258380770683 Accuracy 98.8731249538166\n",
      "Training:: Epoch 150, Iteration 50, Current loss 0.049675267189741135 Accuracy 98.5758552693355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 150, Iteration 60, Current loss 0.06618651002645493 Accuracy 98.34875314012622\n",
      "Training:: Epoch 150, Iteration 70, Current loss 0.034737128764390945 Accuracy 98.86354636889206\n",
      "Training:: Epoch 150, Iteration 80, Current loss 0.04851087927818298 Accuracy 98.65435840911178\n",
      "Training:: Epoch 150, Iteration 90, Current loss 0.050779424607753754 Accuracy 98.38527445626589\n",
      "Training:: Epoch 150, Iteration 100, Current loss 0.03558338060975075 Accuracy 98.79939969984993\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 150, Probability Accuracy 47.498446368645645\n",
      "Starting Training\n",
      "Training:: Epoch 151, Iteration 0, Current loss 0.04465875402092934 Accuracy 98.47366655484737\n",
      "Training:: Epoch 151, Iteration 10, Current loss 0.033937886357307434 Accuracy 98.80047124344007\n",
      "Training:: Epoch 151, Iteration 20, Current loss 0.07672357559204102 Accuracy 98.05420143766008\n",
      "Training:: Epoch 151, Iteration 30, Current loss 0.03646674379706383 Accuracy 98.88029442954276\n",
      "Training:: Epoch 151, Iteration 40, Current loss 0.031479958444833755 Accuracy 99.05763751917598\n",
      "Training:: Epoch 151, Iteration 50, Current loss 0.0420348234474659 Accuracy 98.86426695275794\n",
      "Training:: Epoch 151, Iteration 60, Current loss 0.07513873279094696 Accuracy 96.84801711622221\n",
      "Training:: Epoch 151, Iteration 70, Current loss 0.05162879452109337 Accuracy 98.3446825001384\n",
      "Training:: Epoch 151, Iteration 80, Current loss 0.0452028289437294 Accuracy 98.30663197772604\n",
      "Training:: Epoch 151, Iteration 90, Current loss 0.7516602277755737 Accuracy 84.3598738842755\n",
      "Training:: Epoch 151, Iteration 100, Current loss 0.045974839478731155 Accuracy 98.89940828402366\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 151, Probability Accuracy 48.30239880681112\n",
      "Starting Training\n",
      "Training:: Epoch 152, Iteration 0, Current loss 0.04539675638079643 Accuracy 98.58750827631869\n",
      "Training:: Epoch 152, Iteration 10, Current loss 0.06545622646808624 Accuracy 97.8493484479288\n",
      "Training:: Epoch 152, Iteration 20, Current loss 0.051067933440208435 Accuracy 98.30052400509842\n",
      "Training:: Epoch 152, Iteration 30, Current loss 0.03554341569542885 Accuracy 98.968219729652\n",
      "Training:: Epoch 152, Iteration 40, Current loss 0.026809008792042732 Accuracy 99.11991199119912\n",
      "Training:: Epoch 152, Iteration 50, Current loss 0.04368090257048607 Accuracy 98.61714325423023\n",
      "Training:: Epoch 152, Iteration 60, Current loss 0.0554739385843277 Accuracy 98.47963129734597\n",
      "Training:: Epoch 152, Iteration 70, Current loss 0.03507177159190178 Accuracy 98.83155883155884\n",
      "Training:: Epoch 152, Iteration 80, Current loss 0.042371902614831924 Accuracy 98.76118868600072\n",
      "Training:: Epoch 152, Iteration 90, Current loss 0.03750478848814964 Accuracy 98.74806458748064\n",
      "Training:: Epoch 152, Iteration 100, Current loss 0.030370408669114113 Accuracy 99.10680941117889\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 152, Probability Accuracy 48.9815428595103\n",
      "Starting Training\n",
      "Training:: Epoch 153, Iteration 0, Current loss 0.06659261882305145 Accuracy 97.78207799940458\n",
      "Training:: Epoch 153, Iteration 10, Current loss 0.0502852201461792 Accuracy 98.56486796785305\n",
      "Training:: Epoch 153, Iteration 20, Current loss 0.05687926709651947 Accuracy 98.36138175376439\n",
      "Training:: Epoch 153, Iteration 30, Current loss 0.03601066768169403 Accuracy 98.93848886018728\n",
      "Training:: Epoch 153, Iteration 40, Current loss 0.03769410029053688 Accuracy 98.88239191174353\n",
      "Training:: Epoch 153, Iteration 50, Current loss 0.03763846680521965 Accuracy 98.82192168135337\n",
      "Training:: Epoch 153, Iteration 60, Current loss 0.040226638317108154 Accuracy 98.7236427924626\n",
      "Training:: Epoch 153, Iteration 70, Current loss 0.1676805019378662 Accuracy 95.12903022272698\n",
      "Training:: Epoch 153, Iteration 80, Current loss 0.045673783868551254 Accuracy 98.58460987189824\n",
      "Training:: Epoch 153, Iteration 90, Current loss 0.03986279293894768 Accuracy 98.7144291931704\n",
      "Training:: Epoch 153, Iteration 100, Current loss 0.02667328156530857 Accuracy 99.09511027478761\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 153, Probability Accuracy 47.42200770601152\n",
      "Starting Training\n",
      "Training:: Epoch 154, Iteration 0, Current loss 0.03262251615524292 Accuracy 98.9707900411684\n",
      "Training:: Epoch 154, Iteration 10, Current loss 0.10144546627998352 Accuracy 96.83711969173959\n",
      "Training:: Epoch 154, Iteration 20, Current loss 0.15535831451416016 Accuracy 95.5002343627936\n",
      "Training:: Epoch 154, Iteration 30, Current loss 0.10420878231525421 Accuracy 96.78263035536366\n",
      "Training:: Epoch 154, Iteration 40, Current loss 0.11588280647993088 Accuracy 96.16429598849288\n",
      "Training:: Epoch 154, Iteration 50, Current loss 0.04941064491868019 Accuracy 98.48343443770415\n",
      "Training:: Epoch 154, Iteration 60, Current loss 0.07227642834186554 Accuracy 97.7079324353218\n",
      "Training:: Epoch 154, Iteration 70, Current loss 0.1322236955165863 Accuracy 96.02763385146805\n",
      "Training:: Epoch 154, Iteration 80, Current loss 0.045156028121709824 Accuracy 98.44001610305958\n",
      "Training:: Epoch 154, Iteration 90, Current loss 0.09231425821781158 Accuracy 96.78019728270985\n",
      "Training:: Epoch 154, Iteration 100, Current loss 0.05514770746231079 Accuracy 98.3205319270893\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 154, Probability Accuracy 46.593404317023655\n",
      "Starting Training\n",
      "Training:: Epoch 155, Iteration 0, Current loss 0.04541851207613945 Accuracy 98.61400793917782\n",
      "Training:: Epoch 155, Iteration 10, Current loss 0.044748783111572266 Accuracy 98.57032952942684\n",
      "Training:: Epoch 155, Iteration 20, Current loss 0.06670238822698593 Accuracy 97.71906699524328\n",
      "Training:: Epoch 155, Iteration 30, Current loss 0.06522036343812943 Accuracy 98.19162899227067\n",
      "Training:: Epoch 155, Iteration 40, Current loss 0.06699291616678238 Accuracy 97.61273209549071\n",
      "Training:: Epoch 155, Iteration 50, Current loss 0.05103519931435585 Accuracy 98.38354681580371\n",
      "Training:: Epoch 155, Iteration 60, Current loss 0.07220862060785294 Accuracy 98.04230977858869\n",
      "Training:: Epoch 155, Iteration 70, Current loss 0.05203854292631149 Accuracy 98.72571345510926\n",
      "Training:: Epoch 155, Iteration 80, Current loss 0.09421663731336594 Accuracy 97.11888457030045\n",
      "Training:: Epoch 155, Iteration 90, Current loss 0.039729323238134384 Accuracy 98.68788270087892\n",
      "Training:: Epoch 155, Iteration 100, Current loss 0.050992775708436966 Accuracy 98.39754353979754\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 155, Probability Accuracy 48.961552802750965\n",
      "Starting Training\n",
      "Training:: Epoch 156, Iteration 0, Current loss 0.043313611298799515 Accuracy 98.83653803454861\n",
      "Training:: Epoch 156, Iteration 10, Current loss 0.2333671599626541 Accuracy 92.87658539410437\n",
      "Training:: Epoch 156, Iteration 20, Current loss 0.052513882517814636 Accuracy 98.41584158415841\n",
      "Training:: Epoch 156, Iteration 30, Current loss 0.042336560785770416 Accuracy 98.65780026109661\n",
      "Training:: Epoch 156, Iteration 40, Current loss 0.044171981513500214 Accuracy 98.49899642202635\n",
      "Training:: Epoch 156, Iteration 50, Current loss 0.03659228980541229 Accuracy 98.65078469007565\n",
      "Training:: Epoch 156, Iteration 60, Current loss 0.05118998512625694 Accuracy 98.43247353336842\n",
      "Training:: Epoch 156, Iteration 70, Current loss 0.03377290815114975 Accuracy 98.94005609549279\n",
      "Training:: Epoch 156, Iteration 80, Current loss 0.06398848444223404 Accuracy 98.11348948410475\n",
      "Training:: Epoch 156, Iteration 90, Current loss 0.041428808122873306 Accuracy 98.77920560747664\n",
      "Training:: Epoch 156, Iteration 100, Current loss 0.038165133446455 Accuracy 98.84352135945244\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 156, Probability Accuracy 49.468140199693416\n",
      "Starting Training\n",
      "Training:: Epoch 157, Iteration 0, Current loss 0.06458044797182083 Accuracy 97.86974655117099\n",
      "Training:: Epoch 157, Iteration 10, Current loss 0.03768199309706688 Accuracy 98.70014018096087\n",
      "Training:: Epoch 157, Iteration 20, Current loss 0.03668428584933281 Accuracy 98.8335008082485\n",
      "Training:: Epoch 157, Iteration 30, Current loss 0.04096639156341553 Accuracy 98.79131580454832\n",
      "Training:: Epoch 157, Iteration 40, Current loss 0.034727390855550766 Accuracy 98.85757029682772\n",
      "Training:: Epoch 157, Iteration 50, Current loss 0.04299856349825859 Accuracy 98.56854232042299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 157, Iteration 60, Current loss 0.04599160701036453 Accuracy 98.52121307975169\n",
      "Training:: Epoch 157, Iteration 70, Current loss 0.0639168843626976 Accuracy 97.3884389147239\n",
      "Training:: Epoch 157, Iteration 80, Current loss 0.0912519097328186 Accuracy 97.65195871310436\n",
      "Training:: Epoch 157, Iteration 90, Current loss 0.0486786887049675 Accuracy 98.37205358656944\n",
      "Training:: Epoch 157, Iteration 100, Current loss 0.03633388131856918 Accuracy 98.95803421257743\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 157, Probability Accuracy 48.54911546588225\n",
      "Starting Training\n",
      "Training:: Epoch 158, Iteration 0, Current loss 0.024446390569210052 Accuracy 99.21052631578948\n",
      "Training:: Epoch 158, Iteration 10, Current loss 0.046461790800094604 Accuracy 98.62367921972366\n",
      "Training:: Epoch 158, Iteration 20, Current loss 0.042724814265966415 Accuracy 98.73688209316681\n",
      "Training:: Epoch 158, Iteration 30, Current loss 0.05776875838637352 Accuracy 98.48058226637625\n",
      "Training:: Epoch 158, Iteration 40, Current loss 0.038055263459682465 Accuracy 98.88636760342119\n",
      "Training:: Epoch 158, Iteration 50, Current loss 0.04346165806055069 Accuracy 98.61843853190116\n",
      "Training:: Epoch 158, Iteration 60, Current loss 0.043741654604673386 Accuracy 98.55017169019459\n",
      "Training:: Epoch 158, Iteration 70, Current loss 0.028499213978648186 Accuracy 99.07595549104983\n",
      "Training:: Epoch 158, Iteration 80, Current loss 0.038016580045223236 Accuracy 98.7150761351685\n",
      "Training:: Epoch 158, Iteration 90, Current loss 0.03111926279962063 Accuracy 98.88921125709786\n",
      "Training:: Epoch 158, Iteration 100, Current loss 0.07915172725915909 Accuracy 97.93413630126544\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 158, Probability Accuracy 48.323321042383064\n",
      "Starting Training\n",
      "Training:: Epoch 159, Iteration 0, Current loss 0.033086929470300674 Accuracy 98.84997386304234\n",
      "Training:: Epoch 159, Iteration 10, Current loss 0.0491144061088562 Accuracy 98.54969932790945\n",
      "Training:: Epoch 159, Iteration 20, Current loss 0.05276315659284592 Accuracy 98.39114457291386\n",
      "Training:: Epoch 159, Iteration 30, Current loss 0.023520076647400856 Accuracy 99.22428330522766\n",
      "Training:: Epoch 159, Iteration 40, Current loss 0.03197730332612991 Accuracy 99.16553901668922\n",
      "Training:: Epoch 159, Iteration 50, Current loss 0.034669019281864166 Accuracy 98.8318548114038\n",
      "Training:: Epoch 159, Iteration 60, Current loss 0.027916263788938522 Accuracy 99.07395158089695\n",
      "Training:: Epoch 159, Iteration 70, Current loss 0.029976800084114075 Accuracy 99.05134556919266\n",
      "Training:: Epoch 159, Iteration 80, Current loss 0.03940413519740105 Accuracy 98.8745397022976\n",
      "Training:: Epoch 159, Iteration 90, Current loss 0.02366519719362259 Accuracy 99.30898928165597\n",
      "Training:: Epoch 159, Iteration 100, Current loss 0.07034581154584885 Accuracy 98.19381729767281\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 159, Probability Accuracy 48.81820441645606\n",
      "Starting Training\n",
      "Training:: Epoch 160, Iteration 0, Current loss 0.021862315014004707 Accuracy 99.39525882922109\n",
      "Training:: Epoch 160, Iteration 10, Current loss 0.07703353464603424 Accuracy 97.82241123123431\n",
      "Training:: Epoch 160, Iteration 20, Current loss 0.046326812356710434 Accuracy 98.58286314132003\n",
      "Training:: Epoch 160, Iteration 30, Current loss 0.048195980489254 Accuracy 98.52259660615353\n",
      "Training:: Epoch 160, Iteration 40, Current loss 0.033083025366067886 Accuracy 98.87059789862761\n",
      "Training:: Epoch 160, Iteration 50, Current loss 0.035246822983026505 Accuracy 98.80267992219581\n",
      "Training:: Epoch 160, Iteration 60, Current loss 0.07752098143100739 Accuracy 97.25833136374379\n",
      "Training:: Epoch 160, Iteration 70, Current loss 0.10880092531442642 Accuracy 95.7129574284386\n",
      "Training:: Epoch 160, Iteration 80, Current loss 0.13981083035469055 Accuracy 95.2999347967833\n",
      "Training:: Epoch 160, Iteration 90, Current loss 0.051966018974781036 Accuracy 98.53327297419646\n",
      "Training:: Epoch 160, Iteration 100, Current loss 0.09278715401887894 Accuracy 96.80798532731377\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 160, Probability Accuracy 48.57749513195509\n",
      "Starting Training\n",
      "Training:: Epoch 161, Iteration 0, Current loss 0.04299330711364746 Accuracy 98.56230031948881\n",
      "Training:: Epoch 161, Iteration 10, Current loss 0.07423213869333267 Accuracy 97.77864968460447\n",
      "Training:: Epoch 161, Iteration 20, Current loss 0.05018966645002365 Accuracy 98.3238771709472\n",
      "Training:: Epoch 161, Iteration 30, Current loss 0.04192660376429558 Accuracy 98.74048158057214\n",
      "Training:: Epoch 161, Iteration 40, Current loss 0.0489109568297863 Accuracy 98.33868819058067\n",
      "Training:: Epoch 161, Iteration 50, Current loss 0.03926204890012741 Accuracy 98.65499302119021\n",
      "Training:: Epoch 161, Iteration 60, Current loss 0.04106735438108444 Accuracy 98.67475030119311\n",
      "Training:: Epoch 161, Iteration 70, Current loss 0.02392067015171051 Accuracy 99.22864060878588\n",
      "Training:: Epoch 161, Iteration 80, Current loss 0.08004004508256912 Accuracy 97.88414845646895\n",
      "Training:: Epoch 161, Iteration 90, Current loss 0.03885740786790848 Accuracy 98.71967654986523\n",
      "Training:: Epoch 161, Iteration 100, Current loss 0.03263557329773903 Accuracy 98.99375316562553\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 161, Probability Accuracy 49.276214939719104\n",
      "Starting Training\n",
      "Training:: Epoch 162, Iteration 0, Current loss 0.037664659321308136 Accuracy 98.85988179999069\n",
      "Training:: Epoch 162, Iteration 10, Current loss 0.06914163380861282 Accuracy 97.82522451040502\n",
      "Training:: Epoch 162, Iteration 20, Current loss 0.050384003669023514 Accuracy 98.54172190447332\n",
      "Training:: Epoch 162, Iteration 30, Current loss 0.05372755602002144 Accuracy 98.18742985409652\n",
      "Training:: Epoch 162, Iteration 40, Current loss 0.041518859565258026 Accuracy 98.70141554702495\n",
      "Training:: Epoch 162, Iteration 50, Current loss 0.05246039852499962 Accuracy 98.21277701956197\n",
      "Training:: Epoch 162, Iteration 60, Current loss 0.04458095505833626 Accuracy 98.61080116045525\n",
      "Training:: Epoch 162, Iteration 70, Current loss 0.07006846368312836 Accuracy 97.70550867545273\n",
      "Training:: Epoch 162, Iteration 80, Current loss 0.09624847024679184 Accuracy 97.32349841938883\n",
      "Training:: Epoch 162, Iteration 90, Current loss 0.07877866923809052 Accuracy 98.08322645004988\n",
      "Training:: Epoch 162, Iteration 100, Current loss 0.05642231926321983 Accuracy 98.24734214191093\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 162, Probability Accuracy 46.74058499399263\n",
      "Starting Training\n",
      "Training:: Epoch 163, Iteration 0, Current loss 0.04394717141985893 Accuracy 98.56305751502258\n",
      "Training:: Epoch 163, Iteration 10, Current loss 0.0477125458419323 Accuracy 98.36898509884291\n",
      "Training:: Epoch 163, Iteration 20, Current loss 0.05666289106011391 Accuracy 98.43367300656163\n",
      "Training:: Epoch 163, Iteration 30, Current loss 0.037168487906455994 Accuracy 98.92028578309385\n",
      "Training:: Epoch 163, Iteration 40, Current loss 0.04003996029496193 Accuracy 98.73827727769199\n",
      "Training:: Epoch 163, Iteration 50, Current loss 0.059201449155807495 Accuracy 98.00310020972007\n",
      "Training:: Epoch 163, Iteration 60, Current loss 0.029861610382795334 Accuracy 99.17307131043876\n",
      "Training:: Epoch 163, Iteration 70, Current loss 0.051584936678409576 Accuracy 98.24918566775244\n",
      "Training:: Epoch 163, Iteration 80, Current loss 0.054199717938899994 Accuracy 98.14365272026275\n",
      "Training:: Epoch 163, Iteration 90, Current loss 0.026627616956830025 Accuracy 99.20451629458557\n",
      "Training:: Epoch 163, Iteration 100, Current loss 0.06755350530147552 Accuracy 97.50012549570805\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 163, Probability Accuracy 48.29121266105978\n",
      "Starting Training\n",
      "Training:: Epoch 164, Iteration 0, Current loss 0.032444458454847336 Accuracy 99.0005576287908\n",
      "Training:: Epoch 164, Iteration 10, Current loss 0.037291087210178375 Accuracy 98.80885069169588\n",
      "Training:: Epoch 164, Iteration 20, Current loss 0.03766203299164772 Accuracy 98.62802335279399\n",
      "Training:: Epoch 164, Iteration 30, Current loss 0.044502824544906616 Accuracy 98.65663109756098\n",
      "Training:: Epoch 164, Iteration 40, Current loss 0.03247717767953873 Accuracy 98.94919837743868\n",
      "Training:: Epoch 164, Iteration 50, Current loss 0.04955858737230301 Accuracy 98.68956479757225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 164, Iteration 60, Current loss 0.03866495564579964 Accuracy 98.60467987898994\n",
      "Training:: Epoch 164, Iteration 70, Current loss 0.11486944556236267 Accuracy 96.25740328474818\n",
      "Training:: Epoch 164, Iteration 80, Current loss 0.041856393218040466 Accuracy 98.80093761269383\n",
      "Training:: Epoch 164, Iteration 90, Current loss 0.19029632210731506 Accuracy 93.8009193937463\n",
      "Training:: Epoch 164, Iteration 100, Current loss 0.041643183678388596 Accuracy 98.64384111274575\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 164, Probability Accuracy 49.54385383436218\n",
      "Starting Training\n",
      "Training:: Epoch 165, Iteration 0, Current loss 0.029897287487983704 Accuracy 99.07572692292435\n",
      "Training:: Epoch 165, Iteration 10, Current loss 0.10923472791910172 Accuracy 97.04115607730066\n",
      "Training:: Epoch 165, Iteration 20, Current loss 0.0406673401594162 Accuracy 98.77053845861951\n",
      "Training:: Epoch 165, Iteration 30, Current loss 0.06275136023759842 Accuracy 97.89612491383703\n",
      "Training:: Epoch 165, Iteration 40, Current loss 0.042470596730709076 Accuracy 98.53890587835542\n",
      "Training:: Epoch 165, Iteration 50, Current loss 0.05931445211172104 Accuracy 97.97387809778968\n",
      "Training:: Epoch 165, Iteration 60, Current loss 0.030876701697707176 Accuracy 98.99950795473184\n",
      "Training:: Epoch 165, Iteration 70, Current loss 0.041229069232940674 Accuracy 98.62839422732876\n",
      "Training:: Epoch 165, Iteration 80, Current loss 0.17621727287769318 Accuracy 96.18720808311886\n",
      "Training:: Epoch 165, Iteration 90, Current loss 0.061491258442401886 Accuracy 97.87346686667254\n",
      "Training:: Epoch 165, Iteration 100, Current loss 0.057930588722229004 Accuracy 98.18730127511719\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 165, Probability Accuracy 46.230476032646976\n",
      "Starting Training\n",
      "Training:: Epoch 166, Iteration 0, Current loss 0.03739822283387184 Accuracy 98.73098814966875\n",
      "Training:: Epoch 166, Iteration 10, Current loss 0.03507727012038231 Accuracy 98.97697096338862\n",
      "Training:: Epoch 166, Iteration 20, Current loss 0.05884648114442825 Accuracy 98.08184143222506\n",
      "Training:: Epoch 166, Iteration 30, Current loss 0.03582358732819557 Accuracy 98.8607793840352\n",
      "Training:: Epoch 166, Iteration 40, Current loss 0.063860222697258 Accuracy 98.31306628516928\n",
      "Training:: Epoch 166, Iteration 50, Current loss 0.03603215515613556 Accuracy 98.97439708669317\n",
      "Training:: Epoch 166, Iteration 60, Current loss 0.05531147122383118 Accuracy 98.0485624906897\n",
      "Training:: Epoch 166, Iteration 70, Current loss 0.0518815815448761 Accuracy 98.38627168302098\n",
      "Training:: Epoch 166, Iteration 80, Current loss 0.05869609862565994 Accuracy 97.94918233633409\n",
      "Training:: Epoch 166, Iteration 90, Current loss 0.19765548408031464 Accuracy 95.58154235145386\n",
      "Training:: Epoch 166, Iteration 100, Current loss 0.05915055423974991 Accuracy 98.2406510366208\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 166, Probability Accuracy 46.03026473878278\n",
      "Starting Training\n",
      "Training:: Epoch 167, Iteration 0, Current loss 0.05947792902588844 Accuracy 98.07906114885732\n",
      "Training:: Epoch 167, Iteration 10, Current loss 0.04663766548037529 Accuracy 98.38182056091071\n",
      "Training:: Epoch 167, Iteration 20, Current loss 0.06752949208021164 Accuracy 97.84886070440486\n",
      "Training:: Epoch 167, Iteration 30, Current loss 0.04469434916973114 Accuracy 98.4461277077729\n",
      "Training:: Epoch 167, Iteration 40, Current loss 0.0488492026925087 Accuracy 98.47592213114754\n",
      "Training:: Epoch 167, Iteration 50, Current loss 0.059200190007686615 Accuracy 98.19149835206626\n",
      "Training:: Epoch 167, Iteration 60, Current loss 0.026887699961662292 Accuracy 99.22522637418696\n",
      "Training:: Epoch 167, Iteration 70, Current loss 0.042807530611753464 Accuracy 98.42502019204882\n",
      "Training:: Epoch 167, Iteration 80, Current loss 0.0387398861348629 Accuracy 98.55059280147972\n",
      "Training:: Epoch 167, Iteration 90, Current loss 0.057656869292259216 Accuracy 98.07899637361561\n",
      "Training:: Epoch 167, Iteration 100, Current loss 0.05359916761517525 Accuracy 98.03200294279934\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 167, Probability Accuracy 47.09615942329204\n",
      "Starting Training\n",
      "Training:: Epoch 168, Iteration 0, Current loss 0.04081115126609802 Accuracy 98.67912179449038\n",
      "Training:: Epoch 168, Iteration 10, Current loss 0.02549433335661888 Accuracy 99.18448930641615\n",
      "Training:: Epoch 168, Iteration 20, Current loss 0.03357188031077385 Accuracy 98.94289325519935\n",
      "Training:: Epoch 168, Iteration 30, Current loss 0.026080038398504257 Accuracy 99.21102344704967\n",
      "Training:: Epoch 168, Iteration 40, Current loss 0.029819218441843987 Accuracy 99.07720843960121\n",
      "Training:: Epoch 168, Iteration 50, Current loss 0.4294073283672333 Accuracy 91.53449946453159\n",
      "Training:: Epoch 168, Iteration 60, Current loss 0.04265449196100235 Accuracy 98.47869224227534\n",
      "Training:: Epoch 168, Iteration 70, Current loss 0.04666324704885483 Accuracy 98.51241177902166\n",
      "Training:: Epoch 168, Iteration 80, Current loss 0.07798632234334946 Accuracy 97.4484052532833\n",
      "Training:: Epoch 168, Iteration 90, Current loss 0.0340920090675354 Accuracy 98.98777367074211\n",
      "Training:: Epoch 168, Iteration 100, Current loss 0.05928457900881767 Accuracy 98.20926966292134\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 168, Probability Accuracy 48.97895347391971\n",
      "Starting Training\n",
      "Training:: Epoch 169, Iteration 0, Current loss 0.041208334267139435 Accuracy 98.60969387755102\n",
      "Training:: Epoch 169, Iteration 10, Current loss 0.05484984070062637 Accuracy 98.35941148995796\n",
      "Training:: Epoch 169, Iteration 20, Current loss 0.03603653982281685 Accuracy 98.85599566616743\n",
      "Training:: Epoch 169, Iteration 30, Current loss 0.054207004606723785 Accuracy 98.311553619581\n",
      "Training:: Epoch 169, Iteration 40, Current loss 0.03518487885594368 Accuracy 99.01281151281151\n",
      "Training:: Epoch 169, Iteration 50, Current loss 0.041712552309036255 Accuracy 98.6923430556265\n",
      "Training:: Epoch 169, Iteration 60, Current loss 0.028274642303586006 Accuracy 99.20534427804218\n",
      "Training:: Epoch 169, Iteration 70, Current loss 0.04646512120962143 Accuracy 98.34623335416566\n",
      "Training:: Epoch 169, Iteration 80, Current loss 0.034115128219127655 Accuracy 99.03518940952097\n",
      "Training:: Epoch 169, Iteration 90, Current loss 0.037523575127124786 Accuracy 98.90905839220245\n",
      "Training:: Epoch 169, Iteration 100, Current loss 0.03093782067298889 Accuracy 98.97603189705949\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 169, Probability Accuracy 48.989932468823795\n",
      "Starting Training\n",
      "Training:: Epoch 170, Iteration 0, Current loss 0.03515877574682236 Accuracy 98.91571553994733\n",
      "Training:: Epoch 170, Iteration 10, Current loss 0.052024729549884796 Accuracy 98.43832560324077\n",
      "Training:: Epoch 170, Iteration 20, Current loss 0.062316592782735825 Accuracy 97.7837191303736\n",
      "Training:: Epoch 170, Iteration 30, Current loss 0.07435324788093567 Accuracy 97.82530275702139\n",
      "Training:: Epoch 170, Iteration 40, Current loss 0.039330918341875076 Accuracy 98.846048798252\n",
      "Training:: Epoch 170, Iteration 50, Current loss 0.04697377607226372 Accuracy 98.55837563451777\n",
      "Training:: Epoch 170, Iteration 60, Current loss 0.041658516973257065 Accuracy 98.87885393958268\n",
      "Training:: Epoch 170, Iteration 70, Current loss 0.0279464703053236 Accuracy 99.03079289009692\n",
      "Training:: Epoch 170, Iteration 80, Current loss 0.037548791617155075 Accuracy 98.66879909685309\n",
      "Training:: Epoch 170, Iteration 90, Current loss 0.04869137704372406 Accuracy 98.42886467708827\n",
      "Training:: Epoch 170, Iteration 100, Current loss 0.0661841481924057 Accuracy 97.74466699234652\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 170, Probability Accuracy 49.736711273149105\n",
      "Starting Training\n",
      "Training:: Epoch 171, Iteration 0, Current loss 0.026418615132570267 Accuracy 99.09410320450408\n",
      "Training:: Epoch 171, Iteration 10, Current loss 0.026528552174568176 Accuracy 99.11439114391143\n",
      "Training:: Epoch 171, Iteration 20, Current loss 0.02908104844391346 Accuracy 99.13770078980502\n",
      "Training:: Epoch 171, Iteration 30, Current loss 0.03442716598510742 Accuracy 98.89914197830662\n",
      "Training:: Epoch 171, Iteration 40, Current loss 0.06679440289735794 Accuracy 97.77670037401302\n",
      "Training:: Epoch 171, Iteration 50, Current loss 0.04577600955963135 Accuracy 98.54539019642542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 171, Iteration 60, Current loss 0.03697332367300987 Accuracy 98.8589327665287\n",
      "Training:: Epoch 171, Iteration 70, Current loss 0.06023164838552475 Accuracy 98.10296521130049\n",
      "Training:: Epoch 171, Iteration 80, Current loss 0.03495950251817703 Accuracy 98.7784511141862\n",
      "Training:: Epoch 171, Iteration 90, Current loss 0.03876378387212753 Accuracy 98.78425935800362\n",
      "Training:: Epoch 171, Iteration 100, Current loss 0.04657066613435745 Accuracy 98.38061609643249\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 171, Probability Accuracy 48.305609644943445\n",
      "Starting Training\n",
      "Training:: Epoch 172, Iteration 0, Current loss 0.033902935683727264 Accuracy 98.772040302267\n",
      "Training:: Epoch 172, Iteration 10, Current loss 0.04448474198579788 Accuracy 98.46458127309634\n",
      "Training:: Epoch 172, Iteration 20, Current loss 0.036371972411870956 Accuracy 98.92036450079239\n",
      "Training:: Epoch 172, Iteration 30, Current loss 0.04622882604598999 Accuracy 98.68760253105226\n",
      "Training:: Epoch 172, Iteration 40, Current loss 0.04192265868186951 Accuracy 98.74574561956385\n",
      "Training:: Epoch 172, Iteration 50, Current loss 0.035804178565740585 Accuracy 98.89874857792947\n",
      "Training:: Epoch 172, Iteration 60, Current loss 0.041453029960393906 Accuracy 98.59709810704334\n",
      "Training:: Epoch 172, Iteration 70, Current loss 0.04846487194299698 Accuracy 98.4117525132138\n",
      "Training:: Epoch 172, Iteration 80, Current loss 0.025650862604379654 Accuracy 99.16305140707183\n",
      "Training:: Epoch 172, Iteration 90, Current loss 0.032238300889730453 Accuracy 98.99629701812512\n",
      "Training:: Epoch 172, Iteration 100, Current loss 0.028936687856912613 Accuracy 99.21041739628768\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 172, Probability Accuracy 47.88322906740689\n",
      "Starting Training\n",
      "Training:: Epoch 173, Iteration 0, Current loss 0.0444137267768383 Accuracy 98.40793866699642\n",
      "Training:: Epoch 173, Iteration 10, Current loss 0.09063877165317535 Accuracy 97.04778069228\n",
      "Training:: Epoch 173, Iteration 20, Current loss 0.049864303320646286 Accuracy 98.34530807750926\n",
      "Training:: Epoch 173, Iteration 30, Current loss 0.04261397942900658 Accuracy 98.78445843281962\n",
      "Training:: Epoch 173, Iteration 40, Current loss 0.06054889038205147 Accuracy 98.23518053678316\n",
      "Training:: Epoch 173, Iteration 50, Current loss 0.04471036046743393 Accuracy 98.41662737702342\n",
      "Training:: Epoch 173, Iteration 60, Current loss 0.07408536970615387 Accuracy 97.7079059491873\n",
      "Training:: Epoch 173, Iteration 70, Current loss 0.0466168150305748 Accuracy 98.80303620085637\n",
      "Training:: Epoch 173, Iteration 80, Current loss 0.09386275708675385 Accuracy 97.33439148499136\n",
      "Training:: Epoch 173, Iteration 90, Current loss 0.03730173781514168 Accuracy 98.77753895754971\n",
      "Training:: Epoch 173, Iteration 100, Current loss 0.04253983125090599 Accuracy 98.56447688564477\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 173, Probability Accuracy 46.35248788167544\n",
      "Starting Training\n",
      "Training:: Epoch 174, Iteration 0, Current loss 0.030393105000257492 Accuracy 99.04668398049027\n",
      "Training:: Epoch 174, Iteration 10, Current loss 0.04349605739116669 Accuracy 98.70360955770208\n",
      "Training:: Epoch 174, Iteration 20, Current loss 0.13272112607955933 Accuracy 95.90961682433812\n",
      "Training:: Epoch 174, Iteration 30, Current loss 0.046305231750011444 Accuracy 98.67696563242086\n",
      "Training:: Epoch 174, Iteration 40, Current loss 0.12572477757930756 Accuracy 96.00102933607823\n",
      "Training:: Epoch 174, Iteration 50, Current loss 0.0826675221323967 Accuracy 97.3269278147327\n",
      "Training:: Epoch 174, Iteration 60, Current loss 0.06972894817590714 Accuracy 97.75013108346441\n",
      "Training:: Epoch 174, Iteration 70, Current loss 0.041215118020772934 Accuracy 98.81723575876624\n",
      "Training:: Epoch 174, Iteration 80, Current loss 0.04912537708878517 Accuracy 98.44943165257942\n",
      "Training:: Epoch 174, Iteration 90, Current loss 0.036698102951049805 Accuracy 98.80597014925372\n",
      "Training:: Epoch 174, Iteration 100, Current loss 0.03786106035113335 Accuracy 98.82683410442829\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 174, Probability Accuracy 48.21404897046029\n",
      "Starting Training\n",
      "Training:: Epoch 175, Iteration 0, Current loss 0.05820513889193535 Accuracy 98.08699487292574\n",
      "Training:: Epoch 175, Iteration 10, Current loss 0.07120220363140106 Accuracy 97.74615048331471\n",
      "Training:: Epoch 175, Iteration 20, Current loss 0.05756424367427826 Accuracy 98.18233726684431\n",
      "Training:: Epoch 175, Iteration 30, Current loss 0.07695665210485458 Accuracy 97.42038854396155\n",
      "Training:: Epoch 175, Iteration 40, Current loss 0.08068735152482986 Accuracy 97.23858133752127\n",
      "Training:: Epoch 175, Iteration 50, Current loss 0.05593501031398773 Accuracy 98.30629664387541\n",
      "Training:: Epoch 175, Iteration 60, Current loss 0.08418931812047958 Accuracy 97.52051562004719\n",
      "Training:: Epoch 175, Iteration 70, Current loss 0.03584448993206024 Accuracy 98.87354004861564\n",
      "Training:: Epoch 175, Iteration 80, Current loss 0.05728817358613014 Accuracy 98.08888016231428\n",
      "Training:: Epoch 175, Iteration 90, Current loss 0.04582764953374863 Accuracy 98.84368022926455\n",
      "Training:: Epoch 175, Iteration 100, Current loss 0.0434906892478466 Accuracy 98.59029968454259\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 175, Probability Accuracy 48.16619712474624\n",
      "Starting Training\n",
      "Training:: Epoch 176, Iteration 0, Current loss 0.02343163825571537 Accuracy 99.23897009521255\n",
      "Training:: Epoch 176, Iteration 10, Current loss 0.025156233459711075 Accuracy 99.32525726195128\n",
      "Training:: Epoch 176, Iteration 20, Current loss 0.03852878138422966 Accuracy 98.91354958870092\n",
      "Training:: Epoch 176, Iteration 30, Current loss 0.10529837012290955 Accuracy 96.17556760034871\n",
      "Training:: Epoch 176, Iteration 40, Current loss 0.027892667800188065 Accuracy 99.08908813429989\n",
      "Training:: Epoch 176, Iteration 50, Current loss 0.03054380603134632 Accuracy 99.1634668129457\n",
      "Training:: Epoch 176, Iteration 60, Current loss 0.09475795179605484 Accuracy 96.68823559382483\n",
      "Training:: Epoch 176, Iteration 70, Current loss 0.18156732618808746 Accuracy 94.36253685841372\n",
      "Training:: Epoch 176, Iteration 80, Current loss 0.0610014833509922 Accuracy 97.77107509841011\n",
      "Training:: Epoch 176, Iteration 90, Current loss 0.06146690621972084 Accuracy 97.69684313966343\n",
      "Training:: Epoch 176, Iteration 100, Current loss 0.07527121901512146 Accuracy 97.57356054191364\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 176, Probability Accuracy 45.93963624311223\n",
      "Starting Training\n",
      "Training:: Epoch 177, Iteration 0, Current loss 0.06625643372535706 Accuracy 97.9012050115713\n",
      "Training:: Epoch 177, Iteration 10, Current loss 0.18029026687145233 Accuracy 96.02495658326366\n",
      "Training:: Epoch 177, Iteration 20, Current loss 0.044435590505599976 Accuracy 98.73668955957406\n",
      "Training:: Epoch 177, Iteration 30, Current loss 0.04076085239648819 Accuracy 98.70578669196368\n",
      "Training:: Epoch 177, Iteration 40, Current loss 0.03775699809193611 Accuracy 98.78835174764858\n",
      "Training:: Epoch 177, Iteration 50, Current loss 0.04700164869427681 Accuracy 98.49264705882354\n",
      "Training:: Epoch 177, Iteration 60, Current loss 0.04829294979572296 Accuracy 98.34931965201874\n",
      "Training:: Epoch 177, Iteration 70, Current loss 0.04473042115569115 Accuracy 98.720822893314\n",
      "Training:: Epoch 177, Iteration 80, Current loss 0.04725682735443115 Accuracy 98.61353654019156\n",
      "Training:: Epoch 177, Iteration 90, Current loss 0.07251755893230438 Accuracy 97.58306397600536\n",
      "Training:: Epoch 177, Iteration 100, Current loss 0.05316348001360893 Accuracy 98.21614492954814\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 177, Probability Accuracy 47.15115797323611\n",
      "Starting Training\n",
      "Training:: Epoch 178, Iteration 0, Current loss 0.02897012047469616 Accuracy 99.15148325988292\n",
      "Training:: Epoch 178, Iteration 10, Current loss 0.11317573487758636 Accuracy 96.11792891018432\n",
      "Training:: Epoch 178, Iteration 20, Current loss 0.03683391213417053 Accuracy 98.91957696111999\n",
      "Training:: Epoch 178, Iteration 30, Current loss 0.07746929675340652 Accuracy 97.70082154267457\n",
      "Training:: Epoch 178, Iteration 40, Current loss 0.04825930297374725 Accuracy 98.43986113947315\n",
      "Training:: Epoch 178, Iteration 50, Current loss 0.04689785838127136 Accuracy 98.7301881546019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 178, Iteration 60, Current loss 0.03791232407093048 Accuracy 98.88014527845036\n",
      "Training:: Epoch 178, Iteration 70, Current loss 0.07742249220609665 Accuracy 97.43710322125558\n",
      "Training:: Epoch 178, Iteration 80, Current loss 0.10267352312803268 Accuracy 96.9881678020796\n",
      "Training:: Epoch 178, Iteration 90, Current loss 0.06746827065944672 Accuracy 97.43195152447821\n",
      "Training:: Epoch 178, Iteration 100, Current loss 0.034266069531440735 Accuracy 98.93852065321806\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 178, Probability Accuracy 46.66683929237271\n",
      "Starting Training\n",
      "Training:: Epoch 179, Iteration 0, Current loss 0.09811635315418243 Accuracy 96.66114887472264\n",
      "Training:: Epoch 179, Iteration 10, Current loss 0.13606330752372742 Accuracy 95.04231723332848\n",
      "Training:: Epoch 179, Iteration 20, Current loss 0.13771958649158478 Accuracy 95.39579548403842\n",
      "Training:: Epoch 179, Iteration 30, Current loss 0.09092888981103897 Accuracy 96.96969696969697\n",
      "Training:: Epoch 179, Iteration 40, Current loss 0.0614924393594265 Accuracy 98.00995024875621\n",
      "Training:: Epoch 179, Iteration 50, Current loss 0.05395334213972092 Accuracy 98.1923474663909\n",
      "Training:: Epoch 179, Iteration 60, Current loss 0.07210605591535568 Accuracy 97.39360423178648\n",
      "Training:: Epoch 179, Iteration 70, Current loss 0.09388434886932373 Accuracy 96.8860614177748\n",
      "Training:: Epoch 179, Iteration 80, Current loss 0.08449242264032364 Accuracy 96.69140816469435\n",
      "Training:: Epoch 179, Iteration 90, Current loss 0.06008993461728096 Accuracy 98.04742684157416\n",
      "Training:: Epoch 179, Iteration 100, Current loss 0.06359776109457016 Accuracy 98.06643721721645\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 179, Probability Accuracy 46.382317603679\n",
      "Starting Training\n",
      "Training:: Epoch 180, Iteration 0, Current loss 0.0537688285112381 Accuracy 98.28803657607315\n",
      "Training:: Epoch 180, Iteration 10, Current loss 0.06454063206911087 Accuracy 98.17295327903986\n",
      "Training:: Epoch 180, Iteration 20, Current loss 0.07433512806892395 Accuracy 97.33810928501194\n",
      "Training:: Epoch 180, Iteration 30, Current loss 0.06092331185936928 Accuracy 97.98970427986322\n",
      "Training:: Epoch 180, Iteration 40, Current loss 0.030031172558665276 Accuracy 99.02186912694344\n",
      "Training:: Epoch 180, Iteration 50, Current loss 0.041650302708148956 Accuracy 98.8398966782979\n",
      "Training:: Epoch 180, Iteration 60, Current loss 0.07236821204423904 Accuracy 97.49534415263578\n",
      "Training:: Epoch 180, Iteration 70, Current loss 0.04630319029092789 Accuracy 98.69503546099291\n",
      "Training:: Epoch 180, Iteration 80, Current loss 0.03253863379359245 Accuracy 98.97710667316123\n",
      "Training:: Epoch 180, Iteration 90, Current loss 0.03907123580574989 Accuracy 98.65582994685839\n",
      "Training:: Epoch 180, Iteration 100, Current loss 0.03391585871577263 Accuracy 98.89406917488887\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 180, Probability Accuracy 48.53989725317977\n",
      "Starting Training\n",
      "Training:: Epoch 181, Iteration 0, Current loss 0.03672647848725319 Accuracy 98.69530487263691\n",
      "Training:: Epoch 181, Iteration 10, Current loss 0.02785416878759861 Accuracy 99.05389637011574\n",
      "Training:: Epoch 181, Iteration 20, Current loss 0.04619767144322395 Accuracy 98.64315479215368\n",
      "Training:: Epoch 181, Iteration 30, Current loss 0.026170270517468452 Accuracy 98.988019076422\n",
      "Training:: Epoch 181, Iteration 40, Current loss 0.027192646637558937 Accuracy 99.13926603522167\n",
      "Training:: Epoch 181, Iteration 50, Current loss 0.06070571765303612 Accuracy 97.99003142268936\n",
      "Training:: Epoch 181, Iteration 60, Current loss 0.03645163029432297 Accuracy 98.88160914743563\n",
      "Training:: Epoch 181, Iteration 70, Current loss 0.04359596222639084 Accuracy 98.68985116323877\n",
      "Training:: Epoch 181, Iteration 80, Current loss 0.033427346497774124 Accuracy 99.13486899947249\n",
      "Training:: Epoch 181, Iteration 90, Current loss 0.02724781259894371 Accuracy 99.10770390850823\n",
      "Training:: Epoch 181, Iteration 100, Current loss 0.039942916482686996 Accuracy 98.94622272794736\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 181, Probability Accuracy 47.03101048183287\n",
      "Starting Training\n",
      "Training:: Epoch 182, Iteration 0, Current loss 0.023299971595406532 Accuracy 99.28101709776413\n",
      "Training:: Epoch 182, Iteration 10, Current loss 0.033943187445402145 Accuracy 99.12331592857603\n",
      "Training:: Epoch 182, Iteration 20, Current loss 0.10050899535417557 Accuracy 96.87229749207265\n",
      "Training:: Epoch 182, Iteration 30, Current loss 0.0413694866001606 Accuracy 98.50084884758151\n",
      "Training:: Epoch 182, Iteration 40, Current loss 0.03542414680123329 Accuracy 98.82388648607633\n",
      "Training:: Epoch 182, Iteration 50, Current loss 0.033987920731306076 Accuracy 98.8882863340564\n",
      "Training:: Epoch 182, Iteration 60, Current loss 0.022856498137116432 Accuracy 99.29781712715616\n",
      "Training:: Epoch 182, Iteration 70, Current loss 0.031416065990924835 Accuracy 98.98819561551433\n",
      "Training:: Epoch 182, Iteration 80, Current loss 0.02576810121536255 Accuracy 99.15423790469262\n",
      "Training:: Epoch 182, Iteration 90, Current loss 0.025790665298700333 Accuracy 99.16798067568021\n",
      "Training:: Epoch 182, Iteration 100, Current loss 0.042015448212623596 Accuracy 98.84731039091213\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 182, Probability Accuracy 48.787649666487134\n",
      "Starting Training\n",
      "Training:: Epoch 183, Iteration 0, Current loss 0.04415831342339516 Accuracy 98.52631578947368\n",
      "Training:: Epoch 183, Iteration 10, Current loss 0.028733059763908386 Accuracy 99.10849294354838\n",
      "Training:: Epoch 183, Iteration 20, Current loss 0.02786940522491932 Accuracy 99.00212712712712\n",
      "Training:: Epoch 183, Iteration 30, Current loss 0.03492887318134308 Accuracy 98.71982032565974\n",
      "Training:: Epoch 183, Iteration 40, Current loss 0.028306830674409866 Accuracy 99.0417635980946\n",
      "Training:: Epoch 183, Iteration 50, Current loss 0.028999123722314835 Accuracy 99.23578005022017\n",
      "Training:: Epoch 183, Iteration 60, Current loss 0.02831355296075344 Accuracy 99.15345468780583\n",
      "Training:: Epoch 183, Iteration 70, Current loss 0.03612860292196274 Accuracy 98.67795430824539\n",
      "Training:: Epoch 183, Iteration 80, Current loss 0.08968652784824371 Accuracy 97.08881883923604\n",
      "Training:: Epoch 183, Iteration 90, Current loss 0.03161478415131569 Accuracy 98.92340010503413\n",
      "Training:: Epoch 183, Iteration 100, Current loss 0.035720013082027435 Accuracy 98.98010960016236\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 183, Probability Accuracy 46.743070804159586\n",
      "Starting Training\n",
      "Training:: Epoch 184, Iteration 0, Current loss 0.043595604598522186 Accuracy 98.81298785275507\n",
      "Training:: Epoch 184, Iteration 10, Current loss 0.03896235302090645 Accuracy 98.68195885672674\n",
      "Training:: Epoch 184, Iteration 20, Current loss 0.030163923278450966 Accuracy 99.05723905723906\n",
      "Training:: Epoch 184, Iteration 30, Current loss 0.026758883148431778 Accuracy 99.11032028469751\n",
      "Training:: Epoch 184, Iteration 40, Current loss 0.05397528409957886 Accuracy 98.37996096291477\n",
      "Training:: Epoch 184, Iteration 50, Current loss 0.027619637548923492 Accuracy 99.18988982501621\n",
      "Training:: Epoch 184, Iteration 60, Current loss 0.023932967334985733 Accuracy 99.21816664470505\n",
      "Training:: Epoch 184, Iteration 70, Current loss 0.030306704342365265 Accuracy 98.90207339217093\n",
      "Training:: Epoch 184, Iteration 80, Current loss 0.047242846339941025 Accuracy 98.67025550377632\n",
      "Training:: Epoch 184, Iteration 90, Current loss 0.035174813121557236 Accuracy 98.84526558891454\n",
      "Training:: Epoch 184, Iteration 100, Current loss 0.04547460004687309 Accuracy 98.48534522326405\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 184, Probability Accuracy 46.543895264531635\n",
      "Starting Training\n",
      "Training:: Epoch 185, Iteration 0, Current loss 0.0760079175233841 Accuracy 97.93392504930966\n",
      "Training:: Epoch 185, Iteration 10, Current loss 0.07496574521064758 Accuracy 97.90955048503893\n",
      "Training:: Epoch 185, Iteration 20, Current loss 0.0338020846247673 Accuracy 98.78843005275606\n",
      "Training:: Epoch 185, Iteration 30, Current loss 0.045610081404447556 Accuracy 98.35861994492738\n",
      "Training:: Epoch 185, Iteration 40, Current loss 0.04441896080970764 Accuracy 98.60028091248124\n",
      "Training:: Epoch 185, Iteration 50, Current loss 0.026394974440336227 Accuracy 99.24005741788399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 185, Iteration 60, Current loss 0.022318359464406967 Accuracy 99.28283429175755\n",
      "Training:: Epoch 185, Iteration 70, Current loss 0.08724498003721237 Accuracy 97.63490481934029\n",
      "Training:: Epoch 185, Iteration 80, Current loss 0.030987918376922607 Accuracy 99.00984979705798\n",
      "Training:: Epoch 185, Iteration 90, Current loss 0.041406385600566864 Accuracy 98.71112356402354\n",
      "Training:: Epoch 185, Iteration 100, Current loss 0.02338998392224312 Accuracy 99.24690520055593\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 185, Probability Accuracy 48.75854497244894\n",
      "Starting Training\n",
      "Training:: Epoch 186, Iteration 0, Current loss 0.02967178262770176 Accuracy 99.02938489646772\n",
      "Training:: Epoch 186, Iteration 10, Current loss 0.026836790144443512 Accuracy 99.14529914529915\n",
      "Training:: Epoch 186, Iteration 20, Current loss 0.02714988775551319 Accuracy 99.09544603867748\n",
      "Training:: Epoch 186, Iteration 30, Current loss 0.02361690066754818 Accuracy 99.27945693807122\n",
      "Training:: Epoch 186, Iteration 40, Current loss 0.026075443252921104 Accuracy 99.04055655364856\n",
      "Training:: Epoch 186, Iteration 50, Current loss 0.04273633658885956 Accuracy 98.64026179192057\n",
      "Training:: Epoch 186, Iteration 60, Current loss 0.03457490727305412 Accuracy 98.9275836766947\n",
      "Training:: Epoch 186, Iteration 70, Current loss 0.04112125188112259 Accuracy 98.83299658607956\n",
      "Training:: Epoch 186, Iteration 80, Current loss 0.025206798687577248 Accuracy 99.20051379722727\n",
      "Training:: Epoch 186, Iteration 90, Current loss 0.08876252919435501 Accuracy 97.42018303035323\n",
      "Training:: Epoch 186, Iteration 100, Current loss 0.08085977286100388 Accuracy 97.75842522931052\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 186, Probability Accuracy 44.71941417740398\n",
      "Starting Training\n",
      "Training:: Epoch 187, Iteration 0, Current loss 0.13947255909442902 Accuracy 95.45592982883596\n",
      "Training:: Epoch 187, Iteration 10, Current loss 0.07903330773115158 Accuracy 97.64017749092376\n",
      "Training:: Epoch 187, Iteration 20, Current loss 0.11832299828529358 Accuracy 96.06938137422765\n",
      "Training:: Epoch 187, Iteration 30, Current loss 0.12035617232322693 Accuracy 96.07811294709919\n",
      "Training:: Epoch 187, Iteration 40, Current loss 0.10939007997512817 Accuracy 96.41831155592624\n",
      "Training:: Epoch 187, Iteration 50, Current loss 0.07738473266363144 Accuracy 97.35924476951863\n",
      "Training:: Epoch 187, Iteration 60, Current loss 0.05498890206217766 Accuracy 98.20463933905306\n",
      "Training:: Epoch 187, Iteration 70, Current loss 0.11917651444673538 Accuracy 95.85367288125624\n",
      "Training:: Epoch 187, Iteration 80, Current loss 0.5933151841163635 Accuracy 90.83309124947549\n",
      "Training:: Epoch 187, Iteration 90, Current loss 0.19590668380260468 Accuracy 94.91211023761151\n",
      "Training:: Epoch 187, Iteration 100, Current loss 0.09712320566177368 Accuracy 96.62406769311292\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 187, Probability Accuracy 47.4863280440817\n",
      "Starting Training\n",
      "Training:: Epoch 188, Iteration 0, Current loss 0.0626107007265091 Accuracy 97.73983540105638\n",
      "Training:: Epoch 188, Iteration 10, Current loss 0.04846585914492607 Accuracy 98.54052645295803\n",
      "Training:: Epoch 188, Iteration 20, Current loss 0.07188425213098526 Accuracy 97.92509398943511\n",
      "Training:: Epoch 188, Iteration 30, Current loss 0.18922412395477295 Accuracy 95.05020119545259\n",
      "Training:: Epoch 188, Iteration 40, Current loss 0.09390366077423096 Accuracy 96.54432989690721\n",
      "Training:: Epoch 188, Iteration 50, Current loss 0.032539524137973785 Accuracy 99.02383429958873\n",
      "Training:: Epoch 188, Iteration 60, Current loss 0.0732259526848793 Accuracy 97.92099792099792\n",
      "Training:: Epoch 188, Iteration 70, Current loss 0.12375346571207047 Accuracy 95.82517395108538\n",
      "Training:: Epoch 188, Iteration 80, Current loss 0.038673918694257736 Accuracy 99.0344498748361\n",
      "Training:: Epoch 188, Iteration 90, Current loss 0.0338820181787014 Accuracy 98.82437908758978\n",
      "Training:: Epoch 188, Iteration 100, Current loss 0.0419149324297905 Accuracy 98.51619711553836\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 188, Probability Accuracy 49.78497742055765\n",
      "Starting Training\n",
      "Training:: Epoch 189, Iteration 0, Current loss 0.03163571655750275 Accuracy 98.94355215967566\n",
      "Training:: Epoch 189, Iteration 10, Current loss 0.04014875739812851 Accuracy 98.72180132898802\n",
      "Training:: Epoch 189, Iteration 20, Current loss 0.13399919867515564 Accuracy 95.66544899405703\n",
      "Training:: Epoch 189, Iteration 30, Current loss 0.03411451727151871 Accuracy 99.07672015827654\n",
      "Training:: Epoch 189, Iteration 40, Current loss 0.047298550605773926 Accuracy 98.49698571310596\n",
      "Training:: Epoch 189, Iteration 50, Current loss 0.07034242153167725 Accuracy 97.17692621280065\n",
      "Training:: Epoch 189, Iteration 60, Current loss 0.040210381150245667 Accuracy 98.67923836078805\n",
      "Training:: Epoch 189, Iteration 70, Current loss 0.025756247341632843 Accuracy 99.11838790931989\n",
      "Training:: Epoch 189, Iteration 80, Current loss 0.04312879592180252 Accuracy 98.5160093555932\n",
      "Training:: Epoch 189, Iteration 90, Current loss 0.08131665736436844 Accuracy 97.59791384584524\n",
      "Training:: Epoch 189, Iteration 100, Current loss 0.04930343106389046 Accuracy 98.27002053388091\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 189, Probability Accuracy 48.20710941707752\n",
      "Starting Training\n",
      "Training:: Epoch 190, Iteration 0, Current loss 0.05064397677779198 Accuracy 98.43494788109167\n",
      "Training:: Epoch 190, Iteration 10, Current loss 0.07262849062681198 Accuracy 98.03374349866802\n",
      "Training:: Epoch 190, Iteration 20, Current loss 0.03407171368598938 Accuracy 98.97309321388305\n",
      "Training:: Epoch 190, Iteration 30, Current loss 0.07540181279182434 Accuracy 97.72402941929818\n",
      "Training:: Epoch 190, Iteration 40, Current loss 0.031479362398386 Accuracy 98.99885801294252\n",
      "Training:: Epoch 190, Iteration 50, Current loss 0.03019188530743122 Accuracy 99.0806969817781\n",
      "Training:: Epoch 190, Iteration 60, Current loss 0.046879079192876816 Accuracy 98.76661248685343\n",
      "Training:: Epoch 190, Iteration 70, Current loss 0.027686333283782005 Accuracy 99.13840059420649\n",
      "Training:: Epoch 190, Iteration 80, Current loss 0.03695327416062355 Accuracy 98.83121500311084\n",
      "Training:: Epoch 190, Iteration 90, Current loss 0.025080781430006027 Accuracy 99.2313067784766\n",
      "Training:: Epoch 190, Iteration 100, Current loss 0.04557432979345322 Accuracy 98.5552612569227\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 190, Probability Accuracy 45.7304138873928\n",
      "Starting Training\n",
      "Training:: Epoch 191, Iteration 0, Current loss 0.06420569121837616 Accuracy 98.15238718116416\n",
      "Training:: Epoch 191, Iteration 10, Current loss 0.1011538878083229 Accuracy 97.26107829931269\n",
      "Training:: Epoch 191, Iteration 20, Current loss 0.0738036036491394 Accuracy 97.81779528543738\n",
      "Training:: Epoch 191, Iteration 30, Current loss 0.06190364807844162 Accuracy 98.16934298152046\n",
      "Training:: Epoch 191, Iteration 40, Current loss 0.04982496425509453 Accuracy 98.59186844490483\n",
      "Training:: Epoch 191, Iteration 50, Current loss 0.05996564403176308 Accuracy 98.2184415375973\n",
      "Training:: Epoch 191, Iteration 60, Current loss 0.05346912145614624 Accuracy 98.12752756079996\n",
      "Training:: Epoch 191, Iteration 70, Current loss 0.08583541214466095 Accuracy 97.22135335210093\n",
      "Training:: Epoch 191, Iteration 80, Current loss 0.07563258707523346 Accuracy 97.57589749509408\n",
      "Training:: Epoch 191, Iteration 90, Current loss 0.04200437292456627 Accuracy 98.51261047639578\n",
      "Training:: Epoch 191, Iteration 100, Current loss 0.04633196443319321 Accuracy 98.49939098660171\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 191, Probability Accuracy 47.92610929278701\n",
      "Starting Training\n",
      "Training:: Epoch 192, Iteration 0, Current loss 0.06691338866949081 Accuracy 98.46294522337368\n",
      "Training:: Epoch 192, Iteration 10, Current loss 0.134170264005661 Accuracy 96.38437741686002\n",
      "Training:: Epoch 192, Iteration 20, Current loss 0.18711243569850922 Accuracy 96.03119981647167\n",
      "Training:: Epoch 192, Iteration 30, Current loss 0.036268312484025955 Accuracy 98.83258499037845\n",
      "Training:: Epoch 192, Iteration 40, Current loss 0.052448391914367676 Accuracy 98.00367421922842\n",
      "Training:: Epoch 192, Iteration 50, Current loss 0.03272104263305664 Accuracy 98.86724241815625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 192, Iteration 60, Current loss 0.04488615691661835 Accuracy 98.48211058908565\n",
      "Training:: Epoch 192, Iteration 70, Current loss 0.03890063613653183 Accuracy 98.8663120258132\n",
      "Training:: Epoch 192, Iteration 80, Current loss 0.10461780428886414 Accuracy 96.29035134653112\n",
      "Training:: Epoch 192, Iteration 90, Current loss 0.027505729347467422 Accuracy 99.18232522878648\n",
      "Training:: Epoch 192, Iteration 100, Current loss 0.04352188482880592 Accuracy 98.52490141667884\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 192, Probability Accuracy 47.31056055019265\n",
      "Starting Training\n",
      "Training:: Epoch 193, Iteration 0, Current loss 0.052245933562517166 Accuracy 98.49950127932695\n",
      "Training:: Epoch 193, Iteration 10, Current loss 0.13922062516212463 Accuracy 95.47169811320755\n",
      "Training:: Epoch 193, Iteration 20, Current loss 0.07415567338466644 Accuracy 97.54118119726797\n",
      "Training:: Epoch 193, Iteration 30, Current loss 0.2506941556930542 Accuracy 92.59627329192547\n",
      "Training:: Epoch 193, Iteration 40, Current loss 0.07468130439519882 Accuracy 97.73034988954608\n",
      "Training:: Epoch 193, Iteration 50, Current loss 0.11976907402276993 Accuracy 96.03036736794698\n",
      "Training:: Epoch 193, Iteration 60, Current loss 0.10054183751344681 Accuracy 96.2972388344573\n",
      "Training:: Epoch 193, Iteration 70, Current loss 0.09749232977628708 Accuracy 97.2244383791728\n",
      "Training:: Epoch 193, Iteration 80, Current loss 0.10570772737264633 Accuracy 96.47735828761967\n",
      "Training:: Epoch 193, Iteration 90, Current loss 0.1272674798965454 Accuracy 96.58559771845044\n",
      "Training:: Epoch 193, Iteration 100, Current loss 0.05467800796031952 Accuracy 98.04203010562732\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 193, Probability Accuracy 48.41353523635912\n",
      "Starting Training\n",
      "Training:: Epoch 194, Iteration 0, Current loss 0.06301558762788773 Accuracy 97.92569659442725\n",
      "Training:: Epoch 194, Iteration 10, Current loss 0.03138779476284981 Accuracy 98.98858844690827\n",
      "Training:: Epoch 194, Iteration 20, Current loss 0.04207717999815941 Accuracy 98.61615380036902\n",
      "Training:: Epoch 194, Iteration 30, Current loss 0.03835237771272659 Accuracy 98.84794679176935\n",
      "Training:: Epoch 194, Iteration 40, Current loss 0.030223384499549866 Accuracy 98.95491276662574\n",
      "Training:: Epoch 194, Iteration 50, Current loss 0.2899344563484192 Accuracy 92.91621839698763\n",
      "Training:: Epoch 194, Iteration 60, Current loss 0.12214678525924683 Accuracy 95.95811250877978\n",
      "Training:: Epoch 194, Iteration 70, Current loss 0.03884488716721535 Accuracy 98.86986684569766\n",
      "Training:: Epoch 194, Iteration 80, Current loss 0.023122282698750496 Accuracy 99.2650675617025\n",
      "Training:: Epoch 194, Iteration 90, Current loss 0.030300242826342583 Accuracy 99.0025836688097\n",
      "Training:: Epoch 194, Iteration 100, Current loss 0.0631503015756607 Accuracy 97.75209107806691\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 194, Probability Accuracy 49.29278700749886\n",
      "Starting Training\n",
      "Training:: Epoch 195, Iteration 0, Current loss 0.05652627721428871 Accuracy 98.8358700123406\n",
      "Training:: Epoch 195, Iteration 10, Current loss 0.035407654941082 Accuracy 98.7838136655119\n",
      "Training:: Epoch 195, Iteration 20, Current loss 0.05388442426919937 Accuracy 97.97502066305447\n",
      "Training:: Epoch 195, Iteration 30, Current loss 0.06018956005573273 Accuracy 97.84782240298254\n",
      "Training:: Epoch 195, Iteration 40, Current loss 0.034472230821847916 Accuracy 98.91394823417495\n",
      "Training:: Epoch 195, Iteration 50, Current loss 0.04530671611428261 Accuracy 98.60154033238751\n",
      "Training:: Epoch 195, Iteration 60, Current loss 0.0266879890114069 Accuracy 99.19884053303274\n",
      "Training:: Epoch 195, Iteration 70, Current loss 0.02944977395236492 Accuracy 98.96489648964896\n",
      "Training:: Epoch 195, Iteration 80, Current loss 0.02838768996298313 Accuracy 99.14927499790461\n",
      "Training:: Epoch 195, Iteration 90, Current loss 0.0434105321764946 Accuracy 98.49706172967913\n",
      "Training:: Epoch 195, Iteration 100, Current loss 0.030128616839647293 Accuracy 98.9687069474513\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 195, Probability Accuracy 49.31805941086299\n",
      "Starting Training\n",
      "Training:: Epoch 196, Iteration 0, Current loss 0.01964372955262661 Accuracy 99.39129436085908\n",
      "Training:: Epoch 196, Iteration 10, Current loss 0.04846341162919998 Accuracy 98.45228253880775\n",
      "Training:: Epoch 196, Iteration 20, Current loss 0.034328557550907135 Accuracy 99.12574359962429\n",
      "Training:: Epoch 196, Iteration 30, Current loss 0.07327353954315186 Accuracy 97.48366803774498\n",
      "Training:: Epoch 196, Iteration 40, Current loss 0.02819395810365677 Accuracy 99.1046957792801\n",
      "Training:: Epoch 196, Iteration 50, Current loss 0.04273609444499016 Accuracy 98.71912168344008\n",
      "Training:: Epoch 196, Iteration 60, Current loss 0.025359513238072395 Accuracy 99.1913912375096\n",
      "Training:: Epoch 196, Iteration 70, Current loss 0.043947551399469376 Accuracy 98.56945664449742\n",
      "Training:: Epoch 196, Iteration 80, Current loss 0.03239428251981735 Accuracy 98.97387173396675\n",
      "Training:: Epoch 196, Iteration 90, Current loss 0.040050674229860306 Accuracy 98.90269151138716\n",
      "Training:: Epoch 196, Iteration 100, Current loss 0.025192439556121826 Accuracy 99.27246790299571\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 196, Probability Accuracy 49.298587231221774\n",
      "Starting Training\n",
      "Training:: Epoch 197, Iteration 0, Current loss 0.03537767380475998 Accuracy 98.89937772509842\n",
      "Training:: Epoch 197, Iteration 10, Current loss 0.0263657309114933 Accuracy 99.34959349593495\n",
      "Training:: Epoch 197, Iteration 20, Current loss 0.023606907576322556 Accuracy 99.29976954440701\n",
      "Training:: Epoch 197, Iteration 30, Current loss 0.235762357711792 Accuracy 94.63318138814827\n",
      "Training:: Epoch 197, Iteration 40, Current loss 0.032572679221630096 Accuracy 99.06295149638802\n",
      "Training:: Epoch 197, Iteration 50, Current loss 0.053950291126966476 Accuracy 98.14082278481013\n",
      "Training:: Epoch 197, Iteration 60, Current loss 0.026922347024083138 Accuracy 99.25059404130872\n",
      "Training:: Epoch 197, Iteration 70, Current loss 0.02487007901072502 Accuracy 99.27198890649763\n",
      "Training:: Epoch 197, Iteration 80, Current loss 0.02235335297882557 Accuracy 99.34301188988238\n",
      "Training:: Epoch 197, Iteration 90, Current loss 0.03339100629091263 Accuracy 98.83367809515427\n",
      "Training:: Epoch 197, Iteration 100, Current loss 0.02822260931134224 Accuracy 98.957065878672\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 197, Probability Accuracy 49.99016033475577\n",
      "Starting Training\n",
      "Training:: Epoch 198, Iteration 0, Current loss 0.018021389842033386 Accuracy 99.41884467591858\n",
      "Training:: Epoch 198, Iteration 10, Current loss 0.041406724601984024 Accuracy 98.64884929472903\n",
      "Training:: Epoch 198, Iteration 20, Current loss 0.02277972921729088 Accuracy 99.40925760729507\n",
      "Training:: Epoch 198, Iteration 30, Current loss 0.06980528682470322 Accuracy 98.02327262214513\n",
      "Training:: Epoch 198, Iteration 40, Current loss 0.058036696165800095 Accuracy 98.20743339546102\n",
      "Training:: Epoch 198, Iteration 50, Current loss 0.05305188149213791 Accuracy 98.11740290542544\n",
      "Training:: Epoch 198, Iteration 60, Current loss 0.026690591126680374 Accuracy 99.03047641753089\n",
      "Training:: Epoch 198, Iteration 70, Current loss 0.0162114929407835 Accuracy 99.45088665654575\n",
      "Training:: Epoch 198, Iteration 80, Current loss 0.02881666272878647 Accuracy 99.12663755458516\n",
      "Training:: Epoch 198, Iteration 90, Current loss 0.025908080860972404 Accuracy 99.13842384646794\n",
      "Training:: Epoch 198, Iteration 100, Current loss 0.01797235570847988 Accuracy 99.45638386547313\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 198, Probability Accuracy 49.26098935244645\n",
      "Starting Training\n",
      "Training:: Epoch 199, Iteration 0, Current loss 0.030787257477641106 Accuracy 99.08769864626251\n",
      "Training:: Epoch 199, Iteration 10, Current loss 0.029261542484164238 Accuracy 99.18655097613883\n",
      "Training:: Epoch 199, Iteration 20, Current loss 0.1899024397134781 Accuracy 94.97526685758918\n",
      "Training:: Epoch 199, Iteration 30, Current loss 0.06024181842803955 Accuracy 98.14193747349755\n",
      "Training:: Epoch 199, Iteration 40, Current loss 0.02424277924001217 Accuracy 99.2449400496382\n",
      "Training:: Epoch 199, Iteration 50, Current loss 0.022022683173418045 Accuracy 99.30019305019304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 199, Iteration 60, Current loss 0.024644069373607635 Accuracy 99.1385911179173\n",
      "Training:: Epoch 199, Iteration 70, Current loss 0.036819495260715485 Accuracy 98.92615475139748\n",
      "Training:: Epoch 199, Iteration 80, Current loss 0.021078644320368767 Accuracy 99.26820093697637\n",
      "Training:: Epoch 199, Iteration 90, Current loss 0.047191862016916275 Accuracy 98.38056680161944\n",
      "Training:: Epoch 199, Iteration 100, Current loss 0.02462868206202984 Accuracy 99.14685532955313\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 199, Probability Accuracy 49.24327795500684\n",
      "Starting Training\n",
      "Training:: Epoch 200, Iteration 0, Current loss 0.02907494641840458 Accuracy 99.00916003113213\n",
      "Training:: Epoch 200, Iteration 10, Current loss 0.4298819601535797 Accuracy 86.68578941078044\n",
      "Training:: Epoch 200, Iteration 20, Current loss 0.26989418268203735 Accuracy 92.7278951486698\n",
      "Training:: Epoch 200, Iteration 30, Current loss 0.05310109630227089 Accuracy 98.54011845033762\n",
      "Training:: Epoch 200, Iteration 40, Current loss 0.0817396268248558 Accuracy 97.60698689956332\n",
      "Training:: Epoch 200, Iteration 50, Current loss 0.05721935257315636 Accuracy 98.21745442813356\n",
      "Training:: Epoch 200, Iteration 60, Current loss 0.04202556982636452 Accuracy 98.73718808113134\n",
      "Training:: Epoch 200, Iteration 70, Current loss 0.04588133841753006 Accuracy 98.54102992906984\n",
      "Training:: Epoch 200, Iteration 80, Current loss 0.0363474041223526 Accuracy 98.96440815749479\n",
      "Training:: Epoch 200, Iteration 90, Current loss 0.027880216017365456 Accuracy 99.09029541290302\n",
      "Training:: Epoch 200, Iteration 100, Current loss 0.054676275700330734 Accuracy 98.16884266430627\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 200, Probability Accuracy 48.492356133736585\n",
      "Starting Training\n",
      "Training:: Epoch 201, Iteration 0, Current loss 0.03131232410669327 Accuracy 98.91757268068395\n",
      "Training:: Epoch 201, Iteration 10, Current loss 0.11051105707883835 Accuracy 96.68997092123986\n",
      "Training:: Epoch 201, Iteration 20, Current loss 0.24293412268161774 Accuracy 92.40810998245829\n",
      "Training:: Epoch 201, Iteration 30, Current loss 0.10543746501207352 Accuracy 96.61547726588377\n",
      "Training:: Epoch 201, Iteration 40, Current loss 0.1967170387506485 Accuracy 93.42854749539286\n",
      "Training:: Epoch 201, Iteration 50, Current loss 0.12796248495578766 Accuracy 95.56366498515258\n",
      "Training:: Epoch 201, Iteration 60, Current loss 0.2157965451478958 Accuracy 93.40436326737697\n",
      "Training:: Epoch 201, Iteration 70, Current loss 0.1503399759531021 Accuracy 94.20196228852909\n",
      "Training:: Epoch 201, Iteration 80, Current loss 0.06035725027322769 Accuracy 98.20580231539135\n",
      "Training:: Epoch 201, Iteration 90, Current loss 0.07462304085493088 Accuracy 97.78310151249687\n",
      "Training:: Epoch 201, Iteration 100, Current loss 0.12507617473602295 Accuracy 96.2267773990687\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 201, Probability Accuracy 42.3191573103534\n",
      "Starting Training\n",
      "Training:: Epoch 202, Iteration 0, Current loss 0.35037297010421753 Accuracy 91.98343996989085\n",
      "Training:: Epoch 202, Iteration 10, Current loss 0.07095571607351303 Accuracy 97.82866635026103\n",
      "Training:: Epoch 202, Iteration 20, Current loss 0.062059007585048676 Accuracy 98.29002428062199\n",
      "Training:: Epoch 202, Iteration 30, Current loss 0.054797034710645676 Accuracy 98.15436241610739\n",
      "Training:: Epoch 202, Iteration 40, Current loss 0.06912525743246078 Accuracy 97.68090415382358\n",
      "Training:: Epoch 202, Iteration 50, Current loss 0.10755231976509094 Accuracy 96.70499505440158\n",
      "Training:: Epoch 202, Iteration 60, Current loss 0.048310864716768265 Accuracy 98.35195779114919\n",
      "Training:: Epoch 202, Iteration 70, Current loss 0.04826606810092926 Accuracy 98.60974633329694\n",
      "Training:: Epoch 202, Iteration 80, Current loss 0.07242422550916672 Accuracy 97.27127463432622\n",
      "Training:: Epoch 202, Iteration 90, Current loss 0.07925482094287872 Accuracy 97.7971819805517\n",
      "Training:: Epoch 202, Iteration 100, Current loss 0.04576839134097099 Accuracy 98.60621182138863\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 202, Probability Accuracy 48.41902473381116\n",
      "Starting Training\n",
      "Training:: Epoch 203, Iteration 0, Current loss 0.06719349324703217 Accuracy 97.49661705006766\n",
      "Training:: Epoch 203, Iteration 10, Current loss 0.07298842072486877 Accuracy 98.20631140220304\n",
      "Training:: Epoch 203, Iteration 20, Current loss 0.04046653211116791 Accuracy 98.56276138304766\n",
      "Training:: Epoch 203, Iteration 30, Current loss 0.039958786219358444 Accuracy 98.82356526774558\n",
      "Training:: Epoch 203, Iteration 40, Current loss 0.02504396066069603 Accuracy 99.20590887024065\n",
      "Training:: Epoch 203, Iteration 50, Current loss 0.059274960309267044 Accuracy 98.26890921609127\n",
      "Training:: Epoch 203, Iteration 60, Current loss 0.04244617372751236 Accuracy 98.63534969164152\n",
      "Training:: Epoch 203, Iteration 70, Current loss 0.04111199080944061 Accuracy 98.73199158258242\n",
      "Training:: Epoch 203, Iteration 80, Current loss 0.04386192187666893 Accuracy 98.61638241652655\n",
      "Training:: Epoch 203, Iteration 90, Current loss 0.026211708784103394 Accuracy 99.11623726782504\n",
      "Training:: Epoch 203, Iteration 100, Current loss 0.023231199011206627 Accuracy 99.2723385177777\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 203, Probability Accuracy 48.32166383560509\n",
      "Starting Training\n",
      "Training:: Epoch 204, Iteration 0, Current loss 0.049521565437316895 Accuracy 97.92219440188347\n",
      "Training:: Epoch 204, Iteration 10, Current loss 0.05405373498797417 Accuracy 98.4097518717901\n",
      "Training:: Epoch 204, Iteration 20, Current loss 0.03324410319328308 Accuracy 98.9692840782287\n",
      "Training:: Epoch 204, Iteration 30, Current loss 0.05093472823500633 Accuracy 98.63618357761304\n",
      "Training:: Epoch 204, Iteration 40, Current loss 0.03841150552034378 Accuracy 98.8886144727093\n",
      "Training:: Epoch 204, Iteration 50, Current loss 0.05485015735030174 Accuracy 98.22168564622964\n",
      "Training:: Epoch 204, Iteration 60, Current loss 0.049754925072193146 Accuracy 98.45652546732978\n",
      "Training:: Epoch 204, Iteration 70, Current loss 0.0232497937977314 Accuracy 99.22911621243647\n",
      "Training:: Epoch 204, Iteration 80, Current loss 0.02339966595172882 Accuracy 99.17311187938839\n",
      "Training:: Epoch 204, Iteration 90, Current loss 0.030559461563825607 Accuracy 98.94585667097516\n",
      "Training:: Epoch 204, Iteration 100, Current loss 0.03448306396603584 Accuracy 98.86681653438546\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 204, Probability Accuracy 49.37471516758504\n",
      "Starting Training\n",
      "Training:: Epoch 205, Iteration 0, Current loss 0.028658494353294373 Accuracy 99.10336490026194\n",
      "Training:: Epoch 205, Iteration 10, Current loss 0.05793434754014015 Accuracy 98.38101292646748\n",
      "Training:: Epoch 205, Iteration 20, Current loss 0.028780408203601837 Accuracy 99.13175763422508\n",
      "Training:: Epoch 205, Iteration 30, Current loss 0.06943194568157196 Accuracy 97.92689340813465\n",
      "Training:: Epoch 205, Iteration 40, Current loss 0.03587983921170235 Accuracy 98.82375552565827\n",
      "Training:: Epoch 205, Iteration 50, Current loss 0.0325188934803009 Accuracy 98.90849699239135\n",
      "Training:: Epoch 205, Iteration 60, Current loss 0.0672186017036438 Accuracy 97.96942733287702\n",
      "Training:: Epoch 205, Iteration 70, Current loss 0.03275107592344284 Accuracy 98.91284147927894\n",
      "Training:: Epoch 205, Iteration 80, Current loss 0.024219924584031105 Accuracy 99.26060131231026\n",
      "Training:: Epoch 205, Iteration 90, Current loss 0.03467480465769768 Accuracy 98.9155938867398\n",
      "Training:: Epoch 205, Iteration 100, Current loss 0.03935031592845917 Accuracy 98.73594687481797\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 205, Probability Accuracy 49.25871069312674\n",
      "Starting Training\n",
      "Training:: Epoch 206, Iteration 0, Current loss 0.023381240665912628 Accuracy 99.22361088567874\n",
      "Training:: Epoch 206, Iteration 10, Current loss 0.05663323029875755 Accuracy 98.5904857790083\n",
      "Training:: Epoch 206, Iteration 20, Current loss 0.038149766623973846 Accuracy 98.836\n",
      "Training:: Epoch 206, Iteration 30, Current loss 0.06997392326593399 Accuracy 97.66966255852753\n",
      "Training:: Epoch 206, Iteration 40, Current loss 0.043481115251779556 Accuracy 98.63915715539947\n",
      "Training:: Epoch 206, Iteration 50, Current loss 0.08076523244380951 Accuracy 97.59013637212777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 206, Iteration 60, Current loss 0.10649838298559189 Accuracy 96.915530428999\n",
      "Training:: Epoch 206, Iteration 70, Current loss 0.0798625722527504 Accuracy 97.06481086966814\n",
      "Training:: Epoch 206, Iteration 80, Current loss 0.035855866968631744 Accuracy 98.82657463330457\n",
      "Training:: Epoch 206, Iteration 90, Current loss 0.08710415661334991 Accuracy 97.39565367368058\n",
      "Training:: Epoch 206, Iteration 100, Current loss 0.08175694942474365 Accuracy 97.29438600937777\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 206, Probability Accuracy 49.458196959025564\n",
      "Starting Training\n",
      "Training:: Epoch 207, Iteration 0, Current loss 0.025992341339588165 Accuracy 99.17449834899669\n",
      "Training:: Epoch 207, Iteration 10, Current loss 0.04095173999667168 Accuracy 98.67196429543351\n",
      "Training:: Epoch 207, Iteration 20, Current loss 0.04932139441370964 Accuracy 98.65147064028599\n",
      "Training:: Epoch 207, Iteration 30, Current loss 0.03204857558012009 Accuracy 99.09740503948854\n",
      "Training:: Epoch 207, Iteration 40, Current loss 0.03275340050458908 Accuracy 98.89342448236856\n",
      "Training:: Epoch 207, Iteration 50, Current loss 0.030717987567186356 Accuracy 98.96224827508834\n",
      "Training:: Epoch 207, Iteration 60, Current loss 0.05493036285042763 Accuracy 98.10370757863075\n",
      "Training:: Epoch 207, Iteration 70, Current loss 0.03871355950832367 Accuracy 98.83561319638378\n",
      "Training:: Epoch 207, Iteration 80, Current loss 0.02459688112139702 Accuracy 99.2351494322585\n",
      "Training:: Epoch 207, Iteration 90, Current loss 0.0208879504352808 Accuracy 99.32918798051888\n",
      "Training:: Epoch 207, Iteration 100, Current loss 0.03706885129213333 Accuracy 98.6808018811526\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 207, Probability Accuracy 47.82388034967063\n",
      "Starting Training\n",
      "Training:: Epoch 208, Iteration 0, Current loss 0.01750560849905014 Accuracy 99.45400355649059\n",
      "Training:: Epoch 208, Iteration 10, Current loss 0.029126664623618126 Accuracy 99.02117291211562\n",
      "Training:: Epoch 208, Iteration 20, Current loss 0.034840166568756104 Accuracy 98.89081148231953\n",
      "Training:: Epoch 208, Iteration 30, Current loss 0.01961272582411766 Accuracy 99.37100537688951\n",
      "Training:: Epoch 208, Iteration 40, Current loss 0.0259498730301857 Accuracy 99.01594954264769\n",
      "Training:: Epoch 208, Iteration 50, Current loss 0.0266412440687418 Accuracy 99.11848586984704\n",
      "Training:: Epoch 208, Iteration 60, Current loss 0.032955918461084366 Accuracy 98.91964888588791\n",
      "Training:: Epoch 208, Iteration 70, Current loss 0.01906389743089676 Accuracy 99.46069523418723\n",
      "Training:: Epoch 208, Iteration 80, Current loss 0.03704789653420448 Accuracy 99.02092088856048\n",
      "Training:: Epoch 208, Iteration 90, Current loss 0.022726457566022873 Accuracy 99.39014429621562\n",
      "Training:: Epoch 208, Iteration 100, Current loss 0.058876894414424896 Accuracy 98.06839501961453\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 208, Probability Accuracy 49.577205120768944\n",
      "Starting Training\n",
      "Training:: Epoch 209, Iteration 0, Current loss 0.018790144473314285 Accuracy 99.42862466229855\n",
      "Training:: Epoch 209, Iteration 10, Current loss 0.26417967677116394 Accuracy 93.22996996509457\n",
      "Training:: Epoch 209, Iteration 20, Current loss 0.046707455068826675 Accuracy 98.31167642752563\n",
      "Training:: Epoch 209, Iteration 30, Current loss 0.05308570712804794 Accuracy 98.42781421255953\n",
      "Training:: Epoch 209, Iteration 40, Current loss 0.05760308727622032 Accuracy 97.87886802433219\n",
      "Training:: Epoch 209, Iteration 50, Current loss 0.037242691963911057 Accuracy 98.57217223172455\n",
      "Training:: Epoch 209, Iteration 60, Current loss 0.05904692783951759 Accuracy 98.14413111593154\n",
      "Training:: Epoch 209, Iteration 70, Current loss 0.03384950011968613 Accuracy 98.8145609679337\n",
      "Training:: Epoch 209, Iteration 80, Current loss 0.05009767413139343 Accuracy 98.19271286363197\n",
      "Training:: Epoch 209, Iteration 90, Current loss 0.030666660517454147 Accuracy 99.21588957651556\n",
      "Training:: Epoch 209, Iteration 100, Current loss 0.0817304328083992 Accuracy 97.46654322026765\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 209, Probability Accuracy 48.57956664042756\n",
      "Starting Training\n",
      "Training:: Epoch 210, Iteration 0, Current loss 0.02898283861577511 Accuracy 99.2751635779493\n",
      "Training:: Epoch 210, Iteration 10, Current loss 0.028402315452694893 Accuracy 99.11245885877001\n",
      "Training:: Epoch 210, Iteration 20, Current loss 0.02978292666375637 Accuracy 99.01021584449656\n",
      "Training:: Epoch 210, Iteration 30, Current loss 0.03701664134860039 Accuracy 98.86527273774553\n",
      "Training:: Epoch 210, Iteration 40, Current loss 0.023989122360944748 Accuracy 99.28612511418615\n",
      "Training:: Epoch 210, Iteration 50, Current loss 0.044286590069532394 Accuracy 98.46598666773752\n",
      "Training:: Epoch 210, Iteration 60, Current loss 0.018469877541065216 Accuracy 99.40826083548868\n",
      "Training:: Epoch 210, Iteration 70, Current loss 0.023849384859204292 Accuracy 99.22217085044467\n",
      "Training:: Epoch 210, Iteration 80, Current loss 0.03067530132830143 Accuracy 99.08337826577129\n",
      "Training:: Epoch 210, Iteration 90, Current loss 0.047584641724824905 Accuracy 98.60310137126746\n",
      "Training:: Epoch 210, Iteration 100, Current loss 0.035828568041324615 Accuracy 98.81841489243116\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 210, Probability Accuracy 48.98371794340639\n",
      "Starting Training\n",
      "Training:: Epoch 211, Iteration 0, Current loss 0.01775522343814373 Accuracy 99.39807524059492\n",
      "Training:: Epoch 211, Iteration 10, Current loss 0.03961491584777832 Accuracy 99.00469483568075\n",
      "Training:: Epoch 211, Iteration 20, Current loss 0.03503218665719032 Accuracy 98.79708055505496\n",
      "Training:: Epoch 211, Iteration 30, Current loss 0.04131423309445381 Accuracy 98.7299803315538\n",
      "Training:: Epoch 211, Iteration 40, Current loss 0.029205873608589172 Accuracy 99.16770129213225\n",
      "Training:: Epoch 211, Iteration 50, Current loss 0.04174312576651573 Accuracy 98.49727471855738\n",
      "Training:: Epoch 211, Iteration 60, Current loss 0.023560047149658203 Accuracy 99.21913566395907\n",
      "Training:: Epoch 211, Iteration 70, Current loss 0.01815534010529518 Accuracy 99.47908802657406\n",
      "Training:: Epoch 211, Iteration 80, Current loss 0.04144822806119919 Accuracy 98.56672387402013\n",
      "Training:: Epoch 211, Iteration 90, Current loss 0.04833002761006355 Accuracy 98.39081975862297\n",
      "Training:: Epoch 211, Iteration 100, Current loss 0.033231183886528015 Accuracy 99.02798547318949\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 211, Probability Accuracy 49.155549571197746\n",
      "Starting Training\n",
      "Training:: Epoch 212, Iteration 0, Current loss 0.021060561761260033 Accuracy 99.36209646135943\n",
      "Training:: Epoch 212, Iteration 10, Current loss 0.02402174100279808 Accuracy 99.30742318585334\n",
      "Training:: Epoch 212, Iteration 20, Current loss 0.021866653114557266 Accuracy 99.27471876850207\n",
      "Training:: Epoch 212, Iteration 30, Current loss 0.029346643015742302 Accuracy 99.10759464556787\n",
      "Training:: Epoch 212, Iteration 40, Current loss 0.06812329590320587 Accuracy 97.28194923069456\n",
      "Training:: Epoch 212, Iteration 50, Current loss 0.030619828030467033 Accuracy 99.04618976154744\n",
      "Training:: Epoch 212, Iteration 60, Current loss 0.03241369500756264 Accuracy 99.07621247113164\n",
      "Training:: Epoch 212, Iteration 70, Current loss 0.035575322806835175 Accuracy 98.75660687981977\n",
      "Training:: Epoch 212, Iteration 80, Current loss 0.03444405272603035 Accuracy 98.89672563930809\n",
      "Training:: Epoch 212, Iteration 90, Current loss 0.034361012279987335 Accuracy 98.87482419127988\n",
      "Training:: Epoch 212, Iteration 100, Current loss 0.03615659847855568 Accuracy 98.78345498783455\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 212, Probability Accuracy 47.678667605750505\n",
      "Starting Training\n",
      "Training:: Epoch 213, Iteration 0, Current loss 0.04106748104095459 Accuracy 98.62272207854694\n",
      "Training:: Epoch 213, Iteration 10, Current loss 0.031622812151908875 Accuracy 98.95492519297353\n",
      "Training:: Epoch 213, Iteration 20, Current loss 0.032693587243556976 Accuracy 98.97827092001849\n",
      "Training:: Epoch 213, Iteration 30, Current loss 0.03363695368170738 Accuracy 98.94736842105263\n",
      "Training:: Epoch 213, Iteration 40, Current loss 0.03520485758781433 Accuracy 98.88270858524788\n",
      "Training:: Epoch 213, Iteration 50, Current loss 0.033040642738342285 Accuracy 99.06360994510817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 213, Iteration 60, Current loss 0.0262455977499485 Accuracy 99.19309491677913\n",
      "Training:: Epoch 213, Iteration 70, Current loss 0.02471432089805603 Accuracy 99.06210032684382\n",
      "Training:: Epoch 213, Iteration 80, Current loss 0.06808388978242874 Accuracy 96.88871988107996\n",
      "Training:: Epoch 213, Iteration 90, Current loss 0.09885434806346893 Accuracy 97.58840814672206\n",
      "Training:: Epoch 213, Iteration 100, Current loss 0.4874374568462372 Accuracy 91.00721643125887\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 213, Probability Accuracy 43.40203836433691\n",
      "Starting Training\n",
      "Training:: Epoch 214, Iteration 0, Current loss 0.07417833805084229 Accuracy 97.64348618281203\n",
      "Training:: Epoch 214, Iteration 10, Current loss 0.106254942715168 Accuracy 96.53369410216095\n",
      "Training:: Epoch 214, Iteration 20, Current loss 0.15204621851444244 Accuracy 95.00601416670379\n",
      "Training:: Epoch 214, Iteration 30, Current loss 0.06670030951499939 Accuracy 98.08410052251804\n",
      "Training:: Epoch 214, Iteration 40, Current loss 0.04598241671919823 Accuracy 98.55363152147267\n",
      "Training:: Epoch 214, Iteration 50, Current loss 0.04206410050392151 Accuracy 98.91793545915924\n",
      "Training:: Epoch 214, Iteration 60, Current loss 0.08718738704919815 Accuracy 96.92041308515243\n",
      "Training:: Epoch 214, Iteration 70, Current loss 0.08515096455812454 Accuracy 96.52444363921853\n",
      "Training:: Epoch 214, Iteration 80, Current loss 0.03268885239958763 Accuracy 99.04505599444396\n",
      "Training:: Epoch 214, Iteration 90, Current loss 0.06379962712526321 Accuracy 97.72111086685719\n",
      "Training:: Epoch 214, Iteration 100, Current loss 0.02907039225101471 Accuracy 99.05339728725262\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 214, Probability Accuracy 47.233293284169534\n",
      "Starting Training\n",
      "Training:: Epoch 215, Iteration 0, Current loss 0.13929888606071472 Accuracy 94.80366654279698\n",
      "Training:: Epoch 215, Iteration 10, Current loss 0.07739892601966858 Accuracy 97.40813989799105\n",
      "Training:: Epoch 215, Iteration 20, Current loss 0.0696222335100174 Accuracy 97.75790349417638\n",
      "Training:: Epoch 215, Iteration 30, Current loss 0.05282014235854149 Accuracy 98.27554351173247\n",
      "Training:: Epoch 215, Iteration 40, Current loss 0.06542539596557617 Accuracy 98.1177899210686\n",
      "Training:: Epoch 215, Iteration 50, Current loss 0.036217838525772095 Accuracy 98.84582675252096\n",
      "Training:: Epoch 215, Iteration 60, Current loss 0.052496250718832016 Accuracy 98.20953550235318\n",
      "Training:: Epoch 215, Iteration 70, Current loss 0.029171070083975792 Accuracy 99.15500817091156\n",
      "Training:: Epoch 215, Iteration 80, Current loss 0.1503179669380188 Accuracy 93.84803422285597\n",
      "Training:: Epoch 215, Iteration 90, Current loss 0.03706108406186104 Accuracy 98.83911435659786\n",
      "Training:: Epoch 215, Iteration 100, Current loss 0.06042753532528877 Accuracy 98.42717492217793\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 215, Probability Accuracy 49.24058499399263\n",
      "Starting Training\n",
      "Training:: Epoch 216, Iteration 0, Current loss 0.054179802536964417 Accuracy 98.67320449956735\n",
      "Training:: Epoch 216, Iteration 10, Current loss 0.1291111260652542 Accuracy 95.32244672017714\n",
      "Training:: Epoch 216, Iteration 20, Current loss 0.043433696031570435 Accuracy 98.49042572968953\n",
      "Training:: Epoch 216, Iteration 30, Current loss 0.06588558852672577 Accuracy 97.77156177156178\n",
      "Training:: Epoch 216, Iteration 40, Current loss 0.08154971897602081 Accuracy 97.15302491103203\n",
      "Training:: Epoch 216, Iteration 50, Current loss 0.07941799610853195 Accuracy 97.22872429359639\n",
      "Training:: Epoch 216, Iteration 60, Current loss 0.07516443729400635 Accuracy 97.92369341213603\n",
      "Training:: Epoch 216, Iteration 70, Current loss 0.036053724586963654 Accuracy 98.78491192796888\n",
      "Training:: Epoch 216, Iteration 80, Current loss 0.033469632267951965 Accuracy 98.82176426459556\n",
      "Training:: Epoch 216, Iteration 90, Current loss 0.08407282084226608 Accuracy 97.25292663588452\n",
      "Training:: Epoch 216, Iteration 100, Current loss 0.07182122021913528 Accuracy 97.73610936912078\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 216, Probability Accuracy 49.19894767369598\n",
      "Starting Training\n",
      "Training:: Epoch 217, Iteration 0, Current loss 0.03441379591822624 Accuracy 99.01917786372154\n",
      "Training:: Epoch 217, Iteration 10, Current loss 0.09076422452926636 Accuracy 96.73613308620767\n",
      "Training:: Epoch 217, Iteration 20, Current loss 0.05568678304553032 Accuracy 98.11938581353158\n",
      "Training:: Epoch 217, Iteration 30, Current loss 0.0289465319365263 Accuracy 99.0922121356904\n",
      "Training:: Epoch 217, Iteration 40, Current loss 0.022781861945986748 Accuracy 99.27398759576796\n",
      "Training:: Epoch 217, Iteration 50, Current loss 0.023966997861862183 Accuracy 99.25234400258648\n",
      "Training:: Epoch 217, Iteration 60, Current loss 0.031760770827531815 Accuracy 98.97191574724172\n",
      "Training:: Epoch 217, Iteration 70, Current loss 0.03361409157514572 Accuracy 98.85651404239032\n",
      "Training:: Epoch 217, Iteration 80, Current loss 0.05804827809333801 Accuracy 98.18157507688194\n",
      "Training:: Epoch 217, Iteration 90, Current loss 0.027908658608794212 Accuracy 99.12468065502367\n",
      "Training:: Epoch 217, Iteration 100, Current loss 0.031307414174079895 Accuracy 98.99068322981367\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 217, Probability Accuracy 50.291254091229234\n",
      "Starting Training\n",
      "Training:: Epoch 218, Iteration 0, Current loss 0.023189403116703033 Accuracy 99.20707732634338\n",
      "Training:: Epoch 218, Iteration 10, Current loss 0.031653255224227905 Accuracy 99.01300656601565\n",
      "Training:: Epoch 218, Iteration 20, Current loss 0.031007079407572746 Accuracy 99.03383563277802\n",
      "Training:: Epoch 218, Iteration 30, Current loss 0.03895954415202141 Accuracy 99.19570375921069\n",
      "Training:: Epoch 218, Iteration 40, Current loss 0.02880331501364708 Accuracy 99.15031013680007\n",
      "Training:: Epoch 218, Iteration 50, Current loss 0.027967413887381554 Accuracy 99.17165336879432\n",
      "Training:: Epoch 218, Iteration 60, Current loss 0.02274387888610363 Accuracy 99.21702818698527\n",
      "Training:: Epoch 218, Iteration 70, Current loss 0.025373272597789764 Accuracy 99.11250044913945\n",
      "Training:: Epoch 218, Iteration 80, Current loss 0.036608051508665085 Accuracy 98.89622392395036\n",
      "Training:: Epoch 218, Iteration 90, Current loss 0.03290310502052307 Accuracy 98.92216589272782\n",
      "Training:: Epoch 218, Iteration 100, Current loss 0.03076559118926525 Accuracy 98.99349707989893\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 218, Probability Accuracy 49.87187720097775\n",
      "Starting Training\n",
      "Training:: Epoch 219, Iteration 0, Current loss 0.020445797592401505 Accuracy 99.4045954045954\n",
      "Training:: Epoch 219, Iteration 10, Current loss 0.026567930355668068 Accuracy 99.10499398315282\n",
      "Training:: Epoch 219, Iteration 20, Current loss 0.035489559173583984 Accuracy 98.97282727471406\n",
      "Training:: Epoch 219, Iteration 30, Current loss 0.024135900661349297 Accuracy 99.17120530428605\n",
      "Training:: Epoch 219, Iteration 40, Current loss 0.025722837075591087 Accuracy 99.22246757689796\n",
      "Training:: Epoch 219, Iteration 50, Current loss 0.01743152178823948 Accuracy 99.43429984739252\n",
      "Training:: Epoch 219, Iteration 60, Current loss 0.021453427150845528 Accuracy 99.26218943064136\n",
      "Training:: Epoch 219, Iteration 70, Current loss 0.021291609853506088 Accuracy 99.30696959954274\n",
      "Training:: Epoch 219, Iteration 80, Current loss 0.02923830971121788 Accuracy 99.11950277803936\n",
      "Training:: Epoch 219, Iteration 90, Current loss 0.02642691880464554 Accuracy 99.21037789058093\n",
      "Training:: Epoch 219, Iteration 100, Current loss 0.02335311286151409 Accuracy 99.1994403855122\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 219, Probability Accuracy 49.898910386543484\n",
      "Starting Training\n",
      "Training:: Epoch 220, Iteration 0, Current loss 0.029946720227599144 Accuracy 99.08526407157642\n",
      "Training:: Epoch 220, Iteration 10, Current loss 0.026061933487653732 Accuracy 99.20663061785302\n",
      "Training:: Epoch 220, Iteration 20, Current loss 0.11881595104932785 Accuracy 96.47478534284677\n",
      "Training:: Epoch 220, Iteration 30, Current loss 0.02326033264398575 Accuracy 99.20998181704182\n",
      "Training:: Epoch 220, Iteration 40, Current loss 0.02754937671124935 Accuracy 99.17460524598721\n",
      "Training:: Epoch 220, Iteration 50, Current loss 0.031354792416095734 Accuracy 98.95361152298241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 220, Iteration 60, Current loss 0.02038538083434105 Accuracy 99.40302940302941\n",
      "Training:: Epoch 220, Iteration 70, Current loss 0.024723194539546967 Accuracy 99.27701540729302\n",
      "Training:: Epoch 220, Iteration 80, Current loss 0.02540343813598156 Accuracy 99.1544506226736\n",
      "Training:: Epoch 220, Iteration 90, Current loss 0.03669154271483421 Accuracy 98.7775343097682\n",
      "Training:: Epoch 220, Iteration 100, Current loss 0.022280192002654076 Accuracy 99.27015891701001\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 220, Probability Accuracy 50.66236483407217\n",
      "Starting Training\n",
      "Training:: Epoch 221, Iteration 0, Current loss 0.03139659762382507 Accuracy 98.9987173652199\n",
      "Training:: Epoch 221, Iteration 10, Current loss 0.026735445484519005 Accuracy 99.11904668266017\n",
      "Training:: Epoch 221, Iteration 20, Current loss 0.03154902160167694 Accuracy 99.07865845782925\n",
      "Training:: Epoch 221, Iteration 30, Current loss 0.03939436003565788 Accuracy 98.8911620294599\n",
      "Training:: Epoch 221, Iteration 40, Current loss 0.02890023961663246 Accuracy 99.13815171942998\n",
      "Training:: Epoch 221, Iteration 50, Current loss 0.0283610001206398 Accuracy 99.22654410940477\n",
      "Training:: Epoch 221, Iteration 60, Current loss 0.02230013534426689 Accuracy 99.30694519010201\n",
      "Training:: Epoch 221, Iteration 70, Current loss 0.030328622087836266 Accuracy 99.0310262529833\n",
      "Training:: Epoch 221, Iteration 80, Current loss 0.04577348753809929 Accuracy 98.36843202668891\n",
      "Training:: Epoch 221, Iteration 90, Current loss 0.02097441628575325 Accuracy 99.26746071688196\n",
      "Training:: Epoch 221, Iteration 100, Current loss 0.08063912391662598 Accuracy 97.80891917002168\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 221, Probability Accuracy 50.227037328582675\n",
      "Starting Training\n",
      "Training:: Epoch 222, Iteration 0, Current loss 0.03607592359185219 Accuracy 98.75150875075437\n",
      "Training:: Epoch 222, Iteration 10, Current loss 0.048920705914497375 Accuracy 98.34336486099168\n",
      "Training:: Epoch 222, Iteration 20, Current loss 0.033071987330913544 Accuracy 98.87052724158681\n",
      "Training:: Epoch 222, Iteration 30, Current loss 0.03199309483170509 Accuracy 99.06407936606976\n",
      "Training:: Epoch 222, Iteration 40, Current loss 0.02845696173608303 Accuracy 99.07507455590613\n",
      "Training:: Epoch 222, Iteration 50, Current loss 0.021933265030384064 Accuracy 99.27653877923039\n",
      "Training:: Epoch 222, Iteration 60, Current loss 0.024781590327620506 Accuracy 99.26464985140593\n",
      "Training:: Epoch 222, Iteration 70, Current loss 0.024782812222838402 Accuracy 99.1960592964357\n",
      "Training:: Epoch 222, Iteration 80, Current loss 0.028452729806303978 Accuracy 99.02127178094591\n",
      "Training:: Epoch 222, Iteration 90, Current loss 0.04819730669260025 Accuracy 98.48484848484848\n",
      "Training:: Epoch 222, Iteration 100, Current loss 0.03685518354177475 Accuracy 98.65111346765642\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 222, Probability Accuracy 48.597588764138045\n",
      "Starting Training\n",
      "Training:: Epoch 223, Iteration 0, Current loss 0.03407946228981018 Accuracy 98.8339222614841\n",
      "Training:: Epoch 223, Iteration 10, Current loss 0.05363655835390091 Accuracy 98.50181232380186\n",
      "Training:: Epoch 223, Iteration 20, Current loss 0.04463560879230499 Accuracy 98.79589186636761\n",
      "Training:: Epoch 223, Iteration 30, Current loss 0.022765088826417923 Accuracy 99.21751400350622\n",
      "Training:: Epoch 223, Iteration 40, Current loss 0.028425831347703934 Accuracy 99.13554633471647\n",
      "Training:: Epoch 223, Iteration 50, Current loss 0.024605462327599525 Accuracy 99.15324156498328\n",
      "Training:: Epoch 223, Iteration 60, Current loss 0.022019121795892715 Accuracy 99.34990596123967\n",
      "Training:: Epoch 223, Iteration 70, Current loss 0.021037273108959198 Accuracy 99.36582501467997\n",
      "Training:: Epoch 223, Iteration 80, Current loss 0.02008350007236004 Accuracy 99.38491774639685\n",
      "Training:: Epoch 223, Iteration 90, Current loss 0.022403210401535034 Accuracy 99.26324737886087\n",
      "Training:: Epoch 223, Iteration 100, Current loss 0.01850130967795849 Accuracy 99.39397846689447\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 223, Probability Accuracy 50.65231801798069\n",
      "Starting Training\n",
      "Training:: Epoch 224, Iteration 0, Current loss 0.019096771255135536 Accuracy 99.3591797500801\n",
      "Training:: Epoch 224, Iteration 10, Current loss 0.023750504478812218 Accuracy 99.33397374319564\n",
      "Training:: Epoch 224, Iteration 20, Current loss 0.02871016599237919 Accuracy 99.00674321122654\n",
      "Training:: Epoch 224, Iteration 30, Current loss 0.03615550324320793 Accuracy 98.72830921505982\n",
      "Training:: Epoch 224, Iteration 40, Current loss 0.03394647687673569 Accuracy 98.97430790679148\n",
      "Training:: Epoch 224, Iteration 50, Current loss 0.017620950937271118 Accuracy 99.43294976764771\n",
      "Training:: Epoch 224, Iteration 60, Current loss 0.02692435123026371 Accuracy 99.08141962421712\n",
      "Training:: Epoch 224, Iteration 70, Current loss 0.02397112548351288 Accuracy 99.26242236024845\n",
      "Training:: Epoch 224, Iteration 80, Current loss 0.061668265610933304 Accuracy 98.32336338360435\n",
      "Training:: Epoch 224, Iteration 90, Current loss 0.01962195336818695 Accuracy 99.42000535379674\n",
      "Training:: Epoch 224, Iteration 100, Current loss 0.021019235253334045 Accuracy 99.28091426614732\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 224, Probability Accuracy 49.72324646807805\n",
      "Starting Training\n",
      "Training:: Epoch 225, Iteration 0, Current loss 0.043216969817876816 Accuracy 98.490860990861\n",
      "Training:: Epoch 225, Iteration 10, Current loss 0.032653745263814926 Accuracy 98.91640866873065\n",
      "Training:: Epoch 225, Iteration 20, Current loss 0.02717677876353264 Accuracy 99.0387439587728\n",
      "Training:: Epoch 225, Iteration 30, Current loss 0.020873401314020157 Accuracy 99.35448984419885\n",
      "Training:: Epoch 225, Iteration 40, Current loss 0.019446961581707 Accuracy 99.47563996696945\n",
      "Training:: Epoch 225, Iteration 50, Current loss 0.03423202410340309 Accuracy 99.06310765423369\n",
      "Training:: Epoch 225, Iteration 60, Current loss 0.017796771600842476 Accuracy 99.39655948842655\n",
      "Training:: Epoch 225, Iteration 70, Current loss 0.02023712359368801 Accuracy 99.31817407015988\n",
      "Training:: Epoch 225, Iteration 80, Current loss 0.026198070496320724 Accuracy 99.19370138493645\n",
      "Training:: Epoch 225, Iteration 90, Current loss 0.020183298736810684 Accuracy 99.34366197183098\n",
      "Training:: Epoch 225, Iteration 100, Current loss 0.02896192856132984 Accuracy 99.0106227869194\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 225, Probability Accuracy 49.79574926461449\n",
      "Starting Training\n",
      "Training:: Epoch 226, Iteration 0, Current loss 0.02377071976661682 Accuracy 99.26098959439372\n",
      "Training:: Epoch 226, Iteration 10, Current loss 0.04598863050341606 Accuracy 98.5379241516966\n",
      "Training:: Epoch 226, Iteration 20, Current loss 0.02877116948366165 Accuracy 99.08045045774934\n",
      "Training:: Epoch 226, Iteration 30, Current loss 0.020335227251052856 Accuracy 99.33502835908469\n",
      "Training:: Epoch 226, Iteration 40, Current loss 0.02414219081401825 Accuracy 99.2698161344242\n",
      "Training:: Epoch 226, Iteration 50, Current loss 0.020311102271080017 Accuracy 99.334654168022\n",
      "Training:: Epoch 226, Iteration 60, Current loss 0.02882656641304493 Accuracy 99.09069713219864\n",
      "Training:: Epoch 226, Iteration 70, Current loss 0.023977920413017273 Accuracy 99.17733089579525\n",
      "Training:: Epoch 226, Iteration 80, Current loss 0.020599205046892166 Accuracy 99.30376823071339\n",
      "Training:: Epoch 226, Iteration 90, Current loss 0.03287454694509506 Accuracy 98.884087303758\n",
      "Training:: Epoch 226, Iteration 100, Current loss 0.028580157086253166 Accuracy 99.12018592297477\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 226, Probability Accuracy 48.9815428595103\n",
      "Starting Training\n",
      "Training:: Epoch 227, Iteration 0, Current loss 0.0296077411621809 Accuracy 99.11529148817284\n",
      "Training:: Epoch 227, Iteration 10, Current loss 0.20937064290046692 Accuracy 94.39066386072317\n",
      "Training:: Epoch 227, Iteration 20, Current loss 0.8413236141204834 Accuracy 82.51193374008156\n",
      "Training:: Epoch 227, Iteration 30, Current loss 0.38412436842918396 Accuracy 88.31518866927387\n",
      "Training:: Epoch 227, Iteration 40, Current loss 0.27258414030075073 Accuracy 90.80321972940573\n",
      "Training:: Epoch 227, Iteration 50, Current loss 0.1821960210800171 Accuracy 94.59275633395681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 227, Iteration 60, Current loss 0.1888325810432434 Accuracy 93.2042145463225\n",
      "Training:: Epoch 227, Iteration 70, Current loss 0.14595304429531097 Accuracy 95.37211493170042\n",
      "Training:: Epoch 227, Iteration 80, Current loss 0.16342084109783173 Accuracy 95.67215108618534\n",
      "Training:: Epoch 227, Iteration 90, Current loss 0.09605782479047775 Accuracy 97.2264381884945\n",
      "Training:: Epoch 227, Iteration 100, Current loss 0.2899245321750641 Accuracy 94.67434751939807\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 227, Probability Accuracy 45.800845175456764\n",
      "Starting Training\n",
      "Training:: Epoch 228, Iteration 0, Current loss 0.10241958498954773 Accuracy 96.80711182254745\n",
      "Training:: Epoch 228, Iteration 10, Current loss 0.12184938043355942 Accuracy 96.39796808456052\n",
      "Training:: Epoch 228, Iteration 20, Current loss 0.06402605772018433 Accuracy 97.79032764107392\n",
      "Training:: Epoch 228, Iteration 30, Current loss 0.03735044226050377 Accuracy 99.075034106412\n",
      "Training:: Epoch 228, Iteration 40, Current loss 0.06296941637992859 Accuracy 98.19672966226887\n",
      "Training:: Epoch 228, Iteration 50, Current loss 0.06778573244810104 Accuracy 97.62713533205337\n",
      "Training:: Epoch 228, Iteration 60, Current loss 0.07234858721494675 Accuracy 97.85430264836072\n",
      "Training:: Epoch 228, Iteration 70, Current loss 0.04178810492157936 Accuracy 98.63501317338415\n",
      "Training:: Epoch 228, Iteration 80, Current loss 0.07181186974048615 Accuracy 97.30985366939298\n",
      "Training:: Epoch 228, Iteration 90, Current loss 0.035488057881593704 Accuracy 98.84339499605314\n",
      "Training:: Epoch 228, Iteration 100, Current loss 0.15371303260326385 Accuracy 95.68930868167203\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 228, Probability Accuracy 47.90570493433318\n",
      "Starting Training\n",
      "Training:: Epoch 229, Iteration 0, Current loss 0.15177728235721588 Accuracy 96.3979558193208\n",
      "Training:: Epoch 229, Iteration 10, Current loss 0.030002208426594734 Accuracy 99.18136908962597\n",
      "Training:: Epoch 229, Iteration 20, Current loss 0.07676106691360474 Accuracy 97.70755493635615\n",
      "Training:: Epoch 229, Iteration 30, Current loss 0.03093382902443409 Accuracy 98.972321183886\n",
      "Training:: Epoch 229, Iteration 40, Current loss 0.0348299965262413 Accuracy 98.83607769862587\n",
      "Training:: Epoch 229, Iteration 50, Current loss 0.039563216269016266 Accuracy 98.76226654004432\n",
      "Training:: Epoch 229, Iteration 60, Current loss 0.034388165920972824 Accuracy 98.81998853430154\n",
      "Training:: Epoch 229, Iteration 70, Current loss 0.029826346784830093 Accuracy 99.07346639385399\n",
      "Training:: Epoch 229, Iteration 80, Current loss 0.02564411610364914 Accuracy 99.20466178439247\n",
      "Training:: Epoch 229, Iteration 90, Current loss 0.023632127791643143 Accuracy 99.24839596700275\n",
      "Training:: Epoch 229, Iteration 100, Current loss 0.0624125674366951 Accuracy 98.11229852838122\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 229, Probability Accuracy 49.988606703401416\n",
      "Starting Training\n",
      "Training:: Epoch 230, Iteration 0, Current loss 0.029829364269971848 Accuracy 98.95229877973622\n",
      "Training:: Epoch 230, Iteration 10, Current loss 0.04323959723114967 Accuracy 98.9659003503974\n",
      "Training:: Epoch 230, Iteration 20, Current loss 0.03888869285583496 Accuracy 98.69633761823914\n",
      "Training:: Epoch 230, Iteration 30, Current loss 0.022263403981924057 Accuracy 99.28322548531608\n",
      "Training:: Epoch 230, Iteration 40, Current loss 0.05346277356147766 Accuracy 98.21015530344752\n",
      "Training:: Epoch 230, Iteration 50, Current loss 0.024111898615956306 Accuracy 99.23277899888028\n",
      "Training:: Epoch 230, Iteration 60, Current loss 0.026953918859362602 Accuracy 99.18640127851228\n",
      "Training:: Epoch 230, Iteration 70, Current loss 0.04371213912963867 Accuracy 98.60203919069619\n",
      "Training:: Epoch 230, Iteration 80, Current loss 0.029113251715898514 Accuracy 99.11070332074672\n",
      "Training:: Epoch 230, Iteration 90, Current loss 0.025524692609906197 Accuracy 99.08367626886145\n",
      "Training:: Epoch 230, Iteration 100, Current loss 0.0281202532351017 Accuracy 99.1532396297566\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 230, Probability Accuracy 50.41160873347972\n",
      "Starting Training\n",
      "Training:: Epoch 231, Iteration 0, Current loss 0.03359757363796234 Accuracy 99.13632022588548\n",
      "Training:: Epoch 231, Iteration 10, Current loss 0.02321707084774971 Accuracy 99.32274978153218\n",
      "Training:: Epoch 231, Iteration 20, Current loss 0.03482542186975479 Accuracy 99.01507725341922\n",
      "Training:: Epoch 231, Iteration 30, Current loss 0.021642502397298813 Accuracy 99.3037639158862\n",
      "Training:: Epoch 231, Iteration 40, Current loss 0.03416205570101738 Accuracy 98.99083787013677\n",
      "Training:: Epoch 231, Iteration 50, Current loss 0.022312333807349205 Accuracy 99.2338036716735\n",
      "Training:: Epoch 231, Iteration 60, Current loss 0.018806226551532745 Accuracy 99.40583510899853\n",
      "Training:: Epoch 231, Iteration 70, Current loss 0.027446631342172623 Accuracy 99.06747120131651\n",
      "Training:: Epoch 231, Iteration 80, Current loss 0.020753853023052216 Accuracy 99.30113202052925\n",
      "Training:: Epoch 231, Iteration 90, Current loss 0.025895362719893456 Accuracy 99.04808892242505\n",
      "Training:: Epoch 231, Iteration 100, Current loss 0.02355041727423668 Accuracy 99.24467073238964\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 231, Probability Accuracy 49.43831047768985\n",
      "Starting Training\n",
      "Training:: Epoch 232, Iteration 0, Current loss 0.04514823481440544 Accuracy 98.84460817146685\n",
      "Training:: Epoch 232, Iteration 10, Current loss 0.028424330055713654 Accuracy 99.05731052565864\n",
      "Training:: Epoch 232, Iteration 20, Current loss 0.014332047663629055 Accuracy 99.56489998539932\n",
      "Training:: Epoch 232, Iteration 30, Current loss 0.02209736965596676 Accuracy 99.29780091799685\n",
      "Training:: Epoch 232, Iteration 40, Current loss 0.052729398012161255 Accuracy 98.23555359505956\n",
      "Training:: Epoch 232, Iteration 50, Current loss 0.07910691946744919 Accuracy 98.28031562931248\n",
      "Training:: Epoch 232, Iteration 60, Current loss 0.027697088196873665 Accuracy 99.0467439990812\n",
      "Training:: Epoch 232, Iteration 70, Current loss 0.017248110845685005 Accuracy 99.48363211223695\n",
      "Training:: Epoch 232, Iteration 80, Current loss 0.03285687789320946 Accuracy 98.88011486001436\n",
      "Training:: Epoch 232, Iteration 90, Current loss 0.02654055878520012 Accuracy 99.11056158264199\n",
      "Training:: Epoch 232, Iteration 100, Current loss 0.059933364391326904 Accuracy 98.05948020233734\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 232, Probability Accuracy 50.54356382317604\n",
      "Starting Training\n",
      "Training:: Epoch 233, Iteration 0, Current loss 0.023864421993494034 Accuracy 99.19853613906679\n",
      "Training:: Epoch 233, Iteration 10, Current loss 0.0488521084189415 Accuracy 98.69658119658119\n",
      "Training:: Epoch 233, Iteration 20, Current loss 0.025190068408846855 Accuracy 99.12974683544304\n",
      "Training:: Epoch 233, Iteration 30, Current loss 0.042606525123119354 Accuracy 98.77462887989203\n",
      "Training:: Epoch 233, Iteration 40, Current loss 0.026524456217885017 Accuracy 99.18428500083236\n",
      "Training:: Epoch 233, Iteration 50, Current loss 0.019598444923758507 Accuracy 99.33829910326872\n",
      "Training:: Epoch 233, Iteration 60, Current loss 0.024614442139863968 Accuracy 99.19781604367913\n",
      "Training:: Epoch 233, Iteration 70, Current loss 0.022211655974388123 Accuracy 99.19745499240835\n",
      "Training:: Epoch 233, Iteration 80, Current loss 0.027409713715314865 Accuracy 99.08560231865127\n",
      "Training:: Epoch 233, Iteration 90, Current loss 0.048368189483881 Accuracy 98.72152769349032\n",
      "Training:: Epoch 233, Iteration 100, Current loss 0.02658236213028431 Accuracy 99.08555171713067\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 233, Probability Accuracy 49.441003438704065\n",
      "Starting Training\n",
      "Training:: Epoch 234, Iteration 0, Current loss 0.019438620656728745 Accuracy 99.3936151239522\n",
      "Training:: Epoch 234, Iteration 10, Current loss 0.046921029686927795 Accuracy 98.47639690179453\n",
      "Training:: Epoch 234, Iteration 20, Current loss 0.04950163885951042 Accuracy 98.55000191798688\n",
      "Training:: Epoch 234, Iteration 30, Current loss 0.08859846740961075 Accuracy 97.44867878008255\n",
      "Training:: Epoch 234, Iteration 40, Current loss 0.03750988841056824 Accuracy 98.68743957760095\n",
      "Training:: Epoch 234, Iteration 50, Current loss 0.0696953535079956 Accuracy 97.71548052350344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 234, Iteration 60, Current loss 0.025356357917189598 Accuracy 99.2615687561536\n",
      "Training:: Epoch 234, Iteration 70, Current loss 0.03023727610707283 Accuracy 98.96377251568508\n",
      "Training:: Epoch 234, Iteration 80, Current loss 0.043650656938552856 Accuracy 98.4627140738077\n",
      "Training:: Epoch 234, Iteration 90, Current loss 0.03813972324132919 Accuracy 98.90595305357235\n",
      "Training:: Epoch 234, Iteration 100, Current loss 0.024912910535931587 Accuracy 99.1586856447431\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 234, Probability Accuracy 48.014977006255954\n",
      "Starting Training\n",
      "Training:: Epoch 235, Iteration 0, Current loss 0.0379619225859642 Accuracy 98.70111828853233\n",
      "Training:: Epoch 235, Iteration 10, Current loss 0.08978534489870071 Accuracy 97.38077479928049\n",
      "Training:: Epoch 235, Iteration 20, Current loss 0.11274801939725876 Accuracy 96.43188653324917\n",
      "Training:: Epoch 235, Iteration 30, Current loss 0.08279143273830414 Accuracy 97.60445875307232\n",
      "Training:: Epoch 235, Iteration 40, Current loss 0.07944412529468536 Accuracy 97.90632787374071\n",
      "Training:: Epoch 235, Iteration 50, Current loss 0.06351696699857712 Accuracy 98.07234746138646\n",
      "Training:: Epoch 235, Iteration 60, Current loss 0.07268085330724716 Accuracy 98.1440716562927\n",
      "Training:: Epoch 235, Iteration 70, Current loss 0.026497410610318184 Accuracy 99.19142680284742\n",
      "Training:: Epoch 235, Iteration 80, Current loss 0.19933867454528809 Accuracy 95.68649685973482\n",
      "Training:: Epoch 235, Iteration 90, Current loss 0.1307365745306015 Accuracy 95.76500784257807\n",
      "Training:: Epoch 235, Iteration 100, Current loss 0.12085086852312088 Accuracy 96.17017698743011\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 235, Probability Accuracy 48.924369225670134\n",
      "Starting Training\n",
      "Training:: Epoch 236, Iteration 0, Current loss 0.038812510669231415 Accuracy 98.7265502866076\n",
      "Training:: Epoch 236, Iteration 10, Current loss 0.04227101057767868 Accuracy 98.65396941672007\n",
      "Training:: Epoch 236, Iteration 20, Current loss 0.03282218053936958 Accuracy 99.0074718411955\n",
      "Training:: Epoch 236, Iteration 30, Current loss 0.041234664618968964 Accuracy 98.74303898170247\n",
      "Training:: Epoch 236, Iteration 40, Current loss 0.025408267974853516 Accuracy 99.17030422178534\n",
      "Training:: Epoch 236, Iteration 50, Current loss 0.0562020018696785 Accuracy 98.04852320675106\n",
      "Training:: Epoch 236, Iteration 60, Current loss 0.04560430720448494 Accuracy 98.42383893478784\n",
      "Training:: Epoch 236, Iteration 70, Current loss 0.03133242577314377 Accuracy 99.02806992094001\n",
      "Training:: Epoch 236, Iteration 80, Current loss 0.029784753918647766 Accuracy 99.07028635180365\n",
      "Training:: Epoch 236, Iteration 90, Current loss 0.02378474548459053 Accuracy 99.19672534777791\n",
      "Training:: Epoch 236, Iteration 100, Current loss 0.039871811866760254 Accuracy 98.71063031415912\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 236, Probability Accuracy 48.71576832249244\n",
      "Starting Training\n",
      "Training:: Epoch 237, Iteration 0, Current loss 0.020485546439886093 Accuracy 99.36378871578425\n",
      "Training:: Epoch 237, Iteration 10, Current loss 0.05804840102791786 Accuracy 98.08854025786674\n",
      "Training:: Epoch 237, Iteration 20, Current loss 0.053972773253917694 Accuracy 98.49896298379443\n",
      "Training:: Epoch 237, Iteration 30, Current loss 0.022446410730481148 Accuracy 99.27002319552463\n",
      "Training:: Epoch 237, Iteration 40, Current loss 0.055383022874593735 Accuracy 98.2655989036872\n",
      "Training:: Epoch 237, Iteration 50, Current loss 0.023996170610189438 Accuracy 99.14952182658715\n",
      "Training:: Epoch 237, Iteration 60, Current loss 0.034162502735853195 Accuracy 98.81196421915766\n",
      "Training:: Epoch 237, Iteration 70, Current loss 0.035732608288526535 Accuracy 99.11089085820896\n",
      "Training:: Epoch 237, Iteration 80, Current loss 0.03596651554107666 Accuracy 99.03216958583127\n",
      "Training:: Epoch 237, Iteration 90, Current loss 0.02478071115911007 Accuracy 99.18834306033317\n",
      "Training:: Epoch 237, Iteration 100, Current loss 0.020623140037059784 Accuracy 99.44109297374024\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 237, Probability Accuracy 50.16955296847164\n",
      "Starting Training\n",
      "Training:: Epoch 238, Iteration 0, Current loss 0.03186848759651184 Accuracy 98.88265764801575\n",
      "Training:: Epoch 238, Iteration 10, Current loss 0.026568522676825523 Accuracy 99.09692417739628\n",
      "Training:: Epoch 238, Iteration 20, Current loss 0.02124217338860035 Accuracy 99.32488908892175\n",
      "Training:: Epoch 238, Iteration 30, Current loss 0.0422229990363121 Accuracy 98.71400933196767\n",
      "Training:: Epoch 238, Iteration 40, Current loss 0.048980578780174255 Accuracy 98.49922640536359\n",
      "Training:: Epoch 238, Iteration 50, Current loss 0.021832264959812164 Accuracy 99.29178470254958\n",
      "Training:: Epoch 238, Iteration 60, Current loss 0.027990538626909256 Accuracy 99.12443618997081\n",
      "Training:: Epoch 238, Iteration 70, Current loss 0.022546665742993355 Accuracy 99.29643420060762\n",
      "Training:: Epoch 238, Iteration 80, Current loss 0.025222426280379295 Accuracy 99.13052642672709\n",
      "Training:: Epoch 238, Iteration 90, Current loss 0.027247827500104904 Accuracy 99.20077657337\n",
      "Training:: Epoch 238, Iteration 100, Current loss 0.02915380522608757 Accuracy 99.1200961455919\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 238, Probability Accuracy 49.08553258482827\n",
      "Starting Training\n",
      "Training:: Epoch 239, Iteration 0, Current loss 0.022703278809785843 Accuracy 99.19979237856309\n",
      "Training:: Epoch 239, Iteration 10, Current loss 0.04403359815478325 Accuracy 98.47394869082254\n",
      "Training:: Epoch 239, Iteration 20, Current loss 0.03836177662014961 Accuracy 98.98047792107549\n",
      "Training:: Epoch 239, Iteration 30, Current loss 0.039371371269226074 Accuracy 98.74292626031362\n",
      "Training:: Epoch 239, Iteration 40, Current loss 0.05015229806303978 Accuracy 98.47835405057866\n",
      "Training:: Epoch 239, Iteration 50, Current loss 0.0399458184838295 Accuracy 98.77939706423949\n",
      "Training:: Epoch 239, Iteration 60, Current loss 0.05486530065536499 Accuracy 98.12505094970246\n",
      "Training:: Epoch 239, Iteration 70, Current loss 0.02525157853960991 Accuracy 99.28400297414149\n",
      "Training:: Epoch 239, Iteration 80, Current loss 0.02702482044696808 Accuracy 98.98793527360056\n",
      "Training:: Epoch 239, Iteration 90, Current loss 0.030454399064183235 Accuracy 98.83247352701602\n",
      "Training:: Epoch 239, Iteration 100, Current loss 0.01768064871430397 Accuracy 99.45702500210456\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 239, Probability Accuracy 47.92206985126569\n",
      "Starting Training\n",
      "Training:: Epoch 240, Iteration 0, Current loss 0.0173577181994915 Accuracy 99.40035936651205\n",
      "Training:: Epoch 240, Iteration 10, Current loss 0.024915656074881554 Accuracy 99.17981072555204\n",
      "Training:: Epoch 240, Iteration 20, Current loss 0.04179416969418526 Accuracy 98.27884344713878\n",
      "Training:: Epoch 240, Iteration 30, Current loss 0.02230127714574337 Accuracy 99.31067202500559\n",
      "Training:: Epoch 240, Iteration 40, Current loss 0.021988440304994583 Accuracy 99.30136551286122\n",
      "Training:: Epoch 240, Iteration 50, Current loss 0.024704821407794952 Accuracy 99.21016483516483\n",
      "Training:: Epoch 240, Iteration 60, Current loss 0.029578791931271553 Accuracy 99.22284703355027\n",
      "Training:: Epoch 240, Iteration 70, Current loss 0.03832777962088585 Accuracy 98.85273146410765\n",
      "Training:: Epoch 240, Iteration 80, Current loss 0.04526958614587784 Accuracy 98.76641631351488\n",
      "Training:: Epoch 240, Iteration 90, Current loss 0.030875258147716522 Accuracy 99.19552819428183\n",
      "Training:: Epoch 240, Iteration 100, Current loss 0.08923158049583435 Accuracy 96.28621133045912\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 240, Probability Accuracy 43.158221817127234\n",
      "Starting Training\n",
      "Training:: Epoch 241, Iteration 0, Current loss 0.03834662213921547 Accuracy 98.84871226334641\n",
      "Training:: Epoch 241, Iteration 10, Current loss 0.08991508930921555 Accuracy 96.64172901080632\n",
      "Training:: Epoch 241, Iteration 20, Current loss 0.38663700222969055 Accuracy 92.21287822573996\n",
      "Training:: Epoch 241, Iteration 30, Current loss 0.07947415858507156 Accuracy 97.44119021654463\n",
      "Training:: Epoch 241, Iteration 40, Current loss 0.2967180907726288 Accuracy 92.43710389859804\n",
      "Training:: Epoch 241, Iteration 50, Current loss 0.07974466681480408 Accuracy 97.51240532776181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 241, Iteration 60, Current loss 0.0714380070567131 Accuracy 98.23119083208769\n",
      "Training:: Epoch 241, Iteration 70, Current loss 0.05858078598976135 Accuracy 98.0854487537355\n",
      "Training:: Epoch 241, Iteration 80, Current loss 0.04735994338989258 Accuracy 98.66234572157485\n",
      "Training:: Epoch 241, Iteration 90, Current loss 0.27587658166885376 Accuracy 91.08726498758425\n",
      "Training:: Epoch 241, Iteration 100, Current loss 0.08187238872051239 Accuracy 97.88772854777113\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 241, Probability Accuracy 45.21688693706757\n",
      "Starting Training\n",
      "Training:: Epoch 242, Iteration 0, Current loss 0.09222251921892166 Accuracy 96.56013575620449\n",
      "Training:: Epoch 242, Iteration 10, Current loss 0.0902891755104065 Accuracy 96.77187690760888\n",
      "Training:: Epoch 242, Iteration 20, Current loss 0.03238867223262787 Accuracy 98.95330112721417\n",
      "Training:: Epoch 242, Iteration 30, Current loss 0.11069749295711517 Accuracy 97.18427268096143\n",
      "Training:: Epoch 242, Iteration 40, Current loss 0.039581138640642166 Accuracy 99.09221033599518\n",
      "Training:: Epoch 242, Iteration 50, Current loss 0.060765814036130905 Accuracy 98.13886568272534\n",
      "Training:: Epoch 242, Iteration 60, Current loss 0.040117476135492325 Accuracy 98.69666541991542\n",
      "Training:: Epoch 242, Iteration 70, Current loss 0.06347599625587463 Accuracy 97.81266609428022\n",
      "Training:: Epoch 242, Iteration 80, Current loss 0.029015053063631058 Accuracy 99.07395923767001\n",
      "Training:: Epoch 242, Iteration 90, Current loss 0.03050909750163555 Accuracy 98.98646417959722\n",
      "Training:: Epoch 242, Iteration 100, Current loss 0.03945520147681236 Accuracy 98.63725107379929\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 242, Probability Accuracy 48.055164270621866\n",
      "Starting Training\n",
      "Training:: Epoch 243, Iteration 0, Current loss 0.03375621885061264 Accuracy 98.89817317932334\n",
      "Training:: Epoch 243, Iteration 10, Current loss 0.027246013283729553 Accuracy 98.99293828676696\n",
      "Training:: Epoch 243, Iteration 20, Current loss 0.041659921407699585 Accuracy 98.55450371294144\n",
      "Training:: Epoch 243, Iteration 30, Current loss 0.084865041077137 Accuracy 97.69140651919234\n",
      "Training:: Epoch 243, Iteration 40, Current loss 0.04591427743434906 Accuracy 98.41372351160445\n",
      "Training:: Epoch 243, Iteration 50, Current loss 0.026702603325247765 Accuracy 99.16799433528058\n",
      "Training:: Epoch 243, Iteration 60, Current loss 0.022198284044861794 Accuracy 99.37147516477543\n",
      "Training:: Epoch 243, Iteration 70, Current loss 0.03495736047625542 Accuracy 98.75835268470689\n",
      "Training:: Epoch 243, Iteration 80, Current loss 0.03863190487027168 Accuracy 98.69865995839582\n",
      "Training:: Epoch 243, Iteration 90, Current loss 0.05018070712685585 Accuracy 98.18002628120894\n",
      "Training:: Epoch 243, Iteration 100, Current loss 0.028035474941134453 Accuracy 99.15440700634194\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 243, Probability Accuracy 48.16889008576045\n",
      "Starting Training\n",
      "Training:: Epoch 244, Iteration 0, Current loss 0.02822432667016983 Accuracy 99.16265746079547\n",
      "Training:: Epoch 244, Iteration 10, Current loss 0.10891999304294586 Accuracy 96.08900876601483\n",
      "Training:: Epoch 244, Iteration 20, Current loss 0.05100415647029877 Accuracy 98.36644014636697\n",
      "Training:: Epoch 244, Iteration 30, Current loss 0.04574942961335182 Accuracy 98.49664040218495\n",
      "Training:: Epoch 244, Iteration 40, Current loss 0.031143052503466606 Accuracy 99.0371040723982\n",
      "Training:: Epoch 244, Iteration 50, Current loss 0.045737072825431824 Accuracy 98.51722964027688\n",
      "Training:: Epoch 244, Iteration 60, Current loss 0.05899430066347122 Accuracy 97.8257820171132\n",
      "Training:: Epoch 244, Iteration 70, Current loss 0.03472059220075607 Accuracy 98.98081534772182\n",
      "Training:: Epoch 244, Iteration 80, Current loss 0.05046261474490166 Accuracy 98.46100238109065\n",
      "Training:: Epoch 244, Iteration 90, Current loss 0.028440916910767555 Accuracy 99.12300597263174\n",
      "Training:: Epoch 244, Iteration 100, Current loss 0.038942851126194 Accuracy 98.68110050108726\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 244, Probability Accuracy 49.53981439284087\n",
      "Starting Training\n",
      "Training:: Epoch 245, Iteration 0, Current loss 0.0513395220041275 Accuracy 98.41722808978848\n",
      "Training:: Epoch 245, Iteration 10, Current loss 0.05884147062897682 Accuracy 98.25974318540212\n",
      "Training:: Epoch 245, Iteration 20, Current loss 0.02537870965898037 Accuracy 99.08833456181867\n",
      "Training:: Epoch 245, Iteration 30, Current loss 0.04559013620018959 Accuracy 98.72128818375562\n",
      "Training:: Epoch 245, Iteration 40, Current loss 0.020593639463186264 Accuracy 99.33108296429346\n",
      "Training:: Epoch 245, Iteration 50, Current loss 0.021851275116205215 Accuracy 99.37679932260797\n",
      "Training:: Epoch 245, Iteration 60, Current loss 0.01801243983209133 Accuracy 99.45874967973354\n",
      "Training:: Epoch 245, Iteration 70, Current loss 0.024758711457252502 Accuracy 99.16202342824552\n",
      "Training:: Epoch 245, Iteration 80, Current loss 0.02890044078230858 Accuracy 99.0711580625872\n",
      "Training:: Epoch 245, Iteration 90, Current loss 0.0278554018586874 Accuracy 99.05154488746216\n",
      "Training:: Epoch 245, Iteration 100, Current loss 0.07426674664020538 Accuracy 97.9951232728258\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 245, Probability Accuracy 49.70077060115176\n",
      "Starting Training\n",
      "Training:: Epoch 246, Iteration 0, Current loss 0.019504830241203308 Accuracy 99.32830930537352\n",
      "Training:: Epoch 246, Iteration 10, Current loss 0.02790815569460392 Accuracy 99.22706235984542\n",
      "Training:: Epoch 246, Iteration 20, Current loss 0.04819142073392868 Accuracy 98.42216853212366\n",
      "Training:: Epoch 246, Iteration 30, Current loss 0.022324131801724434 Accuracy 99.34607504700962\n",
      "Training:: Epoch 246, Iteration 40, Current loss 0.0680488795042038 Accuracy 97.92834267413932\n",
      "Training:: Epoch 246, Iteration 50, Current loss 0.03944557160139084 Accuracy 98.92483628188837\n",
      "Training:: Epoch 246, Iteration 60, Current loss 0.03412872180342674 Accuracy 98.9016373361643\n",
      "Training:: Epoch 246, Iteration 70, Current loss 0.030425047501921654 Accuracy 98.96606901079063\n",
      "Training:: Epoch 246, Iteration 80, Current loss 0.034527044743299484 Accuracy 98.89271219340038\n",
      "Training:: Epoch 246, Iteration 90, Current loss 0.03381054103374481 Accuracy 98.76880238049739\n",
      "Training:: Epoch 246, Iteration 100, Current loss 0.019958045333623886 Accuracy 99.36240665069748\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 246, Probability Accuracy 49.63624311223433\n",
      "Starting Training\n",
      "Training:: Epoch 247, Iteration 0, Current loss 0.053256284445524216 Accuracy 98.13414331762955\n",
      "Training:: Epoch 247, Iteration 10, Current loss 0.034730106592178345 Accuracy 99.07231208372978\n",
      "Training:: Epoch 247, Iteration 20, Current loss 0.052671320736408234 Accuracy 98.00721075057358\n",
      "Training:: Epoch 247, Iteration 30, Current loss 0.02136177010834217 Accuracy 99.34009578038388\n",
      "Training:: Epoch 247, Iteration 40, Current loss 0.03648630902171135 Accuracy 98.74711168164313\n",
      "Training:: Epoch 247, Iteration 50, Current loss 0.03603695333003998 Accuracy 98.84350990699555\n",
      "Training:: Epoch 247, Iteration 60, Current loss 0.02054668962955475 Accuracy 99.35928239628383\n",
      "Training:: Epoch 247, Iteration 70, Current loss 0.02532515674829483 Accuracy 99.16087583584634\n",
      "Training:: Epoch 247, Iteration 80, Current loss 0.030953239649534225 Accuracy 98.84919993534831\n",
      "Training:: Epoch 247, Iteration 90, Current loss 0.024591093882918358 Accuracy 99.29955638571096\n",
      "Training:: Epoch 247, Iteration 100, Current loss 0.02488601580262184 Accuracy 99.1702621971457\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 247, Probability Accuracy 49.48512656916767\n",
      "Starting Training\n",
      "Training:: Epoch 248, Iteration 0, Current loss 0.03275149315595627 Accuracy 99.02209028202218\n",
      "Training:: Epoch 248, Iteration 10, Current loss 0.04487135633826256 Accuracy 98.59306196971325\n",
      "Training:: Epoch 248, Iteration 20, Current loss 0.0626414567232132 Accuracy 97.8407333775127\n",
      "Training:: Epoch 248, Iteration 30, Current loss 0.06811780482530594 Accuracy 98.68231696348263\n",
      "Training:: Epoch 248, Iteration 40, Current loss 0.030025871470570564 Accuracy 99.14953676896351\n",
      "Training:: Epoch 248, Iteration 50, Current loss 0.035303641110658646 Accuracy 98.9226395331438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 248, Iteration 60, Current loss 0.03779599815607071 Accuracy 98.80171543895055\n",
      "Training:: Epoch 248, Iteration 70, Current loss 0.04672352969646454 Accuracy 98.36335805957847\n",
      "Training:: Epoch 248, Iteration 80, Current loss 0.029621263965964317 Accuracy 99.10557966050088\n",
      "Training:: Epoch 248, Iteration 90, Current loss 0.027425691485404968 Accuracy 99.07226375311481\n",
      "Training:: Epoch 248, Iteration 100, Current loss 0.01843198947608471 Accuracy 99.43536228456558\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 248, Probability Accuracy 50.0013464805071\n",
      "Starting Training\n",
      "Training:: Epoch 249, Iteration 0, Current loss 0.023439627140760422 Accuracy 99.30364321210395\n",
      "Training:: Epoch 249, Iteration 10, Current loss 0.024788841605186462 Accuracy 99.170963903213\n",
      "Training:: Epoch 249, Iteration 20, Current loss 0.018885644152760506 Accuracy 99.39015093764293\n",
      "Training:: Epoch 249, Iteration 30, Current loss 0.029787681996822357 Accuracy 99.0461891724808\n",
      "Training:: Epoch 249, Iteration 40, Current loss 0.03346013277769089 Accuracy 98.8762030007771\n",
      "Training:: Epoch 249, Iteration 50, Current loss 0.029062913730740547 Accuracy 99.22315345782484\n",
      "Training:: Epoch 249, Iteration 60, Current loss 0.020033523440361023 Accuracy 99.3270332187858\n",
      "Training:: Epoch 249, Iteration 70, Current loss 0.025571834295988083 Accuracy 99.12227074235808\n",
      "Training:: Epoch 249, Iteration 80, Current loss 0.04233694076538086 Accuracy 98.89866266180361\n",
      "Training:: Epoch 249, Iteration 90, Current loss 0.027324102818965912 Accuracy 99.11253364370408\n",
      "Training:: Epoch 249, Iteration 100, Current loss 0.039774417877197266 Accuracy 98.9403520765681\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 249, Probability Accuracy 49.82029664001326\n",
      "Starting Training\n",
      "Training:: Epoch 250, Iteration 0, Current loss 0.019003422930836678 Accuracy 99.38111399480934\n",
      "Training:: Epoch 250, Iteration 10, Current loss 0.035786017775535583 Accuracy 98.8515947173357\n",
      "Training:: Epoch 250, Iteration 20, Current loss 0.02992728352546692 Accuracy 99.00560004186947\n",
      "Training:: Epoch 250, Iteration 30, Current loss 0.028023645281791687 Accuracy 99.05998209489705\n",
      "Training:: Epoch 250, Iteration 40, Current loss 0.038506973534822464 Accuracy 98.70743389541369\n",
      "Training:: Epoch 250, Iteration 50, Current loss 0.040596794337034225 Accuracy 98.39895635673625\n",
      "Training:: Epoch 250, Iteration 60, Current loss 0.023306148126721382 Accuracy 99.20195250455197\n",
      "Training:: Epoch 250, Iteration 70, Current loss 0.08742543309926987 Accuracy 96.41457040182947\n",
      "Training:: Epoch 250, Iteration 80, Current loss 0.037557438015937805 Accuracy 98.9417452750448\n",
      "Training:: Epoch 250, Iteration 90, Current loss 0.0905228927731514 Accuracy 96.5803059726235\n",
      "Training:: Epoch 250, Iteration 100, Current loss 0.07927223294973373 Accuracy 97.26131608976797\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 250, Probability Accuracy 48.93420889091436\n",
      "Starting Training\n",
      "Training:: Epoch 251, Iteration 0, Current loss 0.03238355740904808 Accuracy 98.8179113824957\n",
      "Training:: Epoch 251, Iteration 10, Current loss 0.13023027777671814 Accuracy 95.59180074939387\n",
      "Training:: Epoch 251, Iteration 20, Current loss 0.03029000572860241 Accuracy 99.12478915375068\n",
      "Training:: Epoch 251, Iteration 30, Current loss 0.22408507764339447 Accuracy 92.39813174424224\n",
      "Training:: Epoch 251, Iteration 40, Current loss 0.06556479632854462 Accuracy 97.63702801461632\n",
      "Training:: Epoch 251, Iteration 50, Current loss 0.09537830948829651 Accuracy 96.29979623778165\n",
      "Training:: Epoch 251, Iteration 60, Current loss 0.3620835542678833 Accuracy 93.87989656163202\n",
      "Training:: Epoch 251, Iteration 70, Current loss 0.05798511579632759 Accuracy 97.91903266751287\n",
      "Training:: Epoch 251, Iteration 80, Current loss 0.03205287456512451 Accuracy 98.96217037946559\n",
      "Training:: Epoch 251, Iteration 90, Current loss 0.05198382958769798 Accuracy 98.36524988323214\n",
      "Training:: Epoch 251, Iteration 100, Current loss 0.16165311634540558 Accuracy 94.63201983075477\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 251, Probability Accuracy 43.896818162986285\n",
      "Starting Training\n",
      "Training:: Epoch 252, Iteration 0, Current loss 0.0571659579873085 Accuracy 98.36258524595839\n",
      "Training:: Epoch 252, Iteration 10, Current loss 0.1092110201716423 Accuracy 97.14705628079882\n",
      "Training:: Epoch 252, Iteration 20, Current loss 0.06654421240091324 Accuracy 97.8468899521531\n",
      "Training:: Epoch 252, Iteration 30, Current loss 0.05644778907299042 Accuracy 98.20296771307136\n",
      "Training:: Epoch 252, Iteration 40, Current loss 0.209791898727417 Accuracy 94.00218102508178\n",
      "Training:: Epoch 252, Iteration 50, Current loss 0.04677128419280052 Accuracy 98.40940883892026\n",
      "Training:: Epoch 252, Iteration 60, Current loss 0.059004705399274826 Accuracy 97.78715902764891\n",
      "Training:: Epoch 252, Iteration 70, Current loss 0.027417659759521484 Accuracy 99.04193920738746\n",
      "Training:: Epoch 252, Iteration 80, Current loss 0.044430699199438095 Accuracy 98.63636363636364\n",
      "Training:: Epoch 252, Iteration 90, Current loss 0.026683801785111427 Accuracy 99.17679716665072\n",
      "Training:: Epoch 252, Iteration 100, Current loss 0.025550130754709244 Accuracy 99.12791899152718\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 252, Probability Accuracy 48.679931225918715\n",
      "Starting Training\n",
      "Training:: Epoch 253, Iteration 0, Current loss 0.04618735983967781 Accuracy 98.63083164300203\n",
      "Training:: Epoch 253, Iteration 10, Current loss 0.07388951629400253 Accuracy 97.82172701949861\n",
      "Training:: Epoch 253, Iteration 20, Current loss 0.08933088183403015 Accuracy 97.05989488473747\n",
      "Training:: Epoch 253, Iteration 30, Current loss 0.3227771520614624 Accuracy 93.6446505677275\n",
      "Training:: Epoch 253, Iteration 40, Current loss 0.04749632254242897 Accuracy 98.60544947436172\n",
      "Training:: Epoch 253, Iteration 50, Current loss 0.03495195508003235 Accuracy 98.93115519253209\n",
      "Training:: Epoch 253, Iteration 60, Current loss 0.04869381710886955 Accuracy 98.48495831823125\n",
      "Training:: Epoch 253, Iteration 70, Current loss 0.14932413399219513 Accuracy 96.36840432294977\n",
      "Training:: Epoch 253, Iteration 80, Current loss 0.034173160791397095 Accuracy 99.02026523617941\n",
      "Training:: Epoch 253, Iteration 90, Current loss 0.03574054688215256 Accuracy 98.89164167427305\n",
      "Training:: Epoch 253, Iteration 100, Current loss 0.05613977834582329 Accuracy 98.23483116591088\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 253, Probability Accuracy 50.10999709988814\n",
      "Starting Training\n",
      "Training:: Epoch 254, Iteration 0, Current loss 0.04563535377383232 Accuracy 98.89253486464315\n",
      "Training:: Epoch 254, Iteration 10, Current loss 0.02812604233622551 Accuracy 98.98176814585483\n",
      "Training:: Epoch 254, Iteration 20, Current loss 0.03268982470035553 Accuracy 99.0059312767156\n",
      "Training:: Epoch 254, Iteration 30, Current loss 0.03492113947868347 Accuracy 98.73575433324783\n",
      "Training:: Epoch 254, Iteration 40, Current loss 0.08594027906656265 Accuracy 96.56690771103315\n",
      "Training:: Epoch 254, Iteration 50, Current loss 0.03522942215204239 Accuracy 98.80921848092918\n",
      "Training:: Epoch 254, Iteration 60, Current loss 0.032861992716789246 Accuracy 98.93040072310937\n",
      "Training:: Epoch 254, Iteration 70, Current loss 0.043242089450359344 Accuracy 98.48428835489834\n",
      "Training:: Epoch 254, Iteration 80, Current loss 0.02687995694577694 Accuracy 99.10459616564971\n",
      "Training:: Epoch 254, Iteration 90, Current loss 0.02570471726357937 Accuracy 99.23690734428261\n",
      "Training:: Epoch 254, Iteration 100, Current loss 0.028976086527109146 Accuracy 99.09449475029164\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 254, Probability Accuracy 48.122902597671626\n",
      "Starting Training\n",
      "Training:: Epoch 255, Iteration 0, Current loss 0.028881175443530083 Accuracy 99.02125033065867\n",
      "Training:: Epoch 255, Iteration 10, Current loss 0.033518701791763306 Accuracy 98.9527490337863\n",
      "Training:: Epoch 255, Iteration 20, Current loss 0.05342870578169823 Accuracy 98.2209469153515\n",
      "Training:: Epoch 255, Iteration 30, Current loss 0.03633267804980278 Accuracy 98.82317135030942\n",
      "Training:: Epoch 255, Iteration 40, Current loss 0.036425936967134476 Accuracy 98.65989098807425\n",
      "Training:: Epoch 255, Iteration 50, Current loss 0.06066958233714104 Accuracy 98.22844524741544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 255, Iteration 60, Current loss 0.044714558869600296 Accuracy 98.59046642747309\n",
      "Training:: Epoch 255, Iteration 70, Current loss 0.024724287912249565 Accuracy 99.24205470714044\n",
      "Training:: Epoch 255, Iteration 80, Current loss 0.03063756786286831 Accuracy 98.97333457214532\n",
      "Training:: Epoch 255, Iteration 90, Current loss 0.025101764127612114 Accuracy 99.15302318041822\n",
      "Training:: Epoch 255, Iteration 100, Current loss 0.035826537758111954 Accuracy 98.8841550010144\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 255, Probability Accuracy 48.46211211003853\n",
      "Starting Training\n",
      "Training:: Epoch 256, Iteration 0, Current loss 0.020857099443674088 Accuracy 99.38658716287996\n",
      "Training:: Epoch 256, Iteration 10, Current loss 0.04665246605873108 Accuracy 98.56594627348011\n",
      "Training:: Epoch 256, Iteration 20, Current loss 0.02577878348529339 Accuracy 99.12163206429344\n",
      "Training:: Epoch 256, Iteration 30, Current loss 0.048870112746953964 Accuracy 98.68773732410096\n",
      "Training:: Epoch 256, Iteration 40, Current loss 0.018331559374928474 Accuracy 99.38607710579207\n",
      "Training:: Epoch 256, Iteration 50, Current loss 0.03136587515473366 Accuracy 99.06971467468377\n",
      "Training:: Epoch 256, Iteration 60, Current loss 0.02907727286219597 Accuracy 99.0053749937208\n",
      "Training:: Epoch 256, Iteration 70, Current loss 0.021694688126444817 Accuracy 99.36009307737056\n",
      "Training:: Epoch 256, Iteration 80, Current loss 0.015162435360252857 Accuracy 99.51857188038093\n",
      "Training:: Epoch 256, Iteration 90, Current loss 0.026983970776200294 Accuracy 99.00654900654901\n",
      "Training:: Epoch 256, Iteration 100, Current loss 0.03588569909334183 Accuracy 98.83251010327795\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 256, Probability Accuracy 49.13431660935493\n",
      "Starting Training\n",
      "Training:: Epoch 257, Iteration 0, Current loss 0.01949046552181244 Accuracy 99.40121833033366\n",
      "Training:: Epoch 257, Iteration 10, Current loss 0.023162640631198883 Accuracy 99.25120599035208\n",
      "Training:: Epoch 257, Iteration 20, Current loss 0.028431808575987816 Accuracy 99.13850231941683\n",
      "Training:: Epoch 257, Iteration 30, Current loss 0.028350315988063812 Accuracy 99.08831512283388\n",
      "Training:: Epoch 257, Iteration 40, Current loss 0.04055515304207802 Accuracy 98.82697947214076\n",
      "Training:: Epoch 257, Iteration 50, Current loss 0.021182266995310783 Accuracy 99.36659816043934\n",
      "Training:: Epoch 257, Iteration 60, Current loss 0.016512125730514526 Accuracy 99.4536667367094\n",
      "Training:: Epoch 257, Iteration 70, Current loss 0.01715667173266411 Accuracy 99.44494449444944\n",
      "Training:: Epoch 257, Iteration 80, Current loss 0.018443919718265533 Accuracy 99.3808500564519\n",
      "Training:: Epoch 257, Iteration 90, Current loss 0.01592528447508812 Accuracy 99.55609575648681\n",
      "Training:: Epoch 257, Iteration 100, Current loss 0.026756880804896355 Accuracy 99.2506478044681\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 257, Probability Accuracy 49.25746778804325\n",
      "Starting Training\n",
      "Training:: Epoch 258, Iteration 0, Current loss 0.02631605602800846 Accuracy 99.18560780344161\n",
      "Training:: Epoch 258, Iteration 10, Current loss 0.030434519052505493 Accuracy 98.84514644019367\n",
      "Training:: Epoch 258, Iteration 20, Current loss 0.02256469428539276 Accuracy 99.23501543008649\n",
      "Training:: Epoch 258, Iteration 30, Current loss 0.02297927252948284 Accuracy 99.29581138177456\n",
      "Training:: Epoch 258, Iteration 40, Current loss 0.030526550486683846 Accuracy 98.98224187548256\n",
      "Training:: Epoch 258, Iteration 50, Current loss 0.026532720774412155 Accuracy 99.12337662337663\n",
      "Training:: Epoch 258, Iteration 60, Current loss 0.013859420083463192 Accuracy 99.56122654831552\n",
      "Training:: Epoch 258, Iteration 70, Current loss 0.01952015794813633 Accuracy 99.35542236797467\n",
      "Training:: Epoch 258, Iteration 80, Current loss 0.016273565590381622 Accuracy 99.48467588825604\n",
      "Training:: Epoch 258, Iteration 90, Current loss 0.019487565383315086 Accuracy 99.34747145187602\n",
      "Training:: Epoch 258, Iteration 100, Current loss 0.01899634301662445 Accuracy 99.34322126049324\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 258, Probability Accuracy 49.18030409744376\n",
      "Starting Training\n",
      "Training:: Epoch 259, Iteration 0, Current loss 0.023296495899558067 Accuracy 99.19844547000243\n",
      "Training:: Epoch 259, Iteration 10, Current loss 0.018946219235658646 Accuracy 99.37063960579597\n",
      "Training:: Epoch 259, Iteration 20, Current loss 0.026537925004959106 Accuracy 99.19816207937295\n",
      "Training:: Epoch 259, Iteration 30, Current loss 0.02729063108563423 Accuracy 99.35113994933559\n",
      "Training:: Epoch 259, Iteration 40, Current loss 0.023935332894325256 Accuracy 99.24946567920871\n",
      "Training:: Epoch 259, Iteration 50, Current loss 0.01871592178940773 Accuracy 99.38488129734127\n",
      "Training:: Epoch 259, Iteration 60, Current loss 0.018714141100645065 Accuracy 99.3369663941871\n",
      "Training:: Epoch 259, Iteration 70, Current loss 0.018921023234725 Accuracy 99.3505735951779\n",
      "Training:: Epoch 259, Iteration 80, Current loss 0.02554774470627308 Accuracy 99.11600707194343\n",
      "Training:: Epoch 259, Iteration 90, Current loss 0.02129412814974785 Accuracy 99.31495263870094\n",
      "Training:: Epoch 259, Iteration 100, Current loss 0.030977614223957062 Accuracy 98.98063200815494\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 259, Probability Accuracy 49.266375274474875\n",
      "Starting Training\n",
      "Training:: Epoch 260, Iteration 0, Current loss 0.013731538318097591 Accuracy 99.58707147525399\n",
      "Training:: Epoch 260, Iteration 10, Current loss 0.017759496346116066 Accuracy 99.48415334714134\n",
      "Training:: Epoch 260, Iteration 20, Current loss 0.019055407494306564 Accuracy 99.41377625793845\n",
      "Training:: Epoch 260, Iteration 30, Current loss 0.024861732497811317 Accuracy 99.25233644859813\n",
      "Training:: Epoch 260, Iteration 40, Current loss 0.01328655518591404 Accuracy 99.6492546661656\n",
      "Training:: Epoch 260, Iteration 50, Current loss 0.019678909331560135 Accuracy 99.35324177579048\n",
      "Training:: Epoch 260, Iteration 60, Current loss 0.019024211913347244 Accuracy 99.3642447418738\n",
      "Training:: Epoch 260, Iteration 70, Current loss 0.01556063536554575 Accuracy 99.4878446240314\n",
      "Training:: Epoch 260, Iteration 80, Current loss 0.015581177547574043 Accuracy 99.46517526970447\n",
      "Training:: Epoch 260, Iteration 90, Current loss 0.025710878893733025 Accuracy 99.2149911369967\n",
      "Training:: Epoch 260, Iteration 100, Current loss 0.017316658049821854 Accuracy 99.48402948402948\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 260, Probability Accuracy 49.13131292206985\n",
      "Starting Training\n",
      "Training:: Epoch 261, Iteration 0, Current loss 0.018991755321621895 Accuracy 99.3846482179697\n",
      "Training:: Epoch 261, Iteration 10, Current loss 0.018337711691856384 Accuracy 99.42859426196755\n",
      "Training:: Epoch 261, Iteration 20, Current loss 0.01839350536465645 Accuracy 99.35954600729632\n",
      "Training:: Epoch 261, Iteration 30, Current loss 0.01913544163107872 Accuracy 99.3332147937411\n",
      "Training:: Epoch 261, Iteration 40, Current loss 0.044026173651218414 Accuracy 98.67824773413898\n",
      "Training:: Epoch 261, Iteration 50, Current loss 0.03177894279360771 Accuracy 99.16176704274591\n",
      "Training:: Epoch 261, Iteration 60, Current loss 0.01851285994052887 Accuracy 99.38192246835443\n",
      "Training:: Epoch 261, Iteration 70, Current loss 0.013835525140166283 Accuracy 99.54103122730574\n",
      "Training:: Epoch 261, Iteration 80, Current loss 0.01940157264471054 Accuracy 99.34870134996447\n",
      "Training:: Epoch 261, Iteration 90, Current loss 0.014034328050911427 Accuracy 99.556151940545\n",
      "Training:: Epoch 261, Iteration 100, Current loss 0.014218802563846111 Accuracy 99.49720670391062\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 261, Probability Accuracy 49.03063761030783\n",
      "Starting Training\n",
      "Training:: Epoch 262, Iteration 0, Current loss 0.019406337291002274 Accuracy 99.32075798217531\n",
      "Training:: Epoch 262, Iteration 10, Current loss 0.04217195510864258 Accuracy 98.68799550971725\n",
      "Training:: Epoch 262, Iteration 20, Current loss 0.03421958163380623 Accuracy 98.80884606962995\n",
      "Training:: Epoch 262, Iteration 30, Current loss 0.01849021017551422 Accuracy 99.36861716809274\n",
      "Training:: Epoch 262, Iteration 40, Current loss 0.02218792214989662 Accuracy 99.23158820534185\n",
      "Training:: Epoch 262, Iteration 50, Current loss 0.03658725321292877 Accuracy 98.82109355193104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 262, Iteration 60, Current loss 0.014790685847401619 Accuracy 99.55445426604084\n",
      "Training:: Epoch 262, Iteration 70, Current loss 0.02875981107354164 Accuracy 99.12686183872624\n",
      "Training:: Epoch 262, Iteration 80, Current loss 0.021789273247122765 Accuracy 99.28134839785146\n",
      "Training:: Epoch 262, Iteration 90, Current loss 0.020151833072304726 Accuracy 99.36835456363828\n",
      "Training:: Epoch 262, Iteration 100, Current loss 0.023037167266011238 Accuracy 99.28240922280605\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 262, Probability Accuracy 49.83655798152214\n",
      "Starting Training\n",
      "Training:: Epoch 263, Iteration 0, Current loss 0.03847755491733551 Accuracy 98.82818331094194\n",
      "Training:: Epoch 263, Iteration 10, Current loss 0.02967071533203125 Accuracy 98.9873417721519\n",
      "Training:: Epoch 263, Iteration 20, Current loss 0.04250745102763176 Accuracy 98.58509712960279\n",
      "Training:: Epoch 263, Iteration 30, Current loss 0.026263438165187836 Accuracy 99.15958913246476\n",
      "Training:: Epoch 263, Iteration 40, Current loss 0.020409582182765007 Accuracy 99.3804718801009\n",
      "Training:: Epoch 263, Iteration 50, Current loss 0.02930288389325142 Accuracy 99.10699241786016\n",
      "Training:: Epoch 263, Iteration 60, Current loss 0.01900370419025421 Accuracy 99.35740104422331\n",
      "Training:: Epoch 263, Iteration 70, Current loss 0.022117774933576584 Accuracy 99.2375982651125\n",
      "Training:: Epoch 263, Iteration 80, Current loss 0.021676935255527496 Accuracy 99.24857985187316\n",
      "Training:: Epoch 263, Iteration 90, Current loss 0.01834219880402088 Accuracy 99.40828402366864\n",
      "Training:: Epoch 263, Iteration 100, Current loss 0.01812518760561943 Accuracy 99.394111906311\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 263, Probability Accuracy 48.31358495256246\n",
      "Starting Training\n",
      "Training:: Epoch 264, Iteration 0, Current loss 0.021168019622564316 Accuracy 99.3680466207523\n",
      "Training:: Epoch 264, Iteration 10, Current loss 0.025940842926502228 Accuracy 99.16822027821597\n",
      "Training:: Epoch 264, Iteration 20, Current loss 0.027018707245588303 Accuracy 99.11294639443297\n",
      "Training:: Epoch 264, Iteration 30, Current loss 0.016890699043869972 Accuracy 99.47553880193395\n",
      "Training:: Epoch 264, Iteration 40, Current loss 0.023771319538354874 Accuracy 99.25032313657906\n",
      "Training:: Epoch 264, Iteration 50, Current loss 0.023262333124876022 Accuracy 99.29213177239313\n",
      "Training:: Epoch 264, Iteration 60, Current loss 0.03356033191084862 Accuracy 98.86846774463527\n",
      "Training:: Epoch 264, Iteration 70, Current loss 0.018114794045686722 Accuracy 99.41240942580528\n",
      "Training:: Epoch 264, Iteration 80, Current loss 0.017752621322870255 Accuracy 99.43777369684983\n",
      "Training:: Epoch 264, Iteration 90, Current loss 0.01899639144539833 Accuracy 99.42461964038728\n",
      "Training:: Epoch 264, Iteration 100, Current loss 0.013395274057984352 Accuracy 99.57336408186798\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 264, Probability Accuracy 49.58331607076273\n",
      "Starting Training\n",
      "Training:: Epoch 265, Iteration 0, Current loss 0.02537488006055355 Accuracy 99.24763214176596\n",
      "Training:: Epoch 265, Iteration 10, Current loss 0.019544359296560287 Accuracy 99.37746071272336\n",
      "Training:: Epoch 265, Iteration 20, Current loss 0.02258184365928173 Accuracy 99.22984948656632\n",
      "Training:: Epoch 265, Iteration 30, Current loss 0.02291606366634369 Accuracy 99.33690882496343\n",
      "Training:: Epoch 265, Iteration 40, Current loss 0.02272948995232582 Accuracy 99.27976501830764\n",
      "Training:: Epoch 265, Iteration 50, Current loss 0.020039496943354607 Accuracy 99.41133779533908\n",
      "Training:: Epoch 265, Iteration 60, Current loss 0.04421176016330719 Accuracy 98.76962737286149\n",
      "Training:: Epoch 265, Iteration 70, Current loss 0.02098754234611988 Accuracy 99.33641912096014\n",
      "Training:: Epoch 265, Iteration 80, Current loss 0.02239915169775486 Accuracy 99.28526645768025\n",
      "Training:: Epoch 265, Iteration 90, Current loss 0.035995062440633774 Accuracy 98.89576817514521\n",
      "Training:: Epoch 265, Iteration 100, Current loss 0.01763986051082611 Accuracy 99.47268134887224\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 265, Probability Accuracy 49.58051953432489\n",
      "Starting Training\n",
      "Training:: Epoch 266, Iteration 0, Current loss 0.01444237306714058 Accuracy 99.48624053780777\n",
      "Training:: Epoch 266, Iteration 10, Current loss 0.04146837443113327 Accuracy 98.82033403599806\n",
      "Training:: Epoch 266, Iteration 20, Current loss 0.023213522508740425 Accuracy 99.38627371569208\n",
      "Training:: Epoch 266, Iteration 30, Current loss 0.022173602133989334 Accuracy 99.25942449252733\n",
      "Training:: Epoch 266, Iteration 40, Current loss 0.02028566040098667 Accuracy 99.35046292973516\n",
      "Training:: Epoch 266, Iteration 50, Current loss 0.016882462427020073 Accuracy 99.48496472544865\n",
      "Training:: Epoch 266, Iteration 60, Current loss 0.022581622004508972 Accuracy 99.20716250471926\n",
      "Training:: Epoch 266, Iteration 70, Current loss 0.021161505952477455 Accuracy 99.3692036195697\n",
      "Training:: Epoch 266, Iteration 80, Current loss 0.03567126393318176 Accuracy 98.77595628415301\n",
      "Training:: Epoch 266, Iteration 90, Current loss 0.019570473581552505 Accuracy 99.33658022903778\n",
      "Training:: Epoch 266, Iteration 100, Current loss 0.019238902255892754 Accuracy 99.37576182912865\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 266, Probability Accuracy 49.63727886647057\n",
      "Starting Training\n",
      "Training:: Epoch 267, Iteration 0, Current loss 0.039935316890478134 Accuracy 98.89461036731892\n",
      "Training:: Epoch 267, Iteration 10, Current loss 0.0197752732783556 Accuracy 99.40911745508262\n",
      "Training:: Epoch 267, Iteration 20, Current loss 0.03407509624958038 Accuracy 99.0080738177624\n",
      "Training:: Epoch 267, Iteration 30, Current loss 0.02900109812617302 Accuracy 99.00490406132457\n",
      "Training:: Epoch 267, Iteration 40, Current loss 0.021872082725167274 Accuracy 99.28098489551003\n",
      "Training:: Epoch 267, Iteration 50, Current loss 0.03076229616999626 Accuracy 98.98373013979675\n",
      "Training:: Epoch 267, Iteration 60, Current loss 0.028334572911262512 Accuracy 99.0414981439415\n",
      "Training:: Epoch 267, Iteration 70, Current loss 0.015791943296790123 Accuracy 99.46476360392506\n",
      "Training:: Epoch 267, Iteration 80, Current loss 0.02414100617170334 Accuracy 99.23959386400598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initialize_epoch = 50\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(400):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y, y_list, x = model(item_0)\n",
    "        probs = torch.softmax(y, dim=1) # get_ensemble_out([y, y_list])\n",
    "#         logits = torch.log(probs + 1e-8)\n",
    "        \n",
    "        loss = 0\n",
    "        loss += ce_criterion(y, item_2)\n",
    "        loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(y[:, :, 1:], dim=1), \n",
    "                                                            F.log_softmax(y.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                        max=16) * src_mask_mse[:, :, 1:])\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(probs, dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "                \n",
    "    # Calculating Expectation Step\n",
    "    model.eval()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        \n",
    "        print(\"Calculating Validation Data Accuracy\")\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for i, item in enumerate(testloader):\n",
    "            with torch.no_grad():\n",
    "                item_0 = item[0].to(device)\n",
    "                item_1 = item[1].to(device)\n",
    "                item_2 = item[2].to(device)\n",
    "                src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "                src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "                y, y_list, x = model(item_0)\n",
    "                probs = torch.softmax(y, dim=1) # get_ensemble_out([y, y_list])\n",
    "                \n",
    "                pred = torch.argmax(probs, dim=1)\n",
    "                correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total += float(torch.sum(src_mask).item())\n",
    "        val_acc = correct * 100.0 / total\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(model.state_dict(), config.output_dir + \"c2f-tcn-emmax-best-model.wt\")\n",
    "        torch.save(model.state_dict(), config.output_dir + \"c2f-tcn-emmax-last-model.wt\")\n",
    "        print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-speed/ms-tcn-em.wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/\"\n",
    "                            f\"/results/em-maximize-mstcn-speed/ms-tcn-em.wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 16, Probability Accuracy 59.57357998094212\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "\n",
    "print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_labels(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            select_item = np.random.randint(start, i, 1)[0]\n",
    "            unique_ids.append(select_item)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    select_item = np.random.randint(start, len(labels_arr), 1)[0]\n",
    "    unique_ids.append(select_item)\n",
    "    return unique_ids\n",
    "# get_selected_labels(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            unique_ids.append(i - 1)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    unique_ids.append(len(labels_arr) - 1)\n",
    "    return unique_ids\n",
    "# get_boundary(np.array([2, 2, 2, 2, 3, 3, 4, 4, 4, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[5]\n",
    "    for i, count in enumerate(count_all):\n",
    "        video_id = video_ids[i]\n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_selected_labels(labels)\n",
    "\n",
    "        loaded_vidid_selected_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_boundary_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[4]\n",
    "    for i, count in enumerate(count_all):\n",
    "        \n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_boundary(labels)\n",
    "        video_id = video_ids[i]\n",
    "        video_id_boundary_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in video_id_boundary_frames.keys():\n",
    "    if len(video_id_boundary_frames[ele]) != len(loaded_vidid_selected_frames[ele + \".txt\"]):\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(video_id_boundary_frames, open(\"dump_dir/video_id_boundary_frames_dict.pkl\", \"wb\"))\n",
    "# pickle.dump(loaded_vidid_selected_frames, open(\"dump_dir/loaded_vidid_selected_frames_dict.pkl\", \"wb\"))\n",
    "pickle.dump(video_id_boundary_frames, open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "    \n",
    "    weights = [1, 1, 1, 1, 0, 0]\n",
    "    ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "    for i, outp_ele in enumerate(outp[1]):\n",
    "        upped_logit = F.upsample(outp_ele, size=outp[0].shape[-1], mode='linear', align_corners=True)\n",
    "        ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i + 1] / sum(weights)\n",
    "    \n",
    "    return ensemble_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/results/c2f-tcn-model/split2_c2ftcn_model.wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration done\n",
      "11 iteration done\n",
      "21 iteration done\n",
      "31 iteration done\n",
      "41 iteration done\n",
      "51 iteration done\n",
      "61 iteration done\n",
      "71 iteration done\n",
      "81 iteration done\n",
      "91 iteration done\n",
      "101 iteration done\n",
      "111 iteration done\n",
      "121 iteration done\n",
      "131 iteration done\n",
      "141 iteration done\n",
      "151 iteration done\n",
      "Train Boundary avergage error = 107.269\n",
      "Train From boundary avergage accuracy = 87.407\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        calculate_element_probb(prob, item_1, item[4], item_2)\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f'{i+1} iteration done')\n",
    "get_boundary_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    idx = 4\n",
    "\n",
    "    cur_vid_feat = torch.softmax(predictions[-1], dim=1).permute(0, 2, 1)[idx]\n",
    "    cur_vidid = item[4][idx]\n",
    "\n",
    "    selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "    \n",
    "    bound_list = video_id_boundary_frames[cur_vidid]\n",
    "    \n",
    "    cumsum_feat = torch.cumsum(cur_vid_feat, dim=0)\n",
    "    prob_each_segment = prob_vals_per_segment(selected_frames, cur_vid_feat, item_2[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min prob 0 = 7.953912266787591e-36\n",
      "Min prob 1 = 2.7495868628582206e-249\n",
      "Min prob 2 = 8.185175464823537e-201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 442)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ymax = 1\n",
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "\n",
    "plt.plot([0, item[1][idx].item()], [0, 0])\n",
    "\n",
    "for pivot in loaded_vidid_selected_frames[cur_vidid + \".txt\"]:\n",
    "    plt.plot([pivot, pivot], [0, ymax], '--')\n",
    "    \n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax])\n",
    "    \n",
    "all_prob = []\n",
    "for i, segment in enumerate(prob_each_segment):\n",
    "    segment = segment.cpu().numpy()\n",
    "    print(f\"Min prob {i} = {np.min(segment).item()}\")\n",
    "    xs = np.arange(loaded_vidid_selected_frames[cur_vidid + \".txt\"][i], \n",
    "                   loaded_vidid_selected_frames[cur_vidid + \".txt\"][i+1])\n",
    "    plt.plot(xs, segment, '*')\n",
    "\n",
    "    \n",
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "\n",
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))\n",
    "loss_arr = -loss_arr[0].detach().cpu().numpy()\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr/np.max(loss_arr), 'k+')\n",
    "\n",
    "plt.ylim([0, ymax])\n",
    "plt.xlim([0, item[1][idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1895, 48])\n"
     ]
    }
   ],
   "source": [
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "print(loss_prob.shape)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1387])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf5b8ecb00>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3987, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([75, 345, 1305, 1415, 1440, 1519],\n",
       " [73, 282, 455, 1312, 1434, 1458],\n",
       " tensor(1520, device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid], loaded_vidid_selected_frames[cur_vidid + \".txt\"], item_1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBAAAAEvCAYAAAD8cTIvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3wUdfrA8c/sbnolPSEJ6QRCC53QREXBgnfYK9iwnp7nneXUn95551nu7AVRLNg7oBQFpfcWQksjCem99y3z+yMkihDSdrOb5Hm/XryULTPfhGRn5pmnKKqqIoQQQgghhBBCCHE2GmsvQAghhBBCCCGEELZPAghCCCGEEEIIIYTokAQQhBBCCCGEEEII0SEJIAghhBBCCCGEEKJDEkAQQgghhBBCCCFEhySAIIQQQgghhBBCiA7prLFTHx8fNSwszBq7FkIIIYQQQgghRDv27dtXqqqq75mes0oAISwsjL1791pj10IIIYQQQgghhGiHoign2ntOShiEEEIIIYQQQgjRIQkgCCGEEEIIIYQQokMSQBBCCCGEEEIIIUSHJIAghBBCCCGEEEKIDkkAQQghhBBCCCGEEB2SAIIQQgghhBBCCCE6JAEEIYQQQgghhBBCdEgCCEIIIYQQQgghhOiQBBCEEEIIIYQQQgjRIQkgCCGEEEJ00e7McjJKaq29DCGEEKJX6ay9ACGEEEKIvkJVVd7enMGza5LRahRunDyEP58fjaezvbWXJoQQQlicBBCEEEIIITrBaFL5x/dHWLbjBBePCsTDyY5lO7JYnpjHR7dMYmSwh7WXKIQQQliUlDAIIYQQQnTCv1cdY9mOEyyaEcFr18TzzB9Hsuq+6TjoNDz8TRIGo8naSxRCCCEsSgIIQgghhBAdKK1t4uNdJ7hyXDB/v2gYGo0CwLBAd568NI6jBdUs23HCyqsUQgghLEsCCEIIIYQQHfhoxwmaDSbuPCfytOfmjghgZowvL65Lpai60QqrE0IIIXpHpwMIiqK8pyhKsaIoh8/w3F8VRVEVRfEx7/KEEEIIIayrUW/ko50nOH+YH5G+rqc9rygK/7wsDr3RxD9/OGqFFQohhBC9oysZCB8Ac37/oKIoIcBsINtMaxJC9KJGvZFDuVXWXoYQQtisb/bnUl7XzO3TI9p9zRBvF+4+J4pVSQXsz67oxdUJIYQQvafTAQRVVTcD5Wd46iXgIUA116KEEL2j2WDi1g/3cNkbW6moa7b2coQQwuaYTCpLt2QyKtiDieFeZ33tbdPD8XKx55X1ab20OiGEEKJ39agHgqIo84A8VVUPmmk9wprWPNLyR3Tac7uf47ndz1l7Gd1iMqk89PVBtqWXYVIht6KBwmeeofCZZ6y9tAFhy5epbPky1drL6Jc2fLCEDR8ssfYyxO9Ufn+cyu+PW3sZAKxZs4Y1a9Z06rU/HS0ko7SO26dHoCjKWV/r4qDjtunhbEotITGn0hxLNZvU1KdJTX3a2ssQQog+44m0XJ5Iy+3y+/7x/RH+8f2Rlr/0w+srXXffqCiKM/AYcEEnX78IWAQQGhra3d0KSyo8ZO0V9DnJ5cnWXkK3Pbc2meWJ+VwyKpAfkgrIq2zA/Vjf/Xr6mtKcWmsvod8qPpFh7SWIM2jOr7P2EtoUFhZ26nWNeiPPrE4m2s+VuSMCOvWem6aE8c7mDF5Zn8r7N0/syTLNqqb2mLWXIIQQfcrh2oZuve9ofvWvf+mH11c9yUCIBMKBg4qiZAHBwH5FUc54hFVVdYmqquNVVR3v6+vbg90KIXoqtaiGtzdncMPkUJ6aFwdAQVX3PiSFEKK/Wro1k+zyep68NA6dtnOnTK4OOm6bHsGGFNvLQhBCCCF6qtsBBFVVD6mq6qeqapiqqmFALjBWVdXOhfWFEFbT2jRxYUI43i72OOg05FdKAEEIIVoVVjXyxoZ0Lhjuz7Torg2ZWpAQhqezHf/7KQVVlRZRQggh+o+ujHH8DNgBDFUUJVdRlFsttywhhCUlF1bjoNMQ5u2MoigEeTqRXymzy4UQotVza5MxmFQev3h4l9/r6qDj/vOi2ZJWyhd7ciywOiGEEMI6Ot0DQVXVazt4PqzHqxFC9Irkwhpi/N3aUnKDPB3JlxIGIYQA4Ms9OXx3II97ZkUS6u3crW0smBLGuqNF/POHo0yJ9GaIt8tprzlRVsfKxHy+T8rH182Bl64ag5+7Y0+XL4QQQlhMj6YwCCH6pmMFNcQGuLX9PdDDSUoYhBAC+HZ/Lg9/m8SMGF/+dG50t7ej0Si8cOVotBqFB788iNH0aylDs8HEE8sPM/OFjfxvXSoeTnYcyK5k3uvbOJxXZY4vQwghhLAICSAIMcCU1DRRWttEbKB722NBnk4U1zRhklJdIcQA9v3BfP761UGmRHiz5MZxONppe7S9wZ5O/POyOPaeqODWD/ewIbmYgqoGrn1nJx/tPMEtU8PZ/si5fHVnAt/clYBWo3DF4u1sTy8101ckhBBCmFe3xzgKIfqmlMIaAIb9JgNhsKcjqgrNRhOOOokrCiEGnoKqBh76OolxQwbx7oLxPQ4etPrDmMHkVTTw/rYsbv5gD4oCjjotr18XzyWjgtpeNyzQneX3TOWKxdt5Zs0xvr93GoqimGUNQgghhLlIAEGIASa5sGU27dDflTAANBuMEkAQQgxIL6xNwaiqvHjVGJztzXd6pCgK954bzaIZkfySXMzOjDKumRhCbID7aa/1dXPgjhmR/P27Q+zOLGdShLfZ1iGEEEKYg1wpCDHAHCuowc/NAW9Xh7bHgjxbAwgmay1LCCGsJjGnkm8P5HH79HBCvLrXNLEj9joNc0YE8NS8uDMGD1rNHzuYQc52vLs10yLrEEIIIXpCAghCDDDJhdWn9D+AlikMAE0SQBBCDDCqqvLP74/g6+bAXedEWXs5ONppuXHyENYfKyKztM7ayxFCCCFOIQEEIQYQg9FEWlHtKf0PAJztdXg620kGghBiwPkhqYD92ZX87cKhuDrYRmXnDVOGYKfR8P42yUIQQghhWySAIMQAkllaR7PRRGyg22nPBXk4SQaCEGLA2ZxagreLPVeMDbb2Utr4uTly2ZggvtqbS2V9s7WXI4QQQrSRAIIQA8ixkxMYzlR/G+TpJBkIQogBx2hScXbQotHY1sSDhVPDaNAbWXkw39pLEUIIIdpIAEGIASS5oBqdRiHS1/W054I8HWkyGK2wKiGEsB6jqqK1wXGJcUEexPi78r0EEIQQQtgQCSAIMYAkF9YQ5eeK/RlGNQZ5OmE0qRhMqhVWJoQQ1mE0qTaXfdDq0lFB7MmqIL+ywdpLEUIIIQAJIAgxoCQXVDM04PT+B/CbUY5GKWMQQgwcJhvNQAC4dHQQAD8kSRaCEEII2yABBCEGiOpGPflVje3OHx98cpSj9EEQQgwkBqOK1kYzEMJ8XBgV7MH3BwusvRQhhBACkACCEANGWlFLA8UY/9P7HwAEerRkIMgkBiHEQGJSVTQ2moEAMG90EIfyqsgsrbP2UoQQQggJIAgxUKQU1gIQ43/mEgY/NwcURaFZGikKIQYQo8l2MxAALh4VCCDNFIUQQtgECSCIAeeHpHzSi2utvYxel1pUg4u9lsEnex38nk6rwV6rkQwEIcSAYlSx2SaK0JIdNjHMi5UH81FVaXIrhBDCuiSAINpU1DeTXV5v7WWgqiprDxdy8atbuPuTfWbtPr0iMY97Pz3AH9/YxubUErNtty9ILaohyt/trCfK9jqN9EAQQgwoJpOK1nbjBwDMGxNEenEth/Oqrb0UIYQQA5wEEAQA5XXNHC+ppai60arryCyt47p3dnHnx/uobzbyS3Ixs1/cxNKtmZh6OF7wRFkdj313mDEhngwe5MTNH+zh013ZZlq57UstqmFoO/0PWtlpFfQyhUEIMYDYegkDtIxztNdp+GpfjrWXIoQQYoCTAIIA4Pm1yRhMKirWTY984ItEjuRX8c/L4lj3wAzWPTCT8WFePP3DUf6z5li3t9tsMHHfZwfQKPD6dfF8fVcC06N9+Pt3h9iVUWbGr8A2ldU2UVrb3G7/g1ZajYKxh4EaIYToS4yq7QcQPJztuDAugBWJ+TTqpU+NEEII65EAgmB/dgWf78lBoyhYs7zyUG4ViTmVPDA7hpumhKHTagjxcuaDmydww+RQ3tmSyYrEvG5t+5WfUzmYW8Vzl48ieJAzrg463rp+HI52Gn5I6v/jsVKLzt5AsZUEEITo30wmlTWHCli86TjPrD7Gaz+nDfisI1MfyEAAuHJcMFUNen4+VmztpQghhBjAdNZegLAug9HE498dJsDdET97BwqrG1FVFcUKI60+3nkCJzst88cGn/K4oij83yVxpBbW8vA3SUT6ujJisEent1ta28TSrZn8YUwQc0cGtj3uZK9lZowvPx0t5B/z4my6iVZPpZ4c4Tg0oBMBBFXFZFL79fdDiIFIVVX+vfoYS7dmAr/2PCmsbuTffxxp5dVZj9HGxzi2mhrlQ6CHI1/ty2mbzCCEEEL0NslAGOBWHSrgaEE1j18yDN3JC0Zr3ICuqtez4mAef4gPwsPJ7rTn7XUa3rh+LIOc7bnjo31UN+o7ve33t2XSZDBx77nRpz13YVwARdVNHMyt7NH6bV1qUQ0eTnb4uTmc9XVajQIq1EuKrBD9zuJNGSzdmsnChDCO/ONCUp6ewx0zI/hkVzYf7Txh7eVZTV/JQNBqFOaPHczm1BIKq9rvV5RcWM2rP6fx9qbjLNuRRY4NNEcWQgjRf3Q6gKAoynuKohQrinL4N4+9oChKsqIoSYqifKcoiqdlliks5Ys9OYR6OXPRiMC2rAODqffTWb/en0uj3sQNk4e0+xpfNwfevH4sBVUN/Gd15/oh1DTqWbbjBBcODyDK7/QGgufF+qPTKPx4pKjba+8LUotqiPF37TCzpPUkuq7J0BvLEkL0ki/35PDc2mTmjQ7i/y4ZjouDDkVReOjCWM6N9eOplUfYfrzU2su0CoNJRdsHMhAArhgXgkmFbw/knvacqqp8uD2Lea9v48V1qfxnTTL/t+II1yzZSZNBgsJCCCHMoysZCB8Ac3732DpghKqqo4BU4FEzrUv0gpzyerYfL+PKccFoNAqt50+9XQNvMql8vPME44YMIi7o7KUJ8aGDuH16BJ/tzmF7escnux/vzKam0cDdsyLP+LyHsx2TI7z56Uhhv52vraoqKYU1HfY/ANpOomsazxxAMJlUSmqazLo+IYRlpRXV8Pjyw0yP9uG/V44+pTxJq1F45ZoxhPu48MAXidQ3D7zgobEPlWyF+7gwOcKLN35JZ8fxXxsAl9U2ccdH+3hy5RGmRnqz57HzOfKPC1ly4zjyKhv4bABNHBJCCGFZnQ4gqKq6GSj/3WM/qaraeraxEwg+7Y3CZn21LxdFgcvHtfyztZ4+6Y29eyG9/XgZmaV13HiW7IPfemB2DOE+Ljz8bdJZT3Yb9UaWbs1kerQPo4LbT465MM6fjNI60otru7z2vqC4ponqRkPnAggnT6Jr28lAeOXnNGa+sIGq+s6XkPQX648WcdXiHRTXWHfUqRBdYTSpPPRNEi4OWl6+egz2utMP+26Odjw7fyRF1U0s3ZJphVVal0ntOxkIAK9cE0+QpxML39/NhpRivjuQy/kvbmJDSjGPXzyMpQsm4OvmgIuDjtnD/UmI9Oa1X9KlQa4QQgizMGcPhFuANWbcnrAgo0nl6705TI/2JcjTCcBqGQg/HS3EyU7LnBEBnXq9o52WZ+ePJKe8gefXprT7uo93nqC0tom7zjlz9kGr2cMDTq6jf5YxpBS2NFDsXACh5SOh9gwZCDWNet7blkl9s5EdGQMr1Tm7rJ4Hvkhkd1Y5/1h51NrLEaLTPtyexYHsSp6aF4e3a/s9UMaHeXFhnD+LNx0fcFlGxj7SA6GVv7sjX9wxhSg/V25+fw8PfHGQMB8XVt03ndumR5ySTaEoCg/NiaWsrpnCqgYrrloIIUR/YZYAgqIojwEG4JOzvGaRoih7FUXZW1JSYo7dih7YfryU/KpGrhr/a9KIgnV6IGxJK2VKpDeOdtpOv2dShDcLE8L4YHsWaw6dPoYxu6ye//2UyswYX6ZEeJ91WwEejowJ8eTHI4VdXntf0DqBIcb/9B4Qv/drBsLpGQaf7mopB7HXaticNnACCM0GE3/6bD8ocMPkUFYdKuCnfvqzIvqX7LJ6XvgxhXNj/Zg3OqjD1z88J5Ymg4lXfk7thdXZDpNKnylhaOXlYs+nt09mfvxg/jEvjq/vTGg3SDwmxJM5cQHkVzViGOAjO4UQQvRcjwMIiqIsAC4BrlfPUkSuquoSVVXHq6o63tfXt6e7FT305d5cPJ3tmD3cv+0xa2Qg5JTXk1lax/Rony6/9+8XDSM+1JO/fnWQ9OKatsdNJpWHvjmITqPwn/kjOzWS8rxYP5Jyq/plan5yYQ0+rvZnvfvY6tcAwqkNtxr1Rt7dmsm0KB9mDvVlc2pJv+0Z8XvPr03mYG4VL1wxiicvjSM2wI0nVhzu0iQQIXqbqqo8+l0SWo3Cv/4wolOfgxG+rlw3KZTPduf025KuMzGaVLR9K34AgIeTHS9ePYYFCWEdZlD85YIYjCaV0tqBlV0ihBDC/HoUQFAUZQ7wMDBPVVWZE9RHFFQ18OORQv4wZjAOutPv+ht6sQfC5rSWbJQZMV0PKtnrNLx5/Vic7LXc8dE+imsaUVWVT3ZnszOjnMcuHtZWntGREcEtzRuPFVZ3eR22TFVVdhwvIz50UKde3xZA+N3F8bf78yipaSkHmRHtQ25FAyfK+v+vfEphDe9uzeTGyUOYMyIQO62G568YRUlNE8+vTbb28oRo11d7c9mWXsYjc2M7/TkIcP950TjZaXlxXfvlYf1NSwlD/55qHeHjAvR+iaIQQoj+pytjHD8DdgBDFUXJVRTlVuB1wA1YpyhKoqIoiy20TmFGz65JRgFunRZ+yuO/jnHsxQBCagmDPZ3aTm66KtDDideuHUtWWT0T//0zI578kX9+f4Tp0T5cPSGk09uJC3QH4FhB/wogHC+pJa+ygXOGdi5A03oX7rdNFI0mlbc3H2dUsAcJkd5Mj27Z1pa0/l+K9PW+HOy0Cg/Mjml7bFSwJ1eMC+bb/Xk0GyQdWNieoupGnl51lEnhXlw3MbRL7/V2deDmqWGsPlTY7z4P22NSVbT9O37QFhyW8IEQQoie6soUhmtVVQ1UVdVOVdVgVVWXqqoapapqiKqqY07+udOSixU9tzernBWJ+dwxI4IQL+dTnvu1hKF3LooMRhPb08uYEePTqfTa9kyJ9OabuxJ4/OJhXDUhhEtGBfHc5aO6tE1fNwe8Xew5mt+/Tpg3prRc5M/sZIaHRlFQFIWa3wQQDudVcaKsnpunhqEoCkO8nQnxcur3fRAMRhPfHcjn3Fg/vFzsT3nu3Fh/6puNHMyttNLqhDgzVVV5fPlhmg0mnr18VLdq+2+dFo6bg45Xf06zwAptT19rotgditIyqnmAVJ4JIYSwIJ21FyB6j8mk8tT3Rwhwd+TOM0wmaD196q0MhIO5ldQ0GdruaPfEmBBPxoS0P6qxI4qiMDzInaP97I7bptQSovxcCR7k3PGLT9JplFOmMJTVtdTMDvFuyRJRFIXp0b6sTMxHbzRh109v3W1OK6G0tonLx54+nXZKhDcaBballzIhzMsKqxPizNYeLmTd0SIenRtLeDczuzyd7bl5ahiv/pLO0fxqhge5m3mVtsWkqmj60BjH7lIAVXIQhBBC9FD/PPMXZ/Tl3hwO51Xz6EWxONufHjtqK2HopR4Im1JL0SgwNbLrDRQtYXigO2lFtej7SZfq+mYDuzLKOaeL/SU0GoW632QgVJ5sLDnI+de78DOifahtMpCY03/vwH+9LxdvF3tmxfqd9pyHsx0jBnuwPb3MCisT4szqmw08/cNRhgW6n1ai1lW3TovAzUE3ICYyGAZABgK0HOMlA0EIIURPSQChD2s2mHhjQzpPrTzC/uyKdrvi1zcb+NcPR/n7d4eYEDao3XFevZ2BsCWthNEhnng42/XK/joyPMidZqOJ4yX9o/v4zowymo0mZnay/0ErrUY5pQdCRVsA4dd/pymRPmgU2JLaP/sgVNY3s/5oMfPGBLWbYZEQ6cOBnArqmw1nfF6I3vbGhnTyqxp5+rI4dD3MDPJwtuPmaeH8eKSII/lVZlqhbTKaBlIGghBCCNEzUsLQR2WV1nHf5wdIyq3CXqvhg+1ZhHk7MybEkyBPJ3zdHKhvNlJZ38yaw4XkVjRw3aRQHpkb225vgN7sgVBVr+dgTiX3nhtt8X111rCTjRSP5lcTG9D3U3Y3ppTgZKftcoq9VqNQ0/jbDIRmFAXcHH8NIHg42TE6xJPNaaX85YKhZluzrfj+YD7NRhNXjDu9fKHV1ChvFm86zu7Mcs4ZenqWghC9KbO0jnc2ZzI/fjDjzVRWc+u0cN7flskr69NYctN4s2zTFpkGTAYCA2b8rhBC2AK90UR5dSOujjq6V1RomySA0AdtTy/l9mV70Wk1LL5hHFOjvFlzuJAfkgrYe6KCwqSCtiwCRzsNkb6uvHjVGCaGn/2kUqH3Shh2Z5VjUmFqpLfF99VZET4u2Os0/abz+KbUEqZEeuNod/qozrPRKadmIFTW6/FwsjvtBHt6tC+v/5JGVb3eZrJIzOXr/XnEBrgRF+TR7mvGD/HCXqthW3qpBBCEVamqylMrj2Cv0/DI3FizbdfDyY5bp4Xz8vo0DudVMWJw+78PfZlRHSgBBClhsBUGo4mjBdXszCgjv7IRVwcd7k46ZsT49osbGEKIFg16I5lldQzxcpYAgrAeo0nlyZVH8HVz4NPbJ7fN975qfAhXjQ9pe01Vgx4XBy0Ous5fPLZmIPRGCcOujDLsdRpG96DxobnptBpiA9z6RSPFzNI6TpTVd6sO+vQShuZT+h+0mhHtw6s/p7H9eClzRwb2aL22JLusnoM5lTzawYWYk72WsUM82TZA+yCoqsrKg/lkl9Xj5+5AgIcTkyO8uvSZI8xj7eFCNqWW8PjFw/BzdzTrtm+eGs7SrZm88nMa7/TTLASTCSlhEL2ioq6Ztzdn8MmuE22Zfm4OOuqaDZhU+O+PqTxx6XBumBTao+lUQgjbYGy9KdvPfp0lgNDHrDpUQFpxLa9fF98WPPg9rUY5bexcZ/RmAGF3VjnxIZ5dvjtuacMC3PnpaCGqqvbpg/eyHVkAnBPT9Tvjv2+iWNWgx/MMGQajQzxxc9CxOa1/BRBWHy4A4KJOfE1TI33437pUyuuau/U711c1NBt59Nsklifmn/J4lJ8rz84fabYUetGx6kY9T648wvBAdxYmhJl9+x5Odtw2LYKX1qf22yyElgwEa6/C8qSEwXpMJpXXN6SzZHMGdc0GLhoZyIVxAUwK98Lf3RFVVSmpaeKhb5J4Yvlh9mSW8+8/jjildFAI0fcYT37mKv0sgjAADpn9h9Gk8vL6VIb6u3HRCPNfsLX+cFu6B0JNo57DeVVM6qCkwhqGB7lTUa+nqLrJ2kvptm3ppby/LYubpgwh1Lvz4xtb6X7XA6GivhlPp9NPYuy0GqZEerM5taRfnZSuPlTA6GAPQrw6/t4lRLVMENlxfOBkIeSU1/PHN7ex4mA+D86OIfnpOWx5aBZvXT+WhmYjVyzewRPLD0tzyV7y/NpkSmub+M/8kT1unNiem6eF4e6o4+X1aRbZvrUZTSpaTf8/HVJQJAPBClRV5elVR3lxXSrTonz48c8zeOO6scwbHYT/yYwhRVHwc3fkvQUT+NuFQ/khKZ8LXtrMz8eKrLz6lvUbe6m5thD9TevvTh++J3lGkoHQh6w8mEdGSR1vXT8WjQXqNdsyECzcA2HviQpMKkyKsJ3+B61a550fLagiwMO8qcC9oapez4NfHiTS14VH5w7r1ja0GoUmg4lmgwl7nYaKOj0xfm5nfO30GF9+OlpEVll9t2fO25LssnqScqv4+0WdqyMfHeyBq4OOHRmlXDyq/2RhtKeqQc9N7+2mrLaJ9xdOaOv9EOLlTIiXMzNifPnvTyl8sD2LHRllvHHdWIYGnPlnR/TcvhMVfLIrm5sTwi1aDubuaMdt0yN4cV3/y0IwnTy50/a3s7szaMlAsPYqBp7FmzJ4f1sWt0wN54lLhp01u1GjUbhnVhRTo3x4+Oskbv1wL5eMCuSZ+SNx76VsBKNJ5et9Oby9OYOSmibqmgzotBomhXtxzlA/LhoZQKDHmTNghRCnas3q7m9HmP4fcu8nDEYTr6xPY1igOxfGBVhkH701xnF3Zjk6jUJ8qO30P2gVe/Ji52h+3+uDoKoqj684TGltEy9fHY+TfffKQ1qbibWWMbSUMJw5PX9GdMsd+C1p/WOcY2v5wtxOZvjotBriQz3Zf6LSksvqlka9kZ0ZZSw/kMfiTcfZll7ao+2ZTCoPfJFITnk97y6YcMbGkS4OOp68NI6PbplEZb2eea9v5d0tGVTWN/do3+J05XXN/O2rgwS6O/LgBTEW39+ChDDcHHS8sSHd4vvqTa3ppQOihAEpYeht3+zL5bm1ycwbHcTjF589ePBbY0I8+f5P0/jL7BjWHi5k/pvbySqts/BqYW9WOZe9sZWHvzmEu6Mdl48N5u5zorhuYih5lQ08/cNRLnhpc7855gthaaa2DIT+FUKQDIQ+4scjLXd5F98wziLZB9B7PRB2ZZQxKtgDZ3vb+/Fzc7Qj1Mu5zzVSbNQb+fu3h/j+YD5/vSCGkcHdv0PYeieutsmAi4OO2ibDGXsgAAzxdiHUy5nNqaXcNCWs2/u0FauSChgd4tmp8oVW8SGevL4hnfpmg038TKuqyg9JBTy7Jpm8yoZTnrt1WjgPz4nFXtf1q6WX1qfyS3IxT18W1+FEl2nRPqy+fxoPfnmQf606xnNrk5kZ48ulo4OYPdzfJr5PfVl1o54F7+0mr7KBZbdMxMXB8t9PDyc7FiSE8fqGdFKLaojx7x+ZJa3ppZY6rtoSRZESht6UWVrH3787REKkN/+9cnSXf8bsdRruOy+a8WGDuPuT/Vz2xjbevH4sU0+WzplTXZOB59cm8+GOEwR6OPLKNeGxk70AACAASURBVGOYNzrotIue9OJa7v10Pwvf38NT8+K4cfKQU54vqm5kQ3Ix2eX15FY04O1qz5XjQtqyO4WwBYVVjWSU1pIQaf7fpd/rrxkIchbXR3y+J5vBnk7MHu5vsX30Rg+EhmYjSblV3DY9wmL76KkYf1eOF1s+0m8u+ZUN3PnxPpJyq3jg/BjuPieqR9trzUCoaTTgYNdy53jQWcY0Tov2YcWBPPRGE3Z9+DZedlk9h/I6X77QKj50ECYVknKrmGzlspyMkloe/iaJPVkVDAt05/8uHU6Unytezva8tD6VpVsz2XuigreuH9tuE9YzWXkwn9d+Seeq8cHc8LsTxvb4uTmy7JaJHMmvZuXBfFYm5rP+WDFOdlouiPPntmkRPQp0DVQNzUZu+2AvxwqqWXLTuF4tBbtlWjjvbcvkzQ3pvHxNfK/t15KMA6yEQSIIvcNkUnn46yQcdBpeunpMt4K2rRIifVhxz1Ru+3AvNy7dxb3nRnPfuVFm6XliNKmsP1bEv1YdJbeigYUJYTw0Z2i7Qd4oP1e+viuB+z87wBPLD7MqKZ/Lxgxm/JBBfLIrm093Z9NsMKHVKAR6OFJc3cT727IYFezBk5fGMW7IoB6vWYjuqm7Us3jjcZZuzaTJYOK1a+O5dHSQRffZdk3Vzw4xEkDoA3LK69mSVsqfz4+27KzqXuiBsD+7AoNJZVKE7TVQbBXm7cKWtFJMJtXm70oVVzdy+VvbqWk08M5N480SYGorYWg2YFff8v/tlTBASxnDp7uyOZBd2eGdaVvW1fKFVmNO1p4fyK60agDhm325PLHiMPY6Df+ZP5Krxoec8nnxz8tGMCXCm4e+TuK6d3by5R1TOjXyb3t6KQ9+mcjEMC/+edmILqXhKYrCiMEejBjswSNzYtmdVc7Kg/n8cDCfFYn5XDQygL/MjiGqnR4b4ld5lQ18tTeHr/bmkl/VwKvXxHNurOUCymfi5WLPDZOH8O6WDP58fgxh/aDvya8lDLb9WW8OCmCSCEKvWLYji91Z5bxwxai2Rok9McTbhe/umcqTK460jE9OL+WVa+MZ3IVA8G9VNej5aEcWn+3OIa+ygTBvZ75YNKVTx3BXBx1LbhrPO1sy+GJPDo9+ewho+R26fOxgbp8eQbiPCzqthoq6ZpYn5vHulkwWvr+br+9MkL44wiqyy1oaQJfVNfOHMUFkl9fz8DdJDA1ws2hGnbF/xg8kgGANLR2fO/+j9MWeHDQKXDU+xIKr6p0Shl2Z5WgUGG/DUegwHxeaDCYKqxu7dJe2tzXqjdy+bC9VDXq+vGOK2Rqbtf5s1jYa2hputVfCADAl0getRmHd0cI+G0BQVZVv9uUSH9q18gWAQS72hPu4cCC7wkKrO7smg5FHvz3Et/vzmBjuxSvXjGm3wdXckYH4uTty49Jd3LB0F18smsKgs4yfPJJfxaKP9hHh48o7C8b3aOyqRqMwOcKbyRHePDI3lne3ZLJ0S0ZLfe/YYO4/L5oQL+e2cWYbU0v46UghuzLKifJ3ZXq0LxcM9+9XDfw6UlzTyNrDhfyQVMCerHIApkX58Mz8kcyM8bXKmm6bHs4H27N4fUM6/71ytFXWYE6t9amaAZGBoEgPhF6QU17Pc2tTOGeoL1eMCzbbdl0ddPzvqtFMj/bhse8OcelrW3n9uvgupWGrqsq3+/P4z5pjlNY2My3KhycuGcZ5w/y7lEGo1SjcOTOSO2ZEcCS/mt2Z5Zw/zP+0yU+DXOy5eWo4F8QFMP/NbSx4bzff3p1g0+dWon96aX0qdc0GVt47lVHBnhRVN3Lxq1u586N9LL93qsWalBpOZiBIDwTRLUXVjSw/kMfGlBL2ZVdw8chAXrhiVIcpaAajia/25TAzxtfiH7itJQwWDSBklBEX5GHTs41bpwlkldbZ7EFOVVX+9nUSSXlVLLlxvFkvqtpKGJoMNJ8MnQ46SwaCh5MdF8b58+XeXB6YHdMn69sP5FSSVlzLs/NHduv98SGebEkvRVXVXj1I1DYZuOOjvWxLL+P+86K577yOs5TGDRnEuzeNZ+EHe7jxvV28fHU8UX6up73uxyOFPPbdIdwddXx4y0Q8zjDKs7vcHe34y+wYFkwZwlsbj7Ns5wlWJOYRF+RBVlkdlfV6AAZ7OjF3ZACpRbW8/ksar/2SxuIbxlmskaytSCmsYfGm46w8mI/RpBLt58r950Vz+djgLge4zM3PzZEFU4bwzpZMrhwXbJPTdLqirYRhgGQgWHjI0oCnN5r48xeJaDUKz/xxpEWOB3+IH8yoYA8WfbSPG5fu5tG5sdw6Lfys+1JVlR0ZZby8Lo3dWeXEh3rywc0Te3zu8Nsss7MZ7OnEBzdP5KrFO1oyEe5K6LWpEkKkF9ewPDGPRdMjGBXckjXq7+7IG9fFc927u/jHyqP87yrLBMSN0gNBdFd5XTPz39xOXmUDsQFuzB7mz3cH8lAU+O8VZ2+ssyGlhKLqJv55WajF19l67DEaLdMDoVFv5EBOJTd1sobaWlrTcjPL6kiwQLOizqptMpCUW8mRvGpK65qobjBQ06inutFASU0TxwqqeWRurNn7Yvw2A6FR01rCcPYD/S1Tw1l9qJBv9ued1lSpL/hidw7O9lou6WYtXHyoJ98eyCOvsoHgQb1zgVde18zC93dzJL+a/105msu7cKcrIcqHxTeM5d5PD3DBS5uYNzqI6yYNwclOS7PRxOJNx1l3tIjYADdev26sxUaaers68Pglw7l1ejiv/5JOWlEtc0cEEuXnyqRwL+KC3NtOiivqmln4wR7+/HkiX91pvowbS2nUGzmcV0VGSR0ZpXUYjCYGudjj7WJPbKA7cUHup9zxM5pUNqUW89GOE2xIKcHZXsuCKWFcOzGEaBtrWPjA7Bh+PFLEw98kseb+Gd2e+GILWksYbL1czRz64g2wlMIa/vdTCjsyypgZ48sfxgxmRoxvj3oKWNJ/f0ph34kKXrs23qI3ICJ8Xfnu7oS2RrUrD+Zz9zlRXDDc/5Sf5aLqRramlbJsRxYHc6vwcXXguctHcuW4kF7/mR8W6M7bN43jxqW7eeSbJN64bmy/uysrbNNL69NwttNyx8zIUx6fFOHN1RNCWH4gj2fmj8BBZ/5jWWsAob9FECSAYGEGo4k/fbafktomvrkroa2BzNCf03hxXSqOdlr+/Yf264o/352Nr5sD58aePjLN3Cw9xjEpt4pmg8nm09wD3R1x0Gl6ZWTSmSQXVvPUyiPsyixvKyGw12lwd7TD3UmHm6MdPq72/PWCGO6YYf5mlJq2KQz6tv2fLQMBWu5qjw724P2tmVw/MbRPnYzXNhn4PimfS0YF4trNbvbxoS2/1weyK3slgFDTqOe6d3aSWVrH2zeM4/xuBJHOjfVny0OzWLIlg2XbT7A8Mb/tOUc7DY+cvKvVG40xAz2c+Pcfz579McjFnnduHMdlb2zjtg/3suLeqWapLTan2iYD29NLWXWogPVHi6hrNgJgr9Wg1Sg06I1tr3W00zAs0B1XBx06jUJqUS15lQ34ujnw5/OjWTAl7KzlJdbkbK/j2ctHct07u3hxXQqPXTzc2kvqttb+VgOjiaKCSbVck2Rzyiqt4+X1qaw4mI+rvY5zYv3YmlbCD0kFjBjsztd3JvSopMoSfkku4u1NGVw/KdTijdmgZWrU4hvG8fW+XN7YmM6dH+9jsKcTPm4OOOo0FNc0kXnyPCbM25ln/jiS+WMHW/X7lhDpw18vGMpza5P5ZFd2p5vyCutpaDZyIKeC/MpG3Bx1eDjZ4eFkh7uTHZ5Odr0yBagnjhVUsyqpgHtnReF1hmPquUP9+HRXNnsyK5gWbf6bhpKBILrlubXJbEsv44UrRp3SffZP50bRoDfy1sbjDAtw48YzjMDLKKllQ0oxd50T2Ssn8a1BDEsFEHZllAHYfABBo1EY4u1MZml9r+63UW/klZ/TeGdzBm6OOv40K4qxQwYxOtizVy8k2jIQmow0G0zYaRWcO7jDqCgKt0wL5/7PE9mUWsKsXgh4mcuqpHzqm41cPaH7WT5DA9xwtNNwILvS4ieOeqOJuz/ZT3pxLe8tnMCMHtTCe7s68OjcYSyaHsHB3EpMppYm7XFB7jZZvuPn7si7C8Zz5eId3P/5AT5fNMWq66mq17Mnq5xdmWXszizncH41RpOKp7Mdl44O4rxh/sT4uxI8yLklgNBspKSmiUN5Vew9UU5yQQ11TQb0RpUoP1ceu3gYs4d3rRbZWhIifbh+UihLt2ZyYVwA48Ns+3O9Pa0ZCLo+FPTsrr7wFeZXNvDaL2l8uTcXO63CHTMiuXNmBJ7O9jQbTCxPzOOhr5N4dk0yT82Ls/Zy2+RXNvDglwcZFujOE5f0XkBNo1G4akII88cOZvXhQtYcKqC+2Uij3kikryvXTQxlcoQ3w4PcbaZM544ZEezIKOOfPxxl3JBBDAuUEY+2aHNqCa/9kkZiTiX6s9Q+TY/24d5ZUUwM97LJjJKX1qXi5qjj9namvyVEeWOv1bAxpdgiAYRfxzja3vemJySAYEHrjxbxzpZMFkwZwpW/a4CoKAoPXTiUpNxKnl+bwgVxAafdTXv9l3TsdRpunhreK+tt/dE2WiiAsDurnNgAt7N29LcVYd4uZPRyBsJfvkxk9aFCrhwXzKMXDTtjpLQ3KLQ0a6ptNNCgN+DpbN+pg8JFIwP5z+pklm7N7FMBhM/35BDl58rYUM9ub8NOq2HUYE8O5Fi2kaKqqjyx/DBb0kp57vKRPQoe/Ja3q0Ovd/TvrrggDx68YChP/3CUxJzKtikYvSWnvJ7VhwpYfbiQpNxKVLUlw2BMiCd3nxPJ5AhvJoZ7nTEI4GSvJdTbmVBvZy4e1bVpH7bokbmxbEkr5eYP9vDhLRMZG2q7zXHb09ZE0UYurixJUcAWeyg26o38eKSlBG5rWglajcINk0K5Z1bUKZNi7HUarhofwtH8aj7YnsWMGB+LfG6V1zXj6qDrdJlEo97Ioo/2YjCqvHFdvFXu8Ou0GuaNDmJeL2Q+9JRGo/DiVaO56JUt3PPpflbcM9Wm+2INNOV1zfzrh6N8eyCPId7O3DItnEnhXkT4uFLbZKC6QU/VyT+5FQ18viebq5fsZGKYF69fH4+fm+1kBqYW1fDT0SLuPy8aj3ZKcZ3tdUyK8GJjagmPW2ANJpOKhr5ZQnY2EkCwEFVVeeXnNCJ8XHi8nWi0oij8+w8jufDlzTy18ghv3TCu7bms0jqWJ+Zxy9RwfFwdemXNbVMYLNBlSW80se9EBVeasSOxJYX7uLAxpaTLEzO6a2NKMasPFfLXC2K499xoi++vI64OOmqb9FQ3GBjUQf+DVnZaDTclDOH5tSnsz67oExcTqUU1HMiu5LGLhvU4ch4f6sn727JoMhgtUkcH8ObG43y+J4d7Z0X1KGOir7t6Qggvr0tl6dZMXrs23uL7U1WVzWmlLNl8nG3pLZlUIwd7cN+50UyO8CY+1NPm0ql7g5ujHZ8tmsx17+zkxnd38d7CCX2uqeKvTRStvJBeoKCg2tgYx0O5Vdz3+QEyS+sY7OnEPbOiuHpCyFlLwR6ZG8vOjDL+9lUSa/483SwXLAajiV+Si/lkVzab00pwddBxbqwfc0cEntZX4LdUVeXRbw9xJL+ad28aT4Tv6Q1pxel8XB149dp4rn93Fw99ncSb10s/BFtwvKSWq9/eSWV9M386N4p7ZkV1eGy7Z1YUX+7N4dk1ydy0dDdfLJrS7sV6b1uyOQMnOy0LE8LO+rqZMb78a9UxcivqzV6GajCp2NP/AggD4JBpHTszyjmUV8Vt0yPOmo4a5uPC/edHs+ZwIeuOFrU9/vqGdOy0GhbNNH+Ne3ta02uMJvPXSB7Oq6K+2cjE8L5xchnm40Kz0UR+ZYPF99WoN/LkyiNE+LqwaEZkx2/oBa6OOmqbDFTUN+Pp1PlMiOsnDWGwpxP3fXaAqpOd9G3Zqz+n4WSnZf7YwT3eVnyoJ81GE0fyq82wstOtPJjPCz+mcNmYIB68IMYi++grXB10XD0hhNWHCiz+O5pSWMNFr25lwXu7SS+u5W8XDmXz32bx/Z+m8cDsGKZEeg/I4EGrwZ5OfHnHFAI8HFnw/m4O5VZZe0ldYhhQYxxtJwPBZFJ5d0sG89/aRqPeyPs3T2DLQ7N48IKhHZ7AO9ppefXaeGqbDNz32QH0PWj83Kg38tHOE5zz340s+mgfKYU13H1OJHNHBLA5tYQ7P97HPZ/up77ZcMb3L92ayXcH8vjL+TGcN6xvZHHZiskR3jwyJ5Y1hwtZsjnD2ssZ8IqrG1nw3m5UVWXlvdN48IKhnTq2OdlrWZAQxpKbxpFRUsfNH+ymrunMvy+9qaCqgRWJeVw9IaTDMuBzhrZkzW5MKTH7OiyV1W1tEkCwkCWbj+Pjat+pC5Pbp0cQG+DGw98k8caGdHZnlvPdgTyunzSk11OBFEBvgR/2XZktM8xtvf9BqzDvk6McyyxfxrBkcwYnyur557wRNtNZ2sVBR02jgaoGfYcTGH7Lw8mO16+Lp6i6kQe/SrTpmeOJOZX8kFTA7dPD8TZDls+4IS0/27syynu8rd/bm1XOX786yMQwL56/YpTcqQEWTg1DVVU+3J5lsX3sySrnysXbKatt4oUrRrHloXO5Z1bUabPOBzp/d0c+XzQFDyc7HvwqkSaDseM32QiTOoDGONpIAKGqQc+ij/byr1XHmDXUj9X3TWfWUL8ulZHE+LvxzB9HsjOjnH+vOtblNZTVNvHGhnRmPL+BJ5YfxtfNgcU3jGPrw7P424WxPH/FaPY8dj5/vyiWtUcKuertHRRWNba932hSefGnFP616hhz4gK499yoLq9BwG3Tw7l4VCDPrU1me3qptZczYNU06ln4/h7Kapt5b+EEhgd1vS/F9GhfXr02nsScSu78eJ/VL5zf35aFSYVbp3VcBh7p60LwICeLBhAGbA8ERVHeAy4BilVVHXHyMS/gCyAMyAKuUlXVskXAfUBqUQ0bUkr4y+yYTkXv7LQaXr02nseXH+aFH1MAcNBpuLMXsw9aKYplomW7MsqI8HXB1613yjF6KvzkKMes0jqmR5unzvxMcsrreWNDOhePCrRI85bucnPQUXcyA2F0cNdqzONDB/H3i4bxj++PsnhTBnedYxtZFb+lqir/WX0MH1d7Fs00z/p83RyIDXBjS1qJWb/mrNI6bl+2l8GeTrx94ziLlUf0NcGDnJk7IpBPd2dz33nRZu8Evf5oEfd8up/Bnk4su3Vir43n7Kt83Rx4dv4obv5gD6+sT+OhObHWXlKntJUwDICgnC2UMBwrqObOj/eRV9HAk5cOZ2FCWLcDopePC+ZYQTXvbs1keKA7V00IOe01dU0GdmeWsyOjjJpGPRpFobJez7qjRTQbTUyL8uHlq8cwJdL7tHXotBoWzYgk0teV+z47wOyXNjF7uD/nD/Pns93ZbEkr5cpxwTx9lkla4uwUReH5y0eRWljD3Z/u59u7EjpdBpJeXMvawwUoioKfmwM+bg642OtwstPiZK/B0U6Ls70ON0edTTemLahqoKy2GU9nO7xc7HG2793q8tLaJu76eB8pRTW8u2A8o3vQV2jOiAD+9YeR/P27QyzedJx7ZlknsFbdqOfTXdlcPDKQEK+Oj92KonDOUF++3Z9n9jLUtiaK/ewjois/pR8ArwPLfvPYI8DPqqo+qyjKIyf//rD5ltc3vbM5A0c7DTd2YTxNjL8bX94xhRNldXy7P49QL+dTmgf1FgXF7D0QjCaVvVkVXNIHmvu08nd3wMlOa/FJDM+uTUajKDx+8TCL7qerXB10FFU3UlHftQyEVgsTwtiTVc5za5PZnVnGQ3NibarT8i/JxezKLOfpP4zo9ujGM5ke7cOH20/Q0GzEqYPJFZ1RUdfMzR/sQVEU3l84wWbH+lnLLdPCWXWogG/3555xkk135ZTXc/cn+xkW6MZ7CyeYJUNlIJgV68eV44JZvOk4F8YF9OhEtLcYpYlir9mWXsptH+7FzVHH54smm2VyxyNzY0kpquHx5YcprWvipilhuDroOJRbxSs/p7ExpbilBlmnwcPJDlVt6Wt07cQQbpwyhCg/tw73cd4wf5bfM5W3Nh3n52PFfLs/D3uthv/MH8k1E0IkeNBDLg46li6YwB/f3MbNH+zh27sS2v3MbTIY+W5/Hp/tyeFgTmWntq8o4O3iQICHA8MC3IkPHcTQADf0RhN1TQac7XW93scmMaeSL/bksON4KVllp55nDvZ0YnzYICaEeTFnRIBF+6AdyK7gro/3U1HfzMtXj2HW0J43wL52Ygg7Msp4cV0qkyO8T5lA11s+2ZlNbZOBRV0YdX5OjB8f78xmb1YFU6PMd0NvwI9xVFV1s6IoYb97+DLgnJP//yGwkQEeQCiuaWR5Yh7XTgzt1sn+EG8XHphtvfrmlgwE8/ZAOFZQTU2TgUl9pHwBWqKRQ7ydLVrCsD+7glVJBdx3XjSBHrY1Ms/VUUdpbRPNBlO3pmYoisKLV41hVHAWb25I56JXtzB3RABXTwhlWpSPVdOFG/VGnl2TTISPC9ec4Y5VT0yL9uWdLZnszipnZg8nJDQZjNzx0T7yKhv49LZJhJ3MihG/GjdkECFeTuzMLDdrAOF/P6Wg0cCSm8ZL8KCLHr9kOFvTS/nrVwf58c8zbP7CvK2EYQBcBCpgtfyDzakl3L5sL+E+Liy7daLZyjN1Wg2vXRvPA18k8vzaFJZszmBEkAdb00vxcLLj1mnhzIjxZdyQQT26QIz2d+PFq8agN5rYk1VOgLujNEw0o1BvZ95ZMJ5rl+zk9mV7+fT2yaf8e9WcvKO8dGsmxTVNxAa48fjFw5g3Jgg3BztKapoorWuiodnY8kff8t/6ZgMV9XqKaxrJq2zk5+RivtqXe9r+He00TAjzammKG+LJiGAPVBNU1DdT3ajHaGrJ3dEoLWOtney0eLnYdznzLSm3kpfXp/FLcjGuDjomR3hxw+QhBA9ypqqhmdLaZo7kV7HjeBkrEvP5x/dHuCAugGsnhDIl0tts506NeiPvbcvk5XVp+Lk78M1dCYwY7GGWbSuKwr//OIID2RXc//kBVt8/HfdenLJRWtvEWxvTmRnj26WvKSGq5fu743iZeQMIamsGQv86xvT01pu/qqoFAKqqFiiK0ndmt1nI9wcL0BtVbprS+ewDW2KJHgh9rf9Bq3AfF1IKayyybVVVeWbVMXxcHbijCxHS3uLqoKPiZBPEzk5h+D1HOy13zozk2gmhvLXpOF/syWb1oUKCPBwZF+ZFmLczoV7ODPF2YYi3M35uDhb/gG3UG7l92V7SS2pZumC82dMaJ4Z5Ya/VsCW1pMcBhEe+OcTurHJeuzbeLHfq+quh/u6kmvH39Gh+NSsO5nPnzMjTRuuKjnk42fHQnKE88MVB9mSV2/xUhl+nMPSvk7szUhSr9KXZmFLMoo/2Eenryie3TTL7iGJPZ3vev3kiB3MqefXnNA7nV/GX2TEsnBpm9gsXO62GhEjbKTfsT8aGDuKlq8dw9yf7mfnCBm6eGs6FcQF8sy+XZTuyqG40MC3Kh5euHkPC70pOWsfjdkRVVbLL60kvrsXRTourQ8vNkq3ppWxNK20rI+4sN0cdgR6ORPm5Eu3nRpSfK/7ujvi6OeCg01DX1NJLamt6KasPFZBaVIuHkx1/u3AoCxLC2s2AVFWVtOJaPt+dwzf7c1mVVICfmwOXjg7iopEBjA72RNeN8xeTSWX14QKeXZNMbkUDF8b589zlo8w+Xt3d0Y5Xronnqrd38Nh3h3n1mjG9dgH97JpkGvRGnmhnAl57nO11BA9yMvuNw9absv3tCNNrhTaKoiwCFgGEhvbf8WMrEvMYMdi9U2lxNklRMJq5hGFPZjnBg5wI8rStu+wdCfNxYd3RIgxGU7c+qM/mxyOF7D1RwX/mjzR77bY5/Pag1tMDi4ezHY/MjeWB2dH8fKyY7w7kcTCnktWHCk7pt+HhZMesob6cP9yfhEifM55kmkwqeZUNVNQ34+3qgI+rfadr1VqDB1vTS3nu8lEWmR/uZK9lfNggtvawGdSGlJbv05/Pj+bSPlT6Yw2xAW5sSCk2W93i8z8m4+ag404bmYjSF10YF4CT3WG+T8q3+QBCawaCrWdKmIPGCiUMKYU13PXxfqL9XPn41kkWLcMaHeLJ0oUTLLZ9YXkXjQzk09sm8ebG4zy7Jpln1ySjKDAnLoA7Z0b2uCyqJbvUhSHep2b0tU7QqKrXk5hbyZH8Kuy1Grxc7HF3tEOrUdp6hDXojdQ3GymrbaawqoG8ykaOFdSw5nBhu79figITwrx46tLhXD4uGLcOAluKohDj78b/XTqch+YMZf2xIlYm5vPRjhMs3ZqJu6OOadE+zBsdxPnD/Ds8R61u1PP13lw+2nmCzNI6YgPc+OS2SWa90/5744YM4i+zY3jhxxRmRPtw5XjzZnyeyd6scr7el8udMyOJ8ut6hlColzPZ5eYtXW4rC+9nh5ieXrkUKYoSeDL7IBAobu+FqqouAZYAjB8/3gb6AJtfRkktSblVPHaRbdWzd4XCrw0/zEFVVfZnVzAl0rZPIs8k3NsFw8kL1t8fbHqioKqB/6xJJsbflSvHBZttu+bk6vjbAIJ57uA46LRcNDKQi0YGAqA3msiraOBEeT3ZZXUcyKlkQ3IxyxPzgZY+FDH+bjjoNDTojVQ16MkoqaO++dQO7z6uDkT5uRDl50qUryuRfq6EebvgoNOg0SgUVDayMaWYVYcKSCmq4fnLR1n0QDYt2ofn16ZQXN3YrT4meqOJf686Rpi3M3efI529OxIT4IbRpJJRUtfjPhs7M8rYmFLCI3NjbWaOdV/kbK/jvGF+rD5UyFOXxpk9AGtOrRMAdQMggAAtJQwmk9orAZPqRj13fbwPV0ed9HARLLreygAAIABJREFUnZYQ5UNClA9H8qvYlFrCBcMDunUx2B0eznbMjPHtVgZhQ7ORE+V1lNQ0UVLTRKPehKujDlcHLSOCPLrd18zRTsslo4K4ZFQQVQ16tqSVsDm1hI0pJaw+VIi/uwNXjQ9hxGAPQr2c8XF1QG800WQwkZhTwdrDhWxKLaFRb2JsqCd/vmYMl4wK6pWsqztnRrIlrYQnVx5h3JBBFi37MRhNPLHiCEEejtx3XvfOnUK9nPkhqcCs62oNUve3I0xPAwgrgQXAsyf/u6LHK+rDVh7MR1Ho03cMzd0DIb+qkeKaJuL7QDOt32utOc8sret2ACEpt5KNKSVMifRmTIgnq5IK+L8Vh9EbVd5bOMFmT6x/m4EwyMypba3stBrCfFxOfp99uXFKS3T/QHYFB7IrOVZYTVpRLSZVxclOi7eLAxPCvIjxd8PbxZ7yumZKaprILq/neEktKxPzqW5sf/bwqGAPXrs2nktGWfb3c0a0L8+vTWFreinzx3Y9QPTZ7mzSi2t5+8ZxNjPW05bFBrRke6UU1vQ4gPDK+jQC3B1ZmBBmhpUNbJeODuKHpAK2Hy9jRg/LeSyprYliP6tPPZPWr9FgUrG38MWDqqr87auDnCiv57PbJ1ulKbTo2+KCPIgLMk9dfm9wstcSG+BObIDl9uHhZNcWTDCaVDYkF/PRzhO89kt6u+/xd3fg6vEhXDEuhJHBvfv91GoUXr46njmvbOZPnx3g27sTLDZJavGm4xwrqOat68d2e5LFEG9nqhr0VNXrzXYT4dcpDP3rGNOVMY6f0dIw0UdRlFzgSVoCB18qinIrkA1caYlF9gWqqrIiMZ/J4d4EePTdA6WiKGbtgXAgu2Wq51grdGHtqTCflnq6rNI6GNr19+dW1LPw/T2U1zXDupYmPY16E+OGDOJ/V4626aZ4bo6/DSD03p1YrUZhfJhXt2r+VVWlpLaJ48V15JTX02w0YVJVPJzsmBrlY9FOxr81PNAdLxd7tqZ1PYBQVa/npXWpTI7w4oLh5i+x6I/CfVyw0yqkFPWsD0JOeT07Msp4sJPjd8XZzYzxxc1Bx/cH8/tEAGFA9EA4yWAyYY9lg5PvbMngxyNFPH7xsD7X/0iIvkCrUTh/uD/nD/ensr6Z7PJ6ssvrKa9rxl6rwV7XcpNmTLCnVUu0AjwceeGK0dy+bC8PfJHIK9fEm73/1PqjRfxvXSrzRgcxZ0T3IzihXi3n5SfK6xjlbJ4bnzKFQVWvbeep88y0lj7tUF4VmaV1NtkQrysUMGsPhP0nKnHQaYgNsJ0Rfp3l69oyyjGnoqHL721oNrJo2T70RhPL75lKfmUD29JLifJz5aYpYTZ/svrbDIS+ksrdMgvaET83R6uWzGg0CgmR3mxJL0VV1S5Fnd/clE5lg54nLhne76LVlmKn1RDp69rjhqfL/7+9Ow+z467vfP/5VZ3T+yapW93aZQnL8oKxjQwGYrMFYrZAAkkgE4ZkACc3ZG6SYUJYbu5MHpInGXKH3MmQIXEIITtJWAKXeAAHHAiExQYL27IW27K1t9St3tez/e4fVXW6pe4+a/U51VXv1/P4aan76OjXcnXVr771XR4+J0l6w607wlhW4rWlXb3yxiF98ciwfuvHblq3p071CjpkRzQZLFTBKSXMMsXVHL0wpd/70nH9yI2DevsPXbOufxcAr1dVX0eLbt4ZzWzfV9wwqP/rNdfrt/7pqIw5rP/xU7eEloH7xMVp/crfHdZN23v1oTfdXNfeaY/fiPP02Fxo/5bFAELMtnTR6962QX3u8Hm1uI5eddO2Zi+lLsaEu7l4+My4bt7ZuyFTsY0x2rmpXWfHq2uoYq3Vr33qBzo6PKWPv+123bKrT7fs6ivW/m8EQWPHjhY3shv/KPuhZ/XrC49c0FMjMxU3VLXW6rPfP6dX3jC4odI2o+DAYLe+d2q85j9vrdVnHz6n51+zWbs2l+/kjcq87jnb9Onvn9XXT4zqFRHNqCkkqIQh+A5zITdKXm4xl9ev/t1h9ba36Hd+vL7NPID4eMed+2St9Nv3HZWs9DtvfHbdU1IuTS/oHX/xkNrSru7998+tO3twt3/9P3U5vEaK+XUO2DbLxruri6CZxZw+d/icXnzdwIZ5WrsWIxNaD4TFXF5Hzk3p1t0br3wh4AUQqstA+PoTo/rCIxf0n195nV56cGNONg0yENar/0Hc3e6n7D70TOU3tUfOT+nS9KJeccM6FlDG1HVD3To3Ma/phWxNf/7wmQmdHJ3VG2voWYG1vehZ/drUkdYXHjnf7KWsKUklDKbYAyG8PkdX+/D9J3RseFofetOzQx/XCGBje+dd+/T+Vx/UPz16QXd96AH9yddPaiGbL/8HV3F2fE4/+Uff0qWpRf3xW5+rbb31T3rrbE2pv6tVp0MMIMS1BwIBhBB85KtPanQmo198ycYf+xVmBsKR81PK5L2urxvVzk0dVQcQHjkzIUl62wZuxBb0QOht39gBsWbZ19+pzZ0terCKAMJXj12SMdJLrotuvXhUXTfoZXmcuDhT05//zPfPqTXl6FXPJngTprTr6K4DA/rWU5dlGz0/sEJBCUOSMhDW64nY906N6d6vn9RbnrdrXcbkAtj47rlrv77wH39IN+/s02/fd1R3fegB/eEDT2p8NlPxezw1MqOf/KNvaWw2o796x/P13BD7rO3Z0qFTY7OhvV9ceyAQQKjT06Oz+tNvnNSbnrtzQz9pDxiFl974fT+leCP/u+zc1K7J+aymqniyeWx4Wrs3d1zRR2CjKWYgdBJAqIUxRof2bNJDp8Yq/jNfOXZJz9nZ17Bmj3Fy3bJJDNXK5Ar6/x45r1feOFR2Njeqd9vuTbo0vagLkwvNXsqqConKQPA+rkcJQy5f0Ac++5i29bTpA6+5IfT3BxAfN+3o1V/8h+fpb975fF031K3f+9JxveB3v6Jf+4cf6N+eHF0zyDk5n9WH7z+hN3zkm8rkC/rkPS8INXggeWUMYWYg0AMBq/rgFx5Xa8rVe+6uoU1/BBljQns68fCZCe3oa9fgBh7ftHOTVw91bnxePdsqu7k4NjxVvKHZqLr8DIQ+Shhqdvvezfry4xd1aWqh7AizkelF/eDMhN79igMNWl287OhrV2eLqxM1TGJ44PglTcxl9eO30TxxPdzqZ6B9//S4ntfktaxmqYlizHZ3qzDLxjiG7c+/dUrHhqf1Rz9z24YOngNonBfu79cL9/fr2PCU/uwbz+ifHr2gf/jeWQ32tOqOfd748z1bOvT06JxODE/rvscuaHohp7tvHNL7Xn2w5hHrpeze3KF/PHxOi7l8KD3A1rtpbbNwlq/DV49d1FePXdIHXn29tnZv3Jvk5bwShnDqIx8+Nb4hxzcut3OTV1N1ZmyuohnzC9m8nh6d1Ws2UMPE1bSmXKVd09ARjnFzaK937D90arxsA81/OX5JkjZsz4xmcxyjawe7dWx4quo/+6Ujw+rrSOvOZ/Wvw8pwcKhHrSlHD5+eiGYAIZFNFMPtgXBxakG/f/8JvfjAgH7kRsqAAFTn4FCP/tubbtZvvv5G/fPRi7rv0Qv6zskxfe7wUv+cLZ0tuvPafr3rpc9a10bTe7Z0yFrp7Pi89g901f1+QV85E7MiBgIINbLW6sP3n9A1/Z0butb9akbh1EcOTy7o/OSC3r6ByxekpQBCpX0Qnrg4o4KVDlYQbIi6X33FAT2f+d01u3F7r9rSjh58ZqxsAOGrxy5psKdVN27f+MdNsxwc6taXjgxXVWtfKFh9/cSo7rp2ILSRUrhSS8rRzTt79fDpccmNXmZWwT9eUonIQPA+hv1E7Lf/6agy+YJ+80dvjF2jMACN05Z29dqbt+u1N2+X5N1LnB2f0zX9ndrSoPLO4ijHy3MhBRDiV74g0QOhZg+dGtdj56b0jjuv2ZAjCtdijJQNoT7y8Jmg/8HGbaAoSZs7W9SedisOIARPQDd6CYMk/eJLnqXn7iGAUKuWlKNbdvWVncSQyRX0r0+M6mUHt7L5rsOBwW6Nz2U1OlN5I6bHL0xpdGZRLz5A48r1dOvuTXrs/FQkGykGD+MTUcLgPwELswfC06Oz+vwPzuvn79qnvf3hpxMDSK6h3jYd2ru5YcEDSdq92TuPnbocTiPFfKEQu+wDiQBCzf7sm0+rtz2tH7s1XnWz3hjH+jcXj5+fkmOkGzb4k3hjjD/KsbKGKseGp9WacrR3HeqysPHcvnezjpyf1Mxibs3XPPjMmGYWc3Qtr9PBoWASQ+V9EL52YkSSdOcByhfW0627+pTJFTSXqW1c13oK0kudJAQQihkI4ZUwfPnIsCTpp27fFdp7AkCz9He1qKPF1amxcBop5go2huEDAgg1OTcxry8duag3375LHS0xqwIJqQfCseFp7e3vVFu6/gYkzbZrc+WjHI8PT+u6oe5EPM1CeYf2blbBSodPT6z5msP+2M8X7N/SqGXF0g6/3Kiabv9fOz6iG7f3xKaHTVQFk3imFyufZtMoxQyEBGT/rEcJw/2PX9SN23uKDYcBYCMzxoQ6iaFQsJQwwPOX3zola63e+oI9zV5K6MLqgXD84nTxieBGV10GwlRxJj1w2+4+OUYlxzlOLWTVknLoXF6nYPzlyPRiRa+fWsjqe6fH9ZLrKF9Yb0O9bdrW26aZhbUzcZolmMLgJGA3FHYJw8j0or53elyvuIHsKQDxsWdLR6gZCHGUgEtmuOYzef3td0/rlTcMxTLiboypuwfCXCan02Nzum5wY5cvBHZuatfUQk6T86Wfno1ML2p0JhOLBooIR3dbWgeHevTgMyUCCPM59bQRPKhXZ2tKnS1uxQGEYNb0iw8w+aIRbt3dV7KUp1kK/uYuSRkIYY1q/uqxi7JWBBAAxMruzR06MzZXvD7UI1+wsexvRQChSl9+fFiT81n97Iv2Nnsp6yKMDIQTF2dkbTwaCUoqBorOlSljOD7s1V5fH5PvG+G4eWdv8dhYzfRCVj1tjMsMw0B3q0ZmKgsgfO3EiLpbUxu+0etGcdvuTVrMFZQNeYRgvYLrXRLKzoLvMBtSD4T7H7+oHX3tG77XEQAst3tLpxZzBV2q8IFEKXl6IEBaGuf3nJ3x3HQaU3+6zXF/EkGcShgklS1jiNMEBoRn30CnRmcymphbfTrA9EJO3WQghGKgu1Uj0+V7IFhr9bXjI3rRs/qVZnxjQwSBmqhlIRSKJQxx3OJdKXgKlg+hhGEuk9O/PjGqV9wwGMunawCSa89mf5RjCGUMeXogQJIm57NqTTlqb9n4zQFXY4wpdqWu1bHhabWnXe3eHI8SjyADoVwjxaMXpjXQ3drQcTOIvmCO8FMjq48EmlrIqqedDIQweAGE8k8MnhqZ0fnJBfofNNCN23sleTeeUZJPUgmD/zGMRsn/+sSoFnMFvZLyBQAxs63Xa6w8PFV5U+a1MIUBkqTx2Yw2dbQ0exnrxqj+BkvHh6d1YKg7Nk90NnWk1dHilg0gHL84FZusC4RnKYAws+rXyUAIz0BXZQGER89NSpJu27NpvZcEXzCRx0asn1TQRDERJQwhTmH48pGL6mlL6fZrNtf9XgAQJVt7vADCpRACCHlrFccUBAIIVRqfy6qvI75PC+stYbDW6tjwtA7GaBKBMabsJIZ8werExRkCCFhh56Z2tbjOmgGEqXl6IIRloLtVUws5lXvAeuLijNKu0TX9nY1ZGCR5UwAKEYsgFJLUA8GEN4Xh2ycv684DA5QAAYidnraU2tOuLoYRQMiTgQBJE3Nxz0AwdTVRHJlZ1NhsJnZ9AHZu6iiZgTA5n1UmV9D2vvYGrgobQcp1tLe/Q09dWr2EgQyE8Ax0e+VD5Rr1nRie1r7+Lm5+GswxEcxA8A+VZJUw1Pc/YT6T17mJeUYWA4glY4wGe1p1car+Joo5eiBAksbnMtrUGd+nhV4GQu31kUG3+bg9iS+XgTDrNwbrauVGECvtH+jSydGVGQhWVvPZvLrJQAhFpQGE4xe9Mis0ljFSxOIHxZ4/cSm5K6VYwlDnJIynR71g6L4BMngAxNPWnrZQMhAK1srEMAeBAEKVJuay6ot1BoJUsKp59mkQQIhfBkK7phZympzPrvr16QUCCFjb/oEunb48t+LGNsj26SEDIRQDXV7dYqkAwuxiTmfH53XdYFejlgWfkZGNWApC3tpElC9Iy0oY6sxACIKhQX8XAIibwZACCGQgQNZaTcxntSnWPRDq22AcG55Wf1f8JhEsTWJYPQth1u8s3sWNIFaxb6BTuYLVqctXHj/BzxkZCOEIMhAyJWq8n7jk3fxcS/p1w5mIljAkoXxBWlbCUGcGwlOXZmWM6CECILYGu/0ShjqvWfVOtosqAghVmFrIKV+w8e6B4O8wau2DcHx4OnblC5K0w+9tcH5i9WhkMNu8kwwErGKtSQzBPHbGOIZjS5d3bi6VgXAiyJIigNBwUSxhKFgrJyE7obCmMJwcndH23vbiZA0AiJvBnjbNZ/PFST21ypOBsDZjzK8aY44YYx4zxvytMaYtjPeNmom5jCSpN8ab/XrmRHuTCKZjV74gqdgc8cLk6o0UZ/wShm4CCFhFUCu8IoBQzEDguAlD2nW0ubNF2VyJAMLFabWlHe3a3NHAlUGKaAlDwSYoA8H7PutplCxJJ0dmtX8r5QsA4mtrj5/RWGI/UYl8gR4IqzLG7JD0f0o6ZK29SZIr6c31vm8Ujc959e9xzkBQsclS9RuMM2NzWswVYvlkb0tni1pcR+cmVg8gzJKBgBK629Ia7GldMYkhV+yBEN+gZKMNdLWWzEA4fnFaz9ralZi69yiJZgmDTUQDRSmcDARrrU6OzGgf5QsAYmywp3xPpUrQA6G0lKR2Y0xKUoek8yG9b6SM+xkIsZ7CoNp7IARPV+P4ZMJxjLb1telCmRIGeiBgLfsHushAaICB7lZlSwRAT1yc1oEYBjk3AmOMbMSKGApJaqLof6zlAUHg4tSiZjN57WcCA4AYCwII9WYgFAo2hvkHIQQQrLXnJP0/kk5LuiBp0lr75XrfN4qCEoZYT2GoowdCMYAQ043Ftt42nV8jA6HYA6GFG0GsLgggLE/hDkqF6IEQnoHuVmXWeGIwOZfVxanFWGZJbQTBlJ8oSVIJg4z3iKCeUc0nR5jAACD+BnvKN2WuBBkIazDGbJL0eknXSNouqdMY8zOrvO4eY8xDxpiHRkZG6v1rm2IiASUM9fRAeOrSrPq7WmIbYNne175mAGF2Maf2tJuYJ1mo3v6BTk0v5DQys1j8XBCoY/xneLwMhMKq3fpOXPIaKJKB0BxRLGFIUgaC5P0/qKeEIXhQsI8AAoAY62hJqbstFUoPBMUwByGMEoYflvS0tXbEWpuV9BlJL7z6Rdbae621h6y1hwYGBkL4axtvfC4rY2LeRDEY41hDxO3k6Iz29cd3U7Gjr10XpxdXHYE1s5ijfAElBaU9y/sg5AtWXa2pRN3ArLeBrlYVrFbtnHzcn8BwIIaNXjcCbwpDtCIIuXzCAggydY1xfGpkVp0tbvHpHADE1WBPW909EJjCsLbTku4wxnQY7+7z5ZKOhvC+kTMxl1FPWzrWm416miw9NTKr/VvjWb4gSdt625UvWF2aXlzxtZnFPE+RUVLwxO7k6FIfhFzBqofAU6gGutfunPzExWl1taa0vTeWg4Iiz5GJXAZC3lo5cdzdraHeDISTo7PaN9BVfNgAAHE12LN2SWSl8vRAWJ219juSPiXp+5Ie9d/z3nrfN4rG57La1BHf7ANpKcmm2h4I47MZjc1mYl0Xub3Pu+lYbZTjzEKWAAJK2tbTpra0o5MjV2YgdDOBIVRBAGG1RorHL07r2kFufpolkiUMhYRlIJj6mig+dWmmOJYWAOJssLut7hKGHAGEtVlr/4u19qC19iZr7VuttSsf0cbAxFwmtvX9gWIJQ5U9EIKnqnHeWGzva5cknVtlEsPsYl6drW6jl4QNxHGMtve1a3hy6fjJF6x62gk8hWkpgLDyHHbi4gwNFJvIm8IQLXmrZAUQZGrOQFjI5nV+cj7WpYoAENgalDDUceHKF6ziWMMQ1hjHRBifyyQmA6HaJxRBXXecMxC2+WnPqzVSnF7MkYGAsoZ62q7IYMmRgRC6ga7VAwjTC1mNzWa0l/n1TWOkK6aQREGhYJWg+IGfgVDbE7WnR2dlrWJdqggAgcEer6dSPWVflDBA47PZWE9gkGrvgfDUyIxaXEc7N3Wsw6qiobstrZ62lC6sEkCYJYCACgz1tq3MQKAHQqh629NytDKAcN7PHNrhZxKh8aJYwpBPYAlDLWOapWUTGMhAAJAAgz3eg8N6yhgY44hklDD4cbJqNxhPjcxqb39H7Ddi2/va1yhhyKmTAALK2NbbpovTi8WfL3oghM9xjNKuo0zuynPYuYk5SUulSGg8r4QhWhGExDVRrKOEIejfcg1ZPAASIJg2U08jxYK1xXurOCGAUKFMrqDZTD7+JQzFDIQqeyCMzMS6fCGwva991SaK04xxRAWG/EkeozNemxivhIHjJmzplFmRgRAE/nZuIoDQLF4JQ7NXcaWkNVGUqf76HjgzNqfBnla1t9DvB0D8be32MhCy9WQg5AtkICTZxFxGktTXGfcMBE81PRAyuYJOjc0lIoCwrbdtRQ8Ea71/g64WbgRR2jY/HW54ckEFa2WtVU97vIOSzZB2nZUBhPF5pV1T7JGAxotkCYO1SiUogGBU+xSG0ZnF4oYaAOJuawgZCLWWjEUdAYQKjc9lJUl9Md/sB1MYqjngT4/NKV+wsZ7AENje167xuazmM/ni54J/K0oYUM5QbzAKdKGYRkwGQvhWCyCcn5jXtt52OQm6WYwaY4wKUSthKNhEHRPG1N4QbGRmUf1d8X6IAgCB1pSrtGvqCyBYeiAk2rifgUATxZWCxkpJyEAIGrCdX1bGkPcfqVHCgHKCAMLw5Hwx8NRDD4TQtbiOsnl7RSD03MQ8DRSbLIolDPmClRvH3d0a6umBMDK9WByTCgBJkHYdZXP1TmGI3zWGAEKFJoIMhJj3QAjkq6iRLHZmTkAGwmqjHIObFKYwoJzNHS1qcR1dmCIDYT21pBxZSWfH54qfOzc+rx30P2iqSJYwJDEDoYanaYWC1eWZjPopAQKQIC2uU1cGAlMYEi7ogbAp7j0Q/IM8W0WN5MmRWW3tbk1EN/mgg/uFiStH8UkEEFCe4xgN9rZqeHKhGKSjB0L4Ovwmb8eHpyV5Ix0vTi8wgaHJnAhOYSjYhGUg1FjCMDGfVa5gyUAAkCgtKaeuMY7WKob5BwQQKhb0QIj9FIYaxjg+lZAJDJKXgm6Mlw4doAcCqjHU0+YHEIISBo6bsLVfFUAYnlyQtdJOAghNFWyiChFqKpVP2BQGI1NTBkIwOYYAAoAkSbveVCdbQ/pc8CfiGKMmgFChibmMWlKO2tPxHl9USw+EZ0ZndU0CyhckrxZqa3frFaMcgx4IpKKjEkO97RpeVsJAD4TwuY5RW8rR8YteAOHsuPfzSgZCcwVNerM1jhFcD3mrxJUw1NIVfGTaCyBQwgAgSbzMudrOm0tBh/hdYwggVGh8LqNNHeniBiiugu+u0h4Ik/NZjc9ltWdzx/otKmK297Xr/ColDGQgoBLbett0YXJB+XwQeCKAsB7aW1yd8AMIQc8SeiA0VzFAXeMYwfVQKFi58b6sX8GouhLFQBBAIAMBQJIE9321Np/13iOs1UQHAYQKjc9lYz+BQVr2hKjCDcbpy16Tsj1bEhRA6G1fvYliCwEElDfU06ZMrqDFXF7GGLWlOQ2vh44WVydHZpXJFYolR0ETVDRHsIeKUgAhcSUMxtT0JC0oYSADAUCSBFeHWhopBgkIcbzCsHOt0MRcJhETGIIoWaUbjFNjs5KkPVuSUcIgSdv72nR+cr6YmrSUgRDv8haEI7iJncvklXJM7LOamqW9JaVcwerk6IzOT8yrv6tVbTEvQYu6KJYwFKyVk6CfQa+JYvX//iPTi2pJOfRsAZAoQXw5W0MjRXogIDkZCP7HSlN1TvkZCLsTVMIw1NuuhWyhONqzYK3a0o5SLj9OKG/QDyDMZ/KJevLZaMsnMZybYIRjFESxhCFfsEolqIbBqLZU3JHpRQ10tRLwBJAo9ZQw0AMBfgZCAgIIJpjCUFmk7fTlOfV3tSaq/n+7fwN4YdLrg5ArWHW1xj87BeEIMhByBasUAYR10552lXKMF0AYn2cCQwQER3u2jpnaYcsXkpaBYGoK4IzMLKqf/gcAEia4PNQyypEMhISz1mpiLhv7EY7S8g1eZRuMZy7PJqr/geSNcpRUnMRQKFh1Ub6ACg10tRZT4shAWD/GSPsGOosZCNv76H/QbGE0owpb3iasB4JqL2EYoP8BgIRxir3h6IGwHAGECkwv5pQr2GSUMFTZA+H02FyiJjBIS6PgggyEfMEmKgMD9Um5jrZ2ezezSbpxaYbrhnr03WfGtJgraAcZCE23VMIQrQwEN46Ph9ZQ6xjH0ZmMBrrjvwcCgOWqfbC66nvE8BpDAKECE7NerXsSmigGKnlCtJDNa3hqQbsTloHQ39WqlGM0HAQQrFUXAQRUIeiDQAnD+rpusEvTCzlJS4E/NE+w4ahnIxa2QsHKSdDPoZGp+t8/X7AamyUDAUDyBPf+tWUgeOfaOF5hCCBUoL3F1S++ZL9u3N7b7KWsOyOjlGMq6oFwdnxO1kp7EzSBQfKeGg/2eJMYJG9zRQAB1djWQwZCIxwY7C7+miaKzbdUwhChDARLBkI5l2cXVbDSAD0QACSMqaeEofgm4a0nKrjrqcBAd6vec/fBZi+jYVynsiZLxQkMCctAkLw+CMOTC2rt8QMInfwooXJDvW2aFwGE9XZwqKf46519yTtPRc3Sk5zoZCDkC0pWBoKpfiM8Op2R5GXfAUCSOHVct+iBgERJu05FJQzP+AHkVVr/AAAgAElEQVSEpPVAkLwbQHogoFbBJAbX4RS8nnZualdHi6vOFlc97fyMNpvxt1FR6oFQsFZJmsBrZKrOQBiZWZREBgKA5KnnumX9HAR6IKzBGNNnjPmUMeaYMeaoMeYFYbwvmsN1KttgnL48q67WlDZ3Jq+x0vbetuIUhry16iaAgCoM0QOhIRzH6NrBbu3Y1B7LC/hGU2yiGKUpDAlsopgr2GXzycsbmfYCCGQgAEia4hjHWgLfMc5ACOuu539I+qK19k3GmBZJyXskHSMpx1RUo3pqbE67N3ckcmM+1NuuhWxB2XxBBTIQUKWgoV/KTd7PTqO99+6DWszlm70MqL5mVOsleU0UPfmCrfj8M0oGAoCEWuqBUEMJQ/E9QlxQRNR912OM6ZF0l6SflSRrbUZSpt73RfNU2gPh9OU5HdzWXfZ1cbTdf4I8n/FuTAggoBq37d6k7/V3qrc9OZNdmuUF+7c0ewnwLaWCRigDwdpEZQItNbK0SrmV/ZmR6UWvFIjrHICEqWf88FIPhPhdY8IoYdgnaUTSnxljHjbGfMwYk6y2/DFTSQ+EfMHqzPicdm9O5v/qIAV9PusFEChhQDWCSR5OHMPSwBqWShiik4GQS2gGQjVlJKMzi5QvAEik4Ea5lhIGG+MahjACCClJt0n6qLX2Vkmzkt579YuMMfcYYx4yxjw0MjISwl+L9VJJD4TzE/PK5q32JHACgyRt6/VS0MlAAIDKRHEKQyGBPRAkKV/F/4OR6UXKFwAkUj0lDDGOH4QSQDgr6ay19jv+7z8lL6BwBWvtvdbaQ9baQwMDAyH8tVgvXg+E0j8op8eSO4FB8mpBXccUMxC62gggAEApxRKGCGUg5K1N1DjV4ma4iv8HI9OL6u9KXrNkAKind0+ceyDUHUCw1g5LOmOMuc7/1MslPV7v+6J5vB4IpX9QTvkjHHcnNAPBdYwGu1s1FwQQWissJgWAhIpiBoK1SlQp0fImipUanSEDAUAyFfvG1NQDwR/jGMMchLAem/5HSX/tT2A4KennQnpfNEGqgh4Ip8Zm1eI6xVT+JNrW167Hc94JhRIGAChtaSMWjQBCsIpkZSB4Hyt9mpbJFTQ+l9VAV9s6rgoAommpBwJTGJYL5a7HWntY0qEw3gvNl6qgB8LZsXnt2NSeqI3X1YZ62/T4hPfrLgIIAFDSUgO/iJQw+Je5JF3HgidhlWYgXJ71Rjj2d1PCACB5lnog1D6FIY7C6IGAmHEr6IFwdmJeO/qSm30gSdt6lp7IEEAAgNKciJUwBKtIVAlDlf8PRqe9qdwDTGEAkEB1jXG86j3ihAACVkhV0APh3DgBhG3Lvn9KGACgtHpqSddDMGLLTdBOqDiFocIMhJGZBUlSPz0QACRQcO9fSwmDYtwDIUGXTVQq5ZbOQFjI5jU6s6gdmxIeQOj1MhAcY5RO0g4UAGqwVMIQjQyE4PFQojIQqpyEQQYCgEQzXvYcUxiuxF0PVkg5TsmnE+cn5iWJDAQ/gJCk+lkAqFU947DWQ3CVSyXoHL6UjlthCYPfA2ELYxwBJJRR+czs1dADAYlSrgfCuSCAkPgMBO/7J4AAAJWI2BSGBDZRDFSaBTI6nVFHi6uOFsr0ACSTMbX17iEDAYlSrgfCuXEvgLAz4QGEge5WGWMSufkEgFoYGWWjMoXB3945CTqHV9uH4vLsovopXwCQYMYYZWrKQKAHAhIk5ZYe43h2fF6uYzTUk+y50K5j1OISQACAShkTwQyEOD4eWkPwnVY8xnEmQ/kCgERzjJTN1R74juMVhgACVkg5TtkShqGeNqVoHKiutrQ60m6zlwEAG4IXQIhGBkJxjGOCgsDFPhSVljDMkIEAINmMamv+GwSp4xijpqgNK7hO6QwERjguuXZrV7OXAAAbhiNT8c1royQqA8H/XvOVTmGYyejW3X3ruSQAiLSaSxiW/fm44REyVkg5pmSX7HMT84lvoAgAqF6kMhD8x0NJKkMLvtNKGoLlC1Zj9EAAkHCmxhIGG+MxDAQQsEKpHgi5fEHDUwtkIAAAqhapHgj+xySWMFTSA2FiLqOClbZ00gMBQHI5pvR0urUUMxDCXU4kEEDACm6JHgjDUwvKFywZCACAqpkolTAksomi972WyjIMjM5kJEn93WQgAEguo8rOmSvEuAcCAQSskCrRAyEY4UgGAgCgWpEqYfA/JqkfcDUZCJdnFiVJWzoJIABILmOkTC0lDGKMIxLELdED4dyEH0AgAwEAUCVjTEX1941QHOPoJGcrFAQQKknHHfEDCP2McQSQYN51q5YeCN5D2ThKzlUTFUuX6IFABgIAoFbeOKyoZCAETRSbvJAGCp6EVdKH4nJQwkATRQAJ5pgaxzgqvk16E3TZRKVK9UA4NzGv/q4WtaXdBq8KALDRRamJYsCJY4HqGpZKGMoHcS7PLsp1jHrb0+u8KgCILiNTWwmDJYCABCnZA2FinuwDAEBNjKmxGdU6WCphiOcGbzXVjHEcnc5oS2dLoqZUAMDVar1uWdnYXl8IIGAF1w8grDa/9Nz4PP0PAAA1MaptHNZ6KDZRTFQGgve9VtREcXZRWyhfAJBwpsYSBokeCEiQtOvXSF71w2KtJQMBAFCzKE1hCFIQkvSEPYiVZCsoYRiZydBAEUDiOcYoW3MJQzxvteP5XaEuwcF+9ROK0ZmMFnMFAggAgJo4prL0+UZYGuOYoACC/zFfURPFRRooAkg8Y6RMDdctL4CwDguKgJh+W6hHkG5zdQbC0gjHjoavCQCw8XklDNHIQAiucElsopgtk45rrdXozCIZCAASr9bpQVZWKTIQkBTB05ir00wZ4QgAqEekpjAksImiZPxGyaU3w3OZvBayBXogAEg8U2MJgxTf60toAQRjjGuMedgY84Ww3hPNsVYPhDPjc5KkXZsJIAAAqmeMqaj+vhGS2ERR8ja05YI4ozOLkkQJA4DEq7X0jjGOlfllSUdDfD80yVo9EM6MzamvI63uNmZCAwCqZyRlc9HIQAgmDcV1g7eWtOuU7Sg+OpORJG2hhAFAwhkZZfKFVafTlWIV3+tLKAEEY8xOSa+R9LEw3g/NtVYPhDPj89pF/wMAQI28cVjRyEAIxHWDt5ZgVHMpl/0MhAEyEAAkXJCkVsn42+WstYxxLOP/lfQeSdHaFaAma/VAODs2R/kCAKBmxpgITmFo6jIaLuUYZcuM0iQDAQA8xo8gVHvtsopvk966L5vGmNdKumSt/V6Z191jjHnIGPPQyMhIvX8t1lFqlR4IhYLVWTIQAAB1MFoZnG6WIBs1rhu8taTcyjMQtnSSgQAg2YIrRKbaa5dduqeKmzDi7i+S9KPGmGckfVLSy4wxf3X1i6y191prD1lrDw0MDITw12K9pFbpgXBpelGZfEE7NxNAAADUxpjyIwQbJ5k9EFKOU/ZJ2ujMonraUmpJJSw9AwCuElwiqg1+W9nYXl/qvjJYa99nrd1prd0r6c2Svmqt/Zm6V4amWSphWNpgFCcwbKKEAQBQG2MMGQhN5mUglClhmM2ov5vsAwCouYTBxnfKD6FlrLDURHFpg3FmLBjhSAYCAKA2RlLBemVxzbbUAyGeG7y1uI4pmwUyOr2ofsoXAKDYRLFc75irMYWhQtbaf7HWvjbM90TjrdYD4czYvCRpRx8ZCACA2gR7qWyEJjHEdYO3lrTjKF/mSdrl2Yz6u2mgCABOMQOhygACPRCQJKv1QDgzPqfBnla1pd1mLQsAsMEFqaC5CExiCGZ6J62EwXVM2VGal2cWaaAIAFpqolj9FAYb2+sLAQSssGoPhLE5JjAAAOoSbKUiEUDwPyYuA8E1V2QYXi2bL2h8LssIRwDQ8h4INUxhiOn1hQACVlgqYVj6QTk7Pk//AwBAXYobsQiUMARNFJMWQHAdUzKAMzy5IEka6mlr1JIAILKCJIJqxzh6PRDieasdz+8KdVlqouhtMLL5gi5MzjOBAQBQF1Mch9X8DIRA0gIIKdcpWcJwcnRWkrRvoKtRSwKAyHJqvG5ZK7kxvdOO6beFehR7IPg/KOcn5lWw0k4yEAAAdViqJY1QBkJMa1TXknLMFT2OrnZyZEaStG+gs1FLAoDIMqqxiaJs8Z4qbuL5XaEu7lUZCMEEBnogAADqUWyiGIExjkEXhJju79bkOqZkM7CTI7PqbktpSyc9EACg1hIG2fhmuCXssolKXN0D4cz4nCRp12ZKGAAAtVsqYYhABoL/MWkZCGnXKZ2BMDqjfQNdxWAPACRZrdODvB4I8TyPEkDACkEPhHwxA2FOKcdoWy8BBABA7YJNR7XjsNZDkpsolkrFfXpkVvv7KV8AAGmpB0LVJQxkICBJgnqdINJ2Znxe2/vaY/tDAABojKUShuZnIEheRkTSnrSn3bV7IMxlcjo/uUD/AwDw1dq7x+uBEM/rCwEErOC6KzMQKF8AANTLFJ/kRCEDwSaufEHyxoqt1YPiaSYwAMAViuOHa5jC4BBAQFIE0bJgTvfZ8TkaKAIA6hZ0s45KD4S4bu5KSTtmzQyQkyNeAOEaShgAQNLywHf11y0yEJAYy3sgzGVyGp3JaBcjHAEAdSo2UYzAFAZrk9dAUfJqctdqBnZyZFbGEEAAgIBTzECotgeCjW35NwEErLC8B8KTl7x50PuphwQA1KmeJznrIa5Ph0pJuaVKGGa0vbddbWm3wasCgGgKrhKZXLU9EOIbpCaAgBWW90A4dmFaknTdUE8zlwQAiIGlEoYIZCDIJrKEIeWYNUtITo7O0kARAJZZav5bfQ+E4J4qbgggYIXlPRCODk+pPe1qNyUMAIA6LZUwRCADIcYjtkpJuWbVjbC1VidHZrWP8gUAKCpmztWQgRDXLDcCCFih2AMh72UgHBjqTuQmCwAQrkhNYdBSbWuSpJzVxziOTC9qZjHHBAYAWKbWMY4SJQxIELeYgWB1bHhK1w91N3lFAIA4KJYwRCADwVrJTeAuyHWcVUtIThZHOJKBAABFRmpxHWVraP7rOvG8yMTzu0JdjDFyHaMLE/Man8vqIAEEAEAIopSBIMX36VApaXf1MY7BCEcyEADgSmnXVFXCYP1LXIoeCEgS1zF67PyUJOngNhooAgDqV2xGFYEAQlKbKLqOUcFKhauepp0cmVFb2tG2nrYmrQwAoinlOlWVMFh559e4lskRQMCq0o7RExe9CQxkIAAAwhBspaJTwhDPzV0pab9u4+pGiidHZ7V3S2cigyoAUEradZSpIvAdvJImikgU1/G6NG/rbVNfR0uzlwMAiAGHEoamC4Imy4M4mVxBD58e1w1kHALACi3u2uNvV+Vf4uIapCaAgFWl/CcUZB8AAMKyVMIQjQyEJD5tTzkrZ5p//cSIxueyeu1ztjVrWQAQWVWXMPhNEAggrMEYs8sY84Ax5qgx5ogx5pfDWBiaKzjgrxviaQQAIBxLJQzNz0CwsrFNLy2lGEBYlgXy2cPntLmzRXdeO9CsZQFAZKVdU1XmXPBKAghry0l6t7X2ekl3SHqXMeaGEN4XTZT2D/jrt5GBAAAIx9IUhuZnIEjxbXBVSqrYA8H7fzC9kNU/P35Rr7t5W7E/AgBgSbrqJoqeuAap675SWGsvWGu/7/96WtJRSTvqfV80l+uPHTlIBgIAIDRGjonIFIaENlEMNrR5Pwvki48NazFX0BtuZesGAKtpSVVbwuB9jGuZXKihZmPMXkm3SvpOmO+Lxks5jtKu0b6BzmYvBQAQIynXUTYKUxgU381dKe5VJQz/ePic9m7p0C27+pq5LACIrJRTXQlDkINABkIZxpguSZ+W9CvW2qlVvn6PMeYhY8xDIyMjYf21WCeuY/Ssrd2kMwIAQpV2TCQyEGQlN557u5KWj3EcnlzQvz11Wa+/ZUexwSUA4EpVlzDEfApDKow3Mcak5QUP/tpa+5nVXmOtvVfSvZJ06NChCOwcUMqL9m/RYG9bs5cBAIiZlOtEYwqDbGw3d6UE33M2X9Anvvm0rBXlCwBQQkvK0exiruLXx72JYt0BBOOFrP9U0lFr7YfrXxKi4Ddff1OzlwAAiKG0a5SNxBSGZDZRTPtpF7/2qUf0gzMTesvzduuafsoVAWAt1ZYwBBkIcS1hCCMD4UWS3irpUWPMYf9z77fW3hfCewMAgBhJOdHIQFBCmyi6jlfC8OjZCX3g1dfrHXde0+QVAUC0VT+FwYsgBOfbuKk7gGCt/YaWRjsDAACsKeVGoweCVTIDCPsHOnX9th695+7r9NLrtjZ7OQAQeekapzDEtZVcKD0QAAAAKpF2nWiUMFibyBKGfQNd+t+/fGezlwEAG0a66ikMnrhmIMTzuwIAAJGUckwkShis4lufCgAIT61TGOJ6jSGAAAAAGiblOjU9yQmdlZyYbu4AAOHxShiqaKLo90CIa5YbAQQAANAwadcoV4hGBoIb080dACA8LbVmILjxvMYQQAAAAA3jlTA0PwPByiayiSIAoDreGMdqpjB44nqNIYAAAAAaJlXlk5x1QwkDAKAC6ZRTXeDbT0GIa5YbAQQAANAwXglDFDIQpJhmlwIAQpR2HWXyBVlb2bWLDAQAAICQpBwnGlMYyEAAAFQg7V8rKg1+0wMBAAAgJGm3tnna6yGu6aUAgPCkU94tc6Xld8UMhJheYwggAACAhkk5TkSmMNBEEQBQXtoNAgiVZiD4PRBieo0hgAAAABom5UZkCgMlDACACqT9UoRqGwCnnHjeasfzuwIAAJGUdh1lI5CBIHmjuQAAKGUpA6HCEgY/Rh7T+AEBBAAA0DgpJyIZCJKcmNanAgDCEwQQKr12Ba8iAwEAAKBOKdeJRBNFa+mBAAAoLyhhyFScgUAPBAAAgFCkXROJJopSfDd3AIDwVF3C4H+M6zWGAAIAAGiYlONEo4TBUsIAACivGEDIVTqFwftIAAEAAKBOaddU3cl6PVhJLrsgAEAZxSkMFWfPeRGEuDbq5dIJAAAaJuUa5QrNz0CQJJcMBABAGUsZCNVNYSADAQAAoE4px1G+YItNpprJienmDgAQnqUeCNVNYSCAAAAAUKdiKmgT+yAUnw6RgQAAKKPaEoa4X2MIIAAAgIZJBfO0mziJIQhdkIEAACin2hKG4CoT12sMAQQAANAwQVOpZmYgxL3BFQAgPFWXMFgpzleXUAIIxpi7jTHHjTFPGmPeG8Z7AgCA+Ak2YrkmTmKIe4MrAEB4ghKGSjPnrBTrCELdAQRjjCvpDyW9StINkt5ijLmh3vcFAADxk4pCDwT/oxPT+lQAQHiCwHemiikMcb66hJGB8DxJT1prT1prM5I+Ken1IbwvAACImbQTpII2LwNBZCAAACpU/RQGSwChjB2Sziz7/Vn/cwAAAFdIFVNBm5mBEO8GVwCA8CxND6ow8G0lE+MMt1QI77Hav86KXYEx5h5J90jS7t27Q/hrEbqhZzd7BRvOwc0Hm72EULVeH6/vJ8r6d3U1ewmxtXXPvmYvAato2d4padkUhiZmIGwZGNTYM5nYjthaTXfX9c1eAgBsKDd1tUtaum5VGkDo7WjR1ELO+00M76/CCCCclbRr2e93Sjp/9YustfdKuleSDh061MzWy1jLq3632SvYcH79eb/e7CWEauj972/2EhLjzp880OwlxNZLf/aeZi8Bq+h73X5JUvrRC5Ka2wPhjhe/XO/+1lf1xgTNojpw4DeavQQA2FA+eO1OSdJ8Ji+p8uvWLbt6NTWf9X4Tw/urMC6dD0q61hhzjTGmRdKbJX0+hPcFAAAxU8xAqLCb9XrI+5tAmigCAMqptoQhX4h3j526MxCstTljzC9J+pIkV9LHrbVH6l4ZAACInaAHwtELU3Ido86WlAa6W9XZGkZSZGXy/hzHOG/wAADhcB0jYyovvcsXCsVrXRyFcrW21t4n6b4w3gsAAMRXb3takvTrn370is+3p11dO9ilO6/t153XDuj2vZvX7QY/XyCAAACojDFGacdRpsIShlzBxrrHTuPC/QAAIPFu3dWnf3zXizQ+l9FitqCZxZxGZxZ1aWpRj5yd0B997aT+8IGntGtzu9555z79xHN3qb3FDXUNBTIQAABVSLum4hKGgrWxvr4QQAAAAA1jjNEtu/rW/PrUQlZfOz6ij3/zaf3fnzui37//hP7TKw7op5+/J7QNWTEDIcZPiAAA4UmnnIpLGHL5eAcQEtR/GAAARF1PW1qve852feb/eKH+4RdeoINDPfqNzx3Ra/7gX/Wtpy6H8ncEAQQnxhs8AEB4UlWUMOQLBBAAAAAayhij2/du1t+88/n6X//uNk0v5PSWP/m2fuqPv6VvPjkqa2sfA1ksYSADAQBQgZYqShjy1ipFAAEAAKDxjDF69bO36Z//04v1G6+9QU+Pzurffew7euNH/00PHLtUUyCBJooAgGqkU04VYxzJQAAAAGiq9hZXb/+ha/T197xUH3zDTbo4taif+8SD+tGPfLPqQEKQgUAJAwCgEinHKFfpFAZ6IAAAAERDW9rVW+/Yowf+80v0oTferMn5rH7uEw/qp+79tr53aryi9wgeIlHCAACoRNp1lKmihIEAAgAAQIS0pBz95O279JV3v1gffMNNOjkyqzd+9N/0W194XJlc6U1eruB93WEXBACoQEvK0eRcVo+dm9SDz4zpsXOTOj8xr4VsfsVr8wWrVIwvMIxxBAAAG1badfTWO/bojbft0O/cd0wf+8bTevCZMf3Pt9ym3Vs6Vv0zBTIQAABV6Ghx9e2TY3rt//zGFZ83Rrqmv1M37+jVwW092rmpXRNzGXVuWv36EwcEEAAAwIbX0ZLSB99wk164f4ve8+lH9LqPfEMf/9lDeu6ezStem7c0UQQAVO63f+zZevTspDpaXLW3uJrL5DU+m9Hw1IKOnJ/St0+O6R8Pny++/sBgdxNXu74IIAAAgNh41bO36cbtvXrbn31XP/0n39FHfvo2veKGwSteU2AKAwCgCvsHurR/oKvkaybns7owOa/zE/O6fltPg1bWePEtzgAAAIm0e0uHPvULL9DBoW79/F8+pE9+9/QVX2eMIwAgbL3taR0c6tHLDg5qW297s5ezbgggAACA2NnS1aq/eecduvPaAb33M4/qD77yRHHUY1DC4NADAQCAqhBAAAAAsdTZmtLH3nZIP37bDn34/hP6jc89pnzBUsIAAECN6IEAAABiK+06+u8/8RwNdLfqj792UseHp/Wyg15PBAIIAABUhwACAACINWOM3veq63Vga7f+6+eP6MFnxiVRwgAAQLUoYQAAAInwxufu1Bd/9S69cP8WtbiO+jrSzV4SAAAbChkIAAAgMXb0teuv3v58TS1k1dfR0uzlAACwoZCBAAAAEsVxDMEDAABqQAABAAAAAACURQABAAAAAACURQABAAAAAACUVVcAwRjze8aYY8aYR4wxnzXG9IW1MAAAAAAAEB31ZiDcL+kma+3Nkk5Iel/9SwIAAAAAAFFTVwDBWvtla23O/+23Je2sf0kAAAAAACBqwuyB8B8k/e8Q3w8AAAAAAEREqtwLjDH/LGlolS99wFr7Of81H5CUk/TXJd7nHkn3SNLu3btrWiwAAAAAAGiOsgEEa+0Pl/q6MeZtkl4r6eXWWlvife6VdK8kHTp0aM3XAQAAAACA6CkbQCjFGHO3pF+X9GJr7Vw4SwIAAAAAAFFjSiQNlP/DxjwpqVXSZf9T37bW/kIFf25E0qma/+Lm6Zc02uxFINI4RlAOxwjK4RhBORwjKIdjBOVwjKCUPdbagdW+UFcAIWmMMQ9Zaw81ex2ILo4RlMMxgnI4RlAOxwjK4RhBORwjqFWYUxgAAAAAAEBMEUAAAAAAAABlEUCozr3NXgAij2ME5XCMoByOEZTDMYJyOEZQDscIakIPBAAAAAAAUBYZCAAAAAAAoCwCCBUwxtxtjDlujHnSGPPeZq8H0WCMecYY86gx5rAx5iH/c5uNMfcbY57wP25q9jrROMaYjxtjLhljHlv2uVWPCeP5A/+88ogx5rbmrRyNtMZx8l+NMef888lhY8yrl33tff5xctwY8yPNWTUaxRizyxjzgDHmqDHmiDHml/3Pcy6BpJLHCOcRFBlj2owx3zXG/MA/Tn7T//w1xpjv+OeSvzPGtPifb/V//6T/9b3NXD+iiwBCGcYYV9IfSnqVpBskvcUYc0NzV4UIeam19pZlY3DeK+kr1tprJX3F/z2S4xOS7r7qc2sdE6+SdK3/3z2SPtqgNaL5PqGVx4kk/b5/PrnFWnufJPnXmzdLutH/M//Lvy4hvnKS3m2tvV7SHZLe5R8HnEsQWOsYkTiPYMmipJdZa58j6RZJdxtj7pD03+QdJ9dKGpf0dv/1b5c0bq19lqTf918HrEAAobznSXrSWnvSWpuR9ElJr2/ymhBdr5f05/6v/1zSG5q4FjSYtfbrksau+vRax8TrJf2F9XxbUp8xZltjVopmWuM4WcvrJX3SWrtorX1a0pPyrkuIKWvtBWvt9/1fT0s6KmmHOJfAV+IYWQvnkQTyzwkz/m/T/n9W0sskfcr//NXnkuAc8ylJLzfGmAYtFxsIAYTydkg6s+z3Z1X6JI3ksJK+bIz5njHmHv9zg9baC5J3gZe0tWmrQ1SsdUxwbsHVfslPQf/4svInjpME81OIb5X0HXEuwSquOkYkziNYxhjjGmMOS7ok6X5JT0masNbm/JcsPxaKx4n/9UlJWxq7YmwEBBDKWy3yxugKSNKLrLW3yUsffZcx5q5mLwgbCucWLPdRSfvlpZlekPTf/c9znCSUMaZL0qcl/Yq1dqrUS1f5HMdIAqxyjHAewRWstXlr7S2SdsrLOrl+tZf5HzlOUBECCOWdlbRr2e93SjrfpLUgQqy15/2PlyR9Vt6J+WKQOup/vNS8FSIi1jomOLegyFp70d/oFST9iZbSizlOEsgYk5Z3Y/jX1trP+J/mXIKi1Y4Rzq7xVSMAAAHSSURBVCNYi7V2QtK/yOuZ0WeMSflfWn4sFI8T/+u9qrzcDglCAKG8ByVd63csbZHXhObzTV4TmswY02mM6Q5+LemVkh6Td2y8zX/Z2yR9rjkrRISsdUx8XtK/9zuo3yFpMkhPRvJcVbP+Y/LOJ5J3nLzZ7459jbxGed9t9PrQOH7N8Z9KOmqt/fCyL3EugaS1jxHOI1jOGDNgjOnzf90u6Yfl9ct4QNKb/JddfS4JzjFvkvRVay0ZCFghVf4lyWatzRljfknSlyS5kj5urT3S5GWh+QYlfdbvLZOS9DfW2i8aYx6U9PfGmLdLOi3pJ5q4RjSYMeZvJb1EUr8x5qyk/yLpd7X6MXGfpFfLa2Y1J+nnGr5gNMUax8lLjDG3yEsXfUbSz0uStfaIMebvJT0ur/P6u6y1+WasGw3zIklvlfSoX7ssSe8X5xIsWesYeQvnESyzTdKf+xM3HEl/b639gjHmcUmfNMb8lqSH5QWj5H/8S2PMk/IyD97cjEUj+gyBJQAAAAAAUA4lDAAAAAAAoCwCCAAAAAAAoCwCCAAAAAAAoCwCCAAAAAAAoCwCCAAAAAAAoCwCCAAAAAAAoCwCCAAAAAAAoCwCCAAAAAAAoKz/H0EpxeQViVMiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_features_shortened = cur_vid_feat[:item[1][idx]]\n",
    "cur_features_shortened[:, item[2][idx][:item[1][idx]]].shape\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 47, 47, 47, 47, 47, 47,  4,  4,  4,  4,  4,  4,  4,  4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[2][idx][video_id_boundary_frames[cur_vidid][1]-6:video_id_boundary_frames[cur_vidid][1]+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid][0]-6,video_id_boundary_frames[cur_vidid][0]+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0794, 13.3350],\n",
       "        [ 1.0831, 13.5102],\n",
       "        [ 1.1767, 13.7662],\n",
       "        [ 1.3750, 14.0177],\n",
       "        [ 1.7298, 14.1727],\n",
       "        [ 2.1706, 14.0367],\n",
       "        [ 2.7338, 13.4563],\n",
       "        [ 4.5600, 12.1745],\n",
       "        [ 7.8450, 10.3246],\n",
       "        [11.0151,  8.3563],\n",
       "        [13.4775,  6.6546],\n",
       "        [14.1836,  5.2898],\n",
       "        [11.2142,  4.2791],\n",
       "        [ 5.9829,  3.5694],\n",
       "        [ 2.3440,  2.9389],\n",
       "        [ 0.7406,  2.3515]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_vid_feat[video_id_boundary_frames[cur_vidid][1]-7:video_id_boundary_frames[cur_vidid][1]+9][:,[47,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frames_and_get_classification_value(model_output, count_list, video_ids_list, labels_all, p_criterion):\n",
    "    new_selected_frames = {}\n",
    "#     new_selected_frames_probs_values = {}\n",
    "#     new_selected_frames_labels = {}\n",
    "    \n",
    "    labels_arr = []\n",
    "    probs_arr = []\n",
    "    \n",
    "    for i, video_id in enumerate(video_ids_list):\n",
    "        current_boundary_frames = boundaries_dict[video_id]\n",
    "        current_video_logits = model_output[i].T\n",
    "        current_video_count = count_list[i]\n",
    "        current_selected_frames = loaded_vidid_selected_frames[video_id]\n",
    "        labels = labels_all[i]\n",
    "        cur_seg_start = 0\n",
    "        for seg_num, ele in enumerate(current_boundary_frames):\n",
    "            cur_seg_end = ele + 1\n",
    "            total_seg_len = cur_seg_end - cur_seg_start\n",
    "            per_frames_far_from_boundary = int(0.3 * total_seg_len)\n",
    "            new_frames_start = cur_seg_start + per_frames_far_from_boundary\n",
    "            new_frames_end = cur_seg_end - per_frames_far_from_boundary\n",
    "            \n",
    "            frames_added = list(range(new_frames_start, new_frames_end))\n",
    "            frames_added.append(current_selected_frames[seg_num])\n",
    "            frames_added = list(set(frames_added))\n",
    "            frames_added.sort()\n",
    "            new_selected_frames[video_id] = frames_added\n",
    "\n",
    "            \n",
    "            labels_seg = torch.stack([labels[current_selected_frames[seg_num]]] * len(frames_added))\n",
    "            frames_added = torch.tensor(np.array(frames_added), dtype=torch.long, device=current_video_logits.device)\n",
    "            prob_seg = current_video_logits[frames_added, :]\n",
    "            \n",
    "            labels_arr.append(labels_seg)\n",
    "            probs_arr.append(prob_seg)\n",
    "            \n",
    "            cur_seg_start = cur_seg_end\n",
    "            \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    probs_arr = torch.cat(probs_arr)\n",
    "    loss = p_criterion(probs_arr, labels_arr)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
