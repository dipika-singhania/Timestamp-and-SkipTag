{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mstcn_model import *\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 8, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 3, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split3/', 'project_name': 'breakfast-split-3', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split3.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split3.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split3_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=3,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split3/\",\n",
    "    project_name=\"breakfast-split-3\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1279\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 433\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", allow_pickle=True).item()\n",
    "# loaded_vidid_selected_frames\n",
    "video_id_boundary_frames = pickle.load(open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"rb\"))\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_var_actions = pickle.load(open(\"dump_dir/mean_var_actions.pkl\", \"rb\"))\n",
    "def get_possion_prob(cur_len, cur_class):\n",
    "    mean_class, std_class = loaded_mean_var_actions[cur_class]\n",
    "    mean_class = mean_class * 10\n",
    "    prob = cur_len * torch.log(torch.tensor(mean_class, device=device) + 1e-5)\n",
    "    prob = prob - mean_class\n",
    "    factorials = torch.cumsum(torch.log(torch.arange(1, torch.max(cur_len)+1, 1).type(torch.float).to(device)), \n",
    "                              dim=0)[min(cur_len)-1:]\n",
    "    prob = prob - factorials\n",
    "    return prob\n",
    "\n",
    "# get_possion_prob(torch.arange(10, 21), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_video_each_segment_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vals_per_segment(selected_frames, cur_vid_feat, labels):\n",
    "    prob_each_segment = []\n",
    "    \n",
    "    log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "    cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "    cur_boundary = 0\n",
    "    for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "\n",
    "        next_ele = selected_frames[i + 1]\n",
    "        label_cur_ele = labels[cur_ele]\n",
    "        label_next_ele = labels[next_ele]\n",
    "\n",
    "        indices = torch.arange(cur_ele, next_ele).to(cumsum_feat.device)\n",
    "        strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "        end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "        left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "        right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "\n",
    "        cur_boundary_len = indices - cur_boundary\n",
    "        len_prob = get_possion_prob(cur_boundary_len, label_cur_ele.item())\n",
    "\n",
    "        prob = torch.softmax((left_sum + right_sum + len_prob), dim=0)\n",
    "#         prob = torch.softmax((left_sum + right_sum), dim=0)\n",
    "\n",
    "        cur_boundary = round(np.sum(np.arange(cur_ele, next_ele, 1) * prob.detach().cpu().numpy()).item())\n",
    "        prob_each_segment.append(prob)\n",
    "    \n",
    "    return prob_each_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_element_probb(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_video = prob_vals_per_segment(selected_frames, cur_vid_feat, labels)\n",
    "        prob_video_each_segment_dict[cur_vidid] = prob_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_loss(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    loss_arr = []\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_each_video = prob_video_each_segment_dict[cur_vidid]\n",
    "        \n",
    "        log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "        cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "        \n",
    "        \n",
    "        current_vid_prob_arr = []\n",
    "        # Start segment log probability\n",
    "        start_ele_idx = selected_frames[0]\n",
    "        if start_ele_idx > 0:\n",
    "            start_ele_label_idx = labels[selected_frames[0]]\n",
    "            first_segment_prob = (cumsum_feat[start_ele_idx - 1, :])[start_ele_label_idx]\n",
    "            current_vid_prob_arr.append(first_segment_prob.unsqueeze(0))\n",
    "        \n",
    "        for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "            next_ele = selected_frames[i + 1]\n",
    "            label_cur_ele = labels[cur_ele]\n",
    "            label_next_ele = labels[next_ele]\n",
    "\n",
    "            indices = torch.arange(cur_ele, next_ele)\n",
    "            strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "            end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "            left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "            right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "            current_vid_prob_arr.append((left_sum + right_sum)  * (prob_each_video[i]))\n",
    "        \n",
    "        # End segment log probability\n",
    "        end_index = cur_vid_count.item() - 1\n",
    "        last_ele_label_idx = labels[selected_frames[-1]]\n",
    "        last_segment_sum_prob = (cumsum_feat[end_index, :] - cumsum_feat[selected_frames[-1] - 1, :])[last_ele_label_idx]\n",
    "        current_vid_prob_arr.append(last_segment_sum_prob.unsqueeze(0))\n",
    "            \n",
    "        loss_arr.append(torch.cat(current_vid_prob_arr))\n",
    "    return -torch.mean(torch.cat(loss_arr)), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_estimated_boundaries():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames, video_id_boundary_frames\n",
    "    estimated_boundary_dict = {}\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        selected_ele_list = loaded_vidid_selected_frames[ele + \".txt\"]\n",
    "        boundary_list = []\n",
    "        for i, segment in enumerate(estimated_boundary_probs):\n",
    "            estimated_boundary = np.sum(np.arange(selected_ele_list[i], selected_ele_list[i + 1], 1) \\\n",
    "                                        * segment.detach().cpu().numpy())\n",
    "            estimated_boundary = round(estimated_boundary.item())\n",
    "            \n",
    "            if (estimated_boundary < selected_ele_list[i]) or (estimated_boundary > selected_ele_list[i + 1]):\n",
    "                print(\"Estimated value wrong\")\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            boundary_list.append(estimated_boundary)\n",
    "\n",
    "        estimated_boundary_dict[ele] = boundary_list\n",
    "    return estimated_boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_boundary_err():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    err_list = []\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    estimated_boundar_dict = get_estimated_boundaries()\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        estimated_boundary_list = estimated_boundar_dict[ele]\n",
    "        \n",
    "        estimated = np.array(estimated_boundary_list) #.detach().cpu().numpy()\n",
    "        actual = np.array(video_id_boundary_frames[ele][:-1])\n",
    "        if len(actual) != len(estimated):\n",
    "            print(ele)\n",
    "            continue\n",
    "        \n",
    "        mse_err = (actual - estimated)**2\n",
    "        err_list.append(mse_err)\n",
    "        \n",
    "        estimated_labels = []\n",
    "        actual_labels = []\n",
    "        start_v1 = 0\n",
    "        start_v2 = 0\n",
    "        for i, v1 in enumerate(estimated):\n",
    "            estimated_labels.extend([i] * (v1 - start_v1))\n",
    "            start_v1 = v1\n",
    "            v2 = actual[i]\n",
    "            actual_labels.extend([i] * (v2 - start_v2))\n",
    "            start_v2 = v2\n",
    "            \n",
    "        last_ele = video_id_boundary_frames[ele][-1]\n",
    "        estimated_labels.extend([i+1] * (last_ele - start_v1))\n",
    "        actual_labels.extend([i+1] * (last_ele - start_v2))\n",
    "        \n",
    "        correct += np.sum(np.array(actual_labels) == np.array(estimated_labels))\n",
    "        total += len(actual_labels)\n",
    "        \n",
    "    print(f\"Train Boundary avergage error = {np.sqrt(np.mean(np.concatenate(err_list))):.3f}\")\n",
    "    print(f\"Train From boundary avergage accuracy = {correct * 100.0 / total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = MultiStageModel(num_stages=4, num_layers=10, num_f_maps=64, dim=2048, num_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(labels_all, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((labels_all.shape[0], labels_all.shape[1]), dtype=torch.long, device=labels_all.device) * (-100)\n",
    "    for iter_num, labels in enumerate(labels_all):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(loaded_vidid_selected_frames[cur_vidid + \".txt\"]))\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = labels[frame_idx_tensor]\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/mstcnnew-full-supervised-split3/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Training:: Epoch 0, Iteration 0, Current loss 15.741896629333496 Accuracy 4.178496481915813\n",
      "Training:: Epoch 0, Iteration 10, Current loss 15.739640235900879 Accuracy 1.219186615644745\n",
      "Training:: Epoch 0, Iteration 20, Current loss 14.998711585998535 Accuracy 0.25430244248624995\n",
      "Training:: Epoch 0, Iteration 30, Current loss 12.849777221679688 Accuracy 9.611134340839854\n",
      "Training:: Epoch 0, Iteration 40, Current loss 13.779166221618652 Accuracy 8.066712049012933\n",
      "Training:: Epoch 0, Iteration 50, Current loss 13.135438919067383 Accuracy 14.73735705986669\n",
      "Training:: Epoch 0, Iteration 60, Current loss 14.933050155639648 Accuracy 3.402646502835539\n",
      "Training:: Epoch 0, Iteration 70, Current loss 11.373003005981445 Accuracy 10.687778327560613\n",
      "Training:: Epoch 0, Iteration 80, Current loss 12.752055168151855 Accuracy 10.082075721472068\n",
      "Training:: Epoch 0, Iteration 90, Current loss 12.322772026062012 Accuracy 8.80356796587163\n",
      "Training:: Epoch 0, Iteration 100, Current loss 12.620841026306152 Accuracy 16.316250547525186\n",
      "Training:: Epoch 0, Iteration 110, Current loss 10.243560791015625 Accuracy 21.1629279811098\n",
      "Training:: Epoch 0, Iteration 120, Current loss 11.838728904724121 Accuracy 20.10654193464562\n",
      "Training:: Epoch 0, Iteration 130, Current loss 11.303476333618164 Accuracy 13.071836217048645\n",
      "Training:: Epoch 0, Iteration 140, Current loss 10.717470169067383 Accuracy 18.57758620689655\n",
      "Training:: Epoch 0, Iteration 150, Current loss 9.170551300048828 Accuracy 25.178596119279163\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 0, Probability Accuracy 19.410159541520414\n",
      "Starting Training\n",
      "Training:: Epoch 1, Iteration 0, Current loss 10.395261764526367 Accuracy 19.55559653328416\n",
      "Training:: Epoch 1, Iteration 10, Current loss 9.551822662353516 Accuracy 17.58402662229617\n",
      "Training:: Epoch 1, Iteration 20, Current loss 10.171926498413086 Accuracy 14.235605559232296\n",
      "Training:: Epoch 1, Iteration 30, Current loss 10.15794563293457 Accuracy 25.37955002743735\n",
      "Training:: Epoch 1, Iteration 40, Current loss 9.387104034423828 Accuracy 19.715139183514438\n",
      "Training:: Epoch 1, Iteration 50, Current loss 9.544687271118164 Accuracy 22.173083593479014\n",
      "Training:: Epoch 1, Iteration 60, Current loss 9.017638206481934 Accuracy 14.620635077653576\n",
      "Training:: Epoch 1, Iteration 70, Current loss 10.455162048339844 Accuracy 16.92698706099815\n",
      "Training:: Epoch 1, Iteration 80, Current loss 8.047331809997559 Accuracy 21.651615711021652\n",
      "Training:: Epoch 1, Iteration 90, Current loss 7.277575492858887 Accuracy 34.24504158669226\n",
      "Training:: Epoch 1, Iteration 100, Current loss 7.77004861831665 Accuracy 20.615708459413234\n",
      "Training:: Epoch 1, Iteration 110, Current loss 8.840377807617188 Accuracy 25.45679012345679\n",
      "Training:: Epoch 1, Iteration 120, Current loss 8.305907249450684 Accuracy 31.761145091115456\n",
      "Training:: Epoch 1, Iteration 130, Current loss 9.476408004760742 Accuracy 27.43577981651376\n",
      "Training:: Epoch 1, Iteration 140, Current loss 9.465960502624512 Accuracy 22.235509846129315\n",
      "Training:: Epoch 1, Iteration 150, Current loss 7.846184253692627 Accuracy 31.333578701999755\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 1, Probability Accuracy 37.541085150194476\n",
      "Starting Training\n",
      "Training:: Epoch 2, Iteration 0, Current loss 6.716311454772949 Accuracy 41.2376672853375\n",
      "Training:: Epoch 2, Iteration 10, Current loss 8.589126586914062 Accuracy 16.252279131443725\n",
      "Training:: Epoch 2, Iteration 20, Current loss 7.1284003257751465 Accuracy 43.01930446099342\n",
      "Training:: Epoch 2, Iteration 30, Current loss 8.35935115814209 Accuracy 20.364131603469286\n",
      "Training:: Epoch 2, Iteration 40, Current loss 8.038475036621094 Accuracy 32.01527746573804\n",
      "Training:: Epoch 2, Iteration 50, Current loss 8.442239761352539 Accuracy 25.01287200082381\n",
      "Training:: Epoch 2, Iteration 60, Current loss 7.136028289794922 Accuracy 33.22520061464914\n",
      "Training:: Epoch 2, Iteration 70, Current loss 7.016582012176514 Accuracy 36.18807100943326\n",
      "Training:: Epoch 2, Iteration 80, Current loss 8.051106452941895 Accuracy 33.45622119815668\n",
      "Training:: Epoch 2, Iteration 90, Current loss 7.7367706298828125 Accuracy 25.862335653518947\n",
      "Training:: Epoch 2, Iteration 100, Current loss 8.11776351928711 Accuracy 31.86355598653813\n",
      "Training:: Epoch 2, Iteration 110, Current loss 6.695687294006348 Accuracy 40.3967344600401\n",
      "Training:: Epoch 2, Iteration 120, Current loss 5.729940891265869 Accuracy 60.662173657004516\n",
      "Training:: Epoch 2, Iteration 130, Current loss 5.049915790557861 Accuracy 61.5667022411953\n",
      "Training:: Epoch 2, Iteration 140, Current loss 8.119878768920898 Accuracy 23.827432058468744\n",
      "Training:: Epoch 2, Iteration 150, Current loss 6.784367084503174 Accuracy 33.228814922013186\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 2, Probability Accuracy 39.701006132778275\n",
      "Starting Training\n",
      "Training:: Epoch 3, Iteration 0, Current loss 7.742799758911133 Accuracy 33.1071784409546\n",
      "Training:: Epoch 3, Iteration 10, Current loss 7.828760623931885 Accuracy 40.123953638119765\n",
      "Training:: Epoch 3, Iteration 20, Current loss 6.5270256996154785 Accuracy 42.49806651198762\n",
      "Training:: Epoch 3, Iteration 30, Current loss 6.057568073272705 Accuracy 49.877729257641924\n",
      "Training:: Epoch 3, Iteration 40, Current loss 6.609679222106934 Accuracy 32.824527071102416\n",
      "Training:: Epoch 3, Iteration 50, Current loss 8.860028266906738 Accuracy 23.524933384088314\n",
      "Training:: Epoch 3, Iteration 60, Current loss 10.036237716674805 Accuracy 25.24248448701729\n",
      "Training:: Epoch 3, Iteration 70, Current loss 7.884339332580566 Accuracy 31.584783350392357\n",
      "Training:: Epoch 3, Iteration 80, Current loss 6.0359416007995605 Accuracy 61.4115050616514\n",
      "Training:: Epoch 3, Iteration 90, Current loss 5.695268154144287 Accuracy 54.85994733062006\n",
      "Training:: Epoch 3, Iteration 100, Current loss 5.405974388122559 Accuracy 60.46805543449839\n",
      "Training:: Epoch 3, Iteration 110, Current loss 5.063746929168701 Accuracy 47.779350614881004\n",
      "Training:: Epoch 3, Iteration 120, Current loss 5.371365070343018 Accuracy 51.29065864104642\n",
      "Training:: Epoch 3, Iteration 130, Current loss 6.0616841316223145 Accuracy 49.63365440026344\n",
      "Training:: Epoch 3, Iteration 140, Current loss 6.061097145080566 Accuracy 55.90790162218734\n",
      "Training:: Epoch 3, Iteration 150, Current loss 7.338083267211914 Accuracy 42.66382290211076\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 3, Probability Accuracy 46.118046216259955\n",
      "Starting Training\n",
      "Training:: Epoch 4, Iteration 0, Current loss 6.0514302253723145 Accuracy 41.79498460184778\n",
      "Training:: Epoch 4, Iteration 10, Current loss 5.763311386108398 Accuracy 54.119994850006435\n",
      "Training:: Epoch 4, Iteration 20, Current loss 6.5760650634765625 Accuracy 41.26996966632963\n",
      "Training:: Epoch 4, Iteration 30, Current loss 5.847301959991455 Accuracy 50.57886635883022\n",
      "Training:: Epoch 4, Iteration 40, Current loss 6.957117557525635 Accuracy 41.52730049637266\n",
      "Training:: Epoch 4, Iteration 50, Current loss 5.478348731994629 Accuracy 44.797687861271676\n",
      "Training:: Epoch 4, Iteration 60, Current loss 5.7098565101623535 Accuracy 48.99169632265718\n",
      "Training:: Epoch 4, Iteration 70, Current loss 4.851297378540039 Accuracy 53.96966993755576\n",
      "Training:: Epoch 4, Iteration 80, Current loss 5.782764434814453 Accuracy 47.617252705831675\n",
      "Training:: Epoch 4, Iteration 90, Current loss 6.397564888000488 Accuracy 42.879671222402756\n",
      "Training:: Epoch 4, Iteration 100, Current loss 7.470897197723389 Accuracy 35.47008547008547\n",
      "Training:: Epoch 4, Iteration 110, Current loss 6.57570743560791 Accuracy 37.27265507108252\n",
      "Training:: Epoch 4, Iteration 120, Current loss 7.821136474609375 Accuracy 32.0979090278466\n",
      "Training:: Epoch 4, Iteration 130, Current loss 6.4143805503845215 Accuracy 46.553059643687064\n",
      "Training:: Epoch 4, Iteration 140, Current loss 5.175656318664551 Accuracy 58.59987386251014\n",
      "Training:: Epoch 4, Iteration 150, Current loss 5.417765140533447 Accuracy 53.40729962410573\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 4, Probability Accuracy 47.30966903086478\n",
      "Starting Training\n",
      "Training:: Epoch 5, Iteration 0, Current loss 6.703211307525635 Accuracy 50.62451903155153\n",
      "Training:: Epoch 5, Iteration 10, Current loss 6.959700584411621 Accuracy 40.780911062906725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 5, Iteration 20, Current loss 5.303276062011719 Accuracy 51.250194129523216\n",
      "Training:: Epoch 5, Iteration 30, Current loss 6.343340873718262 Accuracy 47.31130187252591\n",
      "Training:: Epoch 5, Iteration 40, Current loss 5.740039348602295 Accuracy 54.85038508480283\n",
      "Training:: Epoch 5, Iteration 50, Current loss 4.781839847564697 Accuracy 61.09089254844873\n",
      "Training:: Epoch 5, Iteration 60, Current loss 5.095589637756348 Accuracy 56.36801730920535\n",
      "Training:: Epoch 5, Iteration 70, Current loss 5.256860256195068 Accuracy 54.55243527863098\n",
      "Training:: Epoch 5, Iteration 80, Current loss 4.7979278564453125 Accuracy 66.16551298443632\n",
      "Training:: Epoch 5, Iteration 90, Current loss 7.16542911529541 Accuracy 34.99862210149207\n",
      "Training:: Epoch 5, Iteration 100, Current loss 4.7020392417907715 Accuracy 52.355575712908895\n",
      "Training:: Epoch 5, Iteration 110, Current loss 8.194049835205078 Accuracy 33.897572125779945\n",
      "Training:: Epoch 5, Iteration 120, Current loss 4.88438081741333 Accuracy 32.84126198298753\n",
      "Training:: Epoch 5, Iteration 130, Current loss 6.611037731170654 Accuracy 43.57428260079913\n",
      "Training:: Epoch 5, Iteration 140, Current loss 4.879569053649902 Accuracy 54.49114687045907\n",
      "Training:: Epoch 5, Iteration 150, Current loss 4.926585674285889 Accuracy 58.46153846153846\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 5, Probability Accuracy 49.69548161003566\n",
      "Starting Training\n",
      "Training:: Epoch 6, Iteration 0, Current loss 6.073794841766357 Accuracy 47.04903341441557\n",
      "Training:: Epoch 6, Iteration 10, Current loss 5.282537937164307 Accuracy 56.44387831560018\n",
      "Training:: Epoch 6, Iteration 20, Current loss 5.694967746734619 Accuracy 46.678885167161724\n",
      "Training:: Epoch 6, Iteration 30, Current loss 4.022789001464844 Accuracy 65.22457820337438\n",
      "Training:: Epoch 6, Iteration 40, Current loss 5.067046165466309 Accuracy 59.332688588007734\n",
      "Training:: Epoch 6, Iteration 50, Current loss 5.48839807510376 Accuracy 58.060654429369514\n",
      "Training:: Epoch 6, Iteration 60, Current loss 5.504728317260742 Accuracy 59.006719252118025\n",
      "Training:: Epoch 6, Iteration 70, Current loss 4.170182228088379 Accuracy 68.39023748437603\n",
      "Training:: Epoch 6, Iteration 80, Current loss 6.7695465087890625 Accuracy 44.86239437082826\n",
      "Training:: Epoch 6, Iteration 90, Current loss 5.192381858825684 Accuracy 57.912306605893306\n",
      "Training:: Epoch 6, Iteration 100, Current loss 3.9228806495666504 Accuracy 63.89087870105062\n",
      "Training:: Epoch 6, Iteration 110, Current loss 4.617598056793213 Accuracy 61.137127430995534\n",
      "Training:: Epoch 6, Iteration 120, Current loss 7.087642669677734 Accuracy 40.37678739171277\n",
      "Training:: Epoch 6, Iteration 130, Current loss 4.984292984008789 Accuracy 50.85610613448237\n",
      "Training:: Epoch 6, Iteration 140, Current loss 6.398549556732178 Accuracy 45.24524095352427\n",
      "Training:: Epoch 6, Iteration 150, Current loss 7.734431266784668 Accuracy 41.100501476952665\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 6, Probability Accuracy 54.132845240818966\n",
      "Starting Training\n",
      "Training:: Epoch 7, Iteration 0, Current loss 5.385904788970947 Accuracy 33.62504026629443\n",
      "Training:: Epoch 7, Iteration 10, Current loss 4.478178977966309 Accuracy 61.367924528301884\n",
      "Training:: Epoch 7, Iteration 20, Current loss 5.543618202209473 Accuracy 50.92783505154639\n",
      "Training:: Epoch 7, Iteration 30, Current loss 4.215373516082764 Accuracy 66.29515918745005\n",
      "Training:: Epoch 7, Iteration 40, Current loss 4.964444160461426 Accuracy 55.221094388052734\n",
      "Training:: Epoch 7, Iteration 50, Current loss 4.2016777992248535 Accuracy 67.66176364031593\n",
      "Training:: Epoch 7, Iteration 60, Current loss 6.313215255737305 Accuracy 52.46576493321012\n",
      "Training:: Epoch 7, Iteration 70, Current loss 5.235081672668457 Accuracy 57.049137593952295\n",
      "Training:: Epoch 7, Iteration 80, Current loss 4.09293270111084 Accuracy 64.46978835398619\n",
      "Training:: Epoch 7, Iteration 90, Current loss 6.716819763183594 Accuracy 44.09264305177112\n",
      "Training:: Epoch 7, Iteration 100, Current loss 5.109997272491455 Accuracy 56.0705041291754\n",
      "Training:: Epoch 7, Iteration 110, Current loss 4.326892375946045 Accuracy 64.74450711339338\n",
      "Training:: Epoch 7, Iteration 120, Current loss 4.704423904418945 Accuracy 59.52093640641969\n",
      "Training:: Epoch 7, Iteration 130, Current loss 4.815300464630127 Accuracy 60.8415961305925\n",
      "Training:: Epoch 7, Iteration 140, Current loss 4.3810577392578125 Accuracy 64.87234210734329\n",
      "Training:: Epoch 7, Iteration 150, Current loss 4.318074703216553 Accuracy 67.13026860600912\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 7, Probability Accuracy 61.102449205082564\n",
      "Starting Training\n",
      "Training:: Epoch 8, Iteration 0, Current loss 3.448502779006958 Accuracy 72.52441322597224\n",
      "Training:: Epoch 8, Iteration 10, Current loss 4.567487716674805 Accuracy 64.1190510984461\n",
      "Training:: Epoch 8, Iteration 20, Current loss 4.856701374053955 Accuracy 64.89448272120653\n",
      "Training:: Epoch 8, Iteration 30, Current loss 6.93050479888916 Accuracy 41.57658845241234\n",
      "Training:: Epoch 8, Iteration 40, Current loss 3.4850378036499023 Accuracy 81.10820477232691\n",
      "Training:: Epoch 8, Iteration 50, Current loss 3.9584546089172363 Accuracy 73.02697932425617\n",
      "Training:: Epoch 8, Iteration 60, Current loss 4.005263328552246 Accuracy 69.42605946853699\n",
      "Training:: Epoch 8, Iteration 70, Current loss 5.858053207397461 Accuracy 58.63821865826591\n",
      "Training:: Epoch 8, Iteration 80, Current loss 4.94777250289917 Accuracy 62.56369813152148\n",
      "Training:: Epoch 8, Iteration 90, Current loss 4.374874591827393 Accuracy 60.64196790160492\n",
      "Training:: Epoch 8, Iteration 100, Current loss 4.783949375152588 Accuracy 61.18580559841305\n",
      "Training:: Epoch 8, Iteration 110, Current loss 3.3429348468780518 Accuracy 73.27200397812034\n",
      "Training:: Epoch 8, Iteration 120, Current loss 5.779003143310547 Accuracy 62.42117880035437\n",
      "Training:: Epoch 8, Iteration 130, Current loss 4.247691631317139 Accuracy 61.09130792432136\n",
      "Training:: Epoch 8, Iteration 140, Current loss 4.414325714111328 Accuracy 63.74424486090396\n",
      "Training:: Epoch 8, Iteration 150, Current loss 3.8284478187561035 Accuracy 63.0810689422861\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 8, Probability Accuracy 61.519299557480146\n",
      "Starting Training\n",
      "Training:: Epoch 9, Iteration 0, Current loss 4.462746620178223 Accuracy 58.94274773512655\n",
      "Training:: Epoch 9, Iteration 10, Current loss 3.405423641204834 Accuracy 79.29541595925298\n",
      "Training:: Epoch 9, Iteration 20, Current loss 3.8859190940856934 Accuracy 72.63876596282716\n",
      "Training:: Epoch 9, Iteration 30, Current loss 6.056788921356201 Accuracy 60.498694383288104\n",
      "Training:: Epoch 9, Iteration 40, Current loss 3.7668075561523438 Accuracy 73.4925635577698\n",
      "Training:: Epoch 9, Iteration 50, Current loss 4.223427772521973 Accuracy 66.20871418157348\n",
      "Training:: Epoch 9, Iteration 60, Current loss 3.5534064769744873 Accuracy 72.9872340425532\n",
      "Training:: Epoch 9, Iteration 70, Current loss 5.016690254211426 Accuracy 54.45285519935815\n",
      "Training:: Epoch 9, Iteration 80, Current loss 4.455877304077148 Accuracy 66.252404201805\n",
      "Training:: Epoch 9, Iteration 90, Current loss 3.653473377227783 Accuracy 74.19997785405825\n",
      "Training:: Epoch 9, Iteration 100, Current loss 3.9460434913635254 Accuracy 75.1540175645563\n",
      "Training:: Epoch 9, Iteration 110, Current loss 3.3270926475524902 Accuracy 79.99229732331986\n",
      "Training:: Epoch 9, Iteration 120, Current loss 3.1635992527008057 Accuracy 67.23843367730473\n",
      "Training:: Epoch 9, Iteration 130, Current loss 4.254281520843506 Accuracy 73.03323294764094\n",
      "Training:: Epoch 9, Iteration 140, Current loss 5.594370365142822 Accuracy 64.72901168969182\n",
      "Training:: Epoch 9, Iteration 150, Current loss 3.2786684036254883 Accuracy 76.7841788478074\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 9, Probability Accuracy 65.57441085708227\n",
      "Starting Training\n",
      "Training:: Epoch 10, Iteration 0, Current loss 2.4359264373779297 Accuracy 84.47179203539822\n",
      "Training:: Epoch 10, Iteration 10, Current loss 3.4156930446624756 Accuracy 79.53242451267677\n",
      "Training:: Epoch 10, Iteration 20, Current loss 4.748349189758301 Accuracy 68.45625286303252\n",
      "Training:: Epoch 10, Iteration 30, Current loss 3.113769292831421 Accuracy 80.14637391882901\n",
      "Training:: Epoch 10, Iteration 40, Current loss 3.689432144165039 Accuracy 75.50506768247058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 10, Iteration 50, Current loss 3.9355640411376953 Accuracy 72.2707315869962\n",
      "Training:: Epoch 10, Iteration 60, Current loss 3.374910831451416 Accuracy 75.31279806990966\n",
      "Training:: Epoch 10, Iteration 70, Current loss 3.2447245121002197 Accuracy 82.34112698472849\n",
      "Training:: Epoch 10, Iteration 80, Current loss 4.360922336578369 Accuracy 68.07842065714871\n",
      "Training:: Epoch 10, Iteration 90, Current loss 3.7737386226654053 Accuracy 74.7031583946806\n",
      "Training:: Epoch 10, Iteration 100, Current loss 5.975279808044434 Accuracy 60.707134275416216\n",
      "Training:: Epoch 10, Iteration 110, Current loss 3.7115731239318848 Accuracy 77.47585015314537\n",
      "Training:: Epoch 10, Iteration 120, Current loss 3.866051435470581 Accuracy 75.98610488570148\n",
      "Training:: Epoch 10, Iteration 130, Current loss 5.121955871582031 Accuracy 67.02725020644095\n",
      "Training:: Epoch 10, Iteration 140, Current loss 5.151327610015869 Accuracy 67.98685891586055\n",
      "Training:: Epoch 10, Iteration 150, Current loss 6.266438961029053 Accuracy 58.942642074217545\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 10, Probability Accuracy 54.88127856429372\n",
      "Starting Training\n",
      "Training:: Epoch 11, Iteration 0, Current loss 4.706999778747559 Accuracy 61.86806536211418\n",
      "Training:: Epoch 11, Iteration 10, Current loss 7.457400798797607 Accuracy 53.722151784097086\n",
      "Training:: Epoch 11, Iteration 20, Current loss 4.680065631866455 Accuracy 71.81766864239346\n",
      "Training:: Epoch 11, Iteration 30, Current loss 3.3843798637390137 Accuracy 69.59179371882246\n",
      "Training:: Epoch 11, Iteration 40, Current loss 3.225552558898926 Accuracy 69.05404515831742\n",
      "Training:: Epoch 11, Iteration 50, Current loss 3.415048599243164 Accuracy 77.90681598230333\n",
      "Training:: Epoch 11, Iteration 60, Current loss 3.223102569580078 Accuracy 72.53193267279455\n",
      "Training:: Epoch 11, Iteration 70, Current loss 3.2234416007995605 Accuracy 83.38449731389102\n",
      "Training:: Epoch 11, Iteration 80, Current loss 3.626915693283081 Accuracy 75.46219696424717\n",
      "Training:: Epoch 11, Iteration 90, Current loss 4.398334503173828 Accuracy 66.47919624331112\n",
      "Training:: Epoch 11, Iteration 100, Current loss 3.968994617462158 Accuracy 76.16771159874608\n",
      "Training:: Epoch 11, Iteration 110, Current loss 3.323626756668091 Accuracy 78.39280268374505\n",
      "Training:: Epoch 11, Iteration 120, Current loss 4.2096476554870605 Accuracy 72.08876440805774\n",
      "Training:: Epoch 11, Iteration 130, Current loss 5.378448009490967 Accuracy 61.972642741873514\n",
      "Training:: Epoch 11, Iteration 140, Current loss 3.476780891418457 Accuracy 80.4949414798651\n",
      "Training:: Epoch 11, Iteration 150, Current loss 3.816307544708252 Accuracy 78.50299401197604\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 11, Probability Accuracy 66.39159379690962\n",
      "Starting Training\n",
      "Training:: Epoch 12, Iteration 0, Current loss 3.0209908485412598 Accuracy 78.67175716440423\n",
      "Training:: Epoch 12, Iteration 10, Current loss 3.6653504371643066 Accuracy 75.50456646747097\n",
      "Training:: Epoch 12, Iteration 20, Current loss 3.3919785022735596 Accuracy 80.31377605845691\n",
      "Training:: Epoch 12, Iteration 30, Current loss 3.1793124675750732 Accuracy 78.5522307830138\n",
      "Training:: Epoch 12, Iteration 40, Current loss 3.8726017475128174 Accuracy 76.18218511337106\n",
      "Training:: Epoch 12, Iteration 50, Current loss 3.123713493347168 Accuracy 79.43305186972256\n",
      "Training:: Epoch 12, Iteration 60, Current loss 3.0134129524230957 Accuracy 81.08424336973479\n",
      "Training:: Epoch 12, Iteration 70, Current loss 4.05241060256958 Accuracy 74.57285622179239\n",
      "Training:: Epoch 12, Iteration 80, Current loss 3.2344377040863037 Accuracy 78.61299377011022\n",
      "Training:: Epoch 12, Iteration 90, Current loss 2.7043261528015137 Accuracy 81.6490509059534\n",
      "Training:: Epoch 12, Iteration 100, Current loss 2.7314071655273438 Accuracy 83.27482172243555\n",
      "Training:: Epoch 12, Iteration 110, Current loss 2.897867441177368 Accuracy 83.83776120322999\n",
      "Training:: Epoch 12, Iteration 120, Current loss 3.422067403793335 Accuracy 80.41750841750842\n",
      "Training:: Epoch 12, Iteration 130, Current loss 2.9324698448181152 Accuracy 76.02173580284709\n",
      "Training:: Epoch 12, Iteration 140, Current loss 5.065134525299072 Accuracy 67.52888178449466\n",
      "Training:: Epoch 12, Iteration 150, Current loss 6.2671122550964355 Accuracy 61.92432575707375\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 12, Probability Accuracy 63.30243692836535\n",
      "Starting Training\n",
      "Training:: Epoch 13, Iteration 0, Current loss 5.5447797775268555 Accuracy 66.7860142333024\n",
      "Training:: Epoch 13, Iteration 10, Current loss 2.593390941619873 Accuracy 84.16387630490448\n",
      "Training:: Epoch 13, Iteration 20, Current loss 3.1492271423339844 Accuracy 82.81087092514075\n",
      "Training:: Epoch 13, Iteration 30, Current loss 2.8401715755462646 Accuracy 82.95432458697765\n",
      "Training:: Epoch 13, Iteration 40, Current loss 3.9400103092193604 Accuracy 74.930405196412\n",
      "Training:: Epoch 13, Iteration 50, Current loss 4.889945983886719 Accuracy 72.76605925276128\n",
      "Training:: Epoch 13, Iteration 60, Current loss 6.020560264587402 Accuracy 62.82722513089005\n",
      "Training:: Epoch 13, Iteration 70, Current loss 5.406116962432861 Accuracy 56.252489048187975\n",
      "Training:: Epoch 13, Iteration 80, Current loss 6.448855876922607 Accuracy 55.3298098702725\n",
      "Training:: Epoch 13, Iteration 90, Current loss 4.029114723205566 Accuracy 74.4346444843874\n",
      "Training:: Epoch 13, Iteration 100, Current loss 2.450221300125122 Accuracy 84.47017036111293\n",
      "Training:: Epoch 13, Iteration 110, Current loss 3.2313082218170166 Accuracy 83.3099360474185\n",
      "Training:: Epoch 13, Iteration 120, Current loss 3.2529408931732178 Accuracy 82.91002363828396\n",
      "Training:: Epoch 13, Iteration 130, Current loss 2.690506935119629 Accuracy 86.63538153199394\n",
      "Training:: Epoch 13, Iteration 140, Current loss 2.886751413345337 Accuracy 83.77442365346842\n",
      "Training:: Epoch 13, Iteration 150, Current loss 2.660937786102295 Accuracy 87.67973518154547\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 13, Probability Accuracy 66.95263977321555\n",
      "Starting Training\n",
      "Training:: Epoch 14, Iteration 0, Current loss 2.893423318862915 Accuracy 86.13437889859205\n",
      "Training:: Epoch 14, Iteration 10, Current loss 3.4214460849761963 Accuracy 80.79547997314836\n",
      "Training:: Epoch 14, Iteration 20, Current loss 3.236711263656616 Accuracy 82.49667042311239\n",
      "Training:: Epoch 14, Iteration 30, Current loss 2.6221399307250977 Accuracy 81.3821262306342\n",
      "Training:: Epoch 14, Iteration 40, Current loss 3.3937978744506836 Accuracy 82.80092001022234\n",
      "Training:: Epoch 14, Iteration 50, Current loss 4.2420430183410645 Accuracy 73.0796655214883\n",
      "Training:: Epoch 14, Iteration 60, Current loss 3.152269124984741 Accuracy 77.31871241598868\n",
      "Training:: Epoch 14, Iteration 70, Current loss 2.7395029067993164 Accuracy 86.02952849658676\n",
      "Training:: Epoch 14, Iteration 80, Current loss 2.8253777027130127 Accuracy 88.0163043478261\n",
      "Training:: Epoch 14, Iteration 90, Current loss 3.8897924423217773 Accuracy 76.68017561634584\n",
      "Training:: Epoch 14, Iteration 100, Current loss 5.504118919372559 Accuracy 66.29716634855382\n",
      "Training:: Epoch 14, Iteration 110, Current loss 3.849384069442749 Accuracy 76.01815908941377\n",
      "Training:: Epoch 14, Iteration 120, Current loss 3.325047731399536 Accuracy 79.48865118705974\n",
      "Training:: Epoch 14, Iteration 130, Current loss 2.6986823081970215 Accuracy 85.73817087443571\n",
      "Training:: Epoch 14, Iteration 140, Current loss 2.9091427326202393 Accuracy 83.82634417957195\n",
      "Training:: Epoch 14, Iteration 150, Current loss 2.6751463413238525 Accuracy 84.85706758002898\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 14, Probability Accuracy 65.6736290534093\n",
      "Starting Training\n",
      "Training:: Epoch 15, Iteration 0, Current loss 3.2918882369995117 Accuracy 79.10437235543019\n",
      "Training:: Epoch 15, Iteration 10, Current loss 2.8401596546173096 Accuracy 84.5859872611465\n",
      "Training:: Epoch 15, Iteration 20, Current loss 2.7957513332366943 Accuracy 83.60174533915114\n",
      "Training:: Epoch 15, Iteration 30, Current loss 2.9531869888305664 Accuracy 80.36913767019666\n",
      "Training:: Epoch 15, Iteration 40, Current loss 3.665850877761841 Accuracy 74.44098342418512\n",
      "Training:: Epoch 15, Iteration 50, Current loss 2.254765272140503 Accuracy 88.987714987715\n",
      "Training:: Epoch 15, Iteration 60, Current loss 2.52126145362854 Accuracy 85.42451215337213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 15, Iteration 70, Current loss 4.08786153793335 Accuracy 73.66295527458011\n",
      "Training:: Epoch 15, Iteration 80, Current loss 2.8858590126037598 Accuracy 86.14940265196272\n",
      "Training:: Epoch 15, Iteration 90, Current loss 4.258543014526367 Accuracy 74.13259488516044\n",
      "Training:: Epoch 15, Iteration 100, Current loss 4.76881742477417 Accuracy 68.07854777942687\n",
      "Training:: Epoch 15, Iteration 110, Current loss 4.182324409484863 Accuracy 74.2316366691726\n",
      "Training:: Epoch 15, Iteration 120, Current loss 2.414311170578003 Accuracy 87.94987614745737\n",
      "Training:: Epoch 15, Iteration 130, Current loss 2.6222593784332275 Accuracy 86.9087332135831\n",
      "Training:: Epoch 15, Iteration 140, Current loss 2.6966392993927 Accuracy 82.89681495130297\n",
      "Training:: Epoch 15, Iteration 150, Current loss 3.5905444622039795 Accuracy 77.61215319877327\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 68.66937126466928\n",
      "Starting Training\n",
      "Training:: Epoch 16, Iteration 0, Current loss 2.7145166397094727 Accuracy 87.04766366085022\n",
      "Training:: Epoch 16, Iteration 10, Current loss 4.514500141143799 Accuracy 75.69263982728981\n",
      "Training:: Epoch 16, Iteration 20, Current loss 2.253276824951172 Accuracy 92.8909952606635\n",
      "Training:: Epoch 16, Iteration 30, Current loss 2.234950065612793 Accuracy 89.40050707450723\n",
      "Training:: Epoch 16, Iteration 40, Current loss 2.4821531772613525 Accuracy 86.96365407289692\n",
      "Training:: Epoch 16, Iteration 50, Current loss 2.5077874660491943 Accuracy 84.3144848954299\n",
      "Training:: Epoch 16, Iteration 60, Current loss 3.732895851135254 Accuracy 71.45705670654606\n",
      "Training:: Epoch 16, Iteration 70, Current loss 2.6853623390197754 Accuracy 85.27884089666485\n",
      "Training:: Epoch 16, Iteration 80, Current loss 2.434330463409424 Accuracy 88.09279107326384\n",
      "Training:: Epoch 16, Iteration 90, Current loss 3.237077236175537 Accuracy 79.14878071056718\n",
      "Training:: Epoch 16, Iteration 100, Current loss 3.0578107833862305 Accuracy 86.76507380535402\n",
      "Training:: Epoch 16, Iteration 110, Current loss 4.312821388244629 Accuracy 76.96609877817932\n",
      "Training:: Epoch 16, Iteration 120, Current loss 2.3883471488952637 Accuracy 88.279246941616\n",
      "Training:: Epoch 16, Iteration 130, Current loss 2.7270641326904297 Accuracy 86.98996113989638\n",
      "Training:: Epoch 16, Iteration 140, Current loss 2.6559665203094482 Accuracy 79.92700729927007\n",
      "Training:: Epoch 16, Iteration 150, Current loss 2.819678544998169 Accuracy 85.19426874919324\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 16, Probability Accuracy 68.15520002678556\n",
      "Starting Training\n",
      "Training:: Epoch 17, Iteration 0, Current loss 3.352668523788452 Accuracy 81.1125737409195\n",
      "Training:: Epoch 17, Iteration 10, Current loss 2.356889247894287 Accuracy 86.52935428098925\n",
      "Training:: Epoch 17, Iteration 20, Current loss 6.454104423522949 Accuracy 61.68778206731553\n",
      "Training:: Epoch 17, Iteration 30, Current loss 3.680344820022583 Accuracy 76.28399568145596\n",
      "Training:: Epoch 17, Iteration 40, Current loss 3.3739266395568848 Accuracy 78.03204538485127\n",
      "Training:: Epoch 17, Iteration 50, Current loss 4.298760414123535 Accuracy 72.82163804379931\n",
      "Training:: Epoch 17, Iteration 60, Current loss 2.2528913021087646 Accuracy 92.64738450635602\n",
      "Training:: Epoch 17, Iteration 70, Current loss 2.757709264755249 Accuracy 88.93252639711564\n",
      "Training:: Epoch 17, Iteration 80, Current loss 2.331409215927124 Accuracy 88.31688661673206\n",
      "Training:: Epoch 17, Iteration 90, Current loss 2.4996590614318848 Accuracy 88.34419636666205\n",
      "Training:: Epoch 17, Iteration 100, Current loss 2.340672731399536 Accuracy 88.65062761506276\n",
      "Training:: Epoch 17, Iteration 110, Current loss 2.5007097721099854 Accuracy 88.9291222423389\n",
      "Training:: Epoch 17, Iteration 120, Current loss 2.4692723751068115 Accuracy 85.26869078082855\n",
      "Training:: Epoch 17, Iteration 130, Current loss 2.6686668395996094 Accuracy 83.22116855307434\n",
      "Training:: Epoch 17, Iteration 140, Current loss 3.18092942237854 Accuracy 80.75960629483401\n",
      "Training:: Epoch 17, Iteration 150, Current loss 1.9059269428253174 Accuracy 93.41746248294679\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 17, Probability Accuracy 70.00284596626135\n",
      "Starting Training\n",
      "Training:: Epoch 18, Iteration 0, Current loss 2.027796983718872 Accuracy 90.6670091617433\n",
      "Training:: Epoch 18, Iteration 10, Current loss 2.4500718116760254 Accuracy 88.33660344451711\n",
      "Training:: Epoch 18, Iteration 20, Current loss 2.102836847305298 Accuracy 90.43944439931768\n",
      "Training:: Epoch 18, Iteration 30, Current loss 2.1096277236938477 Accuracy 90.4358353510896\n",
      "Training:: Epoch 18, Iteration 40, Current loss 2.4191861152648926 Accuracy 87.83263455121075\n",
      "Training:: Epoch 18, Iteration 50, Current loss 2.0804386138916016 Accuracy 90.70987654320987\n",
      "Training:: Epoch 18, Iteration 60, Current loss 2.199369430541992 Accuracy 90.43085506797148\n",
      "Training:: Epoch 18, Iteration 70, Current loss 2.534471273422241 Accuracy 83.12967032967033\n",
      "Training:: Epoch 18, Iteration 80, Current loss 3.2444188594818115 Accuracy 83.62881434126409\n",
      "Training:: Epoch 18, Iteration 90, Current loss 2.1844685077667236 Accuracy 89.15653197332287\n",
      "Training:: Epoch 18, Iteration 100, Current loss 2.8195855617523193 Accuracy 84.37460873920121\n",
      "Training:: Epoch 18, Iteration 110, Current loss 2.8093905448913574 Accuracy 84.28459261943588\n",
      "Training:: Epoch 18, Iteration 120, Current loss 2.666273832321167 Accuracy 87.01194508825103\n",
      "Training:: Epoch 18, Iteration 130, Current loss 1.9957809448242188 Accuracy 90.46479308214947\n",
      "Training:: Epoch 18, Iteration 140, Current loss 2.921207904815674 Accuracy 84.39497010925582\n",
      "Training:: Epoch 18, Iteration 150, Current loss 2.516834259033203 Accuracy 84.41488162344983\n",
      "Calculating Validation Data Accuracy\n",
      "Training:: Epoch 19, Iteration 120, Current loss 3.2015044689178467 Accuracy 83.04860283406171\n",
      "Training:: Epoch 19, Iteration 130, Current loss 3.03731369972229 Accuracy 85.29910184714456\n",
      "Training:: Epoch 19, Iteration 140, Current loss 2.7829465866088867 Accuracy 81.64281833001267\n",
      "Training:: Epoch 19, Iteration 150, Current loss 2.299107789993286 Accuracy 89.50962469082697\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 19, Probability Accuracy 65.09941350773713\n",
      "Starting Training\n",
      "Training:: Epoch 20, Iteration 0, Current loss 2.4441380500793457 Accuracy 89.08517350157729\n",
      "Training:: Epoch 20, Iteration 10, Current loss 2.2146360874176025 Accuracy 88.6770284333143\n",
      "Training:: Epoch 20, Iteration 20, Current loss 2.1009013652801514 Accuracy 91.01746603825895\n",
      "Training:: Epoch 20, Iteration 30, Current loss 1.8966633081436157 Accuracy 92.19451188111528\n",
      "Training:: Epoch 20, Iteration 40, Current loss 2.0552775859832764 Accuracy 90.71343978437538\n",
      "Training:: Epoch 20, Iteration 50, Current loss 2.526616096496582 Accuracy 83.43054869006426\n",
      "Training:: Epoch 20, Iteration 60, Current loss 3.323709726333618 Accuracy 80.4093118444615\n",
      "Training:: Epoch 20, Iteration 70, Current loss 1.7928454875946045 Accuracy 90.35181617069864\n",
      "Training:: Epoch 20, Iteration 80, Current loss 2.8141496181488037 Accuracy 83.89076403804849\n",
      "Training:: Epoch 20, Iteration 90, Current loss 2.6003167629241943 Accuracy 85.77782599262314\n",
      "Training:: Epoch 20, Iteration 100, Current loss 2.1210711002349854 Accuracy 88.11448547083242\n",
      "Training:: Epoch 20, Iteration 110, Current loss 3.2270348072052 Accuracy 80.9078553954879\n",
      "Training:: Epoch 20, Iteration 120, Current loss 2.2819981575012207 Accuracy 89.11547446177158\n",
      "Training:: Epoch 20, Iteration 130, Current loss 2.369605779647827 Accuracy 86.86958585498991\n",
      "Training:: Epoch 20, Iteration 140, Current loss 2.8303329944610596 Accuracy 85.74018909467685\n",
      "Training:: Epoch 20, Iteration 150, Current loss 3.662754774093628 Accuracy 83.08\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 20, Probability Accuracy 64.24796736625353\n",
      "Starting Training\n",
      "Training:: Epoch 21, Iteration 0, Current loss 2.079404830932617 Accuracy 87.93689476046944\n",
      "Training:: Epoch 21, Iteration 10, Current loss 2.1263723373413086 Accuracy 87.26952850105559\n",
      "Training:: Epoch 21, Iteration 20, Current loss 2.0857157707214355 Accuracy 93.04154463607766\n",
      "Training:: Epoch 21, Iteration 30, Current loss 2.5238430500030518 Accuracy 89.93488372093023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 21, Iteration 40, Current loss 2.0465922355651855 Accuracy 89.54820756261253\n",
      "Training:: Epoch 21, Iteration 50, Current loss 2.4584708213806152 Accuracy 87.35249811699724\n",
      "Training:: Epoch 21, Iteration 60, Current loss 2.03971266746521 Accuracy 93.40592861464005\n",
      "Training:: Epoch 21, Iteration 70, Current loss 2.3759443759918213 Accuracy 87.08311275807306\n",
      "Training:: Epoch 21, Iteration 80, Current loss 2.2197422981262207 Accuracy 86.59404502541757\n",
      "Training:: Epoch 21, Iteration 90, Current loss 1.8141175508499146 Accuracy 91.69761020660764\n",
      "Training:: Epoch 21, Iteration 100, Current loss 3.8604178428649902 Accuracy 82.5051722260535\n",
      "Training:: Epoch 21, Iteration 110, Current loss 2.2010228633880615 Accuracy 90.60263383910679\n",
      "Training:: Epoch 21, Iteration 120, Current loss 2.5316483974456787 Accuracy 88.65353280995409\n",
      "Training:: Epoch 21, Iteration 130, Current loss 2.8823368549346924 Accuracy 87.95591548438166\n",
      "Training:: Epoch 21, Iteration 140, Current loss 2.1490421295166016 Accuracy 88.58328643781654\n",
      "Training:: Epoch 21, Iteration 150, Current loss 2.9289207458496094 Accuracy 84.21903881700554\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 21, Probability Accuracy 69.78621771083867\n",
      "Starting Training\n",
      "Training:: Epoch 22, Iteration 0, Current loss 2.0169413089752197 Accuracy 88.43953973487652\n",
      "Training:: Epoch 22, Iteration 10, Current loss 1.7957890033721924 Accuracy 92.20951170426862\n",
      "Training:: Epoch 22, Iteration 20, Current loss 2.3322947025299072 Accuracy 87.9946608040201\n",
      "Training:: Epoch 22, Iteration 30, Current loss 2.5883052349090576 Accuracy 81.16170724376315\n",
      "Training:: Epoch 22, Iteration 40, Current loss 1.9634041786193848 Accuracy 91.51682803135085\n",
      "Training:: Epoch 22, Iteration 50, Current loss 1.7855838537216187 Accuracy 92.46051961283749\n",
      "Training:: Epoch 22, Iteration 60, Current loss 1.9782546758651733 Accuracy 92.61870964787765\n",
      "Training:: Epoch 22, Iteration 70, Current loss 1.5558892488479614 Accuracy 93.60833256859686\n",
      "Training:: Epoch 22, Iteration 80, Current loss 1.9449924230575562 Accuracy 93.09436847022849\n",
      "Training:: Epoch 22, Iteration 90, Current loss 1.4249587059020996 Accuracy 91.36236373384166\n",
      "Training:: Epoch 22, Iteration 100, Current loss 2.2207212448120117 Accuracy 86.72171758876961\n",
      "Training:: Epoch 22, Iteration 110, Current loss 1.7765504121780396 Accuracy 92.01216346510421\n",
      "Training:: Epoch 22, Iteration 120, Current loss 2.0979743003845215 Accuracy 91.52949550671768\n",
      "Training:: Epoch 22, Iteration 130, Current loss 1.984107255935669 Accuracy 86.61518661518662\n",
      "Training:: Epoch 22, Iteration 140, Current loss 2.0114357471466064 Accuracy 92.18576812236755\n",
      "Training:: Epoch 22, Iteration 150, Current loss 1.8340386152267456 Accuracy 91.6989914662529\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 22, Probability Accuracy 70.18465298742753\n",
      "Starting Training\n",
      "Training:: Epoch 23, Iteration 0, Current loss 1.7891229391098022 Accuracy 92.92515207617032\n",
      "Training:: Epoch 23, Iteration 10, Current loss 1.8727879524230957 Accuracy 91.84528448947779\n",
      "Training:: Epoch 23, Iteration 20, Current loss 2.371232509613037 Accuracy 87.07274320771253\n",
      "Training:: Epoch 23, Iteration 30, Current loss 1.7100998163223267 Accuracy 93.14961144125905\n",
      "Training:: Epoch 23, Iteration 40, Current loss 5.309926986694336 Accuracy 72.55525520750517\n",
      "Training:: Epoch 23, Iteration 50, Current loss 3.4157958030700684 Accuracy 76.76692774713815\n",
      "Training:: Epoch 23, Iteration 60, Current loss 2.381263494491577 Accuracy 85.24732957410718\n",
      "Training:: Epoch 23, Iteration 70, Current loss 2.1039962768554688 Accuracy 94.93959304790165\n",
      "Training:: Epoch 23, Iteration 80, Current loss 4.014013290405273 Accuracy 83.2576903681291\n",
      "Training:: Epoch 23, Iteration 90, Current loss 2.7006630897521973 Accuracy 87.01181834543164\n",
      "Training:: Epoch 23, Iteration 100, Current loss 3.5232276916503906 Accuracy 80.73749355337803\n",
      "Training:: Epoch 23, Iteration 110, Current loss 2.60823655128479 Accuracy 89.03571428571429\n",
      "Training:: Epoch 23, Iteration 120, Current loss 2.1393532752990723 Accuracy 90.89991778159802\n",
      "Training:: Epoch 23, Iteration 130, Current loss 2.9826462268829346 Accuracy 79.21118162101374\n",
      "Training:: Epoch 23, Iteration 140, Current loss 4.414396286010742 Accuracy 74.1040822686195\n",
      "Training:: Epoch 23, Iteration 150, Current loss 2.490734100341797 Accuracy 86.44189957555858\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 23, Probability Accuracy 68.64928209105976\n",
      "Starting Training\n",
      "Training:: Epoch 24, Iteration 0, Current loss 2.588776111602783 Accuracy 85.75645360382117\n",
      "Training:: Epoch 24, Iteration 10, Current loss 2.354796886444092 Accuracy 88.20015271975925\n",
      "Training:: Epoch 24, Iteration 20, Current loss 1.6860320568084717 Accuracy 94.27768014059754\n",
      "Training:: Epoch 24, Iteration 30, Current loss 2.914837598800659 Accuracy 85.85056294779939\n",
      "Training:: Epoch 24, Iteration 40, Current loss 1.8548728227615356 Accuracy 92.12042808420557\n",
      "Training:: Epoch 24, Iteration 50, Current loss 2.43711519241333 Accuracy 89.74446774814878\n",
      "Training:: Epoch 24, Iteration 60, Current loss 2.243581771850586 Accuracy 89.16977407326168\n",
      "Training:: Epoch 24, Iteration 70, Current loss 1.419593334197998 Accuracy 94.57666385846672\n",
      "Training:: Epoch 24, Iteration 80, Current loss 1.9192659854888916 Accuracy 90.94056957105505\n",
      "Training:: Epoch 24, Iteration 90, Current loss 2.2642762660980225 Accuracy 89.91077074404552\n",
      "Training:: Epoch 24, Iteration 100, Current loss 1.9863121509552002 Accuracy 93.04916169366297\n",
      "Training:: Epoch 24, Iteration 110, Current loss 1.8219248056411743 Accuracy 92.81316671746418\n",
      "Training:: Epoch 24, Iteration 120, Current loss 3.123612642288208 Accuracy 84.92550105485232\n",
      "Training:: Epoch 24, Iteration 130, Current loss 1.7846163511276245 Accuracy 88.4768326417704\n",
      "Training:: Epoch 24, Iteration 140, Current loss 2.697207450866699 Accuracy 86.91176470588235\n",
      "Training:: Epoch 24, Iteration 150, Current loss 1.5060163736343384 Accuracy 94.72648155822627\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 24, Probability Accuracy 70.71154736859728\n",
      "Starting Training\n",
      "Training:: Epoch 25, Iteration 0, Current loss 1.3949006795883179 Accuracy 95.36904835806071\n",
      "Training:: Epoch 25, Iteration 10, Current loss 2.0003232955932617 Accuracy 92.3780487804878\n",
      "Training:: Epoch 25, Iteration 20, Current loss 1.3900426626205444 Accuracy 95.40590595740302\n",
      "Training:: Epoch 25, Iteration 30, Current loss 2.0547101497650146 Accuracy 91.50487930148947\n",
      "Training:: Epoch 25, Iteration 40, Current loss 1.632042407989502 Accuracy 94.07154129405576\n",
      "Training:: Epoch 25, Iteration 50, Current loss 1.687729835510254 Accuracy 94.10094768922472\n",
      "Training:: Epoch 25, Iteration 60, Current loss 1.4696571826934814 Accuracy 94.9497077255253\n",
      "Training:: Epoch 25, Iteration 70, Current loss 1.7847533226013184 Accuracy 90.7912146070389\n",
      "Training:: Epoch 25, Iteration 80, Current loss 1.7041313648223877 Accuracy 92.82572331330152\n",
      "Training:: Epoch 25, Iteration 90, Current loss 2.0267269611358643 Accuracy 90.87378640776699\n",
      "Training:: Epoch 25, Iteration 100, Current loss 2.785404920578003 Accuracy 84.22296631869214\n",
      "Training:: Epoch 25, Iteration 110, Current loss 1.5755226612091064 Accuracy 93.67042179722294\n",
      "Training:: Epoch 25, Iteration 120, Current loss 1.6705396175384521 Accuracy 93.8399016095396\n",
      "Training:: Epoch 25, Iteration 130, Current loss 1.5927224159240723 Accuracy 94.70829909613805\n",
      "Training:: Epoch 25, Iteration 140, Current loss 1.721826195716858 Accuracy 92.05594002306805\n",
      "Training:: Epoch 25, Iteration 150, Current loss 2.0709688663482666 Accuracy 91.0014409221902\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 25, Probability Accuracy 69.16311850938332\n",
      "Starting Training\n",
      "Training:: Epoch 26, Iteration 0, Current loss 1.7376219034194946 Accuracy 92.3377838251552\n",
      "Training:: Epoch 26, Iteration 10, Current loss 1.560600996017456 Accuracy 93.04605563155495\n",
      "Training:: Epoch 26, Iteration 20, Current loss 2.1606271266937256 Accuracy 89.38650697585526\n",
      "Training:: Epoch 26, Iteration 30, Current loss 1.301785945892334 Accuracy 95.78082191780823\n",
      "Training:: Epoch 26, Iteration 40, Current loss 1.5434341430664062 Accuracy 93.67756443521883\n",
      "Training:: Epoch 26, Iteration 50, Current loss 1.9125105142593384 Accuracy 91.4634956487079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 26, Iteration 60, Current loss 2.114187717437744 Accuracy 90.23907304194779\n",
      "Training:: Epoch 26, Iteration 70, Current loss 1.4691003561019897 Accuracy 94.58144892908952\n",
      "Training:: Epoch 26, Iteration 80, Current loss 1.36734139919281 Accuracy 92.93736501079914\n",
      "Training:: Epoch 26, Iteration 90, Current loss 1.587613582611084 Accuracy 95.03942020469125\n",
      "Training:: Epoch 26, Iteration 100, Current loss 1.9400601387023926 Accuracy 89.19831223628692\n",
      "Training:: Epoch 26, Iteration 110, Current loss 1.2828283309936523 Accuracy 96.43144350623241\n",
      "Training:: Epoch 26, Iteration 120, Current loss 1.7997190952301025 Accuracy 93.93901446371548\n",
      "Training:: Epoch 26, Iteration 130, Current loss 4.263506889343262 Accuracy 78.03580695381422\n",
      "Training:: Epoch 26, Iteration 140, Current loss 1.730745553970337 Accuracy 93.38096521973578\n",
      "Training:: Epoch 26, Iteration 150, Current loss 1.4851540327072144 Accuracy 90.61870057373235\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 26, Probability Accuracy 67.8063180451002\n",
      "Starting Training\n",
      "Training:: Epoch 27, Iteration 0, Current loss 1.4902026653289795 Accuracy 94.40014903129658\n",
      "Training:: Epoch 27, Iteration 10, Current loss 1.876389980316162 Accuracy 90.22379269729093\n",
      "Training:: Epoch 27, Iteration 20, Current loss 1.55976402759552 Accuracy 94.33715550191108\n",
      "Training:: Epoch 27, Iteration 30, Current loss 1.7377300262451172 Accuracy 94.93502506906783\n",
      "Training:: Epoch 27, Iteration 40, Current loss 1.5960118770599365 Accuracy 94.08543263964951\n",
      "Training:: Epoch 27, Iteration 50, Current loss 1.5377051830291748 Accuracy 92.728490622209\n",
      "Training:: Epoch 27, Iteration 60, Current loss 2.274306297302246 Accuracy 88.35515668677358\n",
      "Training:: Epoch 27, Iteration 70, Current loss 1.5945931673049927 Accuracy 94.87462270722081\n",
      "Training:: Epoch 27, Iteration 80, Current loss 1.7531288862228394 Accuracy 94.24176843407018\n",
      "Training:: Epoch 27, Iteration 90, Current loss 1.854905366897583 Accuracy 92.31777511825383\n",
      "Training:: Epoch 27, Iteration 100, Current loss 1.5729386806488037 Accuracy 93.53818346136467\n",
      "Training:: Epoch 27, Iteration 110, Current loss 2.129617691040039 Accuracy 93.47319347319348\n",
      "Training:: Epoch 27, Iteration 120, Current loss 1.5775090456008911 Accuracy 94.75464432533694\n",
      "Training:: Epoch 27, Iteration 130, Current loss 1.5081026554107666 Accuracy 96.15133724722766\n",
      "Training:: Epoch 27, Iteration 140, Current loss 1.2979105710983276 Accuracy 95.54250924113938\n",
      "Training:: Epoch 27, Iteration 150, Current loss 1.6265482902526855 Accuracy 95.17977631943802\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 27, Probability Accuracy 70.50116907829755\n",
      "Starting Training\n",
      "Training:: Epoch 28, Iteration 0, Current loss 1.1943612098693848 Accuracy 95.10584858270542\n",
      "Training:: Epoch 28, Iteration 10, Current loss 1.421796202659607 Accuracy 95.46052631578948\n",
      "Training:: Epoch 28, Iteration 20, Current loss 1.459025502204895 Accuracy 93.78634833180288\n",
      "Training:: Epoch 28, Iteration 30, Current loss 2.051708459854126 Accuracy 92.26844583987442\n",
      "Training:: Epoch 28, Iteration 40, Current loss 1.5388647317886353 Accuracy 92.36174763214177\n",
      "Training:: Epoch 28, Iteration 50, Current loss 1.5769453048706055 Accuracy 93.0573174950986\n",
      "Training:: Epoch 28, Iteration 60, Current loss 1.7100855112075806 Accuracy 93.73067830514638\n",
      "Training:: Epoch 28, Iteration 70, Current loss 1.1539186239242554 Accuracy 94.8986975397974\n",
      "Training:: Epoch 28, Iteration 80, Current loss 1.6680645942687988 Accuracy 90.48529411764706\n",
      "Training:: Epoch 28, Iteration 90, Current loss 1.669010877609253 Accuracy 96.01253449862006\n",
      "Training:: Epoch 28, Iteration 100, Current loss 1.8727532625198364 Accuracy 92.91930379746836\n",
      "Training:: Epoch 28, Iteration 110, Current loss 1.5684025287628174 Accuracy 94.21037210665104\n",
      "Training:: Epoch 28, Iteration 120, Current loss 2.0137717723846436 Accuracy 90.7895963456698\n",
      "Training:: Epoch 28, Iteration 130, Current loss 1.616056203842163 Accuracy 93.98000206164313\n",
      "Training:: Epoch 28, Iteration 140, Current loss 1.4666918516159058 Accuracy 94.635093885857\n",
      "Training:: Epoch 28, Iteration 150, Current loss 1.1627243757247925 Accuracy 95.48383632262914\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 28, Probability Accuracy 70.76210512218124\n",
      "Starting Training\n",
      "Training:: Epoch 29, Iteration 0, Current loss 1.1959108114242554 Accuracy 96.18430230172733\n",
      "Training:: Epoch 29, Iteration 10, Current loss 1.5066825151443481 Accuracy 93.79321976724574\n",
      "Training:: Epoch 29, Iteration 20, Current loss 1.1832294464111328 Accuracy 96.32818248712289\n",
      "Training:: Epoch 29, Iteration 30, Current loss 1.2198549509048462 Accuracy 96.17208077260754\n",
      "Training:: Epoch 29, Iteration 40, Current loss 0.9341328144073486 Accuracy 97.06096672334223\n",
      "Training:: Epoch 29, Iteration 50, Current loss 1.7092373371124268 Accuracy 92.83620015637217\n",
      "Training:: Epoch 29, Iteration 60, Current loss 1.4856326580047607 Accuracy 94.56604131135114\n",
      "Training:: Epoch 29, Iteration 70, Current loss 1.588111400604248 Accuracy 95.04031713140232\n",
      "Training:: Epoch 29, Iteration 80, Current loss 1.226660132408142 Accuracy 95.84993563415681\n",
      "Training:: Epoch 29, Iteration 90, Current loss 1.3477299213409424 Accuracy 94.84887459807074\n",
      "Training:: Epoch 29, Iteration 100, Current loss 1.5226452350616455 Accuracy 95.08277063820519\n",
      "Training:: Epoch 29, Iteration 110, Current loss 1.480324149131775 Accuracy 94.99419616947185\n",
      "Training:: Epoch 29, Iteration 120, Current loss 1.441900610923767 Accuracy 95.24364847980009\n",
      "Training:: Epoch 29, Iteration 130, Current loss 1.2011569738388062 Accuracy 96.08428446005267\n",
      "Training:: Epoch 29, Iteration 140, Current loss 1.38246750831604 Accuracy 95.71373654536136\n",
      "Training:: Epoch 29, Iteration 150, Current loss 1.3963583707809448 Accuracy 95.11540273198304\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 29, Probability Accuracy 71.06098738288291\n",
      "Starting Training\n",
      "Training:: Epoch 30, Iteration 0, Current loss 1.1705995798110962 Accuracy 95.41421417903892\n",
      "Training:: Epoch 30, Iteration 10, Current loss 1.3785252571105957 Accuracy 96.50060981330331\n",
      "Training:: Epoch 30, Iteration 20, Current loss 1.0968430042266846 Accuracy 96.45682377185402\n",
      "Training:: Epoch 30, Iteration 30, Current loss 1.455100178718567 Accuracy 95.26847862114684\n",
      "Training:: Epoch 30, Iteration 40, Current loss 1.5990947484970093 Accuracy 93.10364367881226\n",
      "Training:: Epoch 30, Iteration 50, Current loss 1.2572708129882812 Accuracy 95.50751190912422\n",
      "Training:: Epoch 30, Iteration 60, Current loss 1.2410476207733154 Accuracy 94.97803975959316\n",
      "Training:: Epoch 30, Iteration 70, Current loss 1.7156782150268555 Accuracy 95.21266073194856\n",
      "Training:: Epoch 30, Iteration 80, Current loss 1.3819864988327026 Accuracy 93.7542844085612\n",
      "Training:: Epoch 30, Iteration 90, Current loss 1.0676358938217163 Accuracy 97.37613713254642\n",
      "Training:: Epoch 30, Iteration 100, Current loss 1.4623394012451172 Accuracy 92.59611465833447\n",
      "Training:: Epoch 30, Iteration 110, Current loss 1.4049811363220215 Accuracy 95.04832609609188\n",
      "Training:: Epoch 30, Iteration 120, Current loss 1.4821677207946777 Accuracy 92.92119222025764\n",
      "Training:: Epoch 30, Iteration 130, Current loss 1.2219892740249634 Accuracy 95.60968800167515\n",
      "Training:: Epoch 30, Iteration 140, Current loss 1.635696291923523 Accuracy 94.27947036894203\n",
      "Training:: Epoch 30, Iteration 150, Current loss 1.5686839818954468 Accuracy 92.90705800139763\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 30, Probability Accuracy 68.56780933142115\n",
      "Starting Training\n",
      "Training:: Epoch 31, Iteration 0, Current loss 1.0903242826461792 Accuracy 95.55977680161463\n",
      "Training:: Epoch 31, Iteration 10, Current loss 1.330932855606079 Accuracy 95.37681598062954\n",
      "Training:: Epoch 31, Iteration 20, Current loss 1.6259641647338867 Accuracy 94.9877750611247\n",
      "Training:: Epoch 31, Iteration 30, Current loss 1.4726059436798096 Accuracy 92.884451041245\n",
      "Training:: Epoch 31, Iteration 40, Current loss 1.0272216796875 Accuracy 97.08559782608695\n",
      "Training:: Epoch 31, Iteration 50, Current loss 1.1947731971740723 Accuracy 96.26371048155318\n",
      "Training:: Epoch 31, Iteration 60, Current loss 1.2973780632019043 Accuracy 95.23552540282168\n",
      "Training:: Epoch 31, Iteration 70, Current loss 1.271472454071045 Accuracy 95.27589174715501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 31, Iteration 80, Current loss 1.0879417657852173 Accuracy 95.44143104443162\n",
      "Training:: Epoch 31, Iteration 90, Current loss 1.0948883295059204 Accuracy 96.5978214740416\n",
      "Training:: Epoch 31, Iteration 100, Current loss 1.2713900804519653 Accuracy 94.9972885032538\n",
      "Training:: Epoch 31, Iteration 110, Current loss 1.4662339687347412 Accuracy 92.97259311314126\n",
      "Training:: Epoch 31, Iteration 120, Current loss 1.0769336223602295 Accuracy 96.57950530035336\n",
      "Training:: Epoch 31, Iteration 130, Current loss 1.1850574016571045 Accuracy 96.33352370074243\n",
      "Training:: Epoch 31, Iteration 140, Current loss 1.450130581855774 Accuracy 95.50144135942952\n",
      "Training:: Epoch 31, Iteration 150, Current loss 1.1536790132522583 Accuracy 97.1617313822637\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 31, Probability Accuracy 70.43108018370434\n",
      "Starting Training\n",
      "Training:: Epoch 32, Iteration 0, Current loss 1.8120604753494263 Accuracy 94.37925946743484\n",
      "Training:: Epoch 32, Iteration 10, Current loss 1.2098252773284912 Accuracy 96.54245457916203\n",
      "Training:: Epoch 32, Iteration 20, Current loss 1.0691367387771606 Accuracy 96.86059907834101\n",
      "Training:: Epoch 32, Iteration 30, Current loss 1.5736069679260254 Accuracy 95.01669407484926\n",
      "Training:: Epoch 32, Iteration 40, Current loss 1.1178067922592163 Accuracy 96.2117566173943\n",
      "Training:: Epoch 32, Iteration 50, Current loss 1.2251715660095215 Accuracy 96.03468861936781\n",
      "Training:: Epoch 32, Iteration 60, Current loss 0.9864281415939331 Accuracy 96.68512063426971\n",
      "Training:: Epoch 32, Iteration 70, Current loss 1.0644019842147827 Accuracy 97.44257636751125\n",
      "Training:: Epoch 32, Iteration 80, Current loss 1.7382363080978394 Accuracy 94.76559746210786\n",
      "Training:: Epoch 32, Iteration 90, Current loss 1.3496798276901245 Accuracy 95.46369123833912\n",
      "Training:: Epoch 32, Iteration 100, Current loss 1.4762542247772217 Accuracy 95.53713739241313\n",
      "Training:: Epoch 32, Iteration 110, Current loss 1.1703177690505981 Accuracy 96.5302365104093\n",
      "Training:: Epoch 32, Iteration 120, Current loss 1.1371723413467407 Accuracy 97.22689690495353\n",
      "Training:: Epoch 32, Iteration 130, Current loss 1.2089108228683472 Accuracy 95.28274458496874\n",
      "Training:: Epoch 32, Iteration 140, Current loss 1.3347740173339844 Accuracy 95.33314721756933\n",
      "Training:: Epoch 32, Iteration 150, Current loss 0.906278669834137 Accuracy 98.1027998328458\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 32, Probability Accuracy 71.06946947840693\n",
      "Starting Training\n",
      "Training:: Epoch 33, Iteration 0, Current loss 1.1247519254684448 Accuracy 97.29729729729729\n",
      "Training:: Epoch 33, Iteration 10, Current loss 1.063066840171814 Accuracy 95.49810058399954\n",
      "Training:: Epoch 33, Iteration 20, Current loss 1.5287808179855347 Accuracy 95.6715210355987\n",
      "Training:: Epoch 33, Iteration 30, Current loss 1.4676467180252075 Accuracy 94.63078032201587\n",
      "Training:: Epoch 33, Iteration 40, Current loss 1.7138893604278564 Accuracy 93.46188954815153\n",
      "Training:: Epoch 33, Iteration 50, Current loss 0.9925150275230408 Accuracy 97.14215776268283\n",
      "Training:: Epoch 33, Iteration 60, Current loss 1.4230588674545288 Accuracy 93.68775359645888\n",
      "Training:: Epoch 33, Iteration 70, Current loss 1.425130009651184 Accuracy 94.06093612010598\n",
      "Training:: Epoch 33, Iteration 80, Current loss 1.1668267250061035 Accuracy 95.69823691680327\n",
      "Training:: Epoch 33, Iteration 90, Current loss 0.992194414138794 Accuracy 97.37074679628812\n",
      "Training:: Epoch 33, Iteration 100, Current loss 1.6835204362869263 Accuracy 92.7685042828904\n",
      "Training:: Epoch 33, Iteration 110, Current loss 2.0031986236572266 Accuracy 88.76040703052729\n",
      "Training:: Epoch 33, Iteration 120, Current loss 1.9421831369400024 Accuracy 93.90839794123396\n",
      "Training:: Epoch 33, Iteration 130, Current loss 2.385477304458618 Accuracy 88.18357657816071\n",
      "Training:: Epoch 33, Iteration 140, Current loss 1.8169007301330566 Accuracy 92.08200981807681\n",
      "Training:: Epoch 33, Iteration 150, Current loss 10.70335865020752 Accuracy 44.037082092369644\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 33, Probability Accuracy 45.51425494277375\n",
      "Starting Training\n",
      "Training:: Epoch 34, Iteration 0, Current loss 6.35521936416626 Accuracy 53.82109004739336\n",
      "Training:: Epoch 34, Iteration 10, Current loss 5.930395126342773 Accuracy 69.68610429640813\n",
      "Training:: Epoch 34, Iteration 20, Current loss 5.896913051605225 Accuracy 46.332970620239394\n",
      "Training:: Epoch 34, Iteration 30, Current loss 7.10212516784668 Accuracy 53.07775377969762\n",
      "Training:: Epoch 34, Iteration 40, Current loss 3.885716199874878 Accuracy 65.79391321951549\n",
      "Training:: Epoch 34, Iteration 50, Current loss 4.42467737197876 Accuracy 78.57318271119843\n",
      "Training:: Epoch 34, Iteration 60, Current loss 2.8266353607177734 Accuracy 82.88186265627832\n",
      "Training:: Epoch 34, Iteration 70, Current loss 1.6422061920166016 Accuracy 94.76741299707095\n",
      "Training:: Epoch 34, Iteration 80, Current loss 2.9527156352996826 Accuracy 83.87978142076503\n",
      "Training:: Epoch 34, Iteration 90, Current loss 2.345029830932617 Accuracy 93.38167342006113\n",
      "Training:: Epoch 34, Iteration 100, Current loss 2.3985800743103027 Accuracy 86.52534113060429\n",
      "Training:: Epoch 34, Iteration 110, Current loss 2.587172746658325 Accuracy 86.64363845710996\n",
      "Training:: Epoch 34, Iteration 120, Current loss 2.178809404373169 Accuracy 91.40532192919933\n",
      "Training:: Epoch 34, Iteration 130, Current loss 3.367305278778076 Accuracy 87.18360787464846\n",
      "Training:: Epoch 34, Iteration 140, Current loss 1.9234485626220703 Accuracy 91.42559833506763\n",
      "Training:: Epoch 34, Iteration 150, Current loss 1.7200058698654175 Accuracy 94.23964877753387\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 34, Probability Accuracy 70.35942879783038\n",
      "Starting Training\n",
      "Training:: Epoch 35, Iteration 0, Current loss 1.550319790840149 Accuracy 95.33955508598932\n",
      "Training:: Epoch 35, Iteration 10, Current loss 2.089343547821045 Accuracy 91.51698493729123\n",
      "Training:: Epoch 35, Iteration 20, Current loss 1.9197818040847778 Accuracy 93.16114638750643\n",
      "Training:: Epoch 35, Iteration 30, Current loss 1.8647429943084717 Accuracy 91.75049284739423\n",
      "Training:: Epoch 35, Iteration 40, Current loss 1.9518623352050781 Accuracy 90.53023424732159\n",
      "Training:: Epoch 35, Iteration 50, Current loss 1.3861278295516968 Accuracy 94.85965667288289\n",
      "Training:: Epoch 35, Iteration 60, Current loss 1.4677176475524902 Accuracy 95.30238324838132\n",
      "Training:: Epoch 35, Iteration 70, Current loss 1.9812732934951782 Accuracy 94.08076710307948\n",
      "Training:: Epoch 35, Iteration 80, Current loss 1.69483482837677 Accuracy 93.31730370616035\n",
      "Training:: Epoch 35, Iteration 90, Current loss 2.0364789962768555 Accuracy 91.4845599163318\n",
      "Training:: Epoch 35, Iteration 100, Current loss 2.391436815261841 Accuracy 87.30417951974839\n",
      "Training:: Epoch 35, Iteration 110, Current loss 2.2984323501586914 Accuracy 86.33050543673079\n",
      "Training:: Epoch 35, Iteration 120, Current loss 1.5742992162704468 Accuracy 94.01568050961656\n",
      "Training:: Epoch 35, Iteration 130, Current loss 4.2887115478515625 Accuracy 81.23580425356184\n",
      "Training:: Epoch 35, Iteration 140, Current loss 2.198777675628662 Accuracy 89.12670459676691\n",
      "Training:: Epoch 35, Iteration 150, Current loss 1.9735184907913208 Accuracy 90.76112230661913\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 35, Probability Accuracy 69.31122036149353\n",
      "Starting Training\n",
      "Training:: Epoch 36, Iteration 0, Current loss 1.1814392805099487 Accuracy 96.0703678318483\n",
      "Training:: Epoch 36, Iteration 10, Current loss 1.6106011867523193 Accuracy 93.91029856635538\n",
      "Training:: Epoch 36, Iteration 20, Current loss 1.1970692873001099 Accuracy 95.82570744755736\n",
      "Training:: Epoch 36, Iteration 30, Current loss 2.1154937744140625 Accuracy 92.89031059746156\n",
      "Training:: Epoch 36, Iteration 40, Current loss 1.790335774421692 Accuracy 91.48655930072442\n",
      "Training:: Epoch 36, Iteration 50, Current loss 1.4853321313858032 Accuracy 95.09867707655606\n",
      "Training:: Epoch 36, Iteration 60, Current loss 1.6200203895568848 Accuracy 94.81853713009492\n",
      "Training:: Epoch 36, Iteration 70, Current loss 1.5471621751785278 Accuracy 94.68955343972107\n",
      "Training:: Epoch 36, Iteration 80, Current loss 2.239048957824707 Accuracy 89.92105547745214\n",
      "Training:: Epoch 36, Iteration 90, Current loss 1.2642980813980103 Accuracy 96.3530135301353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 36, Iteration 100, Current loss 1.4756090641021729 Accuracy 94.75604195166439\n",
      "Training:: Epoch 36, Iteration 110, Current loss 1.1044081449508667 Accuracy 96.92858488788393\n",
      "Training:: Epoch 36, Iteration 120, Current loss 1.4383094310760498 Accuracy 95.86848801392794\n",
      "Training:: Epoch 36, Iteration 130, Current loss 1.2608469724655151 Accuracy 97.35021447350215\n",
      "Training:: Epoch 36, Iteration 140, Current loss 1.310852289199829 Accuracy 94.84846882107156\n",
      "Training:: Epoch 36, Iteration 150, Current loss 1.003014087677002 Accuracy 97.67715762612053\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 36, Probability Accuracy 69.36590755631944\n",
      "Starting Training\n",
      "Training:: Epoch 37, Iteration 0, Current loss 1.178820252418518 Accuracy 96.37867952769001\n",
      "Training:: Epoch 37, Iteration 10, Current loss 1.5640592575073242 Accuracy 95.8821640712795\n",
      "Training:: Epoch 37, Iteration 20, Current loss 1.2961276769638062 Accuracy 95.42640186915888\n",
      "Training:: Epoch 37, Iteration 30, Current loss 1.3592510223388672 Accuracy 97.10976282404854\n",
      "Training:: Epoch 37, Iteration 40, Current loss 1.507631778717041 Accuracy 95.58897243107769\n",
      "Training:: Epoch 37, Iteration 50, Current loss 1.071302890777588 Accuracy 97.14676357356169\n",
      "Training:: Epoch 37, Iteration 60, Current loss 1.072563886642456 Accuracy 96.86724257769835\n",
      "Training:: Epoch 37, Iteration 70, Current loss 1.0487265586853027 Accuracy 97.049907978781\n",
      "Training:: Epoch 37, Iteration 80, Current loss 1.315861463546753 Accuracy 95.86612913547066\n",
      "Training:: Epoch 37, Iteration 90, Current loss 1.361362099647522 Accuracy 95.93775716316301\n",
      "Training:: Epoch 37, Iteration 100, Current loss 1.0920207500457764 Accuracy 96.33204633204633\n",
      "Training:: Epoch 37, Iteration 110, Current loss 1.0644408464431763 Accuracy 98.0965763924374\n",
      "Training:: Epoch 37, Iteration 120, Current loss 1.301995873451233 Accuracy 93.99160398810565\n",
      "Training:: Epoch 37, Iteration 130, Current loss 1.3553777933120728 Accuracy 95.82329756952048\n",
      "Training:: Epoch 37, Iteration 140, Current loss 1.13419508934021 Accuracy 96.7757049152398\n",
      "Training:: Epoch 37, Iteration 150, Current loss 1.6095036268234253 Accuracy 94.33007676203768\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 37, Probability Accuracy 72.293234970787\n",
      "Starting Training\n",
      "Training:: Epoch 38, Iteration 0, Current loss 1.3121315240859985 Accuracy 96.50094120252464\n",
      "Training:: Epoch 38, Iteration 10, Current loss 1.2545205354690552 Accuracy 97.10928407853737\n",
      "Training:: Epoch 38, Iteration 20, Current loss 1.1130075454711914 Accuracy 96.80914092255607\n",
      "Training:: Epoch 38, Iteration 30, Current loss 1.3985369205474854 Accuracy 95.25870359684555\n",
      "Training:: Epoch 38, Iteration 40, Current loss 1.2629072666168213 Accuracy 95.9894962998329\n",
      "Training:: Epoch 38, Iteration 50, Current loss 0.968328595161438 Accuracy 96.97737398445007\n",
      "Training:: Epoch 38, Iteration 60, Current loss 1.1677682399749756 Accuracy 97.11936952238604\n",
      "Training:: Epoch 38, Iteration 70, Current loss 1.1705743074417114 Accuracy 96.54446957344975\n",
      "Training:: Epoch 38, Iteration 80, Current loss 1.098961353302002 Accuracy 96.958921558221\n",
      "Training:: Epoch 38, Iteration 90, Current loss 1.1493260860443115 Accuracy 97.54508258827094\n",
      "Training:: Epoch 38, Iteration 100, Current loss 1.2992714643478394 Accuracy 96.88875332258486\n",
      "Training:: Epoch 38, Iteration 110, Current loss 1.1049693822860718 Accuracy 97.4021164021164\n",
      "Training:: Epoch 38, Iteration 120, Current loss 1.1101078987121582 Accuracy 96.53741957331528\n",
      "Training:: Epoch 38, Iteration 130, Current loss 1.2421821355819702 Accuracy 96.60865322055953\n",
      "Training:: Epoch 38, Iteration 140, Current loss 0.7942543625831604 Accuracy 98.43357082984073\n",
      "Training:: Epoch 38, Iteration 150, Current loss 1.6671665906906128 Accuracy 95.04371017928582\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 38, Probability Accuracy 72.54780944302766\n",
      "Starting Training\n",
      "Training:: Epoch 39, Iteration 0, Current loss 1.0286741256713867 Accuracy 98.15135555477006\n",
      "Training:: Epoch 39, Iteration 10, Current loss 0.9737576842308044 Accuracy 97.98341387745361\n",
      "Training:: Epoch 39, Iteration 20, Current loss 1.1682757139205933 Accuracy 97.37874613907671\n",
      "Training:: Epoch 39, Iteration 30, Current loss 1.2662508487701416 Accuracy 95.80377052503547\n",
      "Training:: Epoch 39, Iteration 40, Current loss 1.4011056423187256 Accuracy 95.03175496422541\n",
      "Training:: Epoch 39, Iteration 50, Current loss 1.148186445236206 Accuracy 97.95888182221564\n",
      "Training:: Epoch 39, Iteration 60, Current loss 0.8903980255126953 Accuracy 97.98450903639544\n",
      "Training:: Epoch 39, Iteration 70, Current loss 1.0495978593826294 Accuracy 96.78244689099085\n",
      "Training:: Epoch 39, Iteration 80, Current loss 1.2058753967285156 Accuracy 97.0556552962298\n",
      "Training:: Epoch 39, Iteration 90, Current loss 1.2781614065170288 Accuracy 96.68529216781893\n",
      "Training:: Epoch 39, Iteration 100, Current loss 1.0680710077285767 Accuracy 98.1420988158432\n",
      "Training:: Epoch 39, Iteration 110, Current loss 1.0075206756591797 Accuracy 96.65922933124284\n",
      "Training:: Epoch 39, Iteration 120, Current loss 1.114941954612732 Accuracy 96.94672787445286\n",
      "Training:: Epoch 39, Iteration 130, Current loss 1.4722987413406372 Accuracy 97.41062479117942\n",
      "Training:: Epoch 39, Iteration 140, Current loss 1.2578204870224 Accuracy 97.63074984247007\n",
      "Training:: Epoch 39, Iteration 150, Current loss 1.1908313035964966 Accuracy 97.15242881072027\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 39, Probability Accuracy 72.24412810196371\n",
      "Starting Training\n",
      "Training:: Epoch 40, Iteration 0, Current loss 1.7355339527130127 Accuracy 93.1271015467384\n",
      "Training:: Epoch 40, Iteration 10, Current loss 1.0073972940444946 Accuracy 97.25744448906994\n",
      "Training:: Epoch 40, Iteration 20, Current loss 1.098618745803833 Accuracy 97.85069166571395\n",
      "Training:: Epoch 40, Iteration 30, Current loss 1.2648605108261108 Accuracy 95.00037534719615\n",
      "Training:: Epoch 40, Iteration 40, Current loss 1.0547531843185425 Accuracy 97.36719694855336\n",
      "Training:: Epoch 40, Iteration 50, Current loss 0.7363824844360352 Accuracy 98.56477089949271\n",
      "Training:: Epoch 40, Iteration 60, Current loss 1.0748170614242554 Accuracy 97.65303321438375\n",
      "Training:: Epoch 40, Iteration 70, Current loss 1.0332056283950806 Accuracy 97.33263046916183\n",
      "Training:: Epoch 40, Iteration 80, Current loss 1.0418601036071777 Accuracy 97.77703909918145\n",
      "Training:: Epoch 40, Iteration 90, Current loss 1.164238452911377 Accuracy 97.77666999002992\n",
      "Training:: Epoch 40, Iteration 100, Current loss 1.010705590248108 Accuracy 97.62717852096091\n",
      "Training:: Epoch 40, Iteration 110, Current loss 1.630034327507019 Accuracy 94.93589743589743\n",
      "Training:: Epoch 40, Iteration 120, Current loss 0.8528391718864441 Accuracy 97.80184644898286\n",
      "Training:: Epoch 40, Iteration 130, Current loss 1.1459306478500366 Accuracy 98.0161541731614\n",
      "Training:: Epoch 40, Iteration 140, Current loss 1.1896132230758667 Accuracy 95.40757749712974\n",
      "Training:: Epoch 40, Iteration 150, Current loss 1.2112916707992554 Accuracy 95.54672513017265\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 40, Probability Accuracy 71.08498278469428\n",
      "Starting Training\n",
      "Training:: Epoch 41, Iteration 0, Current loss 1.1088405847549438 Accuracy 97.68466345661305\n",
      "Training:: Epoch 41, Iteration 10, Current loss 1.1653873920440674 Accuracy 96.98401667824878\n",
      "Training:: Epoch 41, Iteration 20, Current loss 1.2349799871444702 Accuracy 95.92440427280197\n",
      "Training:: Epoch 41, Iteration 30, Current loss 0.8251685500144958 Accuracy 97.83608613840686\n",
      "Training:: Epoch 41, Iteration 40, Current loss 1.0930136442184448 Accuracy 97.15772822370771\n",
      "Training:: Epoch 41, Iteration 50, Current loss 0.9589701890945435 Accuracy 97.78490920475892\n",
      "Training:: Epoch 41, Iteration 60, Current loss 1.00873863697052 Accuracy 96.89445889493182\n",
      "Training:: Epoch 41, Iteration 70, Current loss 0.809603214263916 Accuracy 97.72751520324336\n",
      "Training:: Epoch 41, Iteration 80, Current loss 1.0423767566680908 Accuracy 97.86880050705118\n",
      "Training:: Epoch 41, Iteration 90, Current loss 0.8766967058181763 Accuracy 98.03607797497818\n",
      "Training:: Epoch 41, Iteration 100, Current loss 0.6931136846542358 Accuracy 98.13971640431897\n",
      "Training:: Epoch 41, Iteration 110, Current loss 0.9057621955871582 Accuracy 98.03053018945073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 41, Iteration 120, Current loss 1.1109238862991333 Accuracy 96.05972898302855\n",
      "Training:: Epoch 41, Iteration 130, Current loss 1.069351077079773 Accuracy 96.97057126370456\n",
      "Training:: Epoch 41, Iteration 140, Current loss 0.7799539566040039 Accuracy 97.91770504513485\n",
      "Training:: Epoch 41, Iteration 150, Current loss 0.952498733997345 Accuracy 98.24421388667199\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 41, Probability Accuracy 71.97113855391432\n",
      "Starting Training\n",
      "Training:: Epoch 42, Iteration 0, Current loss 1.212652564048767 Accuracy 97.36193570068332\n",
      "Training:: Epoch 42, Iteration 10, Current loss 1.1413626670837402 Accuracy 97.5458537845518\n",
      "Training:: Epoch 42, Iteration 20, Current loss 0.9441373944282532 Accuracy 96.38796778777831\n",
      "Training:: Epoch 42, Iteration 30, Current loss 1.3240259885787964 Accuracy 96.35776855693868\n",
      "Training:: Epoch 42, Iteration 40, Current loss 1.0760819911956787 Accuracy 97.1911824461601\n",
      "Training:: Epoch 42, Iteration 50, Current loss 1.0596332550048828 Accuracy 97.23606353161009\n",
      "Training:: Epoch 42, Iteration 60, Current loss 1.0094627141952515 Accuracy 97.71049596309112\n",
      "Training:: Epoch 42, Iteration 70, Current loss 0.7258257269859314 Accuracy 98.36012021429505\n",
      "Training:: Epoch 42, Iteration 80, Current loss 0.9496613144874573 Accuracy 98.06098211255998\n",
      "Training:: Epoch 42, Iteration 90, Current loss 0.8166847825050354 Accuracy 97.67129968300415\n",
      "Training:: Epoch 42, Iteration 100, Current loss 0.8060001134872437 Accuracy 98.73306865950491\n",
      "Training:: Epoch 42, Iteration 110, Current loss 1.014798641204834 Accuracy 96.97143220716421\n",
      "Training:: Epoch 42, Iteration 120, Current loss 0.7892468571662903 Accuracy 98.32798139740892\n",
      "Training:: Epoch 42, Iteration 130, Current loss 0.7203583121299744 Accuracy 98.7377562354842\n",
      "Training:: Epoch 42, Iteration 140, Current loss 1.0796327590942383 Accuracy 97.44017807456872\n",
      "Training:: Epoch 42, Iteration 150, Current loss 1.0947279930114746 Accuracy 97.24517906336088\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 42, Probability Accuracy 70.69402514494897\n",
      "Starting Training\n",
      "Training:: Epoch 43, Iteration 0, Current loss 0.9209558367729187 Accuracy 97.64049764049764\n",
      "Training:: Epoch 43, Iteration 10, Current loss 0.8564388155937195 Accuracy 97.92070484581498\n",
      "Training:: Epoch 43, Iteration 20, Current loss 0.9322323203086853 Accuracy 96.56970362239298\n",
      "Training:: Epoch 43, Iteration 30, Current loss 1.283690333366394 Accuracy 97.14858949416342\n",
      "Training:: Epoch 43, Iteration 40, Current loss 1.4938453435897827 Accuracy 93.59833971902937\n",
      "Training:: Epoch 43, Iteration 50, Current loss 1.2416245937347412 Accuracy 95.63214821715054\n",
      "Training:: Epoch 43, Iteration 60, Current loss 1.2002182006835938 Accuracy 93.4355534259703\n",
      "Training:: Epoch 43, Iteration 70, Current loss 0.7709428668022156 Accuracy 98.20048946686501\n",
      "Training:: Epoch 43, Iteration 80, Current loss 1.1790531873703003 Accuracy 97.34304809904678\n",
      "Training:: Epoch 43, Iteration 90, Current loss 1.0485000610351562 Accuracy 97.02839947338725\n",
      "Training:: Epoch 43, Iteration 100, Current loss 1.0243679285049438 Accuracy 98.03041526863916\n",
      "Training:: Epoch 43, Iteration 110, Current loss 0.8959481716156006 Accuracy 96.97788830621182\n",
      "Training:: Epoch 43, Iteration 120, Current loss 0.8183109760284424 Accuracy 97.55233698058314\n",
      "Training:: Epoch 43, Iteration 130, Current loss 0.7992393374443054 Accuracy 97.94125031383379\n",
      "Training:: Epoch 43, Iteration 140, Current loss 0.9368263483047485 Accuracy 97.35786159639484\n",
      "Training:: Epoch 43, Iteration 150, Current loss 0.92049640417099 Accuracy 97.5724353954581\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 43, Probability Accuracy 69.83264602318067\n",
      "Starting Training\n",
      "Training:: Epoch 44, Iteration 0, Current loss 0.8352334499359131 Accuracy 96.89691613027188\n",
      "Training:: Epoch 44, Iteration 10, Current loss 1.6574517488479614 Accuracy 93.60860623319095\n",
      "Training:: Epoch 44, Iteration 20, Current loss 3.3242275714874268 Accuracy 84.53921458309891\n",
      "Training:: Epoch 44, Iteration 30, Current loss 2.616612672805786 Accuracy 87.72899222170273\n",
      "Training:: Epoch 44, Iteration 40, Current loss 3.1706113815307617 Accuracy 82.99493896993152\n",
      "Training:: Epoch 44, Iteration 50, Current loss 1.868245005607605 Accuracy 93.9168614120813\n",
      "Training:: Epoch 44, Iteration 60, Current loss 1.5262995958328247 Accuracy 92.87833827893175\n",
      "Training:: Epoch 44, Iteration 70, Current loss 1.6031571626663208 Accuracy 93.86214863303812\n",
      "Training:: Epoch 44, Iteration 80, Current loss 1.4415130615234375 Accuracy 94.13393756294059\n",
      "Training:: Epoch 44, Iteration 90, Current loss 1.3081036806106567 Accuracy 95.37872136642989\n",
      "Training:: Epoch 44, Iteration 100, Current loss 1.4111090898513794 Accuracy 95.78090629124505\n",
      "Training:: Epoch 44, Iteration 110, Current loss 1.4274736642837524 Accuracy 93.44774874930516\n",
      "Training:: Epoch 44, Iteration 120, Current loss 1.4386653900146484 Accuracy 94.65661103979461\n",
      "Training:: Epoch 44, Iteration 130, Current loss 0.9379302263259888 Accuracy 96.89347251917998\n",
      "Training:: Epoch 44, Iteration 140, Current loss 1.559692621231079 Accuracy 94.01362244579141\n",
      "Training:: Epoch 44, Iteration 150, Current loss 1.664297103881836 Accuracy 93.40691117644224\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 44, Probability Accuracy 65.32072923700203\n",
      "Starting Training\n",
      "Training:: Epoch 45, Iteration 0, Current loss 1.5379438400268555 Accuracy 91.77177622792699\n",
      "Training:: Epoch 45, Iteration 10, Current loss 4.316103458404541 Accuracy 70.5248286327246\n",
      "Training:: Epoch 45, Iteration 20, Current loss 2.4932808876037598 Accuracy 80.05006447697792\n",
      "Training:: Epoch 45, Iteration 30, Current loss 1.3685214519500732 Accuracy 94.0370159813025\n",
      "Training:: Epoch 45, Iteration 40, Current loss 2.5994668006896973 Accuracy 85.57101667293469\n",
      "Training:: Epoch 45, Iteration 50, Current loss 2.2141494750976562 Accuracy 91.25784222976448\n",
      "Training:: Epoch 45, Iteration 60, Current loss 2.0044162273406982 Accuracy 83.17415730337079\n",
      "Training:: Epoch 45, Iteration 70, Current loss 2.6329357624053955 Accuracy 86.78801513447183\n",
      "Training:: Epoch 45, Iteration 80, Current loss 2.4295661449432373 Accuracy 90.37680505415162\n",
      "Training:: Epoch 45, Iteration 90, Current loss 1.348433017730713 Accuracy 95.12164534737899\n",
      "Training:: Epoch 45, Iteration 100, Current loss 1.170372724533081 Accuracy 95.90660854752242\n",
      "Training:: Epoch 45, Iteration 110, Current loss 1.5620245933532715 Accuracy 95.05713352921973\n",
      "Training:: Epoch 45, Iteration 120, Current loss 1.1493215560913086 Accuracy 96.48064905163908\n",
      "Training:: Epoch 45, Iteration 130, Current loss 1.3630197048187256 Accuracy 94.59277917716204\n",
      "Training:: Epoch 45, Iteration 140, Current loss 1.5426111221313477 Accuracy 94.57094992864677\n",
      "Training:: Epoch 45, Iteration 150, Current loss 0.9423704743385315 Accuracy 96.48134148855749\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 45, Probability Accuracy 71.20942405455327\n",
      "Starting Training\n",
      "Training:: Epoch 46, Iteration 0, Current loss 1.29901921749115 Accuracy 96.0315155035945\n",
      "Training:: Epoch 46, Iteration 10, Current loss 1.3309314250946045 Accuracy 95.12623194275685\n",
      "Training:: Epoch 46, Iteration 20, Current loss 1.017823338508606 Accuracy 96.37490207250195\n",
      "Training:: Epoch 46, Iteration 30, Current loss 0.9294934272766113 Accuracy 96.76801532200143\n",
      "Training:: Epoch 46, Iteration 40, Current loss 1.9085510969161987 Accuracy 92.60280013228972\n",
      "Training:: Epoch 46, Iteration 50, Current loss 1.0229789018630981 Accuracy 97.97450912848777\n",
      "Training:: Epoch 46, Iteration 60, Current loss 1.518403172492981 Accuracy 93.09374820598197\n",
      "Training:: Epoch 46, Iteration 70, Current loss 1.4575371742248535 Accuracy 94.8153143028396\n",
      "Training:: Epoch 46, Iteration 80, Current loss 1.364150047302246 Accuracy 94.59797183640121\n",
      "Training:: Epoch 46, Iteration 90, Current loss 2.348472833633423 Accuracy 88.24129039995204\n",
      "Training:: Epoch 46, Iteration 100, Current loss 2.9723830223083496 Accuracy 85.27951101489876\n",
      "Training:: Epoch 46, Iteration 110, Current loss 2.9826672077178955 Accuracy 85.10717713554442\n",
      "Training:: Epoch 46, Iteration 120, Current loss 1.641843318939209 Accuracy 89.25814347580493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 46, Iteration 130, Current loss 1.564468502998352 Accuracy 94.6459802538787\n",
      "Training:: Epoch 46, Iteration 140, Current loss 2.0976459980010986 Accuracy 93.31246442800227\n",
      "Training:: Epoch 46, Iteration 150, Current loss 2.363323211669922 Accuracy 90.36401187556473\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 46, Probability Accuracy 68.12830285545282\n",
      "Starting Training\n",
      "Training:: Epoch 47, Iteration 0, Current loss 2.489412307739258 Accuracy 89.25619834710744\n",
      "Training:: Epoch 47, Iteration 10, Current loss 3.3610332012176514 Accuracy 83.2535885167464\n",
      "Training:: Epoch 47, Iteration 20, Current loss 3.752472162246704 Accuracy 77.57974215419385\n",
      "Training:: Epoch 47, Iteration 30, Current loss 2.556938409805298 Accuracy 88.14338964231898\n",
      "Training:: Epoch 47, Iteration 40, Current loss 1.826987624168396 Accuracy 92.3302036337032\n",
      "Training:: Epoch 47, Iteration 50, Current loss 1.580244779586792 Accuracy 91.83890712321477\n",
      "Training:: Epoch 47, Iteration 60, Current loss 1.3445106744766235 Accuracy 95.4154603983926\n",
      "Training:: Epoch 47, Iteration 70, Current loss 1.9336884021759033 Accuracy 93.44704375379222\n",
      "Training:: Epoch 47, Iteration 80, Current loss 1.1363637447357178 Accuracy 96.80251148189059\n",
      "Training:: Epoch 47, Iteration 90, Current loss 1.6417325735092163 Accuracy 95.95160539785947\n",
      "Training:: Epoch 47, Iteration 100, Current loss 1.3975812196731567 Accuracy 94.90303678556481\n",
      "Training:: Epoch 47, Iteration 110, Current loss 1.5729352235794067 Accuracy 91.03965032781767\n",
      "Training:: Epoch 47, Iteration 120, Current loss 1.4794611930847168 Accuracy 96.83489774633837\n",
      "Training:: Epoch 47, Iteration 130, Current loss 1.5111384391784668 Accuracy 94.77112135176651\n",
      "Training:: Epoch 47, Iteration 140, Current loss 1.5599488019943237 Accuracy 94.4300730213825\n",
      "Training:: Epoch 47, Iteration 150, Current loss 1.316550612449646 Accuracy 96.08520799296217\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 47, Probability Accuracy 67.68209998828131\n",
      "Starting Training\n",
      "Training:: Epoch 48, Iteration 0, Current loss 2.495114803314209 Accuracy 85.46028724692853\n",
      "Training:: Epoch 48, Iteration 10, Current loss 1.0385520458221436 Accuracy 96.39230578717081\n",
      "Training:: Epoch 48, Iteration 20, Current loss 1.299353837966919 Accuracy 97.22050101796937\n",
      "Training:: Epoch 48, Iteration 30, Current loss 1.0784813165664673 Accuracy 97.73102764621655\n",
      "Training:: Epoch 48, Iteration 40, Current loss 1.1116315126419067 Accuracy 96.84285437018826\n",
      "Training:: Epoch 48, Iteration 50, Current loss 1.030068039894104 Accuracy 97.8769551616267\n",
      "Training:: Epoch 48, Iteration 60, Current loss 1.0283091068267822 Accuracy 96.87146850651281\n",
      "Training:: Epoch 48, Iteration 70, Current loss 0.9868571162223816 Accuracy 97.00094906675103\n",
      "Training:: Epoch 48, Iteration 80, Current loss 1.1319893598556519 Accuracy 97.68122302684515\n",
      "Training:: Epoch 48, Iteration 90, Current loss 1.208884835243225 Accuracy 95.7726409119696\n",
      "Training:: Epoch 48, Iteration 100, Current loss 1.1193426847457886 Accuracy 96.80004646030548\n",
      "Training:: Epoch 48, Iteration 110, Current loss 1.1659832000732422 Accuracy 96.5547703180212\n",
      "Training:: Epoch 48, Iteration 120, Current loss 1.473825454711914 Accuracy 96.53006589785832\n",
      "Training:: Epoch 48, Iteration 130, Current loss 0.9750791788101196 Accuracy 97.72779304029304\n",
      "Training:: Epoch 48, Iteration 140, Current loss 1.2077513933181763 Accuracy 96.68709033912796\n",
      "Training:: Epoch 48, Iteration 150, Current loss 0.8835373520851135 Accuracy 97.96644172898048\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 48, Probability Accuracy 72.97169100618858\n",
      "Starting Training\n",
      "Training:: Epoch 49, Iteration 0, Current loss 1.2152843475341797 Accuracy 97.26039906853568\n",
      "Training:: Epoch 49, Iteration 10, Current loss 0.9764161109924316 Accuracy 96.76846853508837\n",
      "Training:: Epoch 49, Iteration 20, Current loss 1.0481054782867432 Accuracy 97.8230075300082\n",
      "Training:: Epoch 49, Iteration 30, Current loss 1.0933890342712402 Accuracy 96.94277252316805\n",
      "Training:: Epoch 49, Iteration 40, Current loss 0.918693482875824 Accuracy 97.71570668517234\n",
      "Training:: Epoch 49, Iteration 50, Current loss 1.18170964717865 Accuracy 97.75762156714538\n",
      "Training:: Epoch 49, Iteration 60, Current loss 0.786338210105896 Accuracy 98.3710500541265\n",
      "Training:: Epoch 49, Iteration 70, Current loss 0.9638313055038452 Accuracy 97.71854100460087\n",
      "Training:: Epoch 49, Iteration 80, Current loss 1.0136380195617676 Accuracy 97.22046302160298\n",
      "Training:: Epoch 49, Iteration 90, Current loss 0.9352905750274658 Accuracy 98.17414652857691\n",
      "Training:: Epoch 49, Iteration 100, Current loss 1.230589509010315 Accuracy 97.46963562753037\n",
      "Training:: Epoch 49, Iteration 110, Current loss 0.870448648929596 Accuracy 98.47335996208787\n",
      "Training:: Epoch 49, Iteration 120, Current loss 0.9491937756538391 Accuracy 97.73519163763066\n",
      "Training:: Epoch 49, Iteration 130, Current loss 1.0351946353912354 Accuracy 97.67673115169048\n",
      "Training:: Epoch 49, Iteration 140, Current loss 1.0761202573776245 Accuracy 96.83836121723093\n",
      "Training:: Epoch 49, Iteration 150, Current loss 0.9043349623680115 Accuracy 97.74430033029888\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 49, Probability Accuracy 73.5318441303341\n",
      "Starting Training\n",
      "Training:: Epoch 50, Iteration 0, Current loss 1.1023951768875122 Accuracy 96.90654273473241\n",
      "Training:: Epoch 50, Iteration 10, Current loss 0.9741132259368896 Accuracy 97.6429336709847\n",
      "Training:: Epoch 50, Iteration 20, Current loss 0.9032785296440125 Accuracy 97.61635788522368\n",
      "Training:: Epoch 50, Iteration 30, Current loss 0.7408922910690308 Accuracy 98.09590643274854\n",
      "Training:: Epoch 50, Iteration 40, Current loss 1.0577112436294556 Accuracy 97.62815608263197\n",
      "Training:: Epoch 50, Iteration 50, Current loss 0.8532552719116211 Accuracy 98.60299312736849\n",
      "Training:: Epoch 50, Iteration 60, Current loss 1.0474059581756592 Accuracy 97.69362325922306\n",
      "Training:: Epoch 50, Iteration 70, Current loss 0.7812165021896362 Accuracy 97.96591533809786\n",
      "Training:: Epoch 50, Iteration 80, Current loss 0.7713533043861389 Accuracy 98.00478850758181\n",
      "Training:: Epoch 50, Iteration 90, Current loss 1.0106874704360962 Accuracy 97.35354362965998\n",
      "Training:: Epoch 50, Iteration 100, Current loss 1.1429319381713867 Accuracy 96.82645732420244\n",
      "Training:: Epoch 50, Iteration 110, Current loss 0.8792099356651306 Accuracy 97.58520388562647\n",
      "Training:: Epoch 50, Iteration 120, Current loss 0.9111406207084656 Accuracy 98.17550729797081\n",
      "Training:: Epoch 50, Iteration 130, Current loss 1.0047249794006348 Accuracy 97.50071489848442\n",
      "Training:: Epoch 50, Iteration 140, Current loss 1.0690139532089233 Accuracy 97.815362931642\n",
      "Training:: Epoch 50, Iteration 150, Current loss 0.8646496534347534 Accuracy 98.42726081258192\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 50, Probability Accuracy 72.61767512458078\n",
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 1.1351245641708374 Accuracy 97.19594783679409\n",
      "Training:: Epoch 51, Iteration 10, Current loss 1.014059066772461 Accuracy 97.7743414149224\n",
      "Training:: Epoch 51, Iteration 20, Current loss 0.761454164981842 Accuracy 97.97175623163682\n",
      "Training:: Epoch 51, Iteration 30, Current loss 0.9652906656265259 Accuracy 97.98550236008091\n",
      "Training:: Epoch 51, Iteration 40, Current loss 0.7396025061607361 Accuracy 98.26812858210815\n",
      "Training:: Epoch 51, Iteration 50, Current loss 0.8503146767616272 Accuracy 98.00022469385462\n",
      "Training:: Epoch 51, Iteration 60, Current loss 0.8318861126899719 Accuracy 97.43224980439577\n",
      "Training:: Epoch 51, Iteration 70, Current loss 0.5861390233039856 Accuracy 99.01359892740854\n",
      "Training:: Epoch 51, Iteration 80, Current loss 0.9660571813583374 Accuracy 98.05542051531356\n",
      "Training:: Epoch 51, Iteration 90, Current loss 0.8321735262870789 Accuracy 98.38840052541174\n",
      "Training:: Epoch 51, Iteration 100, Current loss 0.9352477788925171 Accuracy 98.48574122083431\n",
      "Training:: Epoch 51, Iteration 110, Current loss 0.8050745725631714 Accuracy 98.73744619799139\n",
      "Training:: Epoch 51, Iteration 120, Current loss 1.0518546104431152 Accuracy 97.54293747325958\n",
      "Training:: Epoch 51, Iteration 130, Current loss 1.1246401071548462 Accuracy 97.46767880441229\n",
      "Training:: Epoch 51, Iteration 140, Current loss 0.8047897815704346 Accuracy 98.3038126688682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 51, Iteration 150, Current loss 0.768423318862915 Accuracy 98.79705083430345\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 51, Probability Accuracy 72.79803126098626\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 0.5833995342254639 Accuracy 98.55899794934324\n",
      "Training:: Epoch 52, Iteration 10, Current loss 0.9097919464111328 Accuracy 98.5536107326276\n",
      "Training:: Epoch 52, Iteration 20, Current loss 1.3583866357803345 Accuracy 97.41059110686618\n",
      "Training:: Epoch 52, Iteration 30, Current loss 0.601100504398346 Accuracy 98.23374671176249\n",
      "Training:: Epoch 52, Iteration 40, Current loss 0.9567152261734009 Accuracy 98.54014598540147\n",
      "Training:: Epoch 52, Iteration 50, Current loss 1.0986524820327759 Accuracy 97.74535809018568\n",
      "Training:: Epoch 52, Iteration 60, Current loss 0.914095401763916 Accuracy 98.56801909307876\n",
      "Training:: Epoch 52, Iteration 70, Current loss 1.0406638383865356 Accuracy 97.51672240802675\n",
      "Training:: Epoch 52, Iteration 80, Current loss 0.6718904376029968 Accuracy 98.39816933638444\n",
      "Training:: Epoch 52, Iteration 90, Current loss 0.5511983036994934 Accuracy 98.71895630304935\n",
      "Training:: Epoch 52, Iteration 100, Current loss 1.0727078914642334 Accuracy 98.34322672100647\n",
      "Training:: Epoch 52, Iteration 110, Current loss 0.8811147809028625 Accuracy 98.23455233291298\n",
      "Training:: Epoch 52, Iteration 120, Current loss 0.6709301471710205 Accuracy 98.2752938591673\n",
      "Training:: Epoch 52, Iteration 130, Current loss 0.8968729376792908 Accuracy 97.24902779778193\n",
      "Training:: Epoch 52, Iteration 140, Current loss 1.148526668548584 Accuracy 94.64762017663446\n",
      "Training:: Epoch 52, Iteration 150, Current loss 0.9664055109024048 Accuracy 97.77873858293249\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 52, Probability Accuracy 73.0816234284407\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 1.009579062461853 Accuracy 98.15346470050893\n",
      "Training:: Epoch 53, Iteration 10, Current loss 0.8292306065559387 Accuracy 98.44687067507304\n",
      "Training:: Epoch 53, Iteration 20, Current loss 0.8813478946685791 Accuracy 97.99008810572687\n",
      "Training:: Epoch 53, Iteration 30, Current loss 0.8532602190971375 Accuracy 98.27283069603888\n",
      "Training:: Epoch 53, Iteration 40, Current loss 0.8786954879760742 Accuracy 98.30102374210412\n",
      "Training:: Epoch 53, Iteration 50, Current loss 0.7631215453147888 Accuracy 98.10037852236086\n",
      "Training:: Epoch 53, Iteration 60, Current loss 0.8569588661193848 Accuracy 97.82832088785366\n",
      "Training:: Epoch 53, Iteration 70, Current loss 0.846197247505188 Accuracy 98.45572013867003\n",
      "Training:: Epoch 53, Iteration 80, Current loss 1.3179587125778198 Accuracy 95.7002457002457\n",
      "Training:: Epoch 53, Iteration 90, Current loss 0.6738151907920837 Accuracy 98.3544363826054\n",
      "Training:: Epoch 53, Iteration 100, Current loss 0.9002581238746643 Accuracy 97.55506732015098\n",
      "Training:: Epoch 53, Iteration 110, Current loss 0.6859305500984192 Accuracy 98.26406767193821\n",
      "Training:: Epoch 53, Iteration 120, Current loss 0.7957416772842407 Accuracy 97.78089887640449\n",
      "Training:: Epoch 53, Iteration 130, Current loss 0.974723219871521 Accuracy 97.90857484314311\n",
      "Training:: Epoch 53, Iteration 140, Current loss 0.750629186630249 Accuracy 97.54098360655738\n",
      "Training:: Epoch 53, Iteration 150, Current loss 1.1698368787765503 Accuracy 97.15687801838293\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 53, Probability Accuracy 73.1879844420511\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 0.8065078258514404 Accuracy 98.70025304807913\n",
      "Training:: Epoch 54, Iteration 10, Current loss 0.8529087901115417 Accuracy 97.40584578003288\n",
      "Training:: Epoch 54, Iteration 20, Current loss 1.0737606287002563 Accuracy 97.95383270782993\n",
      "Training:: Epoch 54, Iteration 30, Current loss 0.6109462380409241 Accuracy 98.85904101675332\n",
      "Training:: Epoch 54, Iteration 40, Current loss 0.841762125492096 Accuracy 98.67066092492043\n",
      "Training:: Epoch 54, Iteration 50, Current loss 0.7751486301422119 Accuracy 98.77690413787626\n",
      "Training:: Epoch 54, Iteration 60, Current loss 0.796323299407959 Accuracy 98.43567006859628\n",
      "Training:: Epoch 54, Iteration 70, Current loss 0.7478523850440979 Accuracy 98.03681663978644\n",
      "Training:: Epoch 54, Iteration 80, Current loss 0.9028852581977844 Accuracy 98.72048542408653\n",
      "Training:: Epoch 54, Iteration 90, Current loss 0.6097005009651184 Accuracy 98.63386717127429\n",
      "Training:: Epoch 54, Iteration 100, Current loss 0.7488357424736023 Accuracy 97.97896229760076\n",
      "Training:: Epoch 54, Iteration 110, Current loss 0.8001514077186584 Accuracy 98.42247213882246\n",
      "Training:: Epoch 54, Iteration 120, Current loss 0.6654198169708252 Accuracy 98.68735717702079\n",
      "Training:: Epoch 54, Iteration 130, Current loss 0.8668650984764099 Accuracy 96.96041470311027\n",
      "Training:: Epoch 54, Iteration 140, Current loss 0.9467708468437195 Accuracy 97.87785051435148\n",
      "Training:: Epoch 54, Iteration 150, Current loss 0.511734664440155 Accuracy 98.94905777593705\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 54, Probability Accuracy 72.34993108297387\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 0.8804286122322083 Accuracy 98.03331430647086\n",
      "Training:: Epoch 55, Iteration 10, Current loss 0.5991306900978088 Accuracy 98.32197168327215\n",
      "Training:: Epoch 55, Iteration 20, Current loss 0.7324692606925964 Accuracy 98.61976847729296\n",
      "Training:: Epoch 55, Iteration 30, Current loss 0.9519009590148926 Accuracy 98.11739559355475\n",
      "Training:: Epoch 55, Iteration 40, Current loss 0.8602954149246216 Accuracy 98.94096881741518\n",
      "Training:: Epoch 55, Iteration 50, Current loss 1.0422859191894531 Accuracy 97.36826850617355\n",
      "Training:: Epoch 55, Iteration 60, Current loss 0.7867128252983093 Accuracy 98.03217114395362\n",
      "Training:: Epoch 55, Iteration 70, Current loss 0.768867552280426 Accuracy 97.9295837023915\n",
      "Training:: Epoch 55, Iteration 80, Current loss 0.7483915090560913 Accuracy 98.02495417184082\n",
      "Training:: Epoch 55, Iteration 90, Current loss 0.5522315502166748 Accuracy 98.96178746249802\n",
      "Training:: Epoch 55, Iteration 100, Current loss 0.7623748779296875 Accuracy 97.63933931669055\n",
      "Training:: Epoch 55, Iteration 110, Current loss 0.6794537305831909 Accuracy 98.56827601054954\n",
      "Training:: Epoch 55, Iteration 120, Current loss 0.8162919282913208 Accuracy 98.1543343911809\n",
      "Training:: Epoch 55, Iteration 130, Current loss 1.0068016052246094 Accuracy 98.20278278794125\n",
      "Training:: Epoch 55, Iteration 140, Current loss 0.8529607057571411 Accuracy 97.44142508651501\n",
      "Training:: Epoch 55, Iteration 150, Current loss 0.7406887412071228 Accuracy 98.67210471402826\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 72.62905898962617\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 0.8723247647285461 Accuracy 97.61274920962643\n",
      "Training:: Epoch 56, Iteration 10, Current loss 0.8066423535346985 Accuracy 98.71545618664075\n",
      "Training:: Epoch 56, Iteration 20, Current loss 1.0157276391983032 Accuracy 97.41095245567237\n",
      "Training:: Epoch 56, Iteration 30, Current loss 0.7384694218635559 Accuracy 98.07503822480103\n",
      "Training:: Epoch 56, Iteration 40, Current loss 1.2155324220657349 Accuracy 98.22789634146342\n",
      "Training:: Epoch 56, Iteration 50, Current loss 0.8828153610229492 Accuracy 98.25777559658917\n",
      "Training:: Epoch 56, Iteration 60, Current loss 0.8386975526809692 Accuracy 97.93124429571037\n",
      "Training:: Epoch 56, Iteration 70, Current loss 0.5227811336517334 Accuracy 98.60251686018216\n",
      "Training:: Epoch 56, Iteration 80, Current loss 0.7563794851303101 Accuracy 98.09635722679201\n",
      "Training:: Epoch 56, Iteration 90, Current loss 0.6845704317092896 Accuracy 98.05005213764338\n",
      "Training:: Epoch 56, Iteration 100, Current loss 0.8779124617576599 Accuracy 98.09104258443466\n",
      "Training:: Epoch 56, Iteration 110, Current loss 0.5651533603668213 Accuracy 98.4770461268763\n",
      "Training:: Epoch 56, Iteration 120, Current loss 1.3984947204589844 Accuracy 93.47037484885126\n",
      "Training:: Epoch 56, Iteration 130, Current loss 0.7928593754768372 Accuracy 97.4236696714161\n",
      "Training:: Epoch 56, Iteration 140, Current loss 1.8460719585418701 Accuracy 86.03024739924818\n",
      "Training:: Epoch 56, Iteration 150, Current loss 0.9203400015830994 Accuracy 96.32287831476907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 56, Probability Accuracy 69.7128922271639\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 1.2152808904647827 Accuracy 94.90238611713666\n",
      "Training:: Epoch 57, Iteration 10, Current loss 0.7685920000076294 Accuracy 97.1987835696235\n",
      "Training:: Epoch 57, Iteration 20, Current loss 1.1178478002548218 Accuracy 93.19371727748691\n",
      "Training:: Epoch 57, Iteration 30, Current loss 1.6744242906570435 Accuracy 92.23039469594944\n",
      "Training:: Epoch 57, Iteration 40, Current loss 0.9935207962989807 Accuracy 96.92627556754327\n",
      "Training:: Epoch 57, Iteration 50, Current loss 0.8486995697021484 Accuracy 97.72788931090614\n",
      "Training:: Epoch 57, Iteration 60, Current loss 0.9485663175582886 Accuracy 96.14267711358282\n",
      "Training:: Epoch 57, Iteration 70, Current loss 0.8155182600021362 Accuracy 97.6839451709469\n",
      "Training:: Epoch 57, Iteration 80, Current loss 1.1151256561279297 Accuracy 96.22493118364137\n",
      "Training:: Epoch 57, Iteration 90, Current loss 0.9716817140579224 Accuracy 97.19954607021269\n",
      "Training:: Epoch 57, Iteration 100, Current loss 0.970789909362793 Accuracy 97.97868592566496\n",
      "Training:: Epoch 57, Iteration 110, Current loss 1.1712732315063477 Accuracy 97.00327766505384\n",
      "Training:: Epoch 57, Iteration 120, Current loss 0.8061994314193726 Accuracy 98.44061437448704\n",
      "Training:: Epoch 57, Iteration 130, Current loss 0.6453351378440857 Accuracy 98.86370224360833\n",
      "Training:: Epoch 57, Iteration 140, Current loss 0.9226903319358826 Accuracy 97.31734732109405\n",
      "Training:: Epoch 57, Iteration 150, Current loss 0.7908036112785339 Accuracy 98.1286700651739\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 57, Probability Accuracy 72.24256561068297\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 0.6371649503707886 Accuracy 98.3872373812222\n",
      "Training:: Epoch 58, Iteration 10, Current loss 0.767573356628418 Accuracy 97.96429689946758\n",
      "Training:: Epoch 58, Iteration 20, Current loss 0.7393279075622559 Accuracy 98.4318118174181\n",
      "Training:: Epoch 58, Iteration 30, Current loss 0.8373658657073975 Accuracy 98.10030395136778\n",
      "Training:: Epoch 58, Iteration 40, Current loss 0.7330389618873596 Accuracy 97.77707208637663\n",
      "Training:: Epoch 58, Iteration 50, Current loss 0.574120283126831 Accuracy 98.64077669902913\n",
      "Training:: Epoch 58, Iteration 60, Current loss 0.5827488899230957 Accuracy 98.56115107913669\n",
      "Training:: Epoch 58, Iteration 70, Current loss 0.8080071210861206 Accuracy 98.7064083457526\n",
      "Training:: Epoch 58, Iteration 80, Current loss 0.7030881643295288 Accuracy 98.55862403100775\n",
      "Training:: Epoch 58, Iteration 90, Current loss 0.5343971252441406 Accuracy 99.11731499538129\n",
      "Training:: Epoch 58, Iteration 100, Current loss 0.8920491337776184 Accuracy 97.47161418705373\n",
      "Training:: Epoch 58, Iteration 110, Current loss 0.5588001608848572 Accuracy 98.63632695876704\n",
      "Training:: Epoch 58, Iteration 120, Current loss 0.5937768816947937 Accuracy 98.72368367053802\n",
      "Training:: Epoch 58, Iteration 130, Current loss 0.5947374105453491 Accuracy 98.28629441624365\n",
      "Training:: Epoch 58, Iteration 140, Current loss 0.8176050782203674 Accuracy 98.41819416157502\n",
      "Training:: Epoch 58, Iteration 150, Current loss 0.5720511674880981 Accuracy 98.9328451509198\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 58, Probability Accuracy 71.79390740007031\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 0.7691983580589294 Accuracy 98.54127453616563\n",
      "Training:: Epoch 59, Iteration 10, Current loss 0.6242991089820862 Accuracy 98.67422590492804\n",
      "Training:: Epoch 59, Iteration 20, Current loss 0.6945710778236389 Accuracy 98.71881321645314\n",
      "Training:: Epoch 59, Iteration 30, Current loss 0.8283856511116028 Accuracy 97.78453857555192\n",
      "Training:: Epoch 59, Iteration 40, Current loss 0.6149477362632751 Accuracy 98.72921681688406\n",
      "Training:: Epoch 59, Iteration 50, Current loss 0.661332368850708 Accuracy 98.62878963202962\n",
      "Training:: Epoch 59, Iteration 60, Current loss 0.7078596353530884 Accuracy 98.45472734238645\n",
      "Training:: Epoch 59, Iteration 70, Current loss 0.729590892791748 Accuracy 98.06761035574596\n",
      "Training:: Epoch 59, Iteration 80, Current loss 0.652681291103363 Accuracy 98.33488687399372\n",
      "Training:: Epoch 59, Iteration 90, Current loss 4.359508037567139 Accuracy 76.92307692307692\n",
      "Training:: Epoch 59, Iteration 100, Current loss 2.029421329498291 Accuracy 86.30434782608695\n",
      "Training:: Epoch 59, Iteration 110, Current loss 3.538455009460449 Accuracy 79.97696945065366\n",
      "Training:: Epoch 59, Iteration 120, Current loss 3.4871938228607178 Accuracy 70.59655371078405\n",
      "Training:: Epoch 59, Iteration 130, Current loss 3.1110055446624756 Accuracy 76.66027108993006\n",
      "Training:: Epoch 59, Iteration 140, Current loss 5.605765342712402 Accuracy 66.6921808761187\n",
      "Training:: Epoch 59, Iteration 150, Current loss 5.048735618591309 Accuracy 71.34578235672892\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 59, Probability Accuracy 53.455058844537696\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 3.999479055404663 Accuracy 72.4967440435149\n",
      "Training:: Epoch 60, Iteration 10, Current loss 4.297577381134033 Accuracy 71.54159909577866\n",
      "Training:: Epoch 60, Iteration 20, Current loss 4.447627067565918 Accuracy 68.80187809172466\n",
      "Training:: Epoch 60, Iteration 30, Current loss 4.333395957946777 Accuracy 65.68417444949019\n",
      "Training:: Epoch 60, Iteration 40, Current loss 2.3418140411376953 Accuracy 91.84785540868627\n",
      "Training:: Epoch 60, Iteration 50, Current loss 3.37724232673645 Accuracy 77.61786923678041\n",
      "Training:: Epoch 60, Iteration 60, Current loss 2.9665801525115967 Accuracy 83.60655737704919\n",
      "Training:: Epoch 60, Iteration 70, Current loss 3.0579164028167725 Accuracy 83.39569691300281\n",
      "Training:: Epoch 60, Iteration 80, Current loss 2.826669216156006 Accuracy 80.2179904785768\n",
      "Training:: Epoch 60, Iteration 90, Current loss 1.6329420804977417 Accuracy 94.74860944486858\n",
      "Training:: Epoch 60, Iteration 100, Current loss 1.728737473487854 Accuracy 92.94043672116155\n",
      "Training:: Epoch 60, Iteration 110, Current loss 2.161219835281372 Accuracy 90.4799703656665\n",
      "Training:: Epoch 60, Iteration 120, Current loss 1.6615999937057495 Accuracy 94.45265895069596\n",
      "Training:: Epoch 60, Iteration 130, Current loss 1.5456936359405518 Accuracy 95.64782845720516\n",
      "Training:: Epoch 60, Iteration 140, Current loss 2.306626558303833 Accuracy 88.18403077647798\n",
      "Training:: Epoch 60, Iteration 150, Current loss 1.461194634437561 Accuracy 94.20222201475119\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 71.48576179820425\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 0.9549258947372437 Accuracy 97.07594422634358\n",
      "Training:: Epoch 61, Iteration 10, Current loss 1.2150042057037354 Accuracy 96.77267531768977\n",
      "Training:: Epoch 61, Iteration 20, Current loss 1.161158561706543 Accuracy 97.01607915893631\n",
      "Training:: Epoch 61, Iteration 30, Current loss 0.9706629514694214 Accuracy 97.40591474144293\n",
      "Training:: Epoch 61, Iteration 40, Current loss 0.9573561549186707 Accuracy 96.69546763680461\n",
      "Training:: Epoch 61, Iteration 50, Current loss 1.035875678062439 Accuracy 97.85296031229669\n",
      "Training:: Epoch 61, Iteration 60, Current loss 1.1403005123138428 Accuracy 96.76050507141379\n",
      "Training:: Epoch 61, Iteration 70, Current loss 1.0656652450561523 Accuracy 97.00366094058012\n",
      "Training:: Epoch 61, Iteration 80, Current loss 0.8187748789787292 Accuracy 97.35176939301617\n",
      "Training:: Epoch 61, Iteration 90, Current loss 0.9753609299659729 Accuracy 97.27021585428665\n",
      "Training:: Epoch 61, Iteration 100, Current loss 1.293864369392395 Accuracy 93.97942056483981\n",
      "Training:: Epoch 61, Iteration 110, Current loss 1.5614769458770752 Accuracy 96.38479441427464\n",
      "Training:: Epoch 61, Iteration 120, Current loss 1.1622858047485352 Accuracy 97.54292480757844\n",
      "Training:: Epoch 61, Iteration 130, Current loss 1.1452202796936035 Accuracy 96.1608641704985\n",
      "Training:: Epoch 61, Iteration 140, Current loss 1.4164462089538574 Accuracy 94.01109057301294\n",
      "Training:: Epoch 61, Iteration 150, Current loss 1.0926532745361328 Accuracy 95.77223669519911\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 61, Probability Accuracy 71.74625141600772\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 62, Iteration 0, Current loss 0.8086600303649902 Accuracy 97.94394024576339\n",
      "Training:: Epoch 62, Iteration 10, Current loss 1.088794231414795 Accuracy 97.65461763476807\n",
      "Training:: Epoch 62, Iteration 20, Current loss 0.8428149223327637 Accuracy 97.53246107158847\n",
      "Training:: Epoch 62, Iteration 30, Current loss 1.3369957208633423 Accuracy 96.30344670944297\n",
      "Training:: Epoch 62, Iteration 40, Current loss 0.9702346324920654 Accuracy 97.04998844642995\n",
      "Training:: Epoch 62, Iteration 50, Current loss 0.9663493633270264 Accuracy 96.81089467333219\n",
      "Training:: Epoch 62, Iteration 60, Current loss 0.675694465637207 Accuracy 97.99392415671485\n",
      "Training:: Epoch 62, Iteration 70, Current loss 0.8772801160812378 Accuracy 98.05573558003888\n",
      "Training:: Epoch 62, Iteration 80, Current loss 0.757576048374176 Accuracy 98.132063136266\n",
      "Training:: Epoch 62, Iteration 90, Current loss 1.0137752294540405 Accuracy 97.43091095189355\n",
      "Training:: Epoch 62, Iteration 100, Current loss 0.8286186456680298 Accuracy 97.87324845294829\n",
      "Training:: Epoch 62, Iteration 110, Current loss 0.9891708493232727 Accuracy 98.4652862362972\n",
      "Training:: Epoch 62, Iteration 120, Current loss 0.914391040802002 Accuracy 98.24317855433222\n",
      "Training:: Epoch 62, Iteration 130, Current loss 1.177648663520813 Accuracy 95.97754191662821\n",
      "Training:: Epoch 62, Iteration 140, Current loss 0.7795110940933228 Accuracy 98.03099149481534\n",
      "Training:: Epoch 62, Iteration 150, Current loss 0.9695104956626892 Accuracy 98.08830223031407\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 62, Probability Accuracy 72.24423970848378\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 0.6963263750076294 Accuracy 98.40293040293041\n",
      "Training:: Epoch 63, Iteration 10, Current loss 0.6906203031539917 Accuracy 98.71684092283763\n",
      "Training:: Epoch 63, Iteration 20, Current loss 0.7299342155456543 Accuracy 99.03064992614476\n",
      "Training:: Epoch 63, Iteration 30, Current loss 0.9285789728164673 Accuracy 98.3604372167422\n",
      "Training:: Epoch 63, Iteration 40, Current loss 0.7556843757629395 Accuracy 98.6254476511481\n",
      "Training:: Epoch 63, Iteration 50, Current loss 0.8718992471694946 Accuracy 97.84177098106925\n",
      "Training:: Epoch 63, Iteration 60, Current loss 0.6329654455184937 Accuracy 98.40051558471994\n",
      "Training:: Epoch 63, Iteration 70, Current loss 0.7028009295463562 Accuracy 98.82385774659119\n",
      "Training:: Epoch 63, Iteration 80, Current loss 0.9556823372840881 Accuracy 97.91987314024533\n",
      "Training:: Epoch 63, Iteration 90, Current loss 0.7518443465232849 Accuracy 98.58064516129032\n",
      "Training:: Epoch 63, Iteration 100, Current loss 0.6932509541511536 Accuracy 98.98954854937335\n",
      "Training:: Epoch 63, Iteration 110, Current loss 1.048836588859558 Accuracy 97.49171111431454\n",
      "Training:: Epoch 63, Iteration 120, Current loss 0.7451971173286438 Accuracy 98.63303943681225\n",
      "Training:: Epoch 63, Iteration 130, Current loss 0.6976120471954346 Accuracy 98.52195423623995\n",
      "Training:: Epoch 63, Iteration 140, Current loss 1.1033971309661865 Accuracy 98.42638125542378\n",
      "Training:: Epoch 63, Iteration 150, Current loss 0.7394371628761292 Accuracy 98.50523168908819\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 63, Probability Accuracy 72.87191477726128\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 0.7912800908088684 Accuracy 97.74492234169654\n",
      "Training:: Epoch 64, Iteration 10, Current loss 0.9067817330360413 Accuracy 98.02980132450331\n",
      "Training:: Epoch 64, Iteration 20, Current loss 0.9843839406967163 Accuracy 98.00991691235593\n",
      "Training:: Epoch 64, Iteration 30, Current loss 0.8002973794937134 Accuracy 97.96135436979259\n",
      "Training:: Epoch 64, Iteration 40, Current loss 0.9261622428894043 Accuracy 98.09841664079478\n",
      "Training:: Epoch 64, Iteration 50, Current loss 0.7020190954208374 Accuracy 98.56012979111742\n",
      "Training:: Epoch 64, Iteration 60, Current loss 0.7212056517601013 Accuracy 98.43202668890743\n",
      "Training:: Epoch 64, Iteration 70, Current loss 0.8159306049346924 Accuracy 97.18890554722638\n",
      "Training:: Epoch 64, Iteration 80, Current loss 0.8693433403968811 Accuracy 98.42983758491924\n",
      "Training:: Epoch 64, Iteration 90, Current loss 0.683405876159668 Accuracy 98.69290941206157\n",
      "Training:: Epoch 64, Iteration 100, Current loss 0.8005030155181885 Accuracy 98.54963519787752\n",
      "Training:: Epoch 64, Iteration 110, Current loss 1.1058380603790283 Accuracy 96.89484280625678\n",
      "Training:: Epoch 64, Iteration 120, Current loss 1.0098741054534912 Accuracy 97.55932857418483\n",
      "Training:: Epoch 64, Iteration 130, Current loss 0.9007242918014526 Accuracy 97.69336026585\n",
      "Training:: Epoch 64, Iteration 140, Current loss 0.9037787318229675 Accuracy 98.26231163025453\n",
      "Training:: Epoch 64, Iteration 150, Current loss 0.9220801591873169 Accuracy 97.5\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 64, Probability Accuracy 73.35561743517056\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 0.9001138210296631 Accuracy 98.41041366793917\n",
      "Training:: Epoch 65, Iteration 10, Current loss 0.9189758896827698 Accuracy 98.65763621941547\n",
      "Training:: Epoch 65, Iteration 20, Current loss 0.8247843384742737 Accuracy 98.40963855421687\n",
      "Training:: Epoch 65, Iteration 30, Current loss 0.7286931276321411 Accuracy 98.4959205391983\n",
      "Training:: Epoch 65, Iteration 40, Current loss 0.6621919274330139 Accuracy 98.49494201825809\n",
      "Training:: Epoch 65, Iteration 50, Current loss 1.0283347368240356 Accuracy 96.58905704307334\n",
      "Training:: Epoch 65, Iteration 60, Current loss 0.8273664712905884 Accuracy 98.0975780975781\n",
      "Training:: Epoch 65, Iteration 70, Current loss 0.8478755354881287 Accuracy 98.79028106661175\n",
      "Training:: Epoch 65, Iteration 80, Current loss 0.7426722049713135 Accuracy 98.78747501665556\n",
      "Training:: Epoch 65, Iteration 90, Current loss 0.527326762676239 Accuracy 98.91185277148908\n",
      "Training:: Epoch 65, Iteration 100, Current loss 0.7163475751876831 Accuracy 99.06398985260027\n",
      "Training:: Epoch 65, Iteration 110, Current loss 0.7569462060928345 Accuracy 98.42632331902718\n",
      "Training:: Epoch 65, Iteration 120, Current loss 0.6619305610656738 Accuracy 98.91461436366318\n",
      "Training:: Epoch 65, Iteration 130, Current loss 0.7661318778991699 Accuracy 98.0502701432934\n",
      "Training:: Epoch 65, Iteration 140, Current loss 0.7314115166664124 Accuracy 98.47638395124429\n",
      "Training:: Epoch 65, Iteration 150, Current loss 0.6027883887290955 Accuracy 98.00490905658002\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 72.74992885084346\n",
      "Starting Training\n",
      "Training:: Epoch 66, Iteration 0, Current loss 0.6175267100334167 Accuracy 98.79223680536163\n",
      "Training:: Epoch 66, Iteration 10, Current loss 0.9028529524803162 Accuracy 97.73465876505448\n",
      "Training:: Epoch 66, Iteration 20, Current loss 0.8935363292694092 Accuracy 98.57208659603869\n",
      "Training:: Epoch 66, Iteration 30, Current loss 0.7230975031852722 Accuracy 98.81984709323208\n",
      "Training:: Epoch 66, Iteration 40, Current loss 0.6092514395713806 Accuracy 99.10413030831879\n",
      "Training:: Epoch 66, Iteration 50, Current loss 0.718949019908905 Accuracy 99.12088956472154\n",
      "Training:: Epoch 66, Iteration 60, Current loss 0.7867523431777954 Accuracy 98.71892528218268\n",
      "Training:: Epoch 66, Iteration 70, Current loss 0.7959372997283936 Accuracy 98.93283841781695\n",
      "Training:: Epoch 66, Iteration 80, Current loss 0.8310713171958923 Accuracy 98.89496677557914\n",
      "Training:: Epoch 66, Iteration 90, Current loss 0.64570552110672 Accuracy 98.64060548166654\n",
      "Training:: Epoch 66, Iteration 100, Current loss 0.5677530765533447 Accuracy 99.13394466650992\n",
      "Training:: Epoch 66, Iteration 110, Current loss 0.5524645447731018 Accuracy 99.01656314699792\n",
      "Training:: Epoch 66, Iteration 120, Current loss 0.7543991804122925 Accuracy 98.75108079546547\n",
      "Training:: Epoch 66, Iteration 130, Current loss 1.0229806900024414 Accuracy 98.08988764044943\n",
      "Training:: Epoch 66, Iteration 140, Current loss 0.8593746423721313 Accuracy 98.74906845523262\n",
      "Training:: Epoch 66, Iteration 150, Current loss 0.7442626953125 Accuracy 98.67060561299853\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 66, Probability Accuracy 72.14245456219552\n",
      "Starting Training\n",
      "Training:: Epoch 67, Iteration 0, Current loss 0.7599610686302185 Accuracy 98.90671951921233\n",
      "Training:: Epoch 67, Iteration 10, Current loss 0.6593462228775024 Accuracy 98.63844830880947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 67, Iteration 20, Current loss 0.7774235010147095 Accuracy 98.15136406923982\n",
      "Training:: Epoch 67, Iteration 30, Current loss 0.7135730385780334 Accuracy 98.90428026971563\n",
      "Training:: Epoch 67, Iteration 40, Current loss 0.7905089855194092 Accuracy 98.40038778477945\n",
      "Training:: Epoch 67, Iteration 50, Current loss 0.6289545297622681 Accuracy 99.19945225680729\n",
      "Training:: Epoch 67, Iteration 60, Current loss 0.7276728749275208 Accuracy 98.84545367442752\n",
      "Training:: Epoch 67, Iteration 70, Current loss 0.5575238466262817 Accuracy 98.79452054794521\n",
      "Training:: Epoch 67, Iteration 80, Current loss 0.6413687467575073 Accuracy 99.10041605757337\n",
      "Training:: Epoch 67, Iteration 90, Current loss 0.8014662861824036 Accuracy 98.7159036528952\n",
      "Training:: Epoch 67, Iteration 100, Current loss 0.5618925094604492 Accuracy 98.85142951578251\n",
      "Training:: Epoch 67, Iteration 110, Current loss 0.47065216302871704 Accuracy 99.14260165167535\n",
      "Training:: Epoch 67, Iteration 120, Current loss 0.588376522064209 Accuracy 98.98344243357721\n",
      "Training:: Epoch 67, Iteration 130, Current loss 0.676124095916748 Accuracy 98.97051195083995\n",
      "Training:: Epoch 67, Iteration 140, Current loss 0.7724876403808594 Accuracy 98.47639912368054\n",
      "Training:: Epoch 67, Iteration 150, Current loss 0.6542431712150574 Accuracy 98.33153928955866\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 67, Probability Accuracy 72.30941791619466\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 0.674239456653595 Accuracy 98.80312818769126\n",
      "Training:: Epoch 68, Iteration 10, Current loss 0.5745327472686768 Accuracy 98.93966913749938\n",
      "Training:: Epoch 68, Iteration 20, Current loss 0.8315053582191467 Accuracy 98.67687630201002\n",
      "Training:: Epoch 68, Iteration 30, Current loss 0.6668652892112732 Accuracy 98.66948422947483\n",
      "Training:: Epoch 68, Iteration 40, Current loss 0.5625665783882141 Accuracy 98.20694019618183\n",
      "Training:: Epoch 68, Iteration 50, Current loss 0.5784211754798889 Accuracy 98.68386417478284\n",
      "Training:: Epoch 68, Iteration 60, Current loss 0.7387188076972961 Accuracy 98.90335846470185\n",
      "Training:: Epoch 68, Iteration 70, Current loss 0.7642484903335571 Accuracy 98.57442064725356\n",
      "Training:: Epoch 68, Iteration 80, Current loss 0.8548182845115662 Accuracy 97.56213769554515\n",
      "Training:: Epoch 68, Iteration 90, Current loss 0.6861769556999207 Accuracy 98.69799314733235\n",
      "Training:: Epoch 68, Iteration 100, Current loss 0.518508791923523 Accuracy 99.12972531955398\n",
      "Training:: Epoch 68, Iteration 110, Current loss 0.7129400968551636 Accuracy 97.86472234880542\n",
      "Training:: Epoch 68, Iteration 120, Current loss 0.7837551236152649 Accuracy 98.12601357438885\n",
      "Training:: Epoch 68, Iteration 130, Current loss 0.7376678586006165 Accuracy 98.80911641913627\n",
      "Training:: Epoch 68, Iteration 140, Current loss 0.5980558395385742 Accuracy 98.56374456397344\n",
      "Training:: Epoch 68, Iteration 150, Current loss 0.8119775056838989 Accuracy 97.96180063440643\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 68, Probability Accuracy 72.22705230439563\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 0.7234464287757874 Accuracy 99.03518728717367\n",
      "Training:: Epoch 69, Iteration 10, Current loss 0.7872270345687866 Accuracy 98.71646559797279\n",
      "Training:: Epoch 69, Iteration 20, Current loss 0.7036985158920288 Accuracy 98.99424121988807\n",
      "Training:: Epoch 69, Iteration 30, Current loss 0.7349151372909546 Accuracy 98.24456413325355\n",
      "Training:: Epoch 69, Iteration 40, Current loss 0.6593014001846313 Accuracy 98.87872849989114\n",
      "Training:: Epoch 69, Iteration 50, Current loss 0.659423291683197 Accuracy 98.84483974185152\n",
      "Training:: Epoch 69, Iteration 60, Current loss 0.5962700843811035 Accuracy 98.67119532832797\n",
      "Training:: Epoch 69, Iteration 70, Current loss 0.7967519760131836 Accuracy 98.46343206543318\n",
      "Training:: Epoch 69, Iteration 80, Current loss 0.6014024019241333 Accuracy 99.02795352582538\n",
      "Training:: Epoch 69, Iteration 90, Current loss 0.6300872564315796 Accuracy 98.93577296489917\n",
      "Training:: Epoch 69, Iteration 100, Current loss 0.6939923763275146 Accuracy 98.81059921710329\n",
      "Training:: Epoch 69, Iteration 110, Current loss 0.7115814089775085 Accuracy 98.92962269199893\n",
      "Training:: Epoch 69, Iteration 120, Current loss 0.632067859172821 Accuracy 98.77765108323831\n",
      "Training:: Epoch 69, Iteration 130, Current loss 0.7453745007514954 Accuracy 99.12135489621801\n",
      "Training:: Epoch 69, Iteration 140, Current loss 0.7183818221092224 Accuracy 98.33291863647673\n",
      "Training:: Epoch 69, Iteration 150, Current loss 0.7859708070755005 Accuracy 98.23573081793965\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 69, Probability Accuracy 71.22817394992215\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 0.7927514314651489 Accuracy 98.18440726237095\n",
      "Training:: Epoch 70, Iteration 10, Current loss 0.6657719612121582 Accuracy 99.01926848967348\n",
      "Training:: Epoch 70, Iteration 20, Current loss 0.6632355451583862 Accuracy 99.13786526283666\n",
      "Training:: Epoch 70, Iteration 30, Current loss 0.6168793439865112 Accuracy 99.0709794369229\n",
      "Training:: Epoch 70, Iteration 40, Current loss 0.6411334872245789 Accuracy 98.84753766584214\n",
      "Training:: Epoch 70, Iteration 50, Current loss 0.6345459222793579 Accuracy 98.89149874314913\n",
      "Training:: Epoch 70, Iteration 60, Current loss 0.8430614471435547 Accuracy 97.88761203466102\n",
      "Training:: Epoch 70, Iteration 70, Current loss 0.5535198450088501 Accuracy 98.6627043090639\n",
      "Training:: Epoch 70, Iteration 80, Current loss 0.6548076868057251 Accuracy 98.36639439906651\n",
      "Training:: Epoch 70, Iteration 90, Current loss 0.5752705335617065 Accuracy 97.73989657153803\n",
      "Training:: Epoch 70, Iteration 100, Current loss 0.7731242775917053 Accuracy 98.31560283687944\n",
      "Training:: Epoch 70, Iteration 110, Current loss 0.6948546767234802 Accuracy 98.21075802901169\n",
      "Training:: Epoch 70, Iteration 120, Current loss 0.8678820133209229 Accuracy 97.70036039128196\n",
      "Training:: Epoch 70, Iteration 130, Current loss 0.6866320371627808 Accuracy 98.574470859229\n",
      "Training:: Epoch 70, Iteration 140, Current loss 0.5185098648071289 Accuracy 99.13697837183602\n",
      "Training:: Epoch 70, Iteration 150, Current loss 0.9399712681770325 Accuracy 98.3849259757739\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 72.35417213073588\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 0.6000750064849854 Accuracy 98.57039011388417\n",
      "Training:: Epoch 71, Iteration 10, Current loss 0.8574165105819702 Accuracy 98.54695293862503\n",
      "Training:: Epoch 71, Iteration 20, Current loss 0.44588467478752136 Accuracy 99.12976426525667\n",
      "Training:: Epoch 71, Iteration 30, Current loss 0.6604130864143372 Accuracy 98.81798138167565\n",
      "Training:: Epoch 71, Iteration 40, Current loss 0.4011615216732025 Accuracy 99.39429237041351\n",
      "Training:: Epoch 71, Iteration 50, Current loss 0.6302083134651184 Accuracy 98.49624060150376\n",
      "Training:: Epoch 71, Iteration 60, Current loss 0.7098965644836426 Accuracy 98.83588221867154\n",
      "Training:: Epoch 71, Iteration 70, Current loss 0.680451512336731 Accuracy 97.82879005260641\n",
      "Training:: Epoch 71, Iteration 80, Current loss 0.5625159740447998 Accuracy 98.6843290979544\n",
      "Training:: Epoch 71, Iteration 90, Current loss 0.572555422782898 Accuracy 98.5988241112149\n",
      "Training:: Epoch 71, Iteration 100, Current loss 0.6191071271896362 Accuracy 98.9136649514008\n",
      "Training:: Epoch 71, Iteration 110, Current loss 0.6931459307670593 Accuracy 98.74958868048701\n",
      "Training:: Epoch 71, Iteration 120, Current loss 0.5859429836273193 Accuracy 97.99416701386822\n",
      "Training:: Epoch 71, Iteration 130, Current loss 0.6894798278808594 Accuracy 98.55922658156561\n",
      "Training:: Epoch 71, Iteration 140, Current loss 0.7825773358345032 Accuracy 98.9211238610804\n",
      "Training:: Epoch 71, Iteration 150, Current loss 0.6947999596595764 Accuracy 98.39580135663712\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 71, Probability Accuracy 71.51533752601827\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 0.6005997657775879 Accuracy 99.20612009237875\n",
      "Training:: Epoch 72, Iteration 10, Current loss 0.6530302166938782 Accuracy 98.96835717381882\n",
      "Training:: Epoch 72, Iteration 20, Current loss 0.6346402764320374 Accuracy 98.45317455180894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 72, Iteration 30, Current loss 0.6824126243591309 Accuracy 98.35782979495555\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.6827861666679382 Accuracy 98.6171132238548\n",
      "Training:: Epoch 72, Iteration 50, Current loss 0.625305712223053 Accuracy 98.6740550168217\n",
      "Training:: Epoch 72, Iteration 60, Current loss 0.6996022462844849 Accuracy 98.97143828629717\n",
      "Training:: Epoch 72, Iteration 70, Current loss 0.8036798238754272 Accuracy 98.96781534460338\n",
      "Training:: Epoch 72, Iteration 80, Current loss 0.5915366411209106 Accuracy 98.92494212735905\n",
      "Training:: Epoch 72, Iteration 90, Current loss 0.6410714983940125 Accuracy 98.45451519330449\n",
      "Training:: Epoch 72, Iteration 100, Current loss 0.6835073828697205 Accuracy 98.48214285714286\n",
      "Training:: Epoch 72, Iteration 110, Current loss 0.7780600786209106 Accuracy 98.33913141061609\n",
      "Training:: Epoch 72, Iteration 120, Current loss 0.8105496168136597 Accuracy 98.3965917893106\n",
      "Training:: Epoch 72, Iteration 130, Current loss 0.5709954500198364 Accuracy 98.77224376180534\n",
      "Training:: Epoch 72, Iteration 140, Current loss 0.6663652062416077 Accuracy 98.43019372077488\n",
      "Training:: Epoch 72, Iteration 150, Current loss 0.6870795488357544 Accuracy 98.18660219030009\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 72, Probability Accuracy 72.61845637022115\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 0.6328549385070801 Accuracy 98.83032888399615\n",
      "Training:: Epoch 73, Iteration 10, Current loss 0.488113671541214 Accuracy 99.27700137329349\n",
      "Training:: Epoch 73, Iteration 20, Current loss 0.6996229887008667 Accuracy 98.36763662171754\n",
      "Training:: Epoch 73, Iteration 30, Current loss 0.6062064170837402 Accuracy 98.49004804392588\n",
      "Training:: Epoch 73, Iteration 40, Current loss 0.6343170404434204 Accuracy 98.08654695319399\n",
      "Training:: Epoch 73, Iteration 50, Current loss 0.6546075940132141 Accuracy 98.4003219477841\n",
      "Training:: Epoch 73, Iteration 60, Current loss 0.5784722566604614 Accuracy 98.42067480258434\n",
      "Training:: Epoch 73, Iteration 70, Current loss 0.6937159299850464 Accuracy 98.51161269218188\n",
      "Training:: Epoch 73, Iteration 80, Current loss 0.581599771976471 Accuracy 99.08965762245215\n",
      "Training:: Epoch 73, Iteration 90, Current loss 0.6335524320602417 Accuracy 98.8714136790101\n",
      "Training:: Epoch 73, Iteration 100, Current loss 0.9276118874549866 Accuracy 98.57123231213079\n",
      "Training:: Epoch 73, Iteration 110, Current loss 0.8480242490768433 Accuracy 98.0985598559856\n",
      "Training:: Epoch 73, Iteration 120, Current loss 0.6050682067871094 Accuracy 98.32274773385953\n",
      "Training:: Epoch 73, Iteration 130, Current loss 0.590828001499176 Accuracy 98.22124512841012\n",
      "Training:: Epoch 73, Iteration 140, Current loss 0.7080935835838318 Accuracy 98.39841755997958\n",
      "Training:: Epoch 73, Iteration 150, Current loss 0.5939383506774902 Accuracy 98.84827463536108\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 73, Probability Accuracy 70.51969576062633\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 0.5377784967422485 Accuracy 99.05367440484991\n",
      "Training:: Epoch 74, Iteration 10, Current loss 0.6621775031089783 Accuracy 98.51007320075144\n",
      "Training:: Epoch 74, Iteration 20, Current loss 0.5038505792617798 Accuracy 99.024997406908\n",
      "Training:: Epoch 74, Iteration 30, Current loss 0.46276864409446716 Accuracy 99.14965145597469\n",
      "Training:: Epoch 74, Iteration 40, Current loss 0.7000406384468079 Accuracy 98.77847360814809\n",
      "Training:: Epoch 74, Iteration 50, Current loss 0.7279082536697388 Accuracy 98.59357060849598\n",
      "Training:: Epoch 74, Iteration 60, Current loss 0.5518887042999268 Accuracy 99.28204083710983\n",
      "Training:: Epoch 74, Iteration 70, Current loss 0.5849494934082031 Accuracy 98.6482973745776\n",
      "Training:: Epoch 74, Iteration 80, Current loss 0.5759096145629883 Accuracy 98.67409684857802\n",
      "Training:: Epoch 74, Iteration 90, Current loss 0.5582400560379028 Accuracy 99.07646841521981\n",
      "Training:: Epoch 74, Iteration 100, Current loss 0.6022642254829407 Accuracy 98.5163589245222\n",
      "Training:: Epoch 74, Iteration 110, Current loss 0.6541168689727783 Accuracy 98.8986684721282\n",
      "Training:: Epoch 74, Iteration 120, Current loss 0.4687961935997009 Accuracy 98.97115561271357\n",
      "Training:: Epoch 74, Iteration 130, Current loss 0.5842450857162476 Accuracy 98.97601601966049\n",
      "Training:: Epoch 74, Iteration 140, Current loss 0.5755512118339539 Accuracy 98.79800468778171\n",
      "Training:: Epoch 74, Iteration 150, Current loss 0.5447357296943665 Accuracy 98.39940901255848\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 74, Probability Accuracy 71.15830826836904\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 0.5226144790649414 Accuracy 99.25082977714557\n",
      "Training:: Epoch 75, Iteration 10, Current loss 0.6929256916046143 Accuracy 98.67874930824571\n",
      "Training:: Epoch 75, Iteration 20, Current loss 0.6238189935684204 Accuracy 98.5244934094039\n",
      "Training:: Epoch 75, Iteration 30, Current loss 0.6687477827072144 Accuracy 99.04712178566767\n",
      "Training:: Epoch 75, Iteration 40, Current loss 0.6468822956085205 Accuracy 98.30776173285199\n",
      "Training:: Epoch 75, Iteration 50, Current loss 0.6461950540542603 Accuracy 98.31362667183562\n",
      "Training:: Epoch 75, Iteration 60, Current loss 0.9923756122589111 Accuracy 97.89906103286386\n",
      "Training:: Epoch 75, Iteration 70, Current loss 0.6021921634674072 Accuracy 98.32651718140366\n",
      "Training:: Epoch 75, Iteration 80, Current loss 0.6092607378959656 Accuracy 98.785657828555\n",
      "Training:: Epoch 75, Iteration 90, Current loss 0.708074152469635 Accuracy 98.04188991719435\n",
      "Training:: Epoch 75, Iteration 100, Current loss 0.7710145115852356 Accuracy 97.05285449572047\n",
      "Training:: Epoch 75, Iteration 110, Current loss 0.5626773834228516 Accuracy 98.92098195354097\n",
      "Training:: Epoch 75, Iteration 120, Current loss 0.679594099521637 Accuracy 97.92650279873449\n",
      "Training:: Epoch 75, Iteration 130, Current loss 0.6183083653450012 Accuracy 98.34777729284868\n",
      "Training:: Epoch 75, Iteration 140, Current loss 0.8231362700462341 Accuracy 97.32792152883476\n",
      "Training:: Epoch 75, Iteration 150, Current loss 0.8005006313323975 Accuracy 98.43794647109249\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 70.79212727607546\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 0.6299451589584351 Accuracy 98.34710743801652\n",
      "Training:: Epoch 76, Iteration 10, Current loss 0.8548964858055115 Accuracy 97.22672257695606\n",
      "Training:: Epoch 76, Iteration 20, Current loss 1.007570743560791 Accuracy 95.74010747435271\n",
      "Training:: Epoch 76, Iteration 30, Current loss 2.7893614768981934 Accuracy 87.63543052651588\n",
      "Training:: Epoch 76, Iteration 40, Current loss 9.197641372680664 Accuracy 52.36237776068564\n",
      "Training:: Epoch 76, Iteration 50, Current loss 5.137594699859619 Accuracy 53.40479583617324\n",
      "Training:: Epoch 76, Iteration 60, Current loss 4.892210483551025 Accuracy 59.38043688381983\n",
      "Training:: Epoch 76, Iteration 70, Current loss 5.227880477905273 Accuracy 68.58172049575676\n",
      "Training:: Epoch 76, Iteration 80, Current loss 3.218484878540039 Accuracy 81.51209267664792\n",
      "Training:: Epoch 76, Iteration 90, Current loss 3.9903528690338135 Accuracy 53.562231759656655\n",
      "Training:: Epoch 76, Iteration 100, Current loss 6.070067405700684 Accuracy 65.68072289156626\n",
      "Training:: Epoch 76, Iteration 110, Current loss 4.854551315307617 Accuracy 73.10376613115618\n",
      "Training:: Epoch 76, Iteration 120, Current loss 1.842415452003479 Accuracy 92.2871287128713\n",
      "Training:: Epoch 76, Iteration 130, Current loss 1.651249885559082 Accuracy 93.71558534833612\n",
      "Training:: Epoch 76, Iteration 140, Current loss 1.6908650398254395 Accuracy 91.66951177876409\n",
      "Training:: Epoch 76, Iteration 150, Current loss 1.5760648250579834 Accuracy 92.88720795461478\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 76, Probability Accuracy 65.62117398898444\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 4.758497714996338 Accuracy 79.56120092378752\n",
      "Training:: Epoch 77, Iteration 10, Current loss 2.3453176021575928 Accuracy 88.32476980210612\n",
      "Training:: Epoch 77, Iteration 20, Current loss 1.9326376914978027 Accuracy 90.56239600665558\n",
      "Training:: Epoch 77, Iteration 30, Current loss 1.8531098365783691 Accuracy 90.1369168356998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 77, Iteration 40, Current loss 1.7124502658843994 Accuracy 92.00760228068421\n",
      "Training:: Epoch 77, Iteration 50, Current loss 1.5175955295562744 Accuracy 95.79681783374457\n",
      "Training:: Epoch 77, Iteration 60, Current loss 1.207585096359253 Accuracy 96.54772265738323\n",
      "Training:: Epoch 77, Iteration 70, Current loss 1.032753586769104 Accuracy 97.82934995958897\n",
      "Training:: Epoch 77, Iteration 80, Current loss 1.2758774757385254 Accuracy 95.54325836010949\n",
      "Training:: Epoch 77, Iteration 90, Current loss 1.465200424194336 Accuracy 95.49718574108817\n",
      "Training:: Epoch 77, Iteration 100, Current loss 1.2561872005462646 Accuracy 95.99261903806811\n",
      "Training:: Epoch 77, Iteration 110, Current loss 1.2646745443344116 Accuracy 95.54531490015361\n",
      "Training:: Epoch 77, Iteration 120, Current loss 1.1198588609695435 Accuracy 96.78767007282786\n",
      "Training:: Epoch 77, Iteration 130, Current loss 1.0965476036071777 Accuracy 96.47289353363814\n",
      "Training:: Epoch 77, Iteration 140, Current loss 0.9741570949554443 Accuracy 97.35567218409366\n",
      "Training:: Epoch 77, Iteration 150, Current loss 2.040867328643799 Accuracy 89.4168072591309\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 77, Probability Accuracy 71.1278396883946\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 1.385249137878418 Accuracy 95.43365232461758\n",
      "Training:: Epoch 78, Iteration 10, Current loss 0.9321973919868469 Accuracy 98.05328876216694\n",
      "Training:: Epoch 78, Iteration 20, Current loss 1.0007786750793457 Accuracy 96.53608995284729\n",
      "Training:: Epoch 78, Iteration 30, Current loss 0.7874940633773804 Accuracy 97.84285527519937\n",
      "Training:: Epoch 78, Iteration 40, Current loss 0.8347625136375427 Accuracy 98.55225244219551\n",
      "Training:: Epoch 78, Iteration 50, Current loss 0.805622398853302 Accuracy 98.34402933761173\n",
      "Training:: Epoch 78, Iteration 60, Current loss 0.8873758316040039 Accuracy 98.13915857605178\n",
      "Training:: Epoch 78, Iteration 70, Current loss 0.7417884469032288 Accuracy 98.10760411630638\n",
      "Training:: Epoch 78, Iteration 80, Current loss 2.689319372177124 Accuracy 88.87867244625421\n",
      "Training:: Epoch 78, Iteration 90, Current loss 1.4674757719039917 Accuracy 95.65111561866125\n",
      "Training:: Epoch 78, Iteration 100, Current loss 1.141566276550293 Accuracy 96.0329305281402\n",
      "Training:: Epoch 78, Iteration 110, Current loss 1.0331807136535645 Accuracy 96.97876334623959\n",
      "Training:: Epoch 78, Iteration 120, Current loss 0.993962824344635 Accuracy 97.71650396757435\n",
      "Training:: Epoch 78, Iteration 130, Current loss 1.0453165769577026 Accuracy 96.67612484799352\n",
      "Training:: Epoch 78, Iteration 140, Current loss 1.054829478263855 Accuracy 96.22631115028646\n",
      "Training:: Epoch 78, Iteration 150, Current loss 1.0207059383392334 Accuracy 96.22611591215413\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 78, Probability Accuracy 70.51545471286433\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 0.9474533200263977 Accuracy 98.06615482306759\n",
      "Training:: Epoch 79, Iteration 10, Current loss 1.1044050455093384 Accuracy 96.87691365584813\n",
      "Training:: Epoch 79, Iteration 20, Current loss 0.9308204650878906 Accuracy 97.55917579610603\n",
      "Training:: Epoch 79, Iteration 30, Current loss 0.7183424234390259 Accuracy 98.15460785416902\n",
      "Training:: Epoch 79, Iteration 40, Current loss 1.1292815208435059 Accuracy 94.62860650705954\n",
      "Training:: Epoch 79, Iteration 50, Current loss 0.9430146813392639 Accuracy 98.3045267489712\n",
      "Training:: Epoch 79, Iteration 60, Current loss 0.7802802920341492 Accuracy 98.13132432232808\n",
      "Training:: Epoch 79, Iteration 70, Current loss 0.6467833518981934 Accuracy 98.62200313690343\n",
      "Training:: Epoch 79, Iteration 80, Current loss 0.6321014761924744 Accuracy 99.26098400485928\n",
      "Training:: Epoch 79, Iteration 90, Current loss 0.9653221964836121 Accuracy 97.89974242124035\n",
      "Training:: Epoch 79, Iteration 100, Current loss 0.7027031183242798 Accuracy 98.81962931337586\n",
      "Training:: Epoch 79, Iteration 110, Current loss 0.7286688089370728 Accuracy 98.43437517856123\n",
      "Training:: Epoch 79, Iteration 120, Current loss 0.6641544699668884 Accuracy 98.65466101694915\n",
      "Training:: Epoch 79, Iteration 130, Current loss 0.6475653052330017 Accuracy 98.73652588796607\n",
      "Training:: Epoch 79, Iteration 140, Current loss 0.7006254196166992 Accuracy 98.19815247183027\n",
      "Training:: Epoch 79, Iteration 150, Current loss 0.6151681542396545 Accuracy 99.03993017674013\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 79, Probability Accuracy 72.78687060898098\n",
      "Starting Training\n",
      "Training:: Epoch 80, Iteration 0, Current loss 0.7944605350494385 Accuracy 98.8154870646473\n",
      "Training:: Epoch 80, Iteration 10, Current loss 0.4626684784889221 Accuracy 99.27375210015717\n",
      "Training:: Epoch 80, Iteration 20, Current loss 0.6007792949676514 Accuracy 98.53558436712993\n",
      "Training:: Epoch 80, Iteration 30, Current loss 1.0312888622283936 Accuracy 98.25313807531381\n",
      "Training:: Epoch 80, Iteration 40, Current loss 1.0086982250213623 Accuracy 98.73265841769779\n",
      "Training:: Epoch 80, Iteration 50, Current loss 0.8810051083564758 Accuracy 99.05312208760485\n",
      "Training:: Epoch 80, Iteration 60, Current loss 0.7342166900634766 Accuracy 98.61336291492448\n",
      "Training:: Epoch 80, Iteration 70, Current loss 0.7266464829444885 Accuracy 98.42431817167343\n",
      "Training:: Epoch 80, Iteration 80, Current loss 0.7868122458457947 Accuracy 98.3898112349329\n",
      "Training:: Epoch 80, Iteration 90, Current loss 0.7760524153709412 Accuracy 98.19350032591488\n",
      "Training:: Epoch 80, Iteration 100, Current loss 0.5755126476287842 Accuracy 98.88879709187046\n",
      "Training:: Epoch 80, Iteration 110, Current loss 0.8258553743362427 Accuracy 98.31697054698458\n",
      "Training:: Epoch 80, Iteration 120, Current loss 0.5986194610595703 Accuracy 98.65357096396514\n",
      "Training:: Epoch 80, Iteration 130, Current loss 0.6091524958610535 Accuracy 99.15323048370259\n",
      "Training:: Epoch 80, Iteration 140, Current loss 0.8086879253387451 Accuracy 98.76977152899825\n",
      "Training:: Epoch 80, Iteration 150, Current loss 0.6370684504508972 Accuracy 99.04984541135661\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 80, Probability Accuracy 72.60640286605543\n",
      "Starting Training\n",
      "Training:: Epoch 81, Iteration 0, Current loss 0.6257864236831665 Accuracy 99.13920088381514\n",
      "Training:: Epoch 81, Iteration 10, Current loss 0.590652346611023 Accuracy 98.9268202423906\n",
      "Training:: Epoch 81, Iteration 20, Current loss 0.6294395923614502 Accuracy 98.9062025261513\n",
      "Training:: Epoch 81, Iteration 30, Current loss 0.9294847249984741 Accuracy 97.84557320333386\n",
      "Training:: Epoch 81, Iteration 40, Current loss 0.6031432747840881 Accuracy 98.98614609571788\n",
      "Training:: Epoch 81, Iteration 50, Current loss 0.5341247320175171 Accuracy 99.14463452566096\n",
      "Training:: Epoch 81, Iteration 60, Current loss 0.5422271490097046 Accuracy 98.77967605946306\n",
      "Training:: Epoch 81, Iteration 70, Current loss 0.7755224108695984 Accuracy 97.88004136504654\n",
      "Training:: Epoch 81, Iteration 80, Current loss 0.6040445566177368 Accuracy 98.5557149689144\n",
      "Training:: Epoch 81, Iteration 90, Current loss 0.8523067831993103 Accuracy 98.6855186126146\n",
      "Training:: Epoch 81, Iteration 100, Current loss 0.9300000667572021 Accuracy 98.91295908067397\n",
      "Training:: Epoch 81, Iteration 110, Current loss 0.5014793872833252 Accuracy 99.05392982993719\n",
      "Training:: Epoch 81, Iteration 120, Current loss 0.6121261119842529 Accuracy 98.56882899719668\n",
      "Training:: Epoch 81, Iteration 130, Current loss 0.561338484287262 Accuracy 99.06117146905999\n",
      "Training:: Epoch 81, Iteration 140, Current loss 0.5501559972763062 Accuracy 98.8670505758638\n",
      "Training:: Epoch 81, Iteration 150, Current loss 0.693734347820282 Accuracy 98.74019312009656\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 81, Probability Accuracy 72.64088928075178\n",
      "Starting Training\n",
      "Training:: Epoch 82, Iteration 0, Current loss 0.702942430973053 Accuracy 99.13133323128731\n",
      "Training:: Epoch 82, Iteration 10, Current loss 0.6262618899345398 Accuracy 98.67909072291624\n",
      "Training:: Epoch 82, Iteration 20, Current loss 0.49583831429481506 Accuracy 99.28263516403962\n",
      "Training:: Epoch 82, Iteration 30, Current loss 0.4640082120895386 Accuracy 99.02182551812679\n",
      "Training:: Epoch 82, Iteration 40, Current loss 0.6563486456871033 Accuracy 98.67151668511782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 82, Iteration 50, Current loss 0.5536450147628784 Accuracy 98.96366083445491\n",
      "Training:: Epoch 82, Iteration 60, Current loss 0.7119664549827576 Accuracy 98.9650919620355\n",
      "Training:: Epoch 82, Iteration 70, Current loss 0.6145573854446411 Accuracy 98.8760631834751\n",
      "Training:: Epoch 82, Iteration 80, Current loss 0.5503330230712891 Accuracy 99.08062628851619\n",
      "Training:: Epoch 82, Iteration 90, Current loss 0.7821388840675354 Accuracy 98.8243805389763\n",
      "Training:: Epoch 82, Iteration 100, Current loss 0.5953540205955505 Accuracy 98.53059043547957\n",
      "Training:: Epoch 82, Iteration 110, Current loss 0.4972429573535919 Accuracy 99.19422676760969\n",
      "Training:: Epoch 82, Iteration 120, Current loss 0.4959011971950531 Accuracy 99.21604375569736\n",
      "Training:: Epoch 82, Iteration 130, Current loss 0.6762685775756836 Accuracy 98.32470716426042\n",
      "Training:: Epoch 82, Iteration 140, Current loss 0.6277530789375305 Accuracy 98.9167656719547\n",
      "Training:: Epoch 82, Iteration 150, Current loss 0.6036760807037354 Accuracy 98.94794370815686\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 82, Probability Accuracy 72.96164641938383\n",
      "Starting Training\n",
      "Training:: Epoch 83, Iteration 0, Current loss 0.5986287593841553 Accuracy 98.4893034573659\n",
      "Training:: Epoch 83, Iteration 10, Current loss 0.5126922130584717 Accuracy 99.0603922180202\n",
      "Training:: Epoch 83, Iteration 20, Current loss 0.5723259449005127 Accuracy 98.79875112451712\n",
      "Training:: Epoch 83, Iteration 30, Current loss 0.6009691953659058 Accuracy 99.30459952698648\n",
      "Training:: Epoch 83, Iteration 40, Current loss 0.806562602519989 Accuracy 99.08768062698996\n",
      "Training:: Epoch 83, Iteration 50, Current loss 0.5760931968688965 Accuracy 99.27127138660062\n",
      "Training:: Epoch 83, Iteration 60, Current loss 0.6891698837280273 Accuracy 99.0870185449358\n",
      "Training:: Epoch 83, Iteration 70, Current loss 0.7690592408180237 Accuracy 98.51610568222947\n",
      "Training:: Epoch 83, Iteration 80, Current loss 0.6064345836639404 Accuracy 98.31318526848467\n",
      "Training:: Epoch 83, Iteration 90, Current loss 0.5763759613037109 Accuracy 98.62350242161611\n",
      "Training:: Epoch 83, Iteration 100, Current loss 0.7357191443443298 Accuracy 98.9849108367627\n",
      "Training:: Epoch 83, Iteration 110, Current loss 0.6329135894775391 Accuracy 99.05644622578491\n",
      "Training:: Epoch 83, Iteration 120, Current loss 0.4163856506347656 Accuracy 99.11540476051911\n",
      "Training:: Epoch 83, Iteration 130, Current loss 0.5561467409133911 Accuracy 98.88854774332208\n",
      "Training:: Epoch 83, Iteration 140, Current loss 0.5578190088272095 Accuracy 98.77803639441387\n",
      "Training:: Epoch 83, Iteration 150, Current loss 0.891008198261261 Accuracy 98.28523630280218\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 83, Probability Accuracy 72.15941875324357\n",
      "Starting Training\n",
      "Training:: Epoch 84, Iteration 0, Current loss 0.6212660074234009 Accuracy 99.05134590300011\n",
      "Training:: Epoch 84, Iteration 10, Current loss 0.5614351034164429 Accuracy 98.82797365362262\n",
      "Training:: Epoch 84, Iteration 20, Current loss 0.48226654529571533 Accuracy 99.15474642392718\n",
      "Training:: Epoch 84, Iteration 30, Current loss 0.3878915011882782 Accuracy 99.20629194025543\n",
      "Training:: Epoch 84, Iteration 40, Current loss 0.6574490666389465 Accuracy 98.89354303327273\n",
      "Training:: Epoch 84, Iteration 50, Current loss 0.65029376745224 Accuracy 99.1162474507138\n",
      "Training:: Epoch 84, Iteration 60, Current loss 0.5945720672607422 Accuracy 99.13316822251858\n",
      "Training:: Epoch 84, Iteration 70, Current loss 0.5162120461463928 Accuracy 99.02229845626071\n",
      "Training:: Epoch 84, Iteration 80, Current loss 0.4846591353416443 Accuracy 99.14980628497632\n",
      "Training:: Epoch 84, Iteration 90, Current loss 0.411090612411499 Accuracy 99.13190676678676\n",
      "Training:: Epoch 84, Iteration 100, Current loss 0.7108259797096252 Accuracy 98.56495468277946\n",
      "Training:: Epoch 84, Iteration 110, Current loss 0.4832366406917572 Accuracy 99.10128243966474\n",
      "Training:: Epoch 84, Iteration 120, Current loss 0.5096597671508789 Accuracy 99.11647349420483\n",
      "Training:: Epoch 84, Iteration 130, Current loss 0.48629626631736755 Accuracy 99.25089736253446\n",
      "Training:: Epoch 84, Iteration 140, Current loss 1.0408885478973389 Accuracy 98.71069182389937\n",
      "Training:: Epoch 84, Iteration 150, Current loss 0.5607410669326782 Accuracy 99.11519853550102\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 84, Probability Accuracy 72.56321114279496\n",
      "Starting Training\n",
      "Training:: Epoch 85, Iteration 0, Current loss 0.46042656898498535 Accuracy 99.17971662938106\n",
      "Training:: Epoch 85, Iteration 10, Current loss 0.6437586545944214 Accuracy 98.70910020449898\n",
      "Training:: Epoch 85, Iteration 20, Current loss 0.458268940448761 Accuracy 99.02225920532557\n",
      "Training:: Epoch 85, Iteration 30, Current loss 0.5330145359039307 Accuracy 99.22297766060774\n",
      "Training:: Epoch 85, Iteration 40, Current loss 0.5035706162452698 Accuracy 99.12857044055606\n",
      "Training:: Epoch 85, Iteration 50, Current loss 0.3761017322540283 Accuracy 99.28008383833782\n",
      "Training:: Epoch 85, Iteration 60, Current loss 0.5589877367019653 Accuracy 99.07596487645498\n",
      "Training:: Epoch 85, Iteration 70, Current loss 0.6379838585853577 Accuracy 98.29737457759293\n",
      "Training:: Epoch 85, Iteration 80, Current loss 0.597420871257782 Accuracy 99.08885663602645\n",
      "Training:: Epoch 85, Iteration 90, Current loss 0.4205470681190491 Accuracy 99.03543119090823\n",
      "Training:: Epoch 85, Iteration 100, Current loss 0.5278559327125549 Accuracy 98.83489822493318\n",
      "Training:: Epoch 85, Iteration 110, Current loss 0.8280274271965027 Accuracy 98.97830329468488\n",
      "Training:: Epoch 85, Iteration 120, Current loss 0.4983191192150116 Accuracy 99.16538920115823\n",
      "Training:: Epoch 85, Iteration 130, Current loss 0.6162397861480713 Accuracy 99.30564568462037\n",
      "Training:: Epoch 85, Iteration 140, Current loss 0.6737180352210999 Accuracy 98.7918883934992\n",
      "Training:: Epoch 85, Iteration 150, Current loss 0.5985746383666992 Accuracy 98.8382866509871\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 85, Probability Accuracy 72.20595867210562\n",
      "Starting Training\n",
      "Training:: Epoch 86, Iteration 0, Current loss 0.6494837403297424 Accuracy 98.8740157480315\n",
      "Training:: Epoch 86, Iteration 10, Current loss 0.4312501549720764 Accuracy 99.14400424210287\n",
      "Training:: Epoch 86, Iteration 20, Current loss 0.7821187376976013 Accuracy 98.850990681263\n",
      "Training:: Epoch 86, Iteration 30, Current loss 0.5694047808647156 Accuracy 99.14765009508285\n",
      "Training:: Epoch 86, Iteration 40, Current loss 0.5614692568778992 Accuracy 99.03037583662262\n",
      "Training:: Epoch 86, Iteration 50, Current loss 0.4335543215274811 Accuracy 99.25650557620818\n",
      "Training:: Epoch 86, Iteration 60, Current loss 0.5797057151794434 Accuracy 99.01669458101465\n",
      "Training:: Epoch 86, Iteration 70, Current loss 0.4965086281299591 Accuracy 99.25963037185483\n",
      "Training:: Epoch 86, Iteration 80, Current loss 0.5524229407310486 Accuracy 99.34280739201259\n",
      "Training:: Epoch 86, Iteration 90, Current loss 0.5889702439308167 Accuracy 98.9648033126294\n",
      "Training:: Epoch 86, Iteration 100, Current loss 0.4165734350681305 Accuracy 99.2576262035454\n",
      "Training:: Epoch 86, Iteration 110, Current loss 0.4409894645214081 Accuracy 98.96281305337718\n",
      "Training:: Epoch 86, Iteration 120, Current loss 0.630557119846344 Accuracy 99.09734398337554\n",
      "Training:: Epoch 86, Iteration 130, Current loss 0.5402476787567139 Accuracy 99.35993463162195\n",
      "Training:: Epoch 86, Iteration 140, Current loss 0.6292501091957092 Accuracy 98.79233836993765\n",
      "Training:: Epoch 86, Iteration 150, Current loss 0.5121183395385742 Accuracy 99.01598375592232\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 86, Probability Accuracy 72.78062064385801\n",
      "Starting Training\n",
      "Training:: Epoch 87, Iteration 0, Current loss 0.5364212989807129 Accuracy 99.20744314266024\n",
      "Training:: Epoch 87, Iteration 10, Current loss 0.5864604115486145 Accuracy 99.0741615493623\n",
      "Training:: Epoch 87, Iteration 20, Current loss 0.4999523162841797 Accuracy 99.43834311218093\n",
      "Training:: Epoch 87, Iteration 30, Current loss 0.7726035118103027 Accuracy 98.70063038723788\n",
      "Training:: Epoch 87, Iteration 40, Current loss 0.8590842485427856 Accuracy 98.444241077265\n",
      "Training:: Epoch 87, Iteration 50, Current loss 0.6479165554046631 Accuracy 98.88214627914404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 87, Iteration 60, Current loss 0.5921062231063843 Accuracy 98.82795851683478\n",
      "Training:: Epoch 87, Iteration 70, Current loss 0.5129291415214539 Accuracy 99.30859261110417\n",
      "Training:: Epoch 87, Iteration 80, Current loss 0.5126921534538269 Accuracy 99.41126997476871\n",
      "Training:: Epoch 87, Iteration 90, Current loss 0.37108314037323 Accuracy 99.1998254164545\n",
      "Training:: Epoch 87, Iteration 100, Current loss 0.5385362505912781 Accuracy 98.69121561668146\n",
      "Training:: Epoch 87, Iteration 110, Current loss 0.43327194452285767 Accuracy 99.17128718774221\n",
      "Training:: Epoch 87, Iteration 120, Current loss 0.5601201057434082 Accuracy 99.20113616190307\n",
      "Training:: Epoch 87, Iteration 130, Current loss 0.39784157276153564 Accuracy 99.025069637883\n",
      "Training:: Epoch 87, Iteration 140, Current loss 0.39798495173454285 Accuracy 99.2347329217897\n",
      "Training:: Epoch 87, Iteration 150, Current loss 0.6263027191162109 Accuracy 98.99633067127131\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 87, Probability Accuracy 71.4858734047243\n",
      "Starting Training\n",
      "Training:: Epoch 88, Iteration 0, Current loss 0.7165917754173279 Accuracy 99.17783646419852\n",
      "Training:: Epoch 88, Iteration 10, Current loss 0.3141358196735382 Accuracy 99.41364927764009\n",
      "Training:: Epoch 88, Iteration 20, Current loss 0.3883275091648102 Accuracy 99.31683619250038\n",
      "Training:: Epoch 88, Iteration 30, Current loss 0.5708777904510498 Accuracy 98.92836999435984\n",
      "Training:: Epoch 88, Iteration 40, Current loss 0.6457909941673279 Accuracy 98.91505264003858\n",
      "Training:: Epoch 88, Iteration 50, Current loss 0.4841423034667969 Accuracy 98.85075322255008\n",
      "Training:: Epoch 88, Iteration 60, Current loss 0.5685449838638306 Accuracy 99.01873327386262\n",
      "Training:: Epoch 88, Iteration 70, Current loss 0.5071755051612854 Accuracy 99.10867329448062\n",
      "Training:: Epoch 88, Iteration 80, Current loss 0.5723410248756409 Accuracy 99.34339843586633\n",
      "Training:: Epoch 88, Iteration 90, Current loss 0.4699159562587738 Accuracy 99.10146582905007\n",
      "Training:: Epoch 88, Iteration 100, Current loss 0.3427684009075165 Accuracy 99.2305483778352\n",
      "Training:: Epoch 88, Iteration 110, Current loss 0.6916351914405823 Accuracy 97.41719077568135\n",
      "Training:: Epoch 88, Iteration 120, Current loss 0.40411362051963806 Accuracy 99.07775073651851\n",
      "Training:: Epoch 88, Iteration 130, Current loss 0.6964910626411438 Accuracy 97.59316188048287\n",
      "Training:: Epoch 88, Iteration 140, Current loss 1.0743130445480347 Accuracy 95.3605220228385\n",
      "Training:: Epoch 88, Iteration 150, Current loss 0.6934281587600708 Accuracy 97.4327122153209\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 88, Probability Accuracy 71.02326437910503\n",
      "Starting Training\n",
      "Training:: Epoch 89, Iteration 0, Current loss 0.6655552983283997 Accuracy 98.74165382639958\n",
      "Training:: Epoch 89, Iteration 10, Current loss 0.6206578612327576 Accuracy 98.42793075179631\n",
      "Training:: Epoch 89, Iteration 20, Current loss 0.8601231575012207 Accuracy 97.32207981660936\n",
      "Training:: Epoch 89, Iteration 30, Current loss 0.8022446632385254 Accuracy 97.02424687729611\n",
      "Training:: Epoch 89, Iteration 40, Current loss 0.7020708322525024 Accuracy 97.14773114977824\n",
      "Training:: Epoch 89, Iteration 50, Current loss 0.7059462070465088 Accuracy 97.81770568123954\n",
      "Training:: Epoch 89, Iteration 60, Current loss 0.6838948726654053 Accuracy 97.79179810725552\n",
      "Training:: Epoch 89, Iteration 70, Current loss 0.6392397880554199 Accuracy 98.73545476133935\n",
      "Training:: Epoch 89, Iteration 80, Current loss 1.0568678379058838 Accuracy 96.25207594595808\n",
      "Training:: Epoch 89, Iteration 90, Current loss 0.6322126984596252 Accuracy 98.78510432256361\n",
      "Training:: Epoch 89, Iteration 100, Current loss 0.9633428454399109 Accuracy 96.68191438519834\n",
      "Training:: Epoch 89, Iteration 110, Current loss 0.8172663450241089 Accuracy 96.8783214580955\n",
      "Training:: Epoch 89, Iteration 120, Current loss 1.078500747680664 Accuracy 96.34421852536455\n",
      "Training:: Epoch 89, Iteration 130, Current loss 0.921903669834137 Accuracy 96.75595238095238\n",
      "Training:: Epoch 89, Iteration 140, Current loss 0.9624568223953247 Accuracy 96.23162671401795\n",
      "Training:: Epoch 89, Iteration 150, Current loss 1.1802939176559448 Accuracy 95.76569417641049\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 89, Probability Accuracy 66.24103660135825\n",
      "Starting Training\n",
      "Training:: Epoch 90, Iteration 0, Current loss 0.8489483594894409 Accuracy 97.20214742927938\n",
      "Training:: Epoch 90, Iteration 10, Current loss 1.6422921419143677 Accuracy 93.38155816080486\n",
      "Training:: Epoch 90, Iteration 20, Current loss 2.9140570163726807 Accuracy 85.74721009218825\n",
      "Training:: Epoch 90, Iteration 30, Current loss 3.0942978858947754 Accuracy 90.21935775667119\n",
      "Training:: Epoch 90, Iteration 40, Current loss 2.697061538696289 Accuracy 87.52969121140143\n",
      "Training:: Epoch 90, Iteration 50, Current loss 3.614241361618042 Accuracy 71.78183498960018\n",
      "Training:: Epoch 90, Iteration 60, Current loss 2.241267681121826 Accuracy 90.63191371681415\n",
      "Training:: Epoch 90, Iteration 70, Current loss 2.4140946865081787 Accuracy 87.14359326691859\n",
      "Training:: Epoch 90, Iteration 80, Current loss 4.704180717468262 Accuracy 79.67125334661603\n",
      "Training:: Epoch 90, Iteration 90, Current loss 2.2403295040130615 Accuracy 90.30010718113613\n",
      "Training:: Epoch 90, Iteration 100, Current loss 1.814062237739563 Accuracy 91.73844061650045\n",
      "Training:: Epoch 90, Iteration 110, Current loss 1.1762847900390625 Accuracy 95.83647324792766\n",
      "Training:: Epoch 90, Iteration 120, Current loss 2.1602208614349365 Accuracy 89.46917808219177\n",
      "Training:: Epoch 90, Iteration 130, Current loss 1.5251940488815308 Accuracy 93.1887007893561\n",
      "Training:: Epoch 90, Iteration 140, Current loss 2.8873181343078613 Accuracy 81.57259756609317\n",
      "Training:: Epoch 90, Iteration 150, Current loss 1.7584996223449707 Accuracy 93.78131447096965\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 90, Probability Accuracy 66.99315293999476\n",
      "Starting Training\n",
      "Training:: Epoch 91, Iteration 0, Current loss 2.1948699951171875 Accuracy 89.9109131403118\n",
      "Training:: Epoch 91, Iteration 10, Current loss 1.5403064489364624 Accuracy 94.01643222781435\n",
      "Training:: Epoch 91, Iteration 20, Current loss 1.8210127353668213 Accuracy 91.79924889040628\n",
      "Training:: Epoch 91, Iteration 30, Current loss 1.7876248359680176 Accuracy 91.9083174162181\n",
      "Training:: Epoch 91, Iteration 40, Current loss 1.558101773262024 Accuracy 95.0215320370612\n",
      "Training:: Epoch 91, Iteration 50, Current loss 1.5797169208526611 Accuracy 95.13994523881959\n",
      "Training:: Epoch 91, Iteration 60, Current loss 1.0258389711380005 Accuracy 97.61645493042953\n",
      "Training:: Epoch 91, Iteration 70, Current loss 0.9566860198974609 Accuracy 97.63310185185185\n",
      "Training:: Epoch 91, Iteration 80, Current loss 0.8617446422576904 Accuracy 98.34877600317266\n",
      "Training:: Epoch 91, Iteration 90, Current loss 0.8235152959823608 Accuracy 97.59101612026807\n",
      "Training:: Epoch 91, Iteration 100, Current loss 0.7849093079566956 Accuracy 97.67577441797194\n",
      "Training:: Epoch 91, Iteration 110, Current loss 0.7967842221260071 Accuracy 96.81150071418698\n",
      "Training:: Epoch 91, Iteration 120, Current loss 0.8333479166030884 Accuracy 97.6688282647585\n",
      "Training:: Epoch 91, Iteration 130, Current loss 0.7386122941970825 Accuracy 98.5480943738657\n",
      "Training:: Epoch 91, Iteration 140, Current loss 0.9260308146476746 Accuracy 96.15725250934855\n",
      "Training:: Epoch 91, Iteration 150, Current loss 0.6762391328811646 Accuracy 98.45470795133484\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 91, Probability Accuracy 72.85104435801139\n",
      "Starting Training\n",
      "Training:: Epoch 92, Iteration 0, Current loss 0.7837420701980591 Accuracy 98.54423896047878\n",
      "Training:: Epoch 92, Iteration 10, Current loss 0.6997231841087341 Accuracy 98.13050349966133\n",
      "Training:: Epoch 92, Iteration 20, Current loss 0.7476959228515625 Accuracy 97.55387794694445\n",
      "Training:: Epoch 92, Iteration 30, Current loss 0.7459595203399658 Accuracy 98.47820609725372\n",
      "Training:: Epoch 92, Iteration 40, Current loss 0.7135751843452454 Accuracy 97.73478055686644\n",
      "Training:: Epoch 92, Iteration 50, Current loss 2.7008402347564697 Accuracy 86.85972132692022\n",
      "Training:: Epoch 92, Iteration 60, Current loss 1.2329912185668945 Accuracy 92.89254336562288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 92, Iteration 70, Current loss 0.7440786957740784 Accuracy 98.0291759250314\n",
      "Training:: Epoch 92, Iteration 80, Current loss 1.413214921951294 Accuracy 91.72798677139313\n",
      "Training:: Epoch 92, Iteration 90, Current loss 0.8258844614028931 Accuracy 98.46821705426356\n",
      "Training:: Epoch 92, Iteration 100, Current loss 0.7697249054908752 Accuracy 98.07975159339762\n",
      "Training:: Epoch 92, Iteration 110, Current loss 0.9111712574958801 Accuracy 96.68819129211687\n",
      "Training:: Epoch 92, Iteration 120, Current loss 0.7621256709098816 Accuracy 98.16666666666667\n",
      "Training:: Epoch 92, Iteration 130, Current loss 0.7164638638496399 Accuracy 97.96778049752376\n",
      "Training:: Epoch 92, Iteration 140, Current loss 0.5024830102920532 Accuracy 98.78563610778131\n",
      "Training:: Epoch 92, Iteration 150, Current loss 0.6874484419822693 Accuracy 98.37606133047564\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 92, Probability Accuracy 73.24680107811898\n",
      "Starting Training\n",
      "Training:: Epoch 93, Iteration 0, Current loss 0.6919792294502258 Accuracy 98.52206042164747\n",
      "Training:: Epoch 93, Iteration 10, Current loss 0.8323960900306702 Accuracy 98.19013831045298\n",
      "Training:: Epoch 93, Iteration 20, Current loss 0.7644545435905457 Accuracy 98.67569451359155\n",
      "Training:: Epoch 93, Iteration 30, Current loss 1.1251556873321533 Accuracy 96.73389080177078\n",
      "Training:: Epoch 93, Iteration 40, Current loss 1.1399633884429932 Accuracy 97.79185603452234\n",
      "Training:: Epoch 93, Iteration 50, Current loss 0.6826460957527161 Accuracy 97.56469765371654\n",
      "Training:: Epoch 93, Iteration 60, Current loss 0.6019623279571533 Accuracy 98.46785225718195\n",
      "Training:: Epoch 93, Iteration 70, Current loss 0.7707697153091431 Accuracy 97.9336595976074\n",
      "Training:: Epoch 93, Iteration 80, Current loss 0.8567690253257751 Accuracy 97.8568456096546\n",
      "Training:: Epoch 93, Iteration 90, Current loss 0.9756582379341125 Accuracy 98.00769906125481\n",
      "Training:: Epoch 93, Iteration 100, Current loss 0.8767901062965393 Accuracy 98.27256013482457\n",
      "Training:: Epoch 93, Iteration 110, Current loss 0.6365761160850525 Accuracy 98.23328091758394\n",
      "Training:: Epoch 93, Iteration 120, Current loss 0.7319220304489136 Accuracy 98.44370860927152\n",
      "Training:: Epoch 93, Iteration 130, Current loss 0.7896856069564819 Accuracy 97.31669865642995\n",
      "Training:: Epoch 93, Iteration 140, Current loss 0.8195803165435791 Accuracy 98.37091252508559\n",
      "Training:: Epoch 93, Iteration 150, Current loss 0.6811543703079224 Accuracy 98.71335355206323\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 93, Probability Accuracy 73.21923426766591\n",
      "Starting Training\n",
      "Training:: Epoch 94, Iteration 0, Current loss 0.47780489921569824 Accuracy 98.99502803342854\n",
      "Training:: Epoch 94, Iteration 10, Current loss 0.622413158416748 Accuracy 98.76830892143808\n",
      "Training:: Epoch 94, Iteration 20, Current loss 0.791266143321991 Accuracy 98.73980726464048\n",
      "Training:: Epoch 94, Iteration 30, Current loss 0.7424928545951843 Accuracy 98.0636961722488\n",
      "Training:: Epoch 94, Iteration 40, Current loss 0.6328551173210144 Accuracy 98.69265732627338\n",
      "Training:: Epoch 94, Iteration 50, Current loss 0.5541696548461914 Accuracy 98.97144501186794\n",
      "Training:: Epoch 94, Iteration 60, Current loss 0.8556252121925354 Accuracy 98.39042509286008\n",
      "Training:: Epoch 94, Iteration 70, Current loss 0.4910133481025696 Accuracy 99.07178828873309\n",
      "Training:: Epoch 94, Iteration 80, Current loss 0.60047847032547 Accuracy 98.86478588288188\n",
      "Training:: Epoch 94, Iteration 90, Current loss 0.6258291602134705 Accuracy 98.88154269972452\n",
      "Training:: Epoch 94, Iteration 100, Current loss 0.4569664001464844 Accuracy 99.12873287455017\n",
      "Training:: Epoch 94, Iteration 110, Current loss 0.7040069103240967 Accuracy 98.81076737645641\n",
      "Training:: Epoch 94, Iteration 120, Current loss 0.5040673613548279 Accuracy 99.1210447011552\n",
      "Training:: Epoch 94, Iteration 130, Current loss 0.6553576588630676 Accuracy 97.64971751412429\n",
      "Training:: Epoch 94, Iteration 140, Current loss 0.738732099533081 Accuracy 98.77156474457945\n",
      "Training:: Epoch 94, Iteration 150, Current loss 0.7096312046051025 Accuracy 98.66867330400555\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 94, Probability Accuracy 73.77760168749059\n",
      "Starting Training\n",
      "Training:: Epoch 95, Iteration 0, Current loss 0.5611352324485779 Accuracy 98.82658620400638\n",
      "Training:: Epoch 95, Iteration 10, Current loss 0.488865464925766 Accuracy 98.45973547631006\n",
      "Training:: Epoch 95, Iteration 20, Current loss 0.49978676438331604 Accuracy 99.0321722946025\n",
      "Training:: Epoch 95, Iteration 30, Current loss 0.5200602412223816 Accuracy 98.58136105063558\n",
      "Training:: Epoch 95, Iteration 40, Current loss 0.5323637127876282 Accuracy 99.13640824337585\n",
      "Training:: Epoch 95, Iteration 50, Current loss 0.5511269569396973 Accuracy 99.07778059285533\n",
      "Training:: Epoch 95, Iteration 60, Current loss 0.5662896633148193 Accuracy 98.83472356121894\n",
      "Training:: Epoch 95, Iteration 70, Current loss 0.42477768659591675 Accuracy 99.07971398305085\n",
      "Training:: Epoch 95, Iteration 80, Current loss 0.5046554207801819 Accuracy 99.36173485081514\n",
      "Training:: Epoch 95, Iteration 90, Current loss 0.5611408948898315 Accuracy 99.04998227578872\n",
      "Training:: Epoch 95, Iteration 100, Current loss 0.4355691373348236 Accuracy 99.35447430000806\n",
      "Training:: Epoch 95, Iteration 110, Current loss 0.7128031849861145 Accuracy 98.67453985803128\n",
      "Training:: Epoch 95, Iteration 120, Current loss 0.6143758893013 Accuracy 98.45184990816058\n",
      "Training:: Epoch 95, Iteration 130, Current loss 0.6036546230316162 Accuracy 99.05588645292104\n",
      "Training:: Epoch 95, Iteration 140, Current loss 0.4696617126464844 Accuracy 99.10970039482852\n",
      "Training:: Epoch 95, Iteration 150, Current loss 0.817699670791626 Accuracy 98.79319178004212\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 95, Probability Accuracy 73.36387631765447\n",
      "Starting Training\n",
      "Training:: Epoch 96, Iteration 0, Current loss 0.5183424353599548 Accuracy 99.44433675776959\n",
      "Training:: Epoch 96, Iteration 10, Current loss 0.5881409645080566 Accuracy 99.39138576779027\n",
      "Training:: Epoch 96, Iteration 20, Current loss 0.7109906077384949 Accuracy 99.19948994049305\n",
      "Training:: Epoch 96, Iteration 30, Current loss 0.6407582759857178 Accuracy 99.00582387199248\n",
      "Training:: Epoch 96, Iteration 40, Current loss 0.5109018087387085 Accuracy 98.89923793395428\n",
      "Training:: Epoch 96, Iteration 50, Current loss 0.6492375731468201 Accuracy 99.0985954859898\n",
      "Training:: Epoch 96, Iteration 60, Current loss 0.38218680024147034 Accuracy 99.37456023766711\n",
      "Training:: Epoch 96, Iteration 70, Current loss 0.5204378366470337 Accuracy 99.3953488372093\n",
      "Training:: Epoch 96, Iteration 80, Current loss 0.4717863202095032 Accuracy 99.17806459898543\n",
      "Training:: Epoch 96, Iteration 90, Current loss 0.4541478157043457 Accuracy 99.06888720666161\n",
      "Training:: Epoch 96, Iteration 100, Current loss 0.5201626420021057 Accuracy 99.15604237744658\n",
      "Training:: Epoch 96, Iteration 110, Current loss 0.5966536402702332 Accuracy 99.0504963314631\n",
      "Training:: Epoch 96, Iteration 120, Current loss 0.6711088418960571 Accuracy 99.09538532005038\n",
      "Training:: Epoch 96, Iteration 130, Current loss 0.5774141550064087 Accuracy 98.8994992430418\n",
      "Training:: Epoch 96, Iteration 140, Current loss 0.5723776817321777 Accuracy 98.51083736160923\n",
      "Training:: Epoch 96, Iteration 150, Current loss 0.5949496030807495 Accuracy 98.64835686833355\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 96, Probability Accuracy 73.15863192727718\n",
      "Starting Training\n",
      "Training:: Epoch 97, Iteration 0, Current loss 0.3620700538158417 Accuracy 99.2910003622626\n",
      "Training:: Epoch 97, Iteration 10, Current loss 0.4827249348163605 Accuracy 99.32309988274172\n",
      "Training:: Epoch 97, Iteration 20, Current loss 0.6036144495010376 Accuracy 99.14163090128756\n",
      "Training:: Epoch 97, Iteration 30, Current loss 0.45556172728538513 Accuracy 99.28106480132129\n",
      "Training:: Epoch 97, Iteration 40, Current loss 0.6075370907783508 Accuracy 99.30786516853932\n",
      "Training:: Epoch 97, Iteration 50, Current loss 0.4165545701980591 Accuracy 99.20914360002166\n",
      "Training:: Epoch 97, Iteration 60, Current loss 0.4943177402019501 Accuracy 98.96111268804995\n",
      "Training:: Epoch 97, Iteration 70, Current loss 0.5662558078765869 Accuracy 99.18213563683146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 97, Iteration 80, Current loss 0.7333576679229736 Accuracy 99.13974383483081\n",
      "Training:: Epoch 97, Iteration 90, Current loss 0.6296099424362183 Accuracy 98.79559770194504\n",
      "Training:: Epoch 97, Iteration 100, Current loss 0.4364111125469208 Accuracy 99.17715392061956\n",
      "Training:: Epoch 97, Iteration 110, Current loss 0.5833075046539307 Accuracy 99.04528241506213\n",
      "Training:: Epoch 97, Iteration 120, Current loss 0.5872718691825867 Accuracy 99.1267727751285\n",
      "Training:: Epoch 97, Iteration 130, Current loss 0.6105779409408569 Accuracy 99.05702077501105\n",
      "Training:: Epoch 97, Iteration 140, Current loss 0.5780901312828064 Accuracy 99.23918618567276\n",
      "Training:: Epoch 97, Iteration 150, Current loss 0.5356330871582031 Accuracy 99.42071304102105\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 97, Probability Accuracy 72.52772026941814\n",
      "Starting Training\n",
      "Training:: Epoch 98, Iteration 0, Current loss 0.6064478754997253 Accuracy 99.20452166631777\n",
      "Training:: Epoch 98, Iteration 10, Current loss 0.4422849416732788 Accuracy 98.92147447735834\n",
      "Training:: Epoch 98, Iteration 20, Current loss 0.543986439704895 Accuracy 99.194438313838\n",
      "Training:: Epoch 98, Iteration 30, Current loss 0.5351332426071167 Accuracy 99.48653616127433\n",
      "Training:: Epoch 98, Iteration 40, Current loss 0.6091253757476807 Accuracy 99.09934011057607\n",
      "Training:: Epoch 98, Iteration 50, Current loss 0.4323831796646118 Accuracy 99.36798658583774\n",
      "Training:: Epoch 98, Iteration 60, Current loss 0.3739333748817444 Accuracy 99.26123289330265\n",
      "Training:: Epoch 98, Iteration 70, Current loss 0.42632198333740234 Accuracy 99.28330273055737\n",
      "Training:: Epoch 98, Iteration 80, Current loss 0.6388788223266602 Accuracy 99.12767644726408\n",
      "Training:: Epoch 98, Iteration 90, Current loss 0.5170553922653198 Accuracy 99.10810810810811\n",
      "Training:: Epoch 98, Iteration 100, Current loss 0.5570847392082214 Accuracy 98.97784180558413\n",
      "Training:: Epoch 98, Iteration 110, Current loss 0.4521010220050812 Accuracy 99.10783125894922\n",
      "Training:: Epoch 98, Iteration 120, Current loss 0.4917105436325073 Accuracy 99.07481898632341\n",
      "Training:: Epoch 98, Iteration 130, Current loss 0.5153414011001587 Accuracy 99.05118473689609\n",
      "Training:: Epoch 98, Iteration 140, Current loss 0.6546743512153625 Accuracy 98.84546001202646\n",
      "Training:: Epoch 98, Iteration 150, Current loss 0.5474920868873596 Accuracy 99.06423628033922\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 98, Probability Accuracy 72.63854554383067\n",
      "Starting Training\n",
      "Training:: Epoch 99, Iteration 0, Current loss 0.4914235770702362 Accuracy 98.9063215758131\n",
      "Training:: Epoch 99, Iteration 10, Current loss 0.5152977108955383 Accuracy 99.38871210852453\n",
      "Training:: Epoch 99, Iteration 20, Current loss 0.5868533849716187 Accuracy 99.5316466777554\n",
      "Training:: Epoch 99, Iteration 30, Current loss 0.5086832046508789 Accuracy 99.08788027745918\n",
      "Training:: Epoch 99, Iteration 40, Current loss 0.4596940577030182 Accuracy 99.30195095758009\n",
      "Training:: Epoch 99, Iteration 50, Current loss 0.3979035019874573 Accuracy 99.24432059698673\n",
      "Training:: Epoch 99, Iteration 60, Current loss 0.5882006287574768 Accuracy 99.03738839285714\n",
      "Training:: Epoch 99, Iteration 70, Current loss 0.46701371669769287 Accuracy 98.9236522849585\n",
      "Training:: Epoch 99, Iteration 80, Current loss 0.5188204050064087 Accuracy 99.18550531914893\n",
      "Training:: Epoch 99, Iteration 90, Current loss 0.4152955114841461 Accuracy 99.49007776314112\n",
      "Training:: Epoch 99, Iteration 100, Current loss 0.4982330799102783 Accuracy 98.77271169009256\n",
      "Training:: Epoch 99, Iteration 110, Current loss 0.4549097716808319 Accuracy 99.16454776607337\n",
      "Training:: Epoch 99, Iteration 120, Current loss 0.5266411304473877 Accuracy 99.43424965547254\n",
      "Training:: Epoch 99, Iteration 130, Current loss 0.6266037821769714 Accuracy 99.0242903930131\n",
      "Training:: Epoch 99, Iteration 140, Current loss 0.4759797751903534 Accuracy 99.43848902501276\n",
      "Training:: Epoch 99, Iteration 150, Current loss 0.6477574110031128 Accuracy 99.12610090803578\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 99, Probability Accuracy 72.2084140155468\n",
      "Starting Training\n",
      "Training:: Epoch 100, Iteration 0, Current loss 0.5588026642799377 Accuracy 99.27932737221407\n",
      "Training:: Epoch 100, Iteration 10, Current loss 0.5234925150871277 Accuracy 99.36262473225014\n",
      "Training:: Epoch 100, Iteration 20, Current loss 0.5128534436225891 Accuracy 99.29426204357165\n",
      "Training:: Epoch 100, Iteration 30, Current loss 0.7033899426460266 Accuracy 99.04436464728367\n",
      "Training:: Epoch 100, Iteration 40, Current loss 0.5350517630577087 Accuracy 98.6791088913715\n",
      "Training:: Epoch 100, Iteration 50, Current loss 0.45642465353012085 Accuracy 99.1869312248247\n",
      "Training:: Epoch 100, Iteration 60, Current loss 0.5004692077636719 Accuracy 99.13351464170515\n",
      "Training:: Epoch 100, Iteration 70, Current loss 0.4563005566596985 Accuracy 99.45225160508238\n",
      "Training:: Epoch 100, Iteration 80, Current loss 0.48948508501052856 Accuracy 99.33192294844672\n",
      "Training:: Epoch 100, Iteration 90, Current loss 0.44881314039230347 Accuracy 99.05372998898179\n",
      "Training:: Epoch 100, Iteration 100, Current loss 0.5026810765266418 Accuracy 99.25521350546177\n",
      "Training:: Epoch 100, Iteration 110, Current loss 0.5108098983764648 Accuracy 98.98318872017353\n",
      "Training:: Epoch 100, Iteration 120, Current loss 0.4200066030025482 Accuracy 99.12679008033531\n",
      "Training:: Epoch 100, Iteration 130, Current loss 0.5342907905578613 Accuracy 99.17733743472351\n",
      "Training:: Epoch 100, Iteration 140, Current loss 0.5855405926704407 Accuracy 99.2274678111588\n",
      "Training:: Epoch 100, Iteration 150, Current loss 0.5184197425842285 Accuracy 99.24588848776574\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 100, Probability Accuracy 72.28173949922154\n",
      "Starting Training\n",
      "Training:: Epoch 101, Iteration 0, Current loss 0.46990877389907837 Accuracy 99.33455827581069\n",
      "Training:: Epoch 101, Iteration 10, Current loss 0.7285093665122986 Accuracy 98.99406016478252\n",
      "Training:: Epoch 101, Iteration 20, Current loss 0.552222728729248 Accuracy 98.91355249601948\n",
      "Training:: Epoch 101, Iteration 30, Current loss 0.553952693939209 Accuracy 98.96972690706372\n",
      "Training:: Epoch 101, Iteration 40, Current loss 0.7680672407150269 Accuracy 98.78624409979771\n",
      "Training:: Epoch 101, Iteration 50, Current loss 0.5073353052139282 Accuracy 99.1655377946356\n",
      "Training:: Epoch 101, Iteration 60, Current loss 0.5124136209487915 Accuracy 99.02576489533011\n",
      "Training:: Epoch 101, Iteration 70, Current loss 0.6214527487754822 Accuracy 98.83761276891047\n",
      "Training:: Epoch 101, Iteration 80, Current loss 0.3906698226928711 Accuracy 99.37323337839499\n",
      "Training:: Epoch 101, Iteration 90, Current loss 0.5305679440498352 Accuracy 99.26254100888876\n",
      "Training:: Epoch 101, Iteration 100, Current loss 0.4459032416343689 Accuracy 99.21458296700077\n",
      "Training:: Epoch 101, Iteration 110, Current loss 0.4896075427532196 Accuracy 99.28232368896926\n",
      "Training:: Epoch 101, Iteration 120, Current loss 0.4612720310688019 Accuracy 98.8565570538601\n",
      "Training:: Epoch 101, Iteration 130, Current loss 0.4526263475418091 Accuracy 99.04320987654322\n",
      "Training:: Epoch 101, Iteration 140, Current loss 0.5755952596664429 Accuracy 98.61774964474874\n",
      "Training:: Epoch 101, Iteration 150, Current loss 0.528372585773468 Accuracy 98.89115844762183\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 101, Probability Accuracy 71.70808198614964\n",
      "Starting Training\n",
      "Training:: Epoch 102, Iteration 0, Current loss 0.4965580105781555 Accuracy 99.26368583929143\n",
      "Training:: Epoch 102, Iteration 10, Current loss 0.40487104654312134 Accuracy 99.32549104252105\n",
      "Training:: Epoch 102, Iteration 20, Current loss 0.5302215814590454 Accuracy 99.02863630629494\n",
      "Training:: Epoch 102, Iteration 30, Current loss 0.42407408356666565 Accuracy 99.29257576885844\n",
      "Training:: Epoch 102, Iteration 40, Current loss 0.6335433721542358 Accuracy 99.138302455838\n",
      "Training:: Epoch 102, Iteration 50, Current loss 0.544235110282898 Accuracy 99.02667290594292\n",
      "Training:: Epoch 102, Iteration 60, Current loss 0.366621732711792 Accuracy 99.47964278180156\n",
      "Training:: Epoch 102, Iteration 70, Current loss 0.5428891181945801 Accuracy 98.9423171090001\n",
      "Training:: Epoch 102, Iteration 80, Current loss 0.5374428033828735 Accuracy 98.76228339959493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 102, Iteration 90, Current loss 0.43368104100227356 Accuracy 99.449786854367\n",
      "Training:: Epoch 102, Iteration 100, Current loss 0.5474727153778076 Accuracy 98.48211848211848\n",
      "Training:: Epoch 102, Iteration 110, Current loss 0.3846713900566101 Accuracy 99.4938775510204\n",
      "Training:: Epoch 102, Iteration 120, Current loss 0.46683403849601746 Accuracy 99.10637297170182\n",
      "Training:: Epoch 102, Iteration 130, Current loss 0.5805294513702393 Accuracy 99.31158489596712\n",
      "Training:: Epoch 102, Iteration 140, Current loss 0.4725301265716553 Accuracy 99.17009693267826\n",
      "Training:: Epoch 102, Iteration 150, Current loss 0.39648672938346863 Accuracy 99.36825627550735\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 102, Probability Accuracy 71.33855279825447\n",
      "Starting Training\n",
      "Training:: Epoch 103, Iteration 0, Current loss 0.3868941068649292 Accuracy 99.35738372462939\n",
      "Training:: Epoch 103, Iteration 10, Current loss 0.6136025190353394 Accuracy 99.52899014463074\n",
      "Training:: Epoch 103, Iteration 20, Current loss 0.4372970461845398 Accuracy 99.13103927702468\n",
      "Training:: Epoch 103, Iteration 30, Current loss 0.5841943025588989 Accuracy 99.06041624570621\n",
      "Training:: Epoch 103, Iteration 40, Current loss 0.4267377555370331 Accuracy 99.03410445981397\n",
      "Training:: Epoch 103, Iteration 50, Current loss 0.5249404311180115 Accuracy 99.27298429848034\n",
      "Training:: Epoch 103, Iteration 60, Current loss 0.6373252868652344 Accuracy 99.2528696597161\n",
      "Training:: Epoch 103, Iteration 70, Current loss 0.5198530554771423 Accuracy 98.89542232236083\n",
      "Training:: Epoch 103, Iteration 80, Current loss 0.5732307434082031 Accuracy 99.06703985072637\n",
      "Training:: Epoch 103, Iteration 90, Current loss 0.5673104524612427 Accuracy 99.00302114803625\n",
      "Training:: Epoch 103, Iteration 100, Current loss 0.6031047105789185 Accuracy 99.11433973529705\n",
      "Training:: Epoch 103, Iteration 110, Current loss 0.36030128598213196 Accuracy 99.32298652359967\n",
      "Training:: Epoch 103, Iteration 120, Current loss 0.6388713717460632 Accuracy 98.93705826701542\n",
      "Training:: Epoch 103, Iteration 130, Current loss 0.4586608111858368 Accuracy 99.20783918677596\n",
      "Training:: Epoch 103, Iteration 140, Current loss 0.45148617029190063 Accuracy 99.08746423323795\n",
      "Training:: Epoch 103, Iteration 150, Current loss 0.4047562777996063 Accuracy 99.35386307396206\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 103, Probability Accuracy 71.38029363675426\n",
      "Starting Training\n",
      "Training:: Epoch 104, Iteration 0, Current loss 0.4147967994213104 Accuracy 99.30009690965866\n",
      "Training:: Epoch 104, Iteration 10, Current loss 0.5349390506744385 Accuracy 99.0910540583639\n",
      "Training:: Epoch 104, Iteration 20, Current loss 0.3219285309314728 Accuracy 99.17472294270219\n",
      "Training:: Epoch 104, Iteration 30, Current loss 0.6531839370727539 Accuracy 98.82987753122347\n",
      "Training:: Epoch 104, Iteration 40, Current loss 0.4973832368850708 Accuracy 98.88503637228908\n",
      "Training:: Epoch 104, Iteration 50, Current loss 0.3952440023422241 Accuracy 99.21347769092586\n",
      "Training:: Epoch 104, Iteration 60, Current loss 0.42965999245643616 Accuracy 99.23469387755102\n",
      "Training:: Epoch 104, Iteration 70, Current loss 0.3797583281993866 Accuracy 98.66297833102813\n",
      "Training:: Epoch 104, Iteration 80, Current loss 0.5099313855171204 Accuracy 99.06363794000347\n",
      "Training:: Epoch 104, Iteration 90, Current loss 0.45336204767227173 Accuracy 99.01234567901234\n",
      "Training:: Epoch 104, Iteration 100, Current loss 0.3919781446456909 Accuracy 99.38314711359403\n",
      "Training:: Epoch 104, Iteration 110, Current loss 0.4902673065662384 Accuracy 99.35120945059066\n",
      "Training:: Epoch 104, Iteration 120, Current loss 0.38646700978279114 Accuracy 99.16194380803087\n",
      "Training:: Epoch 104, Iteration 130, Current loss 0.41598206758499146 Accuracy 99.38721653173502\n",
      "Training:: Epoch 104, Iteration 140, Current loss 0.40534508228302 Accuracy 98.92652380758753\n",
      "Training:: Epoch 104, Iteration 150, Current loss 0.4126318097114563 Accuracy 99.1459584550113\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 104, Probability Accuracy 71.52147588462118\n",
      "Starting Training\n",
      "Training:: Epoch 105, Iteration 0, Current loss 0.5211681723594666 Accuracy 99.25834363411619\n",
      "Training:: Epoch 105, Iteration 10, Current loss 0.34209468960762024 Accuracy 99.53549719004474\n",
      "Training:: Epoch 105, Iteration 20, Current loss 0.41517752408981323 Accuracy 99.23519884829945\n",
      "Training:: Epoch 105, Iteration 30, Current loss 0.52169269323349 Accuracy 99.13186451825058\n",
      "Training:: Epoch 105, Iteration 40, Current loss 0.3628654181957245 Accuracy 99.30444515510074\n",
      "Training:: Epoch 105, Iteration 50, Current loss 0.4358122944831848 Accuracy 98.6487386008121\n",
      "Training:: Epoch 105, Iteration 60, Current loss 0.5700557827949524 Accuracy 99.0502926770995\n",
      "Training:: Epoch 105, Iteration 70, Current loss 0.4533531963825226 Accuracy 99.32964835940257\n",
      "Training:: Epoch 105, Iteration 80, Current loss 0.42205268144607544 Accuracy 99.4330938482263\n",
      "Training:: Epoch 105, Iteration 90, Current loss 0.4236357510089874 Accuracy 99.2389506026944\n",
      "Training:: Epoch 105, Iteration 100, Current loss 0.4003162980079651 Accuracy 99.36515221049412\n",
      "Training:: Epoch 105, Iteration 110, Current loss 0.44372889399528503 Accuracy 99.33785907768451\n",
      "Training:: Epoch 105, Iteration 120, Current loss 0.5445553064346313 Accuracy 99.12027689645227\n",
      "Training:: Epoch 105, Iteration 130, Current loss 0.4740910828113556 Accuracy 99.02316993269403\n",
      "Training:: Epoch 105, Iteration 140, Current loss 0.5231220126152039 Accuracy 99.08454227113556\n",
      "Training:: Epoch 105, Iteration 150, Current loss 0.34269875288009644 Accuracy 99.45843362340307\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 105, Probability Accuracy 70.78822104787362\n",
      "Starting Training\n",
      "Training:: Epoch 106, Iteration 0, Current loss 0.41804176568984985 Accuracy 99.16285824114809\n",
      "Training:: Epoch 106, Iteration 10, Current loss 0.5285777449607849 Accuracy 98.50806220232973\n",
      "Training:: Epoch 106, Iteration 20, Current loss 0.5651883482933044 Accuracy 98.59042553191489\n",
      "Training:: Epoch 106, Iteration 30, Current loss 0.46119722723960876 Accuracy 98.98000540394489\n",
      "Training:: Epoch 106, Iteration 40, Current loss 0.4460901618003845 Accuracy 98.8714387183332\n",
      "Training:: Epoch 106, Iteration 50, Current loss 0.4855341613292694 Accuracy 98.9851395433128\n",
      "Training:: Epoch 106, Iteration 60, Current loss 0.3340796232223511 Accuracy 99.03197111611114\n",
      "Training:: Epoch 106, Iteration 70, Current loss 0.3657257854938507 Accuracy 99.2824064416538\n",
      "Training:: Epoch 106, Iteration 80, Current loss 0.42510417103767395 Accuracy 98.8565358660335\n",
      "Training:: Epoch 106, Iteration 90, Current loss 0.48363620042800903 Accuracy 99.19790220576894\n",
      "Training:: Epoch 106, Iteration 100, Current loss 0.45516157150268555 Accuracy 99.07174059139786\n",
      "Training:: Epoch 106, Iteration 110, Current loss 0.5676498413085938 Accuracy 98.72291703364453\n",
      "Training:: Epoch 106, Iteration 120, Current loss 0.5089613795280457 Accuracy 99.17485265225933\n",
      "Training:: Epoch 106, Iteration 130, Current loss 0.493095338344574 Accuracy 98.82002974714923\n",
      "Training:: Epoch 106, Iteration 140, Current loss 0.521265983581543 Accuracy 98.96210990260894\n",
      "Training:: Epoch 106, Iteration 150, Current loss 0.5130507946014404 Accuracy 98.4438947605127\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 106, Probability Accuracy 68.76267431543351\n",
      "Starting Training\n",
      "Training:: Epoch 107, Iteration 0, Current loss 0.43551376461982727 Accuracy 99.01882258710452\n",
      "Training:: Epoch 107, Iteration 10, Current loss 1.0439852476119995 Accuracy 93.70570570570571\n",
      "Training:: Epoch 107, Iteration 20, Current loss 0.5997697710990906 Accuracy 98.46440204746393\n",
      "Training:: Epoch 107, Iteration 30, Current loss 0.5311431884765625 Accuracy 98.31990887641363\n",
      "Training:: Epoch 107, Iteration 40, Current loss 0.481358140707016 Accuracy 98.58384293530737\n",
      "Training:: Epoch 107, Iteration 50, Current loss 0.8930277228355408 Accuracy 96.11580533176132\n",
      "Training:: Epoch 107, Iteration 60, Current loss 0.9346387386322021 Accuracy 96.84875361150306\n",
      "Training:: Epoch 107, Iteration 70, Current loss 3.004788398742676 Accuracy 85.68675778624244\n",
      "Training:: Epoch 107, Iteration 80, Current loss 7.483117580413818 Accuracy 41.296148922182624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 107, Iteration 90, Current loss 6.018483638763428 Accuracy 68.34957067371202\n",
      "Training:: Epoch 107, Iteration 100, Current loss 4.259667873382568 Accuracy 60.92681984939178\n",
      "Training:: Epoch 107, Iteration 110, Current loss 7.621645927429199 Accuracy 47.889888012357254\n",
      "Training:: Epoch 107, Iteration 120, Current loss 3.610586166381836 Accuracy 77.18524889796797\n",
      "Training:: Epoch 107, Iteration 130, Current loss 3.153897523880005 Accuracy 83.78521845651287\n",
      "Training:: Epoch 107, Iteration 140, Current loss 2.1978704929351807 Accuracy 85.59985268391492\n",
      "Training:: Epoch 107, Iteration 150, Current loss 2.4146037101745605 Accuracy 85.68080235473673\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 107, Probability Accuracy 70.208871602279\n",
      "Starting Training\n",
      "Training:: Epoch 108, Iteration 0, Current loss 1.950533390045166 Accuracy 92.47809481015503\n",
      "Training:: Epoch 108, Iteration 10, Current loss 1.4203135967254639 Accuracy 96.69197870297634\n",
      "Training:: Epoch 108, Iteration 20, Current loss 1.426919937133789 Accuracy 94.29952572053995\n",
      "Training:: Epoch 108, Iteration 30, Current loss 1.5349868535995483 Accuracy 94.79179167166687\n",
      "Training:: Epoch 108, Iteration 40, Current loss 1.1318821907043457 Accuracy 95.4312172263751\n",
      "Training:: Epoch 108, Iteration 50, Current loss 1.267788290977478 Accuracy 93.79564246484398\n",
      "Training:: Epoch 108, Iteration 60, Current loss 1.604000210762024 Accuracy 94.75233369775461\n",
      "Training:: Epoch 108, Iteration 70, Current loss 1.315205454826355 Accuracy 96.58949979330302\n",
      "Training:: Epoch 108, Iteration 80, Current loss 1.1792798042297363 Accuracy 95.84325611604244\n",
      "Training:: Epoch 108, Iteration 90, Current loss 1.4332727193832397 Accuracy 96.66418314819832\n",
      "Training:: Epoch 108, Iteration 100, Current loss 1.5161665678024292 Accuracy 94.93124218661211\n",
      "Training:: Epoch 108, Iteration 110, Current loss 1.6259074211120605 Accuracy 92.80238924838228\n",
      "Training:: Epoch 108, Iteration 120, Current loss 1.548006296157837 Accuracy 94.12577747062889\n",
      "Training:: Epoch 108, Iteration 130, Current loss 1.3594352006912231 Accuracy 96.64749236023728\n",
      "Training:: Epoch 108, Iteration 140, Current loss 0.9180717468261719 Accuracy 96.53800824985268\n",
      "Training:: Epoch 108, Iteration 150, Current loss 0.825921893119812 Accuracy 98.57271530520336\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 108, Probability Accuracy 72.40796647340137\n",
      "Starting Training\n",
      "Training:: Epoch 109, Iteration 0, Current loss 0.9933158159255981 Accuracy 97.57258819856385\n",
      "Training:: Epoch 109, Iteration 10, Current loss 0.789567232131958 Accuracy 98.5062736506672\n",
      "Training:: Epoch 109, Iteration 20, Current loss 0.9310897588729858 Accuracy 98.22970639032815\n",
      "Training:: Epoch 109, Iteration 30, Current loss 0.8345271348953247 Accuracy 97.8677507674438\n",
      "Training:: Epoch 109, Iteration 40, Current loss 0.621830940246582 Accuracy 98.58876822115106\n",
      "Training:: Epoch 109, Iteration 50, Current loss 0.5738535523414612 Accuracy 99.06354051054385\n",
      "Training:: Epoch 109, Iteration 60, Current loss 0.863998532295227 Accuracy 97.36334405144694\n",
      "Training:: Epoch 109, Iteration 70, Current loss 1.0570632219314575 Accuracy 98.26778242677824\n",
      "Training:: Epoch 109, Iteration 80, Current loss 0.6913632750511169 Accuracy 98.68648648648649\n",
      "Training:: Epoch 109, Iteration 90, Current loss 0.7937323451042175 Accuracy 98.9060446780552\n",
      "Training:: Epoch 109, Iteration 100, Current loss 0.8608680367469788 Accuracy 98.59548646621667\n",
      "Training:: Epoch 109, Iteration 110, Current loss 0.8401352763175964 Accuracy 98.19325055714741\n",
      "Training:: Epoch 109, Iteration 120, Current loss 0.618703305721283 Accuracy 98.48455410918982\n",
      "Training:: Epoch 109, Iteration 130, Current loss 0.792486310005188 Accuracy 98.59232898378806\n",
      "Training:: Epoch 109, Iteration 140, Current loss 0.8211466670036316 Accuracy 98.5644965628791\n",
      "Training:: Epoch 109, Iteration 150, Current loss 0.6480202674865723 Accuracy 98.94650655021834\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 109, Probability Accuracy 73.84400756692206\n",
      "Starting Training\n",
      "Training:: Epoch 110, Iteration 0, Current loss 0.6264486908912659 Accuracy 99.32144711788125\n",
      "Training:: Epoch 110, Iteration 10, Current loss 0.5943098664283752 Accuracy 98.86579506090067\n",
      "Training:: Epoch 110, Iteration 20, Current loss 0.6931477785110474 Accuracy 98.43284299586578\n",
      "Training:: Epoch 110, Iteration 30, Current loss 0.5839660167694092 Accuracy 99.14432401597261\n",
      "Training:: Epoch 110, Iteration 40, Current loss 0.7155110836029053 Accuracy 98.75473994663172\n",
      "Training:: Epoch 110, Iteration 50, Current loss 0.5716177821159363 Accuracy 99.18755104672657\n",
      "Training:: Epoch 110, Iteration 60, Current loss 0.49341678619384766 Accuracy 99.17104172423322\n",
      "Training:: Epoch 110, Iteration 70, Current loss 0.6274733543395996 Accuracy 99.28242424242424\n",
      "Training:: Epoch 110, Iteration 80, Current loss 0.5971090197563171 Accuracy 99.06872190109185\n",
      "Training:: Epoch 110, Iteration 90, Current loss 0.49971410632133484 Accuracy 98.74091150913283\n",
      "Training:: Epoch 110, Iteration 100, Current loss 0.5399391055107117 Accuracy 98.98209467745892\n",
      "Training:: Epoch 110, Iteration 110, Current loss 0.418092280626297 Accuracy 99.24225237079722\n",
      "Training:: Epoch 110, Iteration 120, Current loss 0.7496910691261292 Accuracy 98.90911884458785\n",
      "Training:: Epoch 110, Iteration 130, Current loss 0.46229782700538635 Accuracy 98.93657911595132\n",
      "Training:: Epoch 110, Iteration 140, Current loss 0.511551022529602 Accuracy 99.2320854722257\n",
      "Training:: Epoch 110, Iteration 150, Current loss 0.5880189538002014 Accuracy 99.16327853396972\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 110, Probability Accuracy 73.85081556464529\n",
      "Starting Training\n",
      "Training:: Epoch 111, Iteration 0, Current loss 0.5050455331802368 Accuracy 98.95071542130366\n",
      "Training:: Epoch 111, Iteration 10, Current loss 0.515839695930481 Accuracy 98.96177685950413\n",
      "Training:: Epoch 111, Iteration 20, Current loss 0.732200026512146 Accuracy 99.2256899909743\n",
      "Training:: Epoch 111, Iteration 30, Current loss 0.5504652261734009 Accuracy 99.18125770653513\n",
      "Training:: Epoch 111, Iteration 40, Current loss 0.5100477337837219 Accuracy 98.96699266503667\n",
      "Training:: Epoch 111, Iteration 50, Current loss 0.6421763300895691 Accuracy 99.22987800722416\n",
      "Training:: Epoch 111, Iteration 60, Current loss 0.5861778855323792 Accuracy 99.08357449793914\n",
      "Training:: Epoch 111, Iteration 70, Current loss 0.6457524299621582 Accuracy 99.08206225412381\n",
      "Training:: Epoch 111, Iteration 80, Current loss 0.5535488724708557 Accuracy 99.01067430356677\n",
      "Training:: Epoch 111, Iteration 90, Current loss 0.6083008050918579 Accuracy 99.11971830985915\n",
      "Training:: Epoch 111, Iteration 100, Current loss 0.4643280506134033 Accuracy 99.37565036420395\n",
      "Training:: Epoch 111, Iteration 110, Current loss 0.624559760093689 Accuracy 99.1708934841982\n",
      "Training:: Epoch 111, Iteration 120, Current loss 0.5281925201416016 Accuracy 99.19495813782316\n",
      "Training:: Epoch 111, Iteration 130, Current loss 0.5638144016265869 Accuracy 99.32526756630992\n",
      "Training:: Epoch 111, Iteration 140, Current loss 0.6165806651115417 Accuracy 98.7888046265481\n",
      "Training:: Epoch 111, Iteration 150, Current loss 0.6589142680168152 Accuracy 99.17483855536953\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 111, Probability Accuracy 73.84099419088064\n",
      "Starting Training\n",
      "Training:: Epoch 112, Iteration 0, Current loss 0.5491254329681396 Accuracy 98.87126521578753\n",
      "Training:: Epoch 112, Iteration 10, Current loss 0.5245281457901001 Accuracy 99.00054157845503\n",
      "Training:: Epoch 112, Iteration 20, Current loss 0.5019258260726929 Accuracy 99.34080739065583\n",
      "Training:: Epoch 112, Iteration 30, Current loss 0.84731125831604 Accuracy 98.34043848964677\n",
      "Training:: Epoch 112, Iteration 40, Current loss 0.43385836482048035 Accuracy 99.2066640222134\n",
      "Training:: Epoch 112, Iteration 50, Current loss 0.4773702621459961 Accuracy 99.29014392136979\n",
      "Training:: Epoch 112, Iteration 60, Current loss 0.6226959228515625 Accuracy 99.1009071204945\n",
      "Training:: Epoch 112, Iteration 70, Current loss 0.6908780932426453 Accuracy 99.0427667129844\n",
      "Training:: Epoch 112, Iteration 80, Current loss 0.44940274953842163 Accuracy 99.33957072096862\n",
      "Training:: Epoch 112, Iteration 90, Current loss 0.5522893667221069 Accuracy 99.22874220708272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 112, Iteration 100, Current loss 0.6354223489761353 Accuracy 98.92703862660944\n",
      "Training:: Epoch 112, Iteration 110, Current loss 0.42636141180992126 Accuracy 99.36370063629937\n",
      "Training:: Epoch 112, Iteration 120, Current loss 0.6466131806373596 Accuracy 98.88522751174368\n",
      "Training:: Epoch 112, Iteration 130, Current loss 0.46119821071624756 Accuracy 99.4311316955296\n",
      "Training:: Epoch 112, Iteration 140, Current loss 0.4959915280342102 Accuracy 99.2893766303859\n",
      "Training:: Epoch 112, Iteration 150, Current loss 0.5474108457565308 Accuracy 99.32862498910106\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 112, Probability Accuracy 73.81186488914682\n",
      "Starting Training\n",
      "Training:: Epoch 113, Iteration 0, Current loss 0.5803200006484985 Accuracy 99.1034694504145\n",
      "Training:: Epoch 113, Iteration 10, Current loss 0.39496487379074097 Accuracy 99.43280034572169\n",
      "Training:: Epoch 113, Iteration 20, Current loss 0.4212060272693634 Accuracy 99.33115413628363\n",
      "Training:: Epoch 113, Iteration 30, Current loss 0.5587597489356995 Accuracy 98.90925233052191\n",
      "Training:: Epoch 113, Iteration 40, Current loss 0.5706380009651184 Accuracy 99.14123880869724\n",
      "Training:: Epoch 113, Iteration 50, Current loss 0.5384657382965088 Accuracy 99.33269241644517\n",
      "Training:: Epoch 113, Iteration 60, Current loss 0.5669177174568176 Accuracy 99.15936111444698\n",
      "Training:: Epoch 113, Iteration 70, Current loss 0.47748228907585144 Accuracy 99.10928143712574\n",
      "Training:: Epoch 113, Iteration 80, Current loss 0.46012353897094727 Accuracy 99.37137098709151\n",
      "Training:: Epoch 113, Iteration 90, Current loss 0.3729418218135834 Accuracy 99.24803591470258\n",
      "Training:: Epoch 113, Iteration 100, Current loss 0.5661160349845886 Accuracy 99.10636713416905\n",
      "Training:: Epoch 113, Iteration 110, Current loss 0.4426143765449524 Accuracy 99.33535691856814\n",
      "Training:: Epoch 113, Iteration 120, Current loss 0.6380680799484253 Accuracy 98.85805964514911\n",
      "Training:: Epoch 113, Iteration 130, Current loss 0.49700161814689636 Accuracy 99.44622364558313\n",
      "Training:: Epoch 113, Iteration 140, Current loss 0.6697432398796082 Accuracy 98.27524254401725\n",
      "Training:: Epoch 113, Iteration 150, Current loss 0.5155368447303772 Accuracy 99.2421272703515\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 113, Probability Accuracy 73.14311862098984\n",
      "Starting Training\n",
      "Training:: Epoch 114, Iteration 0, Current loss 0.6038233041763306 Accuracy 99.1252204585538\n",
      "Training:: Epoch 114, Iteration 10, Current loss 0.648284912109375 Accuracy 98.91988602478298\n",
      "Training:: Epoch 114, Iteration 20, Current loss 0.5212779641151428 Accuracy 99.31106264869152\n",
      "Training:: Epoch 114, Iteration 30, Current loss 0.5700435042381287 Accuracy 99.17639910889083\n",
      "Training:: Epoch 114, Iteration 40, Current loss 0.3878256380558014 Accuracy 99.59886838660643\n",
      "Training:: Epoch 114, Iteration 50, Current loss 0.7277998924255371 Accuracy 99.02588366267743\n",
      "Training:: Epoch 114, Iteration 60, Current loss 0.43240734934806824 Accuracy 99.33516251582947\n",
      "Training:: Epoch 114, Iteration 70, Current loss 0.4518369734287262 Accuracy 99.33106639166063\n",
      "Training:: Epoch 114, Iteration 80, Current loss 0.5714375972747803 Accuracy 99.19618064013251\n",
      "Training:: Epoch 114, Iteration 90, Current loss 0.4664830267429352 Accuracy 99.19215233698789\n",
      "Training:: Epoch 114, Iteration 100, Current loss 0.6148135662078857 Accuracy 99.1036992775273\n",
      "Training:: Epoch 114, Iteration 110, Current loss 0.4818094074726105 Accuracy 99.21893679764086\n",
      "Training:: Epoch 114, Iteration 120, Current loss 0.5449200868606567 Accuracy 99.16129032258064\n",
      "Training:: Epoch 114, Iteration 130, Current loss 0.6301008462905884 Accuracy 99.3982761424622\n",
      "Training:: Epoch 114, Iteration 140, Current loss 0.4512366056442261 Accuracy 99.01022229433717\n",
      "Training:: Epoch 114, Iteration 150, Current loss 0.4184049367904663 Accuracy 99.31907776848644\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 114, Probability Accuracy 72.63039826786681\n",
      "Starting Training\n",
      "Training:: Epoch 115, Iteration 0, Current loss 0.45272573828697205 Accuracy 99.31832593532023\n",
      "Training:: Epoch 115, Iteration 10, Current loss 0.4938609302043915 Accuracy 99.05973451327434\n",
      "Training:: Epoch 115, Iteration 20, Current loss 0.43960103392601013 Accuracy 99.49420546061677\n",
      "Training:: Epoch 115, Iteration 30, Current loss 0.5129743218421936 Accuracy 99.20324714371617\n",
      "Training:: Epoch 115, Iteration 40, Current loss 0.4628427028656006 Accuracy 99.38514090520923\n",
      "Training:: Epoch 115, Iteration 50, Current loss 0.5957754254341125 Accuracy 99.11993965300478\n",
      "Training:: Epoch 115, Iteration 60, Current loss 0.4146862030029297 Accuracy 99.28155580329494\n",
      "Training:: Epoch 115, Iteration 70, Current loss 0.6802605390548706 Accuracy 99.28244142820091\n",
      "Training:: Epoch 115, Iteration 80, Current loss 0.6130715012550354 Accuracy 99.12837937656965\n",
      "Training:: Epoch 115, Iteration 90, Current loss 0.6434237360954285 Accuracy 99.27555917775966\n",
      "Training:: Epoch 115, Iteration 100, Current loss 0.47214508056640625 Accuracy 99.47348144343135\n",
      "Training:: Epoch 115, Iteration 110, Current loss 0.43452540040016174 Accuracy 99.58955223880596\n",
      "Training:: Epoch 115, Iteration 120, Current loss 0.39188897609710693 Accuracy 99.44420067280971\n",
      "Training:: Epoch 115, Iteration 130, Current loss 0.5657445192337036 Accuracy 99.07244522629138\n",
      "Training:: Epoch 115, Iteration 140, Current loss 0.5605260729789734 Accuracy 99.03745796126638\n",
      "Training:: Epoch 115, Iteration 150, Current loss 0.383497029542923 Accuracy 99.30534940331295\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 115, Probability Accuracy 72.68274172577162\n",
      "Starting Training\n",
      "Training:: Epoch 116, Iteration 0, Current loss 0.4762362241744995 Accuracy 99.37430990062569\n",
      "Training:: Epoch 116, Iteration 10, Current loss 0.4753590226173401 Accuracy 99.33344084287481\n",
      "Training:: Epoch 116, Iteration 20, Current loss 0.3464873135089874 Accuracy 99.39479153930799\n",
      "Training:: Epoch 116, Iteration 30, Current loss 0.5193714499473572 Accuracy 99.2408608490566\n",
      "Training:: Epoch 116, Iteration 40, Current loss 0.458721399307251 Accuracy 99.17610133393117\n",
      "Training:: Epoch 116, Iteration 50, Current loss 0.44231054186820984 Accuracy 99.280619030347\n",
      "Training:: Epoch 116, Iteration 60, Current loss 0.4645243287086487 Accuracy 99.22579894056697\n",
      "Training:: Epoch 116, Iteration 70, Current loss 0.400177001953125 Accuracy 99.36059677634208\n",
      "Training:: Epoch 116, Iteration 80, Current loss 0.3703190088272095 Accuracy 99.48207349198553\n",
      "Training:: Epoch 116, Iteration 90, Current loss 0.4394432008266449 Accuracy 99.38468300186793\n",
      "Training:: Epoch 116, Iteration 100, Current loss 0.5976895689964294 Accuracy 99.42451328517204\n",
      "Training:: Epoch 116, Iteration 110, Current loss 0.5736045241355896 Accuracy 99.35636129585926\n",
      "Training:: Epoch 116, Iteration 120, Current loss 0.36000046133995056 Accuracy 99.47916666666667\n",
      "Training:: Epoch 116, Iteration 130, Current loss 0.5343153476715088 Accuracy 98.91710231516056\n",
      "Training:: Epoch 116, Iteration 140, Current loss 0.5620830655097961 Accuracy 99.00669769553865\n",
      "Training:: Epoch 116, Iteration 150, Current loss 0.5051879286766052 Accuracy 99.1728212703102\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 116, Probability Accuracy 72.60562162041506\n",
      "Starting Training\n",
      "Training:: Epoch 117, Iteration 0, Current loss 0.44321519136428833 Accuracy 99.18378378378378\n",
      "Training:: Epoch 117, Iteration 10, Current loss 0.4630194902420044 Accuracy 98.79043036621225\n",
      "Training:: Epoch 117, Iteration 20, Current loss 0.5309717655181885 Accuracy 99.06289525123606\n",
      "Training:: Epoch 117, Iteration 30, Current loss 0.5076477527618408 Accuracy 99.18978639823226\n",
      "Training:: Epoch 117, Iteration 40, Current loss 0.5172150731086731 Accuracy 99.42488672011153\n",
      "Training:: Epoch 117, Iteration 50, Current loss 0.5384625196456909 Accuracy 98.62468331523706\n",
      "Training:: Epoch 117, Iteration 60, Current loss 0.4595101475715637 Accuracy 99.19561243144425\n",
      "Training:: Epoch 117, Iteration 70, Current loss 0.5525989532470703 Accuracy 99.12695768077307\n",
      "Training:: Epoch 117, Iteration 80, Current loss 0.32156097888946533 Accuracy 99.33596656935143\n",
      "Training:: Epoch 117, Iteration 90, Current loss 0.40992915630340576 Accuracy 99.4881667918858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 117, Iteration 100, Current loss 0.545184850692749 Accuracy 99.02174730980511\n",
      "Training:: Epoch 117, Iteration 110, Current loss 0.44303470849990845 Accuracy 99.43233131208876\n",
      "Training:: Epoch 117, Iteration 120, Current loss 0.44293007254600525 Accuracy 99.28571428571429\n",
      "Training:: Epoch 117, Iteration 130, Current loss 0.4102137088775635 Accuracy 99.43972528465571\n",
      "Training:: Epoch 117, Iteration 140, Current loss 0.5463773012161255 Accuracy 99.35506275735476\n",
      "Training:: Epoch 117, Iteration 150, Current loss 0.5152355432510376 Accuracy 99.40265262731599\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 117, Probability Accuracy 72.52816669549836\n",
      "Starting Training\n",
      "Training:: Epoch 118, Iteration 0, Current loss 0.30324283242225647 Accuracy 99.42806209611528\n",
      "Training:: Epoch 118, Iteration 10, Current loss 0.46691009402275085 Accuracy 99.48920955178139\n",
      "Training:: Epoch 118, Iteration 20, Current loss 0.4578947126865387 Accuracy 99.28623057462956\n",
      "Training:: Epoch 118, Iteration 30, Current loss 0.6574908494949341 Accuracy 98.8905109489051\n",
      "Training:: Epoch 118, Iteration 40, Current loss 0.6207563281059265 Accuracy 99.26459880588321\n",
      "Training:: Epoch 118, Iteration 50, Current loss 0.4866524338722229 Accuracy 98.93468070858951\n",
      "Training:: Epoch 118, Iteration 60, Current loss 0.4738883078098297 Accuracy 98.62986757571784\n",
      "Training:: Epoch 118, Iteration 70, Current loss 0.29146334528923035 Accuracy 99.5738852533861\n",
      "Training:: Epoch 118, Iteration 80, Current loss 0.5433546900749207 Accuracy 98.93892339544513\n",
      "Training:: Epoch 118, Iteration 90, Current loss 0.5228548645973206 Accuracy 98.81384955937017\n",
      "Training:: Epoch 118, Iteration 100, Current loss 0.5596201419830322 Accuracy 98.92987890735004\n",
      "Training:: Epoch 118, Iteration 110, Current loss 0.4965214729309082 Accuracy 98.89023755852263\n",
      "Training:: Epoch 118, Iteration 120, Current loss 0.5191459655761719 Accuracy 98.72108331766033\n",
      "Training:: Epoch 118, Iteration 130, Current loss 0.7801220417022705 Accuracy 98.15000455663902\n",
      "Training:: Epoch 118, Iteration 140, Current loss 0.8944219350814819 Accuracy 96.90205211583249\n",
      "Training:: Epoch 118, Iteration 150, Current loss 6.029844760894775 Accuracy 80.23730422401519\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 118, Probability Accuracy 70.02773422023314\n",
      "Starting Training\n",
      "Training:: Epoch 119, Iteration 0, Current loss 0.9114536643028259 Accuracy 97.71581133194897\n",
      "Training:: Epoch 119, Iteration 10, Current loss 0.809823751449585 Accuracy 97.6219872242203\n",
      "Training:: Epoch 119, Iteration 20, Current loss 0.7674251794815063 Accuracy 98.20227197529502\n",
      "Training:: Epoch 119, Iteration 30, Current loss 0.685956597328186 Accuracy 98.49106358042778\n",
      "Training:: Epoch 119, Iteration 40, Current loss 0.5384856462478638 Accuracy 98.42344431048873\n",
      "Training:: Epoch 119, Iteration 50, Current loss 1.1297833919525146 Accuracy 97.63143275592142\n",
      "Training:: Epoch 119, Iteration 60, Current loss 0.9013249278068542 Accuracy 97.91577772672072\n",
      "Training:: Epoch 119, Iteration 70, Current loss 1.3571281433105469 Accuracy 95.34824062593528\n",
      "Training:: Epoch 119, Iteration 80, Current loss 0.9930764436721802 Accuracy 96.43257468687558\n",
      "Training:: Epoch 119, Iteration 90, Current loss 1.2298263311386108 Accuracy 93.82892057026477\n",
      "Training:: Epoch 119, Iteration 100, Current loss 1.1146986484527588 Accuracy 96.42139628327473\n",
      "Training:: Epoch 119, Iteration 110, Current loss 0.9709187746047974 Accuracy 97.62821812284614\n",
      "Training:: Epoch 119, Iteration 120, Current loss 0.785495400428772 Accuracy 96.23881966210534\n",
      "Training:: Epoch 119, Iteration 130, Current loss 0.7511041164398193 Accuracy 98.07359647743355\n",
      "Training:: Epoch 119, Iteration 140, Current loss 0.6163614392280579 Accuracy 98.08921623123958\n",
      "Training:: Epoch 119, Iteration 150, Current loss 1.037258267402649 Accuracy 97.26407315793419\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 119, Probability Accuracy 69.61691061991841\n",
      "Starting Training\n",
      "Training:: Epoch 120, Iteration 0, Current loss 1.4070079326629639 Accuracy 92.69083969465649\n",
      "Training:: Epoch 120, Iteration 10, Current loss 0.8329450488090515 Accuracy 97.79508906200174\n",
      "Training:: Epoch 120, Iteration 20, Current loss 0.9064429998397827 Accuracy 97.53892897835169\n",
      "Training:: Epoch 120, Iteration 30, Current loss 0.874062716960907 Accuracy 98.25959320612287\n",
      "Training:: Epoch 120, Iteration 40, Current loss 0.6288149952888489 Accuracy 98.69178440607013\n",
      "Training:: Epoch 120, Iteration 50, Current loss 0.6303650736808777 Accuracy 98.45282089184528\n",
      "Training:: Epoch 120, Iteration 60, Current loss 0.7472341060638428 Accuracy 98.08526393737438\n",
      "Training:: Epoch 120, Iteration 70, Current loss 0.7838196158409119 Accuracy 98.81617525031808\n",
      "Training:: Epoch 120, Iteration 80, Current loss 0.5847646594047546 Accuracy 98.99444483510301\n",
      "Training:: Epoch 120, Iteration 90, Current loss 0.681323230266571 Accuracy 98.63814567714424\n",
      "Training:: Epoch 120, Iteration 100, Current loss 0.7216073870658875 Accuracy 98.57175765952546\n",
      "Training:: Epoch 120, Iteration 110, Current loss 0.5884032845497131 Accuracy 98.71397480709622\n",
      "Training:: Epoch 120, Iteration 120, Current loss 0.5243355631828308 Accuracy 98.69880976397015\n",
      "Training:: Epoch 120, Iteration 130, Current loss 0.6179972887039185 Accuracy 98.47838452787258\n",
      "Training:: Epoch 120, Iteration 140, Current loss 0.4468774199485779 Accuracy 98.95438827597563\n",
      "Training:: Epoch 120, Iteration 150, Current loss 0.5121637582778931 Accuracy 99.04096679416818\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 120, Probability Accuracy 72.94245009793472\n",
      "Starting Training\n",
      "Training:: Epoch 121, Iteration 0, Current loss 0.378534734249115 Accuracy 99.20133111480865\n",
      "Training:: Epoch 121, Iteration 10, Current loss 0.6019762754440308 Accuracy 98.94240063683402\n",
      "Training:: Epoch 121, Iteration 20, Current loss 0.49684152007102966 Accuracy 99.04324055666004\n",
      "Training:: Epoch 121, Iteration 30, Current loss 0.8145399689674377 Accuracy 98.95100993192725\n",
      "Training:: Epoch 121, Iteration 40, Current loss 0.6104580760002136 Accuracy 98.94254494183997\n",
      "Training:: Epoch 121, Iteration 50, Current loss 0.4615482985973358 Accuracy 99.08840864440079\n",
      "Training:: Epoch 121, Iteration 60, Current loss 0.5944865345954895 Accuracy 98.80098022054962\n",
      "Training:: Epoch 121, Iteration 70, Current loss 0.5429468750953674 Accuracy 98.92891918208375\n",
      "Training:: Epoch 121, Iteration 80, Current loss 0.46542593836784363 Accuracy 99.14295708955224\n",
      "Training:: Epoch 121, Iteration 90, Current loss 0.50128173828125 Accuracy 99.18077061577242\n",
      "Training:: Epoch 121, Iteration 100, Current loss 0.5760959386825562 Accuracy 98.8225806451613\n",
      "Training:: Epoch 121, Iteration 110, Current loss 0.44106006622314453 Accuracy 99.19146859970725\n",
      "Training:: Epoch 121, Iteration 120, Current loss 0.5381692051887512 Accuracy 99.06065318818041\n",
      "Training:: Epoch 121, Iteration 130, Current loss 0.678532063961029 Accuracy 98.96547969073288\n",
      "Training:: Epoch 121, Iteration 140, Current loss 0.5079996585845947 Accuracy 99.26997783861296\n",
      "Training:: Epoch 121, Iteration 150, Current loss 0.6131107807159424 Accuracy 98.95428138418792\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 121, Probability Accuracy 72.97113297358831\n",
      "Starting Training\n",
      "Training:: Epoch 122, Iteration 0, Current loss 0.3744690418243408 Accuracy 98.97781479518437\n",
      "Training:: Epoch 122, Iteration 10, Current loss 0.49281832575798035 Accuracy 99.11299652911686\n",
      "Training:: Epoch 122, Iteration 20, Current loss 0.4149952232837677 Accuracy 99.36927466586575\n",
      "Training:: Epoch 122, Iteration 30, Current loss 0.4497668743133545 Accuracy 98.93809145416456\n",
      "Training:: Epoch 122, Iteration 40, Current loss 0.3570901155471802 Accuracy 99.20295809367296\n",
      "Training:: Epoch 122, Iteration 50, Current loss 0.8151712417602539 Accuracy 98.93030794165315\n",
      "Training:: Epoch 122, Iteration 60, Current loss 0.49820128083229065 Accuracy 98.94218134034166\n",
      "Training:: Epoch 122, Iteration 70, Current loss 0.5580033659934998 Accuracy 99.14920237722865\n",
      "Training:: Epoch 122, Iteration 80, Current loss 0.5413376688957214 Accuracy 99.33392981409683\n",
      "Training:: Epoch 122, Iteration 90, Current loss 0.3388932943344116 Accuracy 99.34715091589023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 122, Iteration 100, Current loss 0.42942705750465393 Accuracy 99.06386701662292\n",
      "Training:: Epoch 122, Iteration 110, Current loss 0.2811667323112488 Accuracy 99.54638957750936\n",
      "Training:: Epoch 122, Iteration 120, Current loss 0.523615837097168 Accuracy 99.24899381768391\n",
      "Training:: Epoch 122, Iteration 130, Current loss 0.32860004901885986 Accuracy 99.34764617107248\n",
      "Training:: Epoch 122, Iteration 140, Current loss 0.5331856608390808 Accuracy 98.96098604461648\n",
      "Training:: Epoch 122, Iteration 150, Current loss 0.3738378584384918 Accuracy 99.16552996870738\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 122, Probability Accuracy 73.1128732540555\n",
      "Starting Training\n",
      "Training:: Epoch 123, Iteration 0, Current loss 0.40330564975738525 Accuracy 99.20470977070853\n",
      "Training:: Epoch 123, Iteration 10, Current loss 0.5396700501441956 Accuracy 99.12697535639298\n",
      "Training:: Epoch 123, Iteration 20, Current loss 0.5106887817382812 Accuracy 99.14863144493953\n",
      "Training:: Epoch 123, Iteration 30, Current loss 0.38401761651039124 Accuracy 99.48318293683347\n",
      "Training:: Epoch 123, Iteration 40, Current loss 0.4508890211582184 Accuracy 99.0925854037267\n",
      "Training:: Epoch 123, Iteration 50, Current loss 0.4992518424987793 Accuracy 99.37706516440063\n",
      "Training:: Epoch 123, Iteration 60, Current loss 0.4914284944534302 Accuracy 99.50302947784056\n",
      "Training:: Epoch 123, Iteration 70, Current loss 0.4929099380970001 Accuracy 98.87393767705383\n",
      "Training:: Epoch 123, Iteration 80, Current loss 0.37081053853034973 Accuracy 99.41484822625868\n",
      "Training:: Epoch 123, Iteration 90, Current loss 0.6888869404792786 Accuracy 99.00902233397426\n",
      "Training:: Epoch 123, Iteration 100, Current loss 0.4897439479827881 Accuracy 99.53909532161357\n",
      "Training:: Epoch 123, Iteration 110, Current loss 0.32970741391181946 Accuracy 99.50745137661025\n",
      "Training:: Epoch 123, Iteration 120, Current loss 0.4059945344924927 Accuracy 99.53360498797191\n",
      "Training:: Epoch 123, Iteration 130, Current loss 0.3743117153644562 Accuracy 99.33661125419496\n",
      "Training:: Epoch 123, Iteration 140, Current loss 0.40990111231803894 Accuracy 99.29781918844103\n",
      "Training:: Epoch 123, Iteration 150, Current loss 0.4110794961452484 Accuracy 99.43157975050548\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 123, Probability Accuracy 72.89200395087082\n",
      "Starting Training\n",
      "Training:: Epoch 124, Iteration 0, Current loss 0.3885408937931061 Accuracy 99.26741752217681\n",
      "Training:: Epoch 124, Iteration 10, Current loss 0.4613460600376129 Accuracy 99.29351844245461\n",
      "Training:: Epoch 124, Iteration 20, Current loss 0.37232646346092224 Accuracy 99.36575052854123\n",
      "Training:: Epoch 124, Iteration 30, Current loss 0.44716429710388184 Accuracy 99.15404447309055\n",
      "Training:: Epoch 124, Iteration 40, Current loss 0.4278264343738556 Accuracy 99.44591465431691\n",
      "Training:: Epoch 124, Iteration 50, Current loss 0.4919275939464569 Accuracy 99.39725187720283\n",
      "Training:: Epoch 124, Iteration 60, Current loss 0.4391576647758484 Accuracy 99.31481343561481\n",
      "Training:: Epoch 124, Iteration 70, Current loss 0.4367269277572632 Accuracy 99.24948066742613\n",
      "Training:: Epoch 124, Iteration 80, Current loss 0.44682633876800537 Accuracy 99.4601686972821\n",
      "Training:: Epoch 124, Iteration 90, Current loss 0.3843187689781189 Accuracy 99.50113913111792\n",
      "Training:: Epoch 124, Iteration 100, Current loss 0.4014061689376831 Accuracy 99.52826179345516\n",
      "Training:: Epoch 124, Iteration 110, Current loss 0.4185253381729126 Accuracy 99.06729803415124\n",
      "Training:: Epoch 124, Iteration 120, Current loss 0.43976250290870667 Accuracy 99.36106579662861\n",
      "Training:: Epoch 124, Iteration 130, Current loss 0.3893057703971863 Accuracy 99.37270501835985\n",
      "Training:: Epoch 124, Iteration 140, Current loss 0.29112914204597473 Accuracy 99.5248126194092\n",
      "Training:: Epoch 124, Iteration 150, Current loss 0.5005685091018677 Accuracy 99.15732635195943\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 124, Probability Accuracy 72.59624667273062\n",
      "Starting Training\n",
      "Training:: Epoch 125, Iteration 0, Current loss 0.443442702293396 Accuracy 99.25496688741723\n",
      "Training:: Epoch 125, Iteration 10, Current loss 0.4792744815349579 Accuracy 99.37523312196942\n",
      "Training:: Epoch 125, Iteration 20, Current loss 0.33251315355300903 Accuracy 99.48758584257791\n",
      "Training:: Epoch 125, Iteration 30, Current loss 0.3287864029407501 Accuracy 99.50439725960838\n",
      "Training:: Epoch 125, Iteration 40, Current loss 0.320303738117218 Accuracy 99.401601348504\n",
      "Training:: Epoch 125, Iteration 50, Current loss 0.4001714885234833 Accuracy 99.381143065162\n",
      "Training:: Epoch 125, Iteration 60, Current loss 0.43637895584106445 Accuracy 99.23797912062791\n",
      "Training:: Epoch 125, Iteration 70, Current loss 0.3965287506580353 Accuracy 99.52345356008223\n",
      "Training:: Epoch 125, Iteration 80, Current loss 0.462053120136261 Accuracy 99.491024141963\n",
      "Training:: Epoch 125, Iteration 90, Current loss 0.3817850947380066 Accuracy 99.29162575396269\n",
      "Training:: Epoch 125, Iteration 100, Current loss 0.5662648677825928 Accuracy 99.27834470316289\n",
      "Training:: Epoch 125, Iteration 110, Current loss 0.4579697549343109 Accuracy 99.23312883435582\n",
      "Training:: Epoch 125, Iteration 120, Current loss 0.5130388140678406 Accuracy 99.202154183704\n",
      "Training:: Epoch 125, Iteration 130, Current loss 0.2767695486545563 Accuracy 99.66570430025833\n",
      "Training:: Epoch 125, Iteration 140, Current loss 0.41512590646743774 Accuracy 99.41420281845814\n",
      "Training:: Epoch 125, Iteration 150, Current loss 0.42955148220062256 Accuracy 99.05994760363693\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 125, Probability Accuracy 72.71443797746664\n",
      "Starting Training\n",
      "Training:: Epoch 126, Iteration 0, Current loss 0.3451032340526581 Accuracy 99.52095383414357\n",
      "Training:: Epoch 126, Iteration 10, Current loss 0.4040158689022064 Accuracy 99.49864089398973\n",
      "Training:: Epoch 126, Iteration 20, Current loss 0.32682478427886963 Accuracy 99.40805051302289\n",
      "Training:: Epoch 126, Iteration 30, Current loss 0.6858251094818115 Accuracy 98.98950131233596\n",
      "Training:: Epoch 126, Iteration 40, Current loss 0.3809131681919098 Accuracy 99.43783378618944\n",
      "Training:: Epoch 126, Iteration 50, Current loss 0.39329177141189575 Accuracy 99.52588593576965\n",
      "Training:: Epoch 126, Iteration 60, Current loss 0.4546925127506256 Accuracy 99.00709219858156\n",
      "Training:: Epoch 126, Iteration 70, Current loss 0.4039342403411865 Accuracy 99.27959146452672\n",
      "Training:: Epoch 126, Iteration 80, Current loss 0.32555723190307617 Accuracy 99.48974332543037\n",
      "Training:: Epoch 126, Iteration 90, Current loss 0.356067031621933 Accuracy 99.30330752990851\n",
      "Training:: Epoch 126, Iteration 100, Current loss 0.5186008810997009 Accuracy 99.33500812767844\n",
      "Training:: Epoch 126, Iteration 110, Current loss 0.34994006156921387 Accuracy 99.46336429308566\n",
      "Training:: Epoch 126, Iteration 120, Current loss 0.46898502111434937 Accuracy 99.38556067588326\n",
      "Training:: Epoch 126, Iteration 130, Current loss 0.4270489513874054 Accuracy 99.31890419252308\n",
      "Training:: Epoch 126, Iteration 140, Current loss 0.37807998061180115 Accuracy 99.43836509892022\n",
      "Training:: Epoch 126, Iteration 150, Current loss 0.3890765607357025 Accuracy 99.41676958968026\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 126, Probability Accuracy 71.86176416426248\n",
      "Starting Training\n",
      "Training:: Epoch 127, Iteration 0, Current loss 0.3306117355823517 Accuracy 99.36516698868341\n",
      "Training:: Epoch 127, Iteration 10, Current loss 0.4360654950141907 Accuracy 99.39617023036519\n",
      "Training:: Epoch 127, Iteration 20, Current loss 0.38723447918891907 Accuracy 99.20647350561211\n",
      "Training:: Epoch 127, Iteration 30, Current loss 0.42068493366241455 Accuracy 99.41831751083795\n",
      "Training:: Epoch 127, Iteration 40, Current loss 0.4374406039714813 Accuracy 99.45939403719692\n",
      "Training:: Epoch 127, Iteration 50, Current loss 0.36983832716941833 Accuracy 99.16812022004562\n",
      "Training:: Epoch 127, Iteration 60, Current loss 0.36425405740737915 Accuracy 99.34132284328973\n",
      "Training:: Epoch 127, Iteration 70, Current loss 0.4567960500717163 Accuracy 99.54880554324619\n",
      "Training:: Epoch 127, Iteration 80, Current loss 0.42415565252304077 Accuracy 99.35032483758121\n",
      "Training:: Epoch 127, Iteration 90, Current loss 0.32628771662712097 Accuracy 99.58677685950413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 127, Iteration 100, Current loss 0.34524762630462646 Accuracy 99.42769402568398\n",
      "Training:: Epoch 127, Iteration 110, Current loss 0.44502583146095276 Accuracy 99.05417675544794\n",
      "Training:: Epoch 127, Iteration 120, Current loss 0.4517441391944885 Accuracy 99.02969714789768\n",
      "Training:: Epoch 127, Iteration 130, Current loss 0.3732043504714966 Accuracy 99.26264326360503\n",
      "Training:: Epoch 127, Iteration 140, Current loss 0.386974960565567 Accuracy 99.42771084337349\n",
      "Training:: Epoch 127, Iteration 150, Current loss 0.38721543550491333 Accuracy 99.41198786039453\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 127, Probability Accuracy 71.91846027644935\n",
      "Starting Training\n",
      "Training:: Epoch 128, Iteration 0, Current loss 0.40469104051589966 Accuracy 99.33654221412775\n",
      "Training:: Epoch 128, Iteration 10, Current loss 0.3130948841571808 Accuracy 99.37834069254009\n",
      "Training:: Epoch 128, Iteration 20, Current loss 0.42414966225624084 Accuracy 99.33279321355383\n",
      "Training:: Epoch 128, Iteration 30, Current loss 0.45322751998901367 Accuracy 98.95613719339744\n",
      "Training:: Epoch 128, Iteration 40, Current loss 0.37375667691230774 Accuracy 99.42712063016731\n",
      "Training:: Epoch 128, Iteration 50, Current loss 0.35941311717033386 Accuracy 99.4041135387967\n",
      "Training:: Epoch 128, Iteration 60, Current loss 0.4148730933666229 Accuracy 99.04568527918782\n",
      "Training:: Epoch 128, Iteration 70, Current loss 0.517471969127655 Accuracy 99.21740921260383\n",
      "Training:: Epoch 128, Iteration 80, Current loss 0.43717697262763977 Accuracy 99.26383713532672\n",
      "Training:: Epoch 128, Iteration 90, Current loss 0.38086098432540894 Accuracy 99.2437006252574\n",
      "Training:: Epoch 128, Iteration 100, Current loss 0.2883029580116272 Accuracy 99.57008436080467\n",
      "Training:: Epoch 128, Iteration 110, Current loss 0.25575533509254456 Accuracy 99.608914604093\n",
      "Training:: Epoch 128, Iteration 120, Current loss 0.33641698956489563 Accuracy 99.5400847686897\n",
      "Training:: Epoch 128, Iteration 130, Current loss 0.3435925543308258 Accuracy 99.24751841178355\n",
      "Training:: Epoch 128, Iteration 140, Current loss 0.4068235754966736 Accuracy 99.44831591173055\n",
      "Training:: Epoch 128, Iteration 150, Current loss 0.39004677534103394 Accuracy 99.3678821439766\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 128, Probability Accuracy 72.03899531810649\n",
      "Starting Training\n",
      "Training:: Epoch 129, Iteration 0, Current loss 0.3891102075576782 Accuracy 99.48519948519949\n",
      "Training:: Epoch 129, Iteration 10, Current loss 0.36382436752319336 Accuracy 99.08264328624777\n",
      "Training:: Epoch 129, Iteration 20, Current loss 0.3181014358997345 Accuracy 99.56300547873728\n",
      "Training:: Epoch 129, Iteration 30, Current loss 0.39728066325187683 Accuracy 99.4170364569738\n",
      "Training:: Epoch 129, Iteration 40, Current loss 0.30684560537338257 Accuracy 99.48988644428673\n",
      "Training:: Epoch 129, Iteration 50, Current loss 0.40669506788253784 Accuracy 99.55336866010597\n",
      "Training:: Epoch 129, Iteration 60, Current loss 0.6076714396476746 Accuracy 99.04084158415841\n",
      "Training:: Epoch 129, Iteration 70, Current loss 0.3141891062259674 Accuracy 99.35162362019076\n",
      "Training:: Epoch 129, Iteration 80, Current loss 0.3995615541934967 Accuracy 99.35532802427001\n",
      "Training:: Epoch 129, Iteration 90, Current loss 0.4456216096878052 Accuracy 99.05504587155963\n",
      "Training:: Epoch 129, Iteration 100, Current loss 0.44652289152145386 Accuracy 99.15169660678643\n",
      "Training:: Epoch 129, Iteration 110, Current loss 0.30747953057289124 Accuracy 99.4140184809556\n",
      "Training:: Epoch 129, Iteration 120, Current loss 0.34403595328330994 Accuracy 99.32594597354849\n",
      "Training:: Epoch 129, Iteration 130, Current loss 0.39044666290283203 Accuracy 99.39108773872128\n",
      "Training:: Epoch 129, Iteration 140, Current loss 0.40089911222457886 Accuracy 99.14851662633325\n",
      "Training:: Epoch 129, Iteration 150, Current loss 0.3522542715072632 Accuracy 99.41134296818989\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 129, Probability Accuracy 71.47917701352112\n",
      "Starting Training\n",
      "Training:: Epoch 130, Iteration 0, Current loss 0.266787052154541 Accuracy 99.41570822313795\n",
      "Training:: Epoch 130, Iteration 10, Current loss 0.3517593741416931 Accuracy 99.39350180505416\n",
      "Training:: Epoch 130, Iteration 20, Current loss 0.3674066364765167 Accuracy 99.30550918196995\n",
      "Training:: Epoch 130, Iteration 30, Current loss 0.4174165725708008 Accuracy 99.41075021557918\n",
      "Training:: Epoch 130, Iteration 40, Current loss 0.4281451106071472 Accuracy 99.171974522293\n",
      "Training:: Epoch 130, Iteration 50, Current loss 0.49488282203674316 Accuracy 99.11614948602171\n",
      "Training:: Epoch 130, Iteration 60, Current loss 0.38159725069999695 Accuracy 99.38879658200807\n",
      "Training:: Epoch 130, Iteration 70, Current loss 0.3322537839412689 Accuracy 99.53551774625133\n",
      "Training:: Epoch 130, Iteration 80, Current loss 0.3950963616371155 Accuracy 99.25156766232891\n",
      "Training:: Epoch 130, Iteration 90, Current loss 0.5473777055740356 Accuracy 99.05025996533796\n",
      "Training:: Epoch 130, Iteration 100, Current loss 0.41388487815856934 Accuracy 99.39376443418014\n",
      "Training:: Epoch 130, Iteration 110, Current loss 0.3931070566177368 Accuracy 99.02838796890842\n",
      "Training:: Epoch 130, Iteration 120, Current loss 0.43371909856796265 Accuracy 98.5338785046729\n",
      "Training:: Epoch 130, Iteration 130, Current loss 0.44981738924980164 Accuracy 99.28665677057562\n",
      "Training:: Epoch 130, Iteration 140, Current loss 0.3200598359107971 Accuracy 99.1180597314091\n",
      "Training:: Epoch 130, Iteration 150, Current loss 0.5345064997673035 Accuracy 98.75737152485257\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 130, Probability Accuracy 68.32227498730475\n",
      "Starting Training\n",
      "Training:: Epoch 131, Iteration 0, Current loss 0.728567361831665 Accuracy 96.9546144761262\n",
      "Training:: Epoch 131, Iteration 10, Current loss 1.3012932538986206 Accuracy 93.95194135215857\n",
      "Training:: Epoch 131, Iteration 20, Current loss 4.673437118530273 Accuracy 64.98973305954826\n",
      "Training:: Epoch 131, Iteration 30, Current loss 20.314298629760742 Accuracy 26.279034461818735\n",
      "Training:: Epoch 131, Iteration 40, Current loss 7.664161682128906 Accuracy 51.448707909162096\n",
      "Training:: Epoch 131, Iteration 50, Current loss 4.714508056640625 Accuracy 77.77191129883843\n",
      "Training:: Epoch 131, Iteration 60, Current loss 5.573398590087891 Accuracy 52.86383386942046\n",
      "Training:: Epoch 131, Iteration 70, Current loss 3.0469110012054443 Accuracy 84.73353647161247\n",
      "Training:: Epoch 131, Iteration 80, Current loss 4.41115665435791 Accuracy 77.95418462236334\n",
      "Training:: Epoch 131, Iteration 90, Current loss 3.1237030029296875 Accuracy 80.27562446167097\n",
      "Training:: Epoch 131, Iteration 100, Current loss 1.6372413635253906 Accuracy 93.79748008152677\n",
      "Training:: Epoch 131, Iteration 110, Current loss 2.125272512435913 Accuracy 85.54684275691292\n",
      "Training:: Epoch 131, Iteration 120, Current loss 1.9282299280166626 Accuracy 94.02784445511281\n",
      "Training:: Epoch 131, Iteration 130, Current loss 1.956626534461975 Accuracy 88.87109321374639\n",
      "Training:: Epoch 131, Iteration 140, Current loss 1.8189646005630493 Accuracy 91.65539337842648\n",
      "Training:: Epoch 131, Iteration 150, Current loss 1.7576515674591064 Accuracy 93.17614770459082\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 131, Probability Accuracy 60.063950535990315\n",
      "Starting Training\n",
      "Training:: Epoch 132, Iteration 0, Current loss 1.4574487209320068 Accuracy 93.57967667436489\n",
      "Training:: Epoch 132, Iteration 10, Current loss 1.9975370168685913 Accuracy 90.67472793228536\n",
      "Training:: Epoch 132, Iteration 20, Current loss 2.4365811347961426 Accuracy 85.72700296735906\n",
      "Training:: Epoch 132, Iteration 30, Current loss 1.7746920585632324 Accuracy 94.88465396188566\n",
      "Training:: Epoch 132, Iteration 40, Current loss 1.8560268878936768 Accuracy 92.82542650151905\n",
      "Training:: Epoch 132, Iteration 50, Current loss 1.4796944856643677 Accuracy 96.2566844919786\n",
      "Training:: Epoch 132, Iteration 60, Current loss 0.8628729581832886 Accuracy 97.42804654011023\n",
      "Training:: Epoch 132, Iteration 70, Current loss 1.5689204931259155 Accuracy 94.62018801893913\n",
      "Training:: Epoch 132, Iteration 80, Current loss 0.9655911922454834 Accuracy 96.15405046480744\n",
      "Training:: Epoch 132, Iteration 90, Current loss 1.510870099067688 Accuracy 91.48062420667513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 132, Iteration 100, Current loss 1.159472107887268 Accuracy 95.44454225352112\n",
      "Training:: Epoch 132, Iteration 110, Current loss 1.455166220664978 Accuracy 94.06536907822255\n",
      "Training:: Epoch 132, Iteration 120, Current loss 1.2079472541809082 Accuracy 96.000546000546\n",
      "Training:: Epoch 132, Iteration 130, Current loss 1.5510269403457642 Accuracy 93.83033419023137\n",
      "Training:: Epoch 132, Iteration 140, Current loss 1.0414824485778809 Accuracy 96.9332098384275\n",
      "Training:: Epoch 132, Iteration 150, Current loss 0.9557971358299255 Accuracy 98.32219536470923\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 132, Probability Accuracy 68.16323569622938\n",
      "Starting Training\n",
      "Training:: Epoch 133, Iteration 0, Current loss 0.9155832529067993 Accuracy 96.23957434570032\n",
      "Training:: Epoch 133, Iteration 10, Current loss 0.8322080373764038 Accuracy 97.75886955861125\n",
      "Training:: Epoch 133, Iteration 20, Current loss 1.30800199508667 Accuracy 97.57869249394673\n",
      "Training:: Epoch 133, Iteration 30, Current loss 0.798124372959137 Accuracy 97.36925716464383\n",
      "Training:: Epoch 133, Iteration 40, Current loss 0.6753352880477905 Accuracy 98.38803934937178\n",
      "Training:: Epoch 133, Iteration 50, Current loss 0.9672269225120544 Accuracy 96.12415831827886\n",
      "Training:: Epoch 133, Iteration 60, Current loss 0.8060504198074341 Accuracy 97.62860008597221\n",
      "Training:: Epoch 133, Iteration 70, Current loss 1.4677504301071167 Accuracy 94.4359756097561\n",
      "Training:: Epoch 133, Iteration 80, Current loss 0.9726575613021851 Accuracy 97.92063410365948\n",
      "Training:: Epoch 133, Iteration 90, Current loss 0.6443817615509033 Accuracy 98.60460272831406\n",
      "Training:: Epoch 133, Iteration 100, Current loss 0.8570948839187622 Accuracy 97.97839933536416\n",
      "Training:: Epoch 133, Iteration 110, Current loss 0.9464120864868164 Accuracy 98.78891711466888\n",
      "Training:: Epoch 133, Iteration 120, Current loss 0.7302632927894592 Accuracy 98.00936768149883\n",
      "Training:: Epoch 133, Iteration 130, Current loss 0.6291342973709106 Accuracy 98.73780324053352\n",
      "Training:: Epoch 133, Iteration 140, Current loss 0.8024705052375793 Accuracy 98.22977201609298\n",
      "Training:: Epoch 133, Iteration 150, Current loss 0.5486052632331848 Accuracy 98.52237545735997\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 133, Probability Accuracy 73.68976735620895\n",
      "Starting Training\n",
      "Training:: Epoch 134, Iteration 0, Current loss 0.6254870891571045 Accuracy 98.82677956371987\n",
      "Training:: Epoch 134, Iteration 10, Current loss 0.5792028903961182 Accuracy 99.0181702263309\n",
      "Training:: Epoch 134, Iteration 20, Current loss 0.8230049014091492 Accuracy 98.8411367342185\n",
      "Training:: Epoch 134, Iteration 30, Current loss 0.806714653968811 Accuracy 98.29751604800447\n",
      "Training:: Epoch 134, Iteration 40, Current loss 0.6341930627822876 Accuracy 99.18411396503346\n",
      "Training:: Epoch 134, Iteration 50, Current loss 0.5936854481697083 Accuracy 98.8962079401816\n",
      "Training:: Epoch 134, Iteration 60, Current loss 0.5138143301010132 Accuracy 99.02555330949049\n",
      "Training:: Epoch 134, Iteration 70, Current loss 0.44748276472091675 Accuracy 99.06589245140115\n",
      "Training:: Epoch 134, Iteration 80, Current loss 0.6666358709335327 Accuracy 98.85094939287751\n",
      "Training:: Epoch 134, Iteration 90, Current loss 0.7304677963256836 Accuracy 98.89029358055907\n",
      "Training:: Epoch 134, Iteration 100, Current loss 0.9509101510047913 Accuracy 99.23553245164743\n",
      "Training:: Epoch 134, Iteration 110, Current loss 0.49071261286735535 Accuracy 99.26065272552279\n",
      "Training:: Epoch 134, Iteration 120, Current loss 0.6379193067550659 Accuracy 99.08887818126068\n",
      "Training:: Epoch 134, Iteration 130, Current loss 0.4771731197834015 Accuracy 99.1910160200117\n",
      "Training:: Epoch 134, Iteration 140, Current loss 0.7600956559181213 Accuracy 99.06523547352161\n",
      "Training:: Epoch 134, Iteration 150, Current loss 0.6553205251693726 Accuracy 98.67921738701888\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 134, Probability Accuracy 73.17514969224501\n",
      "Starting Training\n",
      "Training:: Epoch 135, Iteration 0, Current loss 0.850570797920227 Accuracy 97.34299516908213\n",
      "Training:: Epoch 135, Iteration 10, Current loss 0.6956005692481995 Accuracy 98.78088962108731\n",
      "Training:: Epoch 135, Iteration 20, Current loss 0.5770084261894226 Accuracy 98.87640449438203\n",
      "Training:: Epoch 135, Iteration 30, Current loss 0.6076898574829102 Accuracy 99.23545014822905\n",
      "Training:: Epoch 135, Iteration 40, Current loss 0.48630645871162415 Accuracy 99.01156677181913\n",
      "Training:: Epoch 135, Iteration 50, Current loss 0.44684937596321106 Accuracy 99.36717474142624\n",
      "Training:: Epoch 135, Iteration 60, Current loss 0.5483222603797913 Accuracy 99.34719724806504\n",
      "Training:: Epoch 135, Iteration 70, Current loss 0.47993406653404236 Accuracy 99.22975087254784\n",
      "Training:: Epoch 135, Iteration 80, Current loss 0.8825503587722778 Accuracy 98.97904923960438\n",
      "Training:: Epoch 135, Iteration 90, Current loss 0.5512346625328064 Accuracy 98.7933482052322\n",
      "Training:: Epoch 135, Iteration 100, Current loss 0.4090020954608917 Accuracy 99.19380235560874\n",
      "Training:: Epoch 135, Iteration 110, Current loss 0.9829702973365784 Accuracy 99.0121399666746\n",
      "Training:: Epoch 135, Iteration 120, Current loss 0.4980738162994385 Accuracy 99.14632064196688\n",
      "Training:: Epoch 135, Iteration 130, Current loss 0.5939107537269592 Accuracy 99.0151017728168\n",
      "Training:: Epoch 135, Iteration 140, Current loss 0.5880091190338135 Accuracy 99.12123027761135\n",
      "Training:: Epoch 135, Iteration 150, Current loss 0.6027547121047974 Accuracy 99.03971657654299\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 135, Probability Accuracy 72.90807528975843\n",
      "Starting Training\n",
      "Training:: Epoch 136, Iteration 0, Current loss 0.47175881266593933 Accuracy 99.33915211970074\n",
      "Training:: Epoch 136, Iteration 10, Current loss 0.4792826473712921 Accuracy 98.76618122977347\n",
      "Training:: Epoch 136, Iteration 20, Current loss 0.5120428800582886 Accuracy 99.22742207619217\n",
      "Training:: Epoch 136, Iteration 30, Current loss 0.5918306112289429 Accuracy 99.05217631361002\n",
      "Training:: Epoch 136, Iteration 40, Current loss 0.5511872172355652 Accuracy 99.27509061367329\n",
      "Training:: Epoch 136, Iteration 50, Current loss 0.4657919108867645 Accuracy 99.37352081303077\n",
      "Training:: Epoch 136, Iteration 60, Current loss 0.5340824723243713 Accuracy 98.93162393162393\n",
      "Training:: Epoch 136, Iteration 70, Current loss 0.6586247086524963 Accuracy 99.2156259612427\n",
      "Training:: Epoch 136, Iteration 80, Current loss 0.39505279064178467 Accuracy 99.40119760479043\n",
      "Training:: Epoch 136, Iteration 90, Current loss 0.4130777418613434 Accuracy 99.36407844813766\n",
      "Training:: Epoch 136, Iteration 100, Current loss 0.6064270734786987 Accuracy 99.31661839246922\n",
      "Training:: Epoch 136, Iteration 110, Current loss 0.514667809009552 Accuracy 99.33785889160917\n",
      "Training:: Epoch 136, Iteration 120, Current loss 0.5702869296073914 Accuracy 99.45593035908597\n",
      "Training:: Epoch 136, Iteration 130, Current loss 0.4718710780143738 Accuracy 99.10506965944272\n",
      "Training:: Epoch 136, Iteration 140, Current loss 0.4810047447681427 Accuracy 99.32696380665506\n",
      "Training:: Epoch 136, Iteration 150, Current loss 0.5529002547264099 Accuracy 99.25291017547924\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 136, Probability Accuracy 72.71042014274474\n",
      "Starting Training\n",
      "Training:: Epoch 137, Iteration 0, Current loss 0.4664790630340576 Accuracy 99.17719852553975\n",
      "Training:: Epoch 137, Iteration 10, Current loss 0.6281712651252747 Accuracy 98.9951705873189\n",
      "Training:: Epoch 137, Iteration 20, Current loss 0.48829546570777893 Accuracy 99.07304412309973\n",
      "Training:: Epoch 137, Iteration 30, Current loss 0.43960678577423096 Accuracy 99.0560126978823\n",
      "Training:: Epoch 137, Iteration 40, Current loss 0.5921268463134766 Accuracy 99.11074740861974\n",
      "Training:: Epoch 137, Iteration 50, Current loss 0.4937444031238556 Accuracy 99.20963855421687\n",
      "Training:: Epoch 137, Iteration 60, Current loss 0.65884929895401 Accuracy 99.02889902889903\n",
      "Training:: Epoch 137, Iteration 70, Current loss 0.5279659032821655 Accuracy 99.04172598839119\n",
      "Training:: Epoch 137, Iteration 80, Current loss 0.40613654255867004 Accuracy 99.01487942534634\n",
      "Training:: Epoch 137, Iteration 90, Current loss 0.45562291145324707 Accuracy 99.37966731202339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 137, Iteration 100, Current loss 0.3521747589111328 Accuracy 99.40105966367196\n",
      "Training:: Epoch 137, Iteration 110, Current loss 0.522821843624115 Accuracy 98.97039180765806\n",
      "Training:: Epoch 137, Iteration 120, Current loss 0.3872477114200592 Accuracy 99.39404584174936\n",
      "Training:: Epoch 137, Iteration 130, Current loss 0.5022852420806885 Accuracy 99.18957011980268\n",
      "Training:: Epoch 137, Iteration 140, Current loss 0.5614808201789856 Accuracy 99.24895345973898\n",
      "Training:: Epoch 137, Iteration 150, Current loss 0.42939382791519165 Accuracy 99.2272234273319\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 137, Probability Accuracy 72.80037499790738\n",
      "Starting Training\n",
      "Training:: Epoch 138, Iteration 0, Current loss 0.4617418944835663 Accuracy 99.41870810842873\n",
      "Training:: Epoch 138, Iteration 10, Current loss 0.5849484801292419 Accuracy 99.3665913777865\n",
      "Training:: Epoch 138, Iteration 20, Current loss 0.42600807547569275 Accuracy 99.4404476418865\n",
      "Training:: Epoch 138, Iteration 30, Current loss 0.37494033575057983 Accuracy 99.56516259137888\n",
      "Training:: Epoch 138, Iteration 40, Current loss 0.6349862217903137 Accuracy 99.23586691912361\n",
      "Training:: Epoch 138, Iteration 50, Current loss 0.7092415690422058 Accuracy 99.38929906116124\n",
      "Training:: Epoch 138, Iteration 60, Current loss 0.4591720998287201 Accuracy 99.50742309944542\n",
      "Training:: Epoch 138, Iteration 70, Current loss 0.342210978269577 Accuracy 99.51713690588903\n",
      "Training:: Epoch 138, Iteration 80, Current loss 0.4772098958492279 Accuracy 99.41897703358038\n",
      "Training:: Epoch 138, Iteration 90, Current loss 0.498793363571167 Accuracy 99.3578374411351\n",
      "Training:: Epoch 138, Iteration 100, Current loss 0.36403489112854004 Accuracy 99.30993099309931\n",
      "Training:: Epoch 138, Iteration 110, Current loss 0.3232080936431885 Accuracy 99.39681463910539\n",
      "Training:: Epoch 138, Iteration 120, Current loss 0.5342226624488831 Accuracy 99.30589985126426\n",
      "Training:: Epoch 138, Iteration 130, Current loss 0.4787023067474365 Accuracy 99.11034019695613\n",
      "Training:: Epoch 138, Iteration 140, Current loss 0.5473817586898804 Accuracy 99.28452924097884\n",
      "Training:: Epoch 138, Iteration 150, Current loss 0.45242035388946533 Accuracy 99.36720469552458\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 138, Probability Accuracy 72.60495198129475\n",
      "Starting Training\n",
      "Training:: Epoch 139, Iteration 0, Current loss 0.4971334636211395 Accuracy 99.1044776119403\n",
      "Training:: Epoch 139, Iteration 10, Current loss 0.3463859260082245 Accuracy 99.5398773006135\n",
      "Training:: Epoch 139, Iteration 20, Current loss 0.41327255964279175 Accuracy 99.50109136264422\n",
      "Training:: Epoch 139, Iteration 30, Current loss 0.4848363399505615 Accuracy 99.24307586444176\n",
      "Training:: Epoch 139, Iteration 40, Current loss 0.46174296736717224 Accuracy 99.5450315375866\n",
      "Training:: Epoch 139, Iteration 50, Current loss 0.4736550748348236 Accuracy 99.29064417177914\n",
      "Training:: Epoch 139, Iteration 60, Current loss 0.302120178937912 Accuracy 99.60028313278094\n",
      "Training:: Epoch 139, Iteration 70, Current loss 0.45534300804138184 Accuracy 99.30717935394475\n",
      "Training:: Epoch 139, Iteration 80, Current loss 0.31933990120887756 Accuracy 99.50801460085701\n",
      "Training:: Epoch 139, Iteration 90, Current loss 0.46017706394195557 Accuracy 99.4283284512898\n",
      "Training:: Epoch 139, Iteration 100, Current loss 0.3172835111618042 Accuracy 99.48069932490912\n",
      "Training:: Epoch 139, Iteration 110, Current loss 0.49047425389289856 Accuracy 99.31527240250074\n",
      "Training:: Epoch 139, Iteration 120, Current loss 0.3915053904056549 Accuracy 99.56117809843843\n",
      "Training:: Epoch 139, Iteration 130, Current loss 0.3425181806087494 Accuracy 99.45652173913044\n",
      "Training:: Epoch 139, Iteration 140, Current loss 0.48761647939682007 Accuracy 99.23006416131989\n",
      "Training:: Epoch 139, Iteration 150, Current loss 0.4315849840641022 Accuracy 99.60699093767339\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 139, Probability Accuracy 71.83866161461152\n",
      "Starting Training\n",
      "Training:: Epoch 140, Iteration 0, Current loss 0.3578471541404724 Accuracy 99.38260919739196\n",
      "Training:: Epoch 140, Iteration 10, Current loss 0.4562503397464752 Accuracy 99.30155733836716\n",
      "Training:: Epoch 140, Iteration 20, Current loss 0.5114283561706543 Accuracy 99.37762734072173\n",
      "Training:: Epoch 140, Iteration 30, Current loss 0.3319028317928314 Accuracy 99.65687839131823\n",
      "Training:: Epoch 140, Iteration 40, Current loss 0.42139339447021484 Accuracy 99.50410535728803\n",
      "Training:: Epoch 140, Iteration 50, Current loss 0.5783308744430542 Accuracy 99.26274174591302\n",
      "Training:: Epoch 140, Iteration 60, Current loss 0.5079891681671143 Accuracy 99.17034658102503\n",
      "Training:: Epoch 140, Iteration 70, Current loss 0.4171972870826721 Accuracy 99.1506228765572\n",
      "Training:: Epoch 140, Iteration 80, Current loss 0.34695443511009216 Accuracy 99.54881253408696\n",
      "Training:: Epoch 140, Iteration 90, Current loss 0.317125529050827 Accuracy 99.54128440366972\n",
      "Training:: Epoch 140, Iteration 100, Current loss 0.41733941435813904 Accuracy 99.35967137851878\n",
      "Training:: Epoch 140, Iteration 110, Current loss 0.3991796672344208 Accuracy 98.88263729297091\n",
      "Training:: Epoch 140, Iteration 120, Current loss 0.4850894808769226 Accuracy 99.36808846761454\n",
      "Training:: Epoch 140, Iteration 130, Current loss 0.5787684321403503 Accuracy 99.29494712103408\n",
      "Training:: Epoch 140, Iteration 140, Current loss 0.33173176646232605 Accuracy 99.61213464468763\n",
      "Training:: Epoch 140, Iteration 150, Current loss 0.4572453796863556 Accuracy 99.42897930049965\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 140, Probability Accuracy 72.39334601927445\n",
      "Starting Training\n",
      "Training:: Epoch 141, Iteration 0, Current loss 0.5816891193389893 Accuracy 99.12474752332403\n",
      "Training:: Epoch 141, Iteration 10, Current loss 0.4211621880531311 Accuracy 99.67860574533647\n",
      "Training:: Epoch 141, Iteration 20, Current loss 0.5097689628601074 Accuracy 99.25154572079401\n",
      "Training:: Epoch 141, Iteration 30, Current loss 0.4452453851699829 Accuracy 99.46061866389394\n",
      "Training:: Epoch 141, Iteration 40, Current loss 0.40248778462409973 Accuracy 99.1825613079019\n",
      "Training:: Epoch 141, Iteration 50, Current loss 0.5884566307067871 Accuracy 99.1461332032203\n",
      "Training:: Epoch 141, Iteration 60, Current loss 0.5567356944084167 Accuracy 99.44766339957762\n",
      "Training:: Epoch 141, Iteration 70, Current loss 0.4597967863082886 Accuracy 99.45459157787926\n",
      "Training:: Epoch 141, Iteration 80, Current loss 0.4937782287597656 Accuracy 99.16327453640886\n",
      "Training:: Epoch 141, Iteration 90, Current loss 0.35528531670570374 Accuracy 99.58390689525716\n",
      "Training:: Epoch 141, Iteration 100, Current loss 0.3918989300727844 Accuracy 99.26616986176688\n",
      "Training:: Epoch 141, Iteration 110, Current loss 0.5496118664741516 Accuracy 99.44741204641738\n",
      "Training:: Epoch 141, Iteration 120, Current loss 0.44210121035575867 Accuracy 99.404991886253\n",
      "Training:: Epoch 141, Iteration 130, Current loss 0.3169122636318207 Accuracy 99.59349593495935\n",
      "Training:: Epoch 141, Iteration 140, Current loss 0.4577779173851013 Accuracy 99.49391111814012\n",
      "Training:: Epoch 141, Iteration 150, Current loss 0.4428957402706146 Accuracy 99.39100954294325\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 141, Probability Accuracy 72.06254429383765\n",
      "Starting Training\n",
      "Training:: Epoch 142, Iteration 0, Current loss 0.4595258831977844 Accuracy 99.52201814560743\n",
      "Training:: Epoch 142, Iteration 10, Current loss 0.4213334321975708 Accuracy 99.36167496489212\n",
      "Training:: Epoch 142, Iteration 20, Current loss 0.5797893404960632 Accuracy 99.35217091660924\n",
      "Training:: Epoch 142, Iteration 30, Current loss 0.5186012387275696 Accuracy 99.25088823252429\n",
      "Training:: Epoch 142, Iteration 40, Current loss 0.4224555492401123 Accuracy 99.50601924695378\n",
      "Training:: Epoch 142, Iteration 50, Current loss 0.38707470893859863 Accuracy 99.46795347686216\n",
      "Training:: Epoch 142, Iteration 60, Current loss 0.3437484800815582 Accuracy 99.63369963369964\n",
      "Training:: Epoch 142, Iteration 70, Current loss 0.3095468282699585 Accuracy 99.55739156093243\n",
      "Training:: Epoch 142, Iteration 80, Current loss 0.6065348982810974 Accuracy 99.14529914529915\n",
      "Training:: Epoch 142, Iteration 90, Current loss 0.43905431032180786 Accuracy 99.48704796101565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 142, Iteration 100, Current loss 0.5655264258384705 Accuracy 99.17059377945334\n",
      "Training:: Epoch 142, Iteration 110, Current loss 0.3932247757911682 Accuracy 99.40796842498267\n",
      "Training:: Epoch 142, Iteration 120, Current loss 0.49893996119499207 Accuracy 99.47322756569162\n",
      "Training:: Epoch 142, Iteration 130, Current loss 0.41727471351623535 Accuracy 99.317738791423\n",
      "Training:: Epoch 142, Iteration 140, Current loss 0.4756150543689728 Accuracy 99.36555224539336\n",
      "Training:: Epoch 142, Iteration 150, Current loss 0.3907509744167328 Accuracy 99.35629224332153\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 142, Probability Accuracy 71.87671943794956\n",
      "Starting Training\n",
      "Training:: Epoch 143, Iteration 0, Current loss 0.3176540434360504 Accuracy 99.48139435414885\n",
      "Training:: Epoch 143, Iteration 10, Current loss 0.3159729242324829 Accuracy 99.43098542977843\n",
      "Training:: Epoch 143, Iteration 20, Current loss 0.3242344558238983 Accuracy 99.52825504022246\n",
      "Training:: Epoch 143, Iteration 30, Current loss 0.46381279826164246 Accuracy 99.33986137088789\n",
      "Training:: Epoch 143, Iteration 40, Current loss 0.5166957378387451 Accuracy 99.48051948051948\n",
      "Training:: Epoch 143, Iteration 50, Current loss 0.3323695957660675 Accuracy 99.51776915819268\n",
      "Training:: Epoch 143, Iteration 60, Current loss 0.4604294002056122 Accuracy 99.42556634304208\n",
      "Training:: Epoch 143, Iteration 70, Current loss 0.32844677567481995 Accuracy 99.56532582748423\n",
      "Training:: Epoch 143, Iteration 80, Current loss 0.3969070017337799 Accuracy 99.5451187745422\n",
      "Training:: Epoch 143, Iteration 90, Current loss 0.45007196068763733 Accuracy 99.28935860058309\n",
      "Training:: Epoch 143, Iteration 100, Current loss 0.402342289686203 Accuracy 99.33659597607395\n",
      "Training:: Epoch 143, Iteration 110, Current loss 0.389554888010025 Accuracy 99.41158059467918\n",
      "Training:: Epoch 143, Iteration 120, Current loss 0.35085591673851013 Accuracy 99.52810118748307\n",
      "Training:: Epoch 143, Iteration 130, Current loss 0.4028911292552948 Accuracy 99.45860482943657\n",
      "Training:: Epoch 143, Iteration 140, Current loss 0.25467488169670105 Accuracy 99.64519235642962\n",
      "Training:: Epoch 143, Iteration 150, Current loss 0.3947298228740692 Accuracy 99.49107775558848\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 143, Probability Accuracy 71.99747769264681\n",
      "Starting Training\n",
      "Training:: Epoch 144, Iteration 0, Current loss 0.43520408868789673 Accuracy 99.54176804541768\n",
      "Training:: Epoch 144, Iteration 10, Current loss 0.5118196606636047 Accuracy 99.48449520183996\n",
      "Training:: Epoch 144, Iteration 20, Current loss 0.4763917624950409 Accuracy 99.41831966275612\n",
      "Training:: Epoch 144, Iteration 30, Current loss 0.41568586230278015 Accuracy 99.43783783783783\n",
      "Training:: Epoch 144, Iteration 40, Current loss 0.36521586775779724 Accuracy 99.3506902855702\n",
      "Training:: Epoch 144, Iteration 50, Current loss 0.40236765146255493 Accuracy 99.64044253226798\n",
      "Training:: Epoch 144, Iteration 60, Current loss 0.3704381287097931 Accuracy 99.46232244603162\n",
      "Training:: Epoch 144, Iteration 70, Current loss 0.42898279428482056 Accuracy 99.38646259400976\n",
      "Training:: Epoch 144, Iteration 80, Current loss 0.38760632276535034 Accuracy 99.60253013078041\n",
      "Training:: Epoch 144, Iteration 90, Current loss 0.48971933126449585 Accuracy 99.52626038133116\n",
      "Training:: Epoch 144, Iteration 100, Current loss 0.4107402265071869 Accuracy 99.49433527491519\n",
      "Training:: Epoch 144, Iteration 110, Current loss 0.31860995292663574 Accuracy 99.5063922207414\n",
      "Training:: Epoch 144, Iteration 120, Current loss 0.2925628423690796 Accuracy 99.68958121684163\n",
      "Training:: Epoch 144, Iteration 130, Current loss 0.438414990901947 Accuracy 99.18792479477447\n",
      "Training:: Epoch 144, Iteration 140, Current loss 0.4979877173900604 Accuracy 99.35953420669578\n",
      "Training:: Epoch 144, Iteration 150, Current loss 0.4954679310321808 Accuracy 99.25627729257641\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 144, Probability Accuracy 71.50730185657446\n",
      "Starting Training\n",
      "Training:: Epoch 145, Iteration 0, Current loss 0.3342956304550171 Accuracy 99.41157659098177\n",
      "Training:: Epoch 145, Iteration 10, Current loss 0.36000725626945496 Accuracy 99.60359275427768\n",
      "Training:: Epoch 145, Iteration 20, Current loss 0.48669084906578064 Accuracy 99.45322447756722\n",
      "Training:: Epoch 145, Iteration 30, Current loss 0.3515866696834564 Accuracy 99.4936906060134\n",
      "Training:: Epoch 145, Iteration 40, Current loss 0.4261082708835602 Accuracy 99.55382570723371\n",
      "Training:: Epoch 145, Iteration 50, Current loss 0.5303109288215637 Accuracy 99.33923303834808\n",
      "Training:: Epoch 145, Iteration 60, Current loss 0.406251460313797 Accuracy 99.30397367754999\n",
      "Training:: Epoch 145, Iteration 70, Current loss 0.35298973321914673 Accuracy 99.5078065114778\n",
      "Training:: Epoch 145, Iteration 80, Current loss 0.30110105872154236 Accuracy 99.3556830766133\n",
      "Training:: Epoch 145, Iteration 90, Current loss 0.6726229190826416 Accuracy 98.80894897794946\n",
      "Training:: Epoch 145, Iteration 100, Current loss 0.3163861334323883 Accuracy 99.4040640875342\n",
      "Training:: Epoch 145, Iteration 110, Current loss 0.25827133655548096 Accuracy 99.63369963369964\n",
      "Training:: Epoch 145, Iteration 120, Current loss 0.3429621458053589 Accuracy 99.57639085004236\n",
      "Training:: Epoch 145, Iteration 130, Current loss 0.32469508051872253 Accuracy 99.17933414156487\n",
      "Training:: Epoch 145, Iteration 140, Current loss 0.48298272490501404 Accuracy 99.05463645210621\n",
      "Training:: Epoch 145, Iteration 150, Current loss 0.3970874547958374 Accuracy 99.13409643556136\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 145, Probability Accuracy 71.59000228793366\n",
      "Starting Training\n",
      "Training:: Epoch 146, Iteration 0, Current loss 0.3630402088165283 Accuracy 99.25260718424101\n",
      "Training:: Epoch 146, Iteration 10, Current loss 0.38485440611839294 Accuracy 99.23529076071449\n",
      "Training:: Epoch 146, Iteration 20, Current loss 0.4671862721443176 Accuracy 99.23155737704919\n",
      "Training:: Epoch 146, Iteration 30, Current loss 0.44989705085754395 Accuracy 99.15014164305948\n",
      "Training:: Epoch 146, Iteration 40, Current loss 0.49571940302848816 Accuracy 99.21853702862336\n",
      "Training:: Epoch 146, Iteration 50, Current loss 0.6219365000724792 Accuracy 98.08865170986714\n",
      "Training:: Epoch 146, Iteration 60, Current loss 0.5403591394424438 Accuracy 98.76374274524608\n",
      "Training:: Epoch 146, Iteration 70, Current loss 0.6380072236061096 Accuracy 98.51637107776261\n",
      "Training:: Epoch 146, Iteration 80, Current loss 0.42962002754211426 Accuracy 98.80471473631786\n",
      "Training:: Epoch 146, Iteration 90, Current loss 0.6780834794044495 Accuracy 98.33229749326703\n",
      "Training:: Epoch 146, Iteration 100, Current loss 0.47702983021736145 Accuracy 98.8168597485827\n",
      "Training:: Epoch 146, Iteration 110, Current loss 0.5752342939376831 Accuracy 98.19748387778834\n",
      "Training:: Epoch 146, Iteration 120, Current loss 0.4456249177455902 Accuracy 99.16772857597961\n",
      "Training:: Epoch 146, Iteration 130, Current loss 0.4009714126586914 Accuracy 99.55076923076923\n",
      "Training:: Epoch 146, Iteration 140, Current loss 0.44809216260910034 Accuracy 99.20843045843046\n",
      "Training:: Epoch 146, Iteration 150, Current loss 0.5557479858398438 Accuracy 98.55022437003797\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 146, Probability Accuracy 72.20953008074731\n",
      "Starting Training\n",
      "Training:: Epoch 147, Iteration 0, Current loss 0.39105144143104553 Accuracy 99.35494362532523\n",
      "Training:: Epoch 147, Iteration 10, Current loss 0.3796521723270416 Accuracy 99.13570781348017\n",
      "Training:: Epoch 147, Iteration 20, Current loss 0.48104995489120483 Accuracy 99.15574402467232\n",
      "Training:: Epoch 147, Iteration 30, Current loss 0.5071172714233398 Accuracy 98.95318406513522\n",
      "Training:: Epoch 147, Iteration 40, Current loss 0.3754025399684906 Accuracy 99.22592259225922\n",
      "Training:: Epoch 147, Iteration 50, Current loss 0.6324655413627625 Accuracy 98.88926668933469\n",
      "Training:: Epoch 147, Iteration 60, Current loss 0.41382381319999695 Accuracy 99.1918284880458\n",
      "Training:: Epoch 147, Iteration 70, Current loss 0.44574248790740967 Accuracy 99.07916339496039\n",
      "Training:: Epoch 147, Iteration 80, Current loss 0.4378199279308319 Accuracy 99.41448801742919\n",
      "Training:: Epoch 147, Iteration 90, Current loss 0.589468777179718 Accuracy 98.70449109752857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 147, Iteration 100, Current loss 0.5253686308860779 Accuracy 99.20566260322454\n",
      "Training:: Epoch 147, Iteration 110, Current loss 0.3983810245990753 Accuracy 98.86949499968942\n",
      "Training:: Epoch 147, Iteration 120, Current loss 0.41739991307258606 Accuracy 99.1405497517967\n",
      "Training:: Epoch 147, Iteration 130, Current loss 0.38643404841423035 Accuracy 99.0420658059142\n",
      "Training:: Epoch 147, Iteration 140, Current loss 0.39793661236763 Accuracy 99.1461879481145\n",
      "Training:: Epoch 147, Iteration 150, Current loss 0.6257629990577698 Accuracy 97.17022031293106\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 147, Probability Accuracy 70.0628902740498\n",
      "Starting Training\n",
      "Training:: Epoch 148, Iteration 0, Current loss 0.39311397075653076 Accuracy 99.00804661487237\n",
      "Training:: Epoch 148, Iteration 10, Current loss 0.5476323366165161 Accuracy 98.67229402971951\n",
      "Training:: Epoch 148, Iteration 20, Current loss 0.45328348875045776 Accuracy 98.98185438255125\n",
      "Training:: Epoch 148, Iteration 30, Current loss 0.6872429847717285 Accuracy 97.12306872669153\n",
      "Training:: Epoch 148, Iteration 40, Current loss 0.9880407452583313 Accuracy 94.13866249860445\n",
      "Training:: Epoch 148, Iteration 50, Current loss 0.46866855025291443 Accuracy 99.05125470162241\n",
      "Training:: Epoch 148, Iteration 60, Current loss 0.4520193338394165 Accuracy 99.1174628480328\n",
      "Training:: Epoch 148, Iteration 70, Current loss 0.3718200922012329 Accuracy 99.3404545128683\n",
      "Training:: Epoch 148, Iteration 80, Current loss 0.39683666825294495 Accuracy 99.02668040764915\n",
      "Training:: Epoch 148, Iteration 90, Current loss 0.5294052362442017 Accuracy 98.7075634841176\n",
      "Training:: Epoch 148, Iteration 100, Current loss 0.3652994632720947 Accuracy 99.38569907445327\n",
      "Training:: Epoch 148, Iteration 110, Current loss 0.3485206067562103 Accuracy 99.40425184641123\n",
      "Training:: Epoch 148, Iteration 120, Current loss 0.3585841655731201 Accuracy 99.31061987237922\n",
      "Training:: Epoch 148, Iteration 130, Current loss 0.4390096068382263 Accuracy 99.01505445227419\n",
      "Training:: Epoch 148, Iteration 140, Current loss 0.4261012077331543 Accuracy 99.16278689863596\n",
      "Training:: Epoch 148, Iteration 150, Current loss 0.5057435631752014 Accuracy 98.4781285329159\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 148, Probability Accuracy 70.52471805402871\n",
      "Starting Training\n",
      "Training:: Epoch 149, Iteration 0, Current loss 0.26028621196746826 Accuracy 99.2854565138939\n",
      "Training:: Epoch 149, Iteration 10, Current loss 0.44835370779037476 Accuracy 98.9373365944168\n",
      "Training:: Epoch 149, Iteration 20, Current loss 0.4535122811794281 Accuracy 98.69354236655468\n",
      "Training:: Epoch 149, Iteration 30, Current loss 0.3142380118370056 Accuracy 99.47739611345071\n",
      "Training:: Epoch 149, Iteration 40, Current loss 0.4213586449623108 Accuracy 99.00549205877988\n",
      "Training:: Epoch 149, Iteration 50, Current loss 0.4188932478427887 Accuracy 99.22148873207375\n",
      "Training:: Epoch 149, Iteration 60, Current loss 0.46888262033462524 Accuracy 99.31109499637418\n",
      "Training:: Epoch 149, Iteration 70, Current loss 0.2642024755477905 Accuracy 99.43510870597171\n",
      "Training:: Epoch 149, Iteration 80, Current loss 0.3216785192489624 Accuracy 99.53912433623886\n",
      "Training:: Epoch 149, Iteration 90, Current loss 0.42732393741607666 Accuracy 99.32557912227544\n",
      "Training:: Epoch 149, Iteration 100, Current loss 0.40404075384140015 Accuracy 99.39567676454637\n",
      "Training:: Epoch 149, Iteration 110, Current loss 0.46264249086380005 Accuracy 99.5523879883534\n",
      "Training:: Epoch 149, Iteration 120, Current loss 0.5637112855911255 Accuracy 99.02807123453064\n",
      "Training:: Epoch 149, Iteration 130, Current loss 0.3103955090045929 Accuracy 99.48453608247422\n",
      "Training:: Epoch 149, Iteration 140, Current loss 0.38524311780929565 Accuracy 99.52197378565921\n",
      "Training:: Epoch 149, Iteration 150, Current loss 0.3314742147922516 Accuracy 99.4428658978216\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 149, Probability Accuracy 71.61857355706721\n",
      "Starting Training\n",
      "Training:: Epoch 150, Iteration 0, Current loss 0.5908948183059692 Accuracy 98.62793572311496\n",
      "Training:: Epoch 150, Iteration 10, Current loss 0.3772036135196686 Accuracy 99.4811272764269\n",
      "Training:: Epoch 150, Iteration 20, Current loss 0.3774457573890686 Accuracy 99.29094776648546\n",
      "Training:: Epoch 150, Iteration 30, Current loss 0.32469984889030457 Accuracy 99.35874599216245\n",
      "Training:: Epoch 150, Iteration 40, Current loss 0.390428364276886 Accuracy 99.34281276138128\n",
      "Training:: Epoch 150, Iteration 50, Current loss 0.41762691736221313 Accuracy 99.29734160908771\n",
      "Training:: Epoch 150, Iteration 60, Current loss 0.43601298332214355 Accuracy 99.21545407445785\n",
      "Training:: Epoch 150, Iteration 70, Current loss 0.3083992600440979 Accuracy 99.65579604457149\n",
      "Training:: Epoch 150, Iteration 80, Current loss 0.42888376116752625 Accuracy 99.50015620118712\n",
      "Training:: Epoch 150, Iteration 90, Current loss 0.3227536380290985 Accuracy 99.19043374925728\n",
      "Training:: Epoch 150, Iteration 100, Current loss 0.2943533658981323 Accuracy 99.63676869430454\n",
      "Training:: Epoch 150, Iteration 110, Current loss 0.31215062737464905 Accuracy 99.44829078321074\n",
      "Training:: Epoch 150, Iteration 120, Current loss 0.3661661446094513 Accuracy 99.30539148513915\n",
      "Training:: Epoch 150, Iteration 130, Current loss 0.2893553078174591 Accuracy 99.56136820925553\n",
      "Training:: Epoch 150, Iteration 140, Current loss 0.3399046063423157 Accuracy 99.44352844187964\n",
      "Training:: Epoch 150, Iteration 150, Current loss 0.4832290709018707 Accuracy 99.19775811857794\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 150, Probability Accuracy 71.54714538423335\n",
      "Starting Training\n",
      "Training:: Epoch 151, Iteration 0, Current loss 0.4647534489631653 Accuracy 99.45897607869439\n",
      "Training:: Epoch 151, Iteration 10, Current loss 0.45981186628341675 Accuracy 99.31825224666873\n",
      "Training:: Epoch 151, Iteration 20, Current loss 0.40482833981513977 Accuracy 99.47045117559838\n",
      "Training:: Epoch 151, Iteration 30, Current loss 0.4099573493003845 Accuracy 99.42799291318653\n",
      "Training:: Epoch 151, Iteration 40, Current loss 0.35884928703308105 Accuracy 99.34821657685399\n",
      "Training:: Epoch 151, Iteration 50, Current loss 0.29263633489608765 Accuracy 99.52099905910529\n",
      "Training:: Epoch 151, Iteration 60, Current loss 0.566148579120636 Accuracy 98.88362134883009\n",
      "Training:: Epoch 151, Iteration 70, Current loss 0.3605501651763916 Accuracy 99.6272346899962\n",
      "Training:: Epoch 151, Iteration 80, Current loss 0.366285115480423 Accuracy 99.4658249362632\n",
      "Training:: Epoch 151, Iteration 90, Current loss 0.2977341413497925 Accuracy 99.43199747554434\n",
      "Training:: Epoch 151, Iteration 100, Current loss 0.35293880105018616 Accuracy 99.17109898459626\n",
      "Training:: Epoch 151, Iteration 110, Current loss 0.3653298318386078 Accuracy 99.55913723631193\n",
      "Training:: Epoch 151, Iteration 120, Current loss 0.7130818963050842 Accuracy 98.40419468824804\n",
      "Training:: Epoch 151, Iteration 130, Current loss 0.3892762362957001 Accuracy 99.26877344693838\n",
      "Training:: Epoch 151, Iteration 140, Current loss 0.40135252475738525 Accuracy 99.45167422137739\n",
      "Training:: Epoch 151, Iteration 150, Current loss 0.43709200620651245 Accuracy 99.11286780851998\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 151, Probability Accuracy 71.0124385466599\n",
      "Starting Training\n",
      "Training:: Epoch 152, Iteration 0, Current loss 0.35688209533691406 Accuracy 99.42446043165468\n",
      "Training:: Epoch 152, Iteration 10, Current loss 0.3216555118560791 Accuracy 99.40903002127492\n",
      "Training:: Epoch 152, Iteration 20, Current loss 0.32876935601234436 Accuracy 99.49235975797355\n",
      "Training:: Epoch 152, Iteration 30, Current loss 0.3802937865257263 Accuracy 99.17584727945506\n",
      "Training:: Epoch 152, Iteration 40, Current loss 0.4842296242713928 Accuracy 99.4614607876136\n",
      "Training:: Epoch 152, Iteration 50, Current loss 0.4888164699077606 Accuracy 99.38019898874572\n",
      "Training:: Epoch 152, Iteration 60, Current loss 0.3524145185947418 Accuracy 99.38011247443762\n",
      "Training:: Epoch 152, Iteration 70, Current loss 0.2354678213596344 Accuracy 99.48572897917202\n",
      "Training:: Epoch 152, Iteration 80, Current loss 0.26302850246429443 Accuracy 99.47927624658111\n",
      "Training:: Epoch 152, Iteration 90, Current loss 0.40456703305244446 Accuracy 99.56625547154795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 152, Iteration 100, Current loss 0.32827332615852356 Accuracy 99.4842903575298\n",
      "Training:: Epoch 152, Iteration 110, Current loss 0.3507588505744934 Accuracy 99.37712101035268\n",
      "Training:: Epoch 152, Iteration 120, Current loss 0.4056452810764313 Accuracy 98.85816634949066\n",
      "Training:: Epoch 152, Iteration 130, Current loss 0.5039398074150085 Accuracy 99.45327102803738\n",
      "Training:: Epoch 152, Iteration 140, Current loss 0.37982437014579773 Accuracy 99.43756422043157\n",
      "Training:: Epoch 152, Iteration 150, Current loss 0.6274574995040894 Accuracy 99.02490249024902\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 152, Probability Accuracy 71.11243798862729\n",
      "Starting Training\n",
      "Training:: Epoch 153, Iteration 0, Current loss 0.49892204999923706 Accuracy 98.93543161305253\n",
      "Training:: Epoch 153, Iteration 10, Current loss 0.3196718990802765 Accuracy 99.58806640868805\n",
      "Training:: Epoch 153, Iteration 20, Current loss 0.37136906385421753 Accuracy 99.506711024435\n",
      "Training:: Epoch 153, Iteration 30, Current loss 0.2868918776512146 Accuracy 99.39065981537902\n",
      "Training:: Epoch 153, Iteration 40, Current loss 0.39572450518608093 Accuracy 99.41417147108052\n",
      "Training:: Epoch 153, Iteration 50, Current loss 0.3683990240097046 Accuracy 99.33115490656591\n",
      "Training:: Epoch 153, Iteration 60, Current loss 0.5991266369819641 Accuracy 99.12632821723732\n",
      "Training:: Epoch 153, Iteration 70, Current loss 0.33050957322120667 Accuracy 99.40848214285714\n",
      "Training:: Epoch 153, Iteration 80, Current loss 0.47024932503700256 Accuracy 99.17582417582418\n",
      "Training:: Epoch 153, Iteration 90, Current loss 0.3033272624015808 Accuracy 99.56760466712423\n",
      "Training:: Epoch 153, Iteration 100, Current loss 0.3905297517776489 Accuracy 99.30615784908933\n",
      "Training:: Epoch 153, Iteration 110, Current loss 0.35046833753585815 Accuracy 99.07252096694623\n",
      "Training:: Epoch 153, Iteration 120, Current loss 0.4564286470413208 Accuracy 99.33316557624559\n",
      "Training:: Epoch 153, Iteration 130, Current loss 0.35412198305130005 Accuracy 99.37621373506738\n",
      "Training:: Epoch 153, Iteration 140, Current loss 0.43579742312431335 Accuracy 98.94188127285459\n",
      "Training:: Epoch 153, Iteration 150, Current loss 0.589495062828064 Accuracy 99.27573278800273\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 153, Probability Accuracy 70.73911417905035\n",
      "Starting Training\n",
      "Training:: Epoch 154, Iteration 0, Current loss 0.3465578556060791 Accuracy 99.22816519972918\n",
      "Training:: Epoch 154, Iteration 10, Current loss 0.26570233702659607 Accuracy 99.33271974229176\n",
      "Training:: Epoch 154, Iteration 20, Current loss 0.407199501991272 Accuracy 99.06773488710851\n",
      "Training:: Epoch 154, Iteration 30, Current loss 0.37826094031333923 Accuracy 98.8923797859311\n",
      "Training:: Epoch 154, Iteration 40, Current loss 0.3382022976875305 Accuracy 99.17122492955411\n",
      "Training:: Epoch 154, Iteration 50, Current loss 0.5101847648620605 Accuracy 98.8775597635164\n",
      "Training:: Epoch 154, Iteration 60, Current loss 0.332165390253067 Accuracy 99.42667741646511\n",
      "Training:: Epoch 154, Iteration 70, Current loss 0.3693868815898895 Accuracy 99.23220233726641\n",
      "Training:: Epoch 154, Iteration 80, Current loss 0.4355405867099762 Accuracy 99.42459978351279\n",
      "Training:: Epoch 154, Iteration 90, Current loss 0.407372385263443 Accuracy 99.28172827228023\n",
      "Training:: Epoch 154, Iteration 100, Current loss 0.6000108122825623 Accuracy 98.26535055886195\n",
      "Training:: Epoch 154, Iteration 110, Current loss 0.3660370409488678 Accuracy 99.04955147372918\n",
      "Training:: Epoch 154, Iteration 120, Current loss 1.496361494064331 Accuracy 94.77966351286676\n",
      "Training:: Epoch 154, Iteration 130, Current loss 2.5176422595977783 Accuracy 88.84106488123705\n",
      "Training:: Epoch 154, Iteration 140, Current loss 2.144632577896118 Accuracy 90.09578722217056\n",
      "Training:: Epoch 154, Iteration 150, Current loss 4.723691463470459 Accuracy 70.56518462697815\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 154, Probability Accuracy 54.86989469924833\n",
      "Starting Training\n",
      "Training:: Epoch 155, Iteration 0, Current loss 1.9209673404693604 Accuracy 92.99323909035034\n",
      "Training:: Epoch 155, Iteration 10, Current loss 6.023050785064697 Accuracy 52.44868563197695\n",
      "Training:: Epoch 155, Iteration 20, Current loss 2.4297096729278564 Accuracy 88.13899279015558\n",
      "Training:: Epoch 155, Iteration 30, Current loss 3.1291632652282715 Accuracy 80.61903384437373\n",
      "Training:: Epoch 155, Iteration 40, Current loss 2.71915864944458 Accuracy 86.09235118580179\n",
      "Training:: Epoch 155, Iteration 50, Current loss 2.1987013816833496 Accuracy 87.0230634778531\n",
      "Training:: Epoch 155, Iteration 60, Current loss 2.137554407119751 Accuracy 86.8474773986585\n",
      "Training:: Epoch 155, Iteration 70, Current loss 1.8513073921203613 Accuracy 92.65603465249065\n",
      "Training:: Epoch 155, Iteration 80, Current loss 1.5657742023468018 Accuracy 93.25039325039324\n",
      "Training:: Epoch 155, Iteration 90, Current loss 1.415208339691162 Accuracy 94.72765072765073\n",
      "Training:: Epoch 155, Iteration 100, Current loss 0.9405537247657776 Accuracy 97.41574243566275\n",
      "Training:: Epoch 155, Iteration 110, Current loss 0.8242871165275574 Accuracy 97.70174641303642\n",
      "Training:: Epoch 155, Iteration 120, Current loss 1.2615208625793457 Accuracy 96.13015978695073\n",
      "Training:: Epoch 155, Iteration 130, Current loss 1.044368863105774 Accuracy 96.8892900120337\n",
      "Training:: Epoch 155, Iteration 140, Current loss 1.0263479948043823 Accuracy 96.42710164989337\n",
      "Training:: Epoch 155, Iteration 150, Current loss 0.8311974406242371 Accuracy 97.32416535498302\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 155, Probability Accuracy 72.78597775682056\n",
      "Starting Training\n",
      "Training:: Epoch 156, Iteration 0, Current loss 0.7734362483024597 Accuracy 98.54573287409109\n",
      "Training:: Epoch 156, Iteration 10, Current loss 1.1370453834533691 Accuracy 96.01376936316696\n",
      "Training:: Epoch 156, Iteration 20, Current loss 0.6829856634140015 Accuracy 97.78120184899846\n",
      "Training:: Epoch 156, Iteration 30, Current loss 0.8020200133323669 Accuracy 98.20871217261501\n",
      "Training:: Epoch 156, Iteration 40, Current loss 0.846152663230896 Accuracy 96.81801031316816\n",
      "Training:: Epoch 156, Iteration 50, Current loss 1.3362236022949219 Accuracy 94.77477477477477\n",
      "Training:: Epoch 156, Iteration 60, Current loss 0.8914843797683716 Accuracy 97.77706358133804\n",
      "Training:: Epoch 156, Iteration 70, Current loss 1.2437642812728882 Accuracy 93.48885793871867\n",
      "Training:: Epoch 156, Iteration 80, Current loss 0.6845762133598328 Accuracy 98.53810775295663\n",
      "Training:: Epoch 156, Iteration 90, Current loss 1.0682393312454224 Accuracy 97.06177440160528\n",
      "Training:: Epoch 156, Iteration 100, Current loss 1.424567699432373 Accuracy 94.56970466814862\n",
      "Training:: Epoch 156, Iteration 110, Current loss 0.712141752243042 Accuracy 97.4073302790504\n",
      "Training:: Epoch 156, Iteration 120, Current loss 0.7893722057342529 Accuracy 98.89371560961483\n",
      "Training:: Epoch 156, Iteration 130, Current loss 0.6537520289421082 Accuracy 98.16366961292289\n",
      "Training:: Epoch 156, Iteration 140, Current loss 0.6584087610244751 Accuracy 97.76226647505645\n",
      "Training:: Epoch 156, Iteration 150, Current loss 0.6269388794898987 Accuracy 98.64497170222029\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 156, Probability Accuracy 69.11468127968035\n",
      "Starting Training\n",
      "Training:: Epoch 157, Iteration 0, Current loss 2.4458980560302734 Accuracy 90.97785977859779\n",
      "Training:: Epoch 157, Iteration 10, Current loss 0.8342607021331787 Accuracy 97.92827899176244\n",
      "Training:: Epoch 157, Iteration 20, Current loss 0.8332603573799133 Accuracy 98.36565189725042\n",
      "Training:: Epoch 157, Iteration 30, Current loss 0.5961332321166992 Accuracy 97.93138892457921\n",
      "Training:: Epoch 157, Iteration 40, Current loss 0.6417058706283569 Accuracy 98.9227060418864\n",
      "Training:: Epoch 157, Iteration 50, Current loss 0.5612007975578308 Accuracy 99.04013070560605\n",
      "Training:: Epoch 157, Iteration 60, Current loss 0.6942716240882874 Accuracy 98.53618000511378\n",
      "Training:: Epoch 157, Iteration 70, Current loss 0.738379180431366 Accuracy 98.02524429967427\n",
      "Training:: Epoch 157, Iteration 80, Current loss 0.7047348022460938 Accuracy 98.45819331883771\n",
      "Training:: Epoch 157, Iteration 90, Current loss 0.5920838117599487 Accuracy 98.95167619688306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 157, Iteration 100, Current loss 0.5682530403137207 Accuracy 98.65765683499224\n",
      "Training:: Epoch 157, Iteration 110, Current loss 0.44959989190101624 Accuracy 99.36696627041654\n",
      "Training:: Epoch 157, Iteration 120, Current loss 0.5619943141937256 Accuracy 99.23346473937801\n",
      "Training:: Epoch 157, Iteration 130, Current loss 0.5267006158828735 Accuracy 99.13269731136167\n",
      "Training:: Epoch 157, Iteration 140, Current loss 0.5986085534095764 Accuracy 99.17287014061208\n",
      "Training:: Epoch 157, Iteration 150, Current loss 0.5381394028663635 Accuracy 99.09127051740785\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 157, Probability Accuracy 73.64914258290969\n",
      "Starting Training\n",
      "Training:: Epoch 158, Iteration 0, Current loss 0.3666696846485138 Accuracy 99.45125297237973\n",
      "Training:: Epoch 158, Iteration 10, Current loss 0.3873249888420105 Accuracy 99.15912855140782\n",
      "Training:: Epoch 158, Iteration 20, Current loss 0.5375040769577026 Accuracy 99.2442748091603\n",
      "Training:: Epoch 158, Iteration 30, Current loss 0.5691444277763367 Accuracy 98.82043180621379\n",
      "Training:: Epoch 158, Iteration 40, Current loss 0.5676603317260742 Accuracy 99.43336179461306\n",
      "Training:: Epoch 158, Iteration 50, Current loss 0.435668021440506 Accuracy 99.2580546803537\n",
      "Training:: Epoch 158, Iteration 60, Current loss 0.4505773186683655 Accuracy 99.14895710350255\n",
      "Training:: Epoch 158, Iteration 70, Current loss 0.5261355042457581 Accuracy 98.73828856964397\n",
      "Training:: Epoch 158, Iteration 80, Current loss 0.5645876526832581 Accuracy 99.32432432432432\n",
      "Training:: Epoch 158, Iteration 90, Current loss 0.6354010105133057 Accuracy 98.82669274016133\n",
      "Training:: Epoch 158, Iteration 100, Current loss 0.4722180962562561 Accuracy 99.14008525650448\n",
      "Training:: Epoch 158, Iteration 110, Current loss 0.46760648488998413 Accuracy 99.09281228192603\n",
      "Training:: Epoch 158, Iteration 120, Current loss 0.49943751096725464 Accuracy 98.66594275963702\n",
      "Training:: Epoch 158, Iteration 130, Current loss 0.40293776988983154 Accuracy 99.3995949074074\n",
      "Training:: Epoch 158, Iteration 140, Current loss 0.4308319091796875 Accuracy 99.33726067746686\n",
      "Training:: Epoch 158, Iteration 150, Current loss 0.4513159394264221 Accuracy 98.96832579185521\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 158, Probability Accuracy 73.40182253447246\n",
      "Starting Training\n",
      "Training:: Epoch 159, Iteration 0, Current loss 0.40432801842689514 Accuracy 99.36598417868777\n",
      "Training:: Epoch 159, Iteration 10, Current loss 0.4956167936325073 Accuracy 99.16782246879335\n",
      "Training:: Epoch 159, Iteration 20, Current loss 0.6524748802185059 Accuracy 99.28934769599043\n",
      "Training:: Epoch 159, Iteration 30, Current loss 0.4122674763202667 Accuracy 99.44515578318395\n",
      "Training:: Epoch 159, Iteration 40, Current loss 0.29370203614234924 Accuracy 99.49789439585358\n",
      "Training:: Epoch 159, Iteration 50, Current loss 0.46834951639175415 Accuracy 99.42406954631893\n",
      "Training:: Epoch 159, Iteration 60, Current loss 0.5082136988639832 Accuracy 99.35465366413308\n",
      "Training:: Epoch 159, Iteration 70, Current loss 0.3963353633880615 Accuracy 99.17568983583654\n",
      "Training:: Epoch 159, Iteration 80, Current loss 0.2916913330554962 Accuracy 99.62020737882807\n",
      "Training:: Epoch 159, Iteration 90, Current loss 0.5280767679214478 Accuracy 99.47596031544137\n",
      "Training:: Epoch 159, Iteration 100, Current loss 0.6367186903953552 Accuracy 99.326825984517\n",
      "Training:: Epoch 159, Iteration 110, Current loss 0.4181177616119385 Accuracy 99.27749393610982\n",
      "Training:: Epoch 159, Iteration 120, Current loss 0.4133304953575134 Accuracy 99.20450281425892\n",
      "Training:: Epoch 159, Iteration 130, Current loss 0.4394713342189789 Accuracy 99.31481962107449\n",
      "Training:: Epoch 159, Iteration 140, Current loss 0.47968366742134094 Accuracy 99.50968066381695\n",
      "Training:: Epoch 159, Iteration 150, Current loss 0.434048056602478 Accuracy 99.39519768911356\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 159, Probability Accuracy 72.99088732763768\n",
      "Starting Training\n",
      "Training:: Epoch 160, Iteration 0, Current loss 0.4967573881149292 Accuracy 99.35773064829063\n",
      "Training:: Epoch 160, Iteration 10, Current loss 0.4865066111087799 Accuracy 99.25610448608745\n",
      "Training:: Epoch 160, Iteration 20, Current loss 0.5747183561325073 Accuracy 98.87602179836512\n",
      "Training:: Epoch 160, Iteration 30, Current loss 0.5395643711090088 Accuracy 99.30625285257874\n",
      "Training:: Epoch 160, Iteration 40, Current loss 0.4183971881866455 Accuracy 99.45667715184443\n",
      "Training:: Epoch 160, Iteration 50, Current loss 0.5475605726242065 Accuracy 99.391034677192\n",
      "Training:: Epoch 160, Iteration 60, Current loss 0.6766206622123718 Accuracy 99.0782713810819\n",
      "Training:: Epoch 160, Iteration 70, Current loss 0.33964574337005615 Accuracy 99.47316834109287\n",
      "Training:: Epoch 160, Iteration 80, Current loss 0.40032759308815 Accuracy 99.34897804693414\n",
      "Training:: Epoch 160, Iteration 90, Current loss 0.5489482879638672 Accuracy 99.00279703271312\n",
      "Training:: Epoch 160, Iteration 100, Current loss 0.6578757166862488 Accuracy 99.3949394939494\n",
      "Training:: Epoch 160, Iteration 110, Current loss 0.4600915014743805 Accuracy 99.21101225449051\n",
      "Training:: Epoch 160, Iteration 120, Current loss 0.514578640460968 Accuracy 99.27313677313677\n",
      "Training:: Epoch 160, Iteration 130, Current loss 0.43363550305366516 Accuracy 99.48643410852713\n",
      "Training:: Epoch 160, Iteration 140, Current loss 0.38919520378112793 Accuracy 99.6127181448721\n",
      "Training:: Epoch 160, Iteration 150, Current loss 0.4594075083732605 Accuracy 99.2083028407957\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 160, Probability Accuracy 72.8668924838589\n",
      "Starting Training\n",
      "Training:: Epoch 161, Iteration 0, Current loss 0.38290083408355713 Accuracy 99.50575711492505\n",
      "Training:: Epoch 161, Iteration 10, Current loss 0.520732045173645 Accuracy 99.44373127941806\n",
      "Training:: Epoch 161, Iteration 20, Current loss 0.5513954758644104 Accuracy 99.31601492331076\n",
      "Training:: Epoch 161, Iteration 30, Current loss 0.5663891434669495 Accuracy 98.84417358411264\n",
      "Training:: Epoch 161, Iteration 40, Current loss 0.4306279420852661 Accuracy 99.34932580746316\n",
      "Training:: Epoch 161, Iteration 50, Current loss 0.4742482900619507 Accuracy 99.39197803275474\n",
      "Training:: Epoch 161, Iteration 60, Current loss 0.3073280453681946 Accuracy 99.53467759805008\n",
      "Training:: Epoch 161, Iteration 70, Current loss 0.4266461730003357 Accuracy 99.08476792329483\n",
      "Training:: Epoch 161, Iteration 80, Current loss 0.5379623770713806 Accuracy 99.52364149611856\n",
      "Training:: Epoch 161, Iteration 90, Current loss 0.4083910286426544 Accuracy 99.20070446386237\n",
      "Training:: Epoch 161, Iteration 100, Current loss 0.2977489233016968 Accuracy 99.61449498843486\n",
      "Training:: Epoch 161, Iteration 110, Current loss 0.4217654764652252 Accuracy 99.4854295309284\n",
      "Training:: Epoch 161, Iteration 120, Current loss 0.3504679799079895 Accuracy 99.49041989400733\n",
      "Training:: Epoch 161, Iteration 130, Current loss 0.40896397829055786 Accuracy 99.39116316827091\n",
      "Training:: Epoch 161, Iteration 140, Current loss 0.3823745846748352 Accuracy 99.35333866162901\n",
      "Training:: Epoch 161, Iteration 150, Current loss 0.4570433497428894 Accuracy 99.42781287192163\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 161, Probability Accuracy 72.72024151650939\n",
      "Starting Training\n",
      "Training:: Epoch 162, Iteration 0, Current loss 0.4258537292480469 Accuracy 99.44437006292677\n",
      "Training:: Epoch 162, Iteration 10, Current loss 0.3542436957359314 Accuracy 99.5080564665621\n",
      "Training:: Epoch 162, Iteration 20, Current loss 0.3600923717021942 Accuracy 99.54764776839566\n",
      "Training:: Epoch 162, Iteration 30, Current loss 0.3784721791744232 Accuracy 99.40809122997557\n",
      "Training:: Epoch 162, Iteration 40, Current loss 0.4106300473213196 Accuracy 99.57684807005924\n",
      "Training:: Epoch 162, Iteration 50, Current loss 0.5013127326965332 Accuracy 99.40448923499771\n",
      "Training:: Epoch 162, Iteration 60, Current loss 0.5041990876197815 Accuracy 99.37859957562898\n",
      "Training:: Epoch 162, Iteration 70, Current loss 0.544062614440918 Accuracy 99.30177803036453\n",
      "Training:: Epoch 162, Iteration 80, Current loss 0.38757917284965515 Accuracy 99.32397959183673\n",
      "Training:: Epoch 162, Iteration 90, Current loss 0.41780999302864075 Accuracy 99.22435068750734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 162, Iteration 100, Current loss 0.4282940924167633 Accuracy 99.35253590105421\n",
      "Training:: Epoch 162, Iteration 110, Current loss 0.39778372645378113 Accuracy 99.43178346673184\n",
      "Training:: Epoch 162, Iteration 120, Current loss 0.32245486974716187 Accuracy 99.48538493207082\n",
      "Training:: Epoch 162, Iteration 130, Current loss 0.3552052676677704 Accuracy 99.43136945190334\n",
      "Training:: Epoch 162, Iteration 140, Current loss 0.35016822814941406 Accuracy 99.55976115777756\n",
      "Training:: Epoch 162, Iteration 150, Current loss 0.39070838689804077 Accuracy 99.32449337002753\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 162, Probability Accuracy 72.5799521208029\n",
      "Starting Training\n",
      "Training:: Epoch 163, Iteration 0, Current loss 0.44945836067199707 Accuracy 99.16066395239586\n",
      "Training:: Epoch 163, Iteration 10, Current loss 0.4431230425834656 Accuracy 99.50229461573267\n",
      "Training:: Epoch 163, Iteration 20, Current loss 0.3640705347061157 Accuracy 99.52611982398736\n",
      "Training:: Epoch 163, Iteration 30, Current loss 0.38886135816574097 Accuracy 99.40652818991099\n",
      "Training:: Epoch 163, Iteration 40, Current loss 0.39256519079208374 Accuracy 99.53548617624726\n",
      "Training:: Epoch 163, Iteration 50, Current loss 0.46928825974464417 Accuracy 99.59096858638743\n",
      "Training:: Epoch 163, Iteration 60, Current loss 0.42193442583084106 Accuracy 99.2056557311939\n",
      "Training:: Epoch 163, Iteration 70, Current loss 0.4800092577934265 Accuracy 99.31614349775785\n",
      "Training:: Epoch 163, Iteration 80, Current loss 0.4087803065776825 Accuracy 99.49280226747221\n",
      "Training:: Epoch 163, Iteration 90, Current loss 0.410731703042984 Accuracy 99.23358299947745\n",
      "Training:: Epoch 163, Iteration 100, Current loss 0.4764265716075897 Accuracy 99.39217116460004\n",
      "Training:: Epoch 163, Iteration 110, Current loss 0.4321802258491516 Accuracy 99.33048852266039\n",
      "Training:: Epoch 163, Iteration 120, Current loss 0.4041012227535248 Accuracy 99.25181849670939\n",
      "Training:: Epoch 163, Iteration 130, Current loss 0.3411334156990051 Accuracy 99.39511995078942\n",
      "Training:: Epoch 163, Iteration 140, Current loss 0.3526240885257721 Accuracy 99.31255728689275\n",
      "Training:: Epoch 163, Iteration 150, Current loss 0.297313928604126 Accuracy 99.47693425354159\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 163, Probability Accuracy 72.62559918750453\n",
      "Starting Training\n",
      "Training:: Epoch 164, Iteration 0, Current loss 0.383160799741745 Accuracy 99.51398294956577\n",
      "Training:: Epoch 164, Iteration 10, Current loss 0.49449437856674194 Accuracy 99.45077507442768\n",
      "Training:: Epoch 164, Iteration 20, Current loss 0.2640896141529083 Accuracy 99.5679939894816\n",
      "Training:: Epoch 164, Iteration 30, Current loss 0.3980658948421478 Accuracy 99.26522043386984\n",
      "Training:: Epoch 164, Iteration 40, Current loss 0.38081833720207214 Accuracy 99.58030319476103\n",
      "Training:: Epoch 164, Iteration 50, Current loss 0.5771840810775757 Accuracy 99.26882768705825\n",
      "Training:: Epoch 164, Iteration 60, Current loss 0.32010912895202637 Accuracy 99.5727301288093\n",
      "Training:: Epoch 164, Iteration 70, Current loss 0.443554162979126 Accuracy 99.438156167979\n",
      "Training:: Epoch 164, Iteration 80, Current loss 0.4007856249809265 Accuracy 99.47520335869851\n",
      "Training:: Epoch 164, Iteration 90, Current loss 0.30743980407714844 Accuracy 99.54921617439278\n",
      "Training:: Epoch 164, Iteration 100, Current loss 0.2708069980144501 Accuracy 99.56282241846638\n",
      "Training:: Epoch 164, Iteration 110, Current loss 0.5936511754989624 Accuracy 99.2342147088571\n",
      "Training:: Epoch 164, Iteration 120, Current loss 0.3195798397064209 Accuracy 99.53901430411497\n",
      "Training:: Epoch 164, Iteration 130, Current loss 0.3641532063484192 Accuracy 99.52608999301606\n",
      "Training:: Epoch 164, Iteration 140, Current loss 0.34025469422340393 Accuracy 99.63826749167063\n",
      "Training:: Epoch 164, Iteration 150, Current loss 0.3745405673980713 Accuracy 99.55749454658772\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 164, Probability Accuracy 72.48318926791703\n",
      "Starting Training\n",
      "Training:: Epoch 165, Iteration 0, Current loss 0.4105800688266754 Accuracy 99.38712972420838\n",
      "Training:: Epoch 165, Iteration 10, Current loss 0.3305317461490631 Accuracy 99.3647656691135\n",
      "Training:: Epoch 165, Iteration 20, Current loss 0.39366206526756287 Accuracy 99.56132105032275\n",
      "Training:: Epoch 165, Iteration 30, Current loss 0.3040443956851959 Accuracy 99.59939301972686\n",
      "Training:: Epoch 165, Iteration 40, Current loss 0.34265875816345215 Accuracy 99.38946832866955\n",
      "Training:: Epoch 165, Iteration 50, Current loss 0.5918439030647278 Accuracy 99.30399640772339\n",
      "Training:: Epoch 165, Iteration 60, Current loss 0.35819295048713684 Accuracy 99.61476725521669\n",
      "Training:: Epoch 165, Iteration 70, Current loss 0.41782915592193604 Accuracy 99.58827803342214\n",
      "Training:: Epoch 165, Iteration 80, Current loss 0.4527193307876587 Accuracy 99.38926588525601\n",
      "Training:: Epoch 165, Iteration 90, Current loss 0.432198166847229 Accuracy 99.29375835083032\n",
      "Training:: Epoch 165, Iteration 100, Current loss 0.4345114231109619 Accuracy 99.47184958276117\n",
      "Training:: Epoch 165, Iteration 110, Current loss 0.3856118321418762 Accuracy 99.1899852724595\n",
      "Training:: Epoch 165, Iteration 120, Current loss 0.5080279111862183 Accuracy 99.42695436858861\n",
      "Training:: Epoch 165, Iteration 130, Current loss 0.39458373188972473 Accuracy 99.45161772770328\n",
      "Training:: Epoch 165, Iteration 140, Current loss 0.735788881778717 Accuracy 99.02399349328995\n",
      "Training:: Epoch 165, Iteration 150, Current loss 0.5916624665260315 Accuracy 99.057478368356\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 165, Probability Accuracy 72.22392732183414\n",
      "Starting Training\n",
      "Training:: Epoch 166, Iteration 0, Current loss 0.4877329468727112 Accuracy 99.44668310579179\n",
      "Training:: Epoch 166, Iteration 10, Current loss 0.3253723680973053 Accuracy 99.51548632494114\n",
      "Training:: Epoch 166, Iteration 20, Current loss 0.49791672825813293 Accuracy 99.24674486172388\n",
      "Training:: Epoch 166, Iteration 30, Current loss 0.3662106394767761 Accuracy 99.48310774805499\n",
      "Training:: Epoch 166, Iteration 40, Current loss 0.43304532766342163 Accuracy 99.2294991744634\n",
      "Training:: Epoch 166, Iteration 50, Current loss 0.41123104095458984 Accuracy 99.45086705202313\n",
      "Training:: Epoch 166, Iteration 60, Current loss 0.4104488790035248 Accuracy 99.38924792273276\n",
      "Training:: Epoch 166, Iteration 70, Current loss 0.3397141396999359 Accuracy 99.43617648987085\n",
      "Training:: Epoch 166, Iteration 80, Current loss 0.35165271162986755 Accuracy 99.50388844194154\n",
      "Training:: Epoch 166, Iteration 90, Current loss 0.4616480767726898 Accuracy 99.33196853787308\n",
      "Training:: Epoch 166, Iteration 100, Current loss 0.2928757071495056 Accuracy 99.41322021915872\n",
      "Training:: Epoch 166, Iteration 110, Current loss 0.3523257374763489 Accuracy 99.55744053107136\n",
      "Training:: Epoch 166, Iteration 120, Current loss 0.44068458676338196 Accuracy 99.62881005068171\n",
      "Training:: Epoch 166, Iteration 130, Current loss 0.4609769880771637 Accuracy 99.45859344787581\n",
      "Training:: Epoch 166, Iteration 140, Current loss 0.3078998923301697 Accuracy 99.18335768933207\n",
      "Training:: Epoch 166, Iteration 150, Current loss 0.3901582658290863 Accuracy 99.32830930537352\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 166, Probability Accuracy 72.08430756524797\n",
      "Starting Training\n",
      "Training:: Epoch 167, Iteration 0, Current loss 0.3787016272544861 Accuracy 99.51194777207327\n",
      "Training:: Epoch 167, Iteration 10, Current loss 0.3633970618247986 Accuracy 99.33924534863502\n",
      "Training:: Epoch 167, Iteration 20, Current loss 0.39238429069519043 Accuracy 99.39899638230833\n",
      "Training:: Epoch 167, Iteration 30, Current loss 0.3635447323322296 Accuracy 99.52407614781634\n",
      "Training:: Epoch 167, Iteration 40, Current loss 0.3188938498497009 Accuracy 99.54202412436302\n",
      "Training:: Epoch 167, Iteration 50, Current loss 0.3455932140350342 Accuracy 99.54269175108539\n",
      "Training:: Epoch 167, Iteration 60, Current loss 0.5266406536102295 Accuracy 99.29994166180515\n",
      "Training:: Epoch 167, Iteration 70, Current loss 0.41818150877952576 Accuracy 99.48927477017365\n",
      "Training:: Epoch 167, Iteration 80, Current loss 0.3643355965614319 Accuracy 99.57618139436322\n",
      "Training:: Epoch 167, Iteration 90, Current loss 0.2672475576400757 Accuracy 99.63672969187675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 167, Iteration 100, Current loss 0.3259432315826416 Accuracy 99.4701260734515\n",
      "Training:: Epoch 167, Iteration 110, Current loss 0.3366253972053528 Accuracy 99.50710900473933\n",
      "Training:: Epoch 167, Iteration 120, Current loss 0.2855475842952728 Accuracy 99.5761347784097\n",
      "Training:: Epoch 167, Iteration 130, Current loss 0.2809855043888092 Accuracy 99.5333433689598\n",
      "Training:: Epoch 167, Iteration 140, Current loss 0.41337642073631287 Accuracy 99.2021022455805\n",
      "Training:: Epoch 167, Iteration 150, Current loss 0.38925936818122864 Accuracy 99.2566844919786\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 167, Probability Accuracy 71.94245567826073\n",
      "Starting Training\n",
      "Training:: Epoch 168, Iteration 0, Current loss 0.2587372064590454 Accuracy 99.4105676794611\n",
      "Training:: Epoch 168, Iteration 10, Current loss 0.2955576777458191 Accuracy 99.47586958001497\n",
      "Training:: Epoch 168, Iteration 20, Current loss 0.34278222918510437 Accuracy 99.30766935243211\n",
      "Training:: Epoch 168, Iteration 30, Current loss 0.2837773263454437 Accuracy 99.55393157120486\n",
      "Training:: Epoch 168, Iteration 40, Current loss 0.25604790449142456 Accuracy 99.60823263256636\n",
      "Training:: Epoch 168, Iteration 50, Current loss 0.40724849700927734 Accuracy 99.50372208436724\n",
      "Training:: Epoch 168, Iteration 60, Current loss 0.2854134142398834 Accuracy 99.6257184868333\n",
      "Training:: Epoch 168, Iteration 70, Current loss 0.37773871421813965 Accuracy 99.54242558862728\n",
      "Training:: Epoch 168, Iteration 80, Current loss 0.2639547288417816 Accuracy 99.52678571428571\n",
      "Training:: Epoch 168, Iteration 90, Current loss 0.3832022547721863 Accuracy 99.23023578363384\n",
      "Training:: Epoch 168, Iteration 100, Current loss 0.3570248484611511 Accuracy 99.27719941744431\n",
      "Training:: Epoch 168, Iteration 110, Current loss 0.3876979351043701 Accuracy 99.23084708025503\n",
      "Training:: Epoch 168, Iteration 120, Current loss 0.2524142563343048 Accuracy 99.56846030900373\n",
      "Training:: Epoch 168, Iteration 130, Current loss 0.4010874330997467 Accuracy 99.617773530817\n",
      "Training:: Epoch 168, Iteration 140, Current loss 0.424050897359848 Accuracy 99.65093014345919\n",
      "Training:: Epoch 168, Iteration 150, Current loss 0.4955900013446808 Accuracy 99.39222281229716\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 168, Probability Accuracy 71.61556018102577\n",
      "Starting Training\n",
      "Training:: Epoch 169, Iteration 0, Current loss 0.246315136551857 Accuracy 99.56804699648679\n",
      "Training:: Epoch 169, Iteration 10, Current loss 0.4272004961967468 Accuracy 99.47092832987892\n",
      "Training:: Epoch 169, Iteration 20, Current loss 0.38004979491233826 Accuracy 99.54836995711412\n",
      "Training:: Epoch 169, Iteration 30, Current loss 0.41117894649505615 Accuracy 99.68220056082254\n",
      "Training:: Epoch 169, Iteration 40, Current loss 0.439742773771286 Accuracy 99.19463734567901\n",
      "Training:: Epoch 169, Iteration 50, Current loss 0.5491470098495483 Accuracy 99.18327917801001\n",
      "Training:: Epoch 169, Iteration 60, Current loss 0.4776400923728943 Accuracy 98.89325894082137\n",
      "Training:: Epoch 169, Iteration 70, Current loss 0.484902560710907 Accuracy 99.20901720387582\n",
      "Training:: Epoch 169, Iteration 80, Current loss 0.5746248364448547 Accuracy 99.54457282617862\n",
      "Training:: Epoch 169, Iteration 90, Current loss 0.3371422290802002 Accuracy 99.34637801831806\n",
      "Training:: Epoch 169, Iteration 100, Current loss 0.32560843229293823 Accuracy 99.38106044976274\n",
      "Training:: Epoch 169, Iteration 110, Current loss 0.3924601674079895 Accuracy 99.35524725116574\n",
      "Training:: Epoch 169, Iteration 120, Current loss 0.42911964654922485 Accuracy 99.4159808866472\n",
      "Training:: Epoch 169, Iteration 130, Current loss 0.3826324939727783 Accuracy 99.53090795371625\n",
      "Training:: Epoch 169, Iteration 140, Current loss 0.2981073260307312 Accuracy 99.37978358405911\n",
      "Training:: Epoch 169, Iteration 150, Current loss 0.36750057339668274 Accuracy 99.377995891349\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 169, Probability Accuracy 72.37336845218498\n",
      "Starting Training\n",
      "Training:: Epoch 170, Iteration 0, Current loss 0.4591715931892395 Accuracy 99.20285871357889\n",
      "Training:: Epoch 170, Iteration 10, Current loss 0.32701021432876587 Accuracy 99.49192155268773\n",
      "Training:: Epoch 170, Iteration 20, Current loss 0.33413562178611755 Accuracy 99.54645889056867\n",
      "Training:: Epoch 170, Iteration 30, Current loss 0.32170212268829346 Accuracy 99.47895465277212\n",
      "Training:: Epoch 170, Iteration 40, Current loss 0.5248245000839233 Accuracy 99.22149673530889\n",
      "Training:: Epoch 170, Iteration 50, Current loss 0.3622693121433258 Accuracy 99.61764930794524\n",
      "Training:: Epoch 170, Iteration 60, Current loss 0.4387885332107544 Accuracy 99.11937377690802\n",
      "Training:: Epoch 170, Iteration 70, Current loss 0.39978963136672974 Accuracy 99.28311892369635\n",
      "Training:: Epoch 170, Iteration 80, Current loss 0.27645090222358704 Accuracy 99.17664670658682\n",
      "Training:: Epoch 170, Iteration 90, Current loss 0.35301947593688965 Accuracy 99.27167630057804\n",
      "Training:: Epoch 170, Iteration 100, Current loss 0.351519376039505 Accuracy 99.58011162885964\n",
      "Training:: Epoch 170, Iteration 110, Current loss 0.2773653268814087 Accuracy 99.57569121270188\n",
      "Training:: Epoch 170, Iteration 120, Current loss 0.26714321970939636 Accuracy 99.61345505386956\n",
      "Training:: Epoch 170, Iteration 130, Current loss 0.3168128728866577 Accuracy 99.52823830704946\n",
      "Training:: Epoch 170, Iteration 140, Current loss 0.3771109879016876 Accuracy 99.47327652982185\n",
      "Training:: Epoch 170, Iteration 150, Current loss 0.4227539598941803 Accuracy 99.4067237969677\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 170, Probability Accuracy 71.98408491024045\n",
      "Starting Training\n",
      "Training:: Epoch 171, Iteration 0, Current loss 0.3884754180908203 Accuracy 99.55368528436624\n",
      "Training:: Epoch 171, Iteration 10, Current loss 0.35467636585235596 Accuracy 99.41988950276243\n",
      "Training:: Epoch 171, Iteration 20, Current loss 0.3867950439453125 Accuracy 99.33814282877755\n",
      "Training:: Epoch 171, Iteration 30, Current loss 0.43273964524269104 Accuracy 99.3031055359554\n",
      "Training:: Epoch 171, Iteration 40, Current loss 0.32981181144714355 Accuracy 99.39314910476955\n",
      "Training:: Epoch 171, Iteration 50, Current loss 0.3286845088005066 Accuracy 99.06047274625307\n",
      "Training:: Epoch 171, Iteration 60, Current loss 0.44655027985572815 Accuracy 99.01374775851764\n",
      "Training:: Epoch 171, Iteration 70, Current loss 0.3958752751350403 Accuracy 99.32535099981766\n",
      "Training:: Epoch 171, Iteration 80, Current loss 0.3224339485168457 Accuracy 99.66227257871228\n",
      "Training:: Epoch 171, Iteration 90, Current loss 0.3961447477340698 Accuracy 98.81626351003602\n",
      "Training:: Epoch 171, Iteration 100, Current loss 0.42897048592567444 Accuracy 99.30356920780997\n",
      "Training:: Epoch 171, Iteration 110, Current loss 0.33474111557006836 Accuracy 99.30975647588025\n",
      "Training:: Epoch 171, Iteration 120, Current loss 0.26175737380981445 Accuracy 99.40924207946789\n",
      "Training:: Epoch 171, Iteration 130, Current loss 0.3876977860927582 Accuracy 99.18036540134622\n",
      "Training:: Epoch 171, Iteration 140, Current loss 0.3396761119365692 Accuracy 99.54460660594755\n",
      "Training:: Epoch 171, Iteration 150, Current loss 0.4446845054626465 Accuracy 99.3626550298576\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 171, Probability Accuracy 71.47895380048102\n",
      "Starting Training\n",
      "Training:: Epoch 172, Iteration 0, Current loss 0.34281426668167114 Accuracy 99.322204091879\n",
      "Training:: Epoch 172, Iteration 10, Current loss 0.381121426820755 Accuracy 99.22344322344323\n",
      "Training:: Epoch 172, Iteration 20, Current loss 0.37808549404144287 Accuracy 99.0694191892181\n",
      "Training:: Epoch 172, Iteration 30, Current loss 0.33915287256240845 Accuracy 99.47855559900925\n",
      "Training:: Epoch 172, Iteration 40, Current loss 0.4378494322299957 Accuracy 99.42037528244425\n",
      "Training:: Epoch 172, Iteration 50, Current loss 0.3620895743370056 Accuracy 99.04564315352697\n",
      "Training:: Epoch 172, Iteration 60, Current loss 0.30114826560020447 Accuracy 99.32859399684044\n",
      "Training:: Epoch 172, Iteration 70, Current loss 0.41999515891075134 Accuracy 99.07378820623649\n",
      "Training:: Epoch 172, Iteration 80, Current loss 0.2982977032661438 Accuracy 99.32199798718152\n",
      "Training:: Epoch 172, Iteration 90, Current loss 0.341177761554718 Accuracy 99.62038858404942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 172, Iteration 100, Current loss 0.4876995384693146 Accuracy 99.23008706868752\n",
      "Training:: Epoch 172, Iteration 110, Current loss 0.3490484058856964 Accuracy 99.40623738975039\n",
      "Training:: Epoch 172, Iteration 120, Current loss 0.28614962100982666 Accuracy 99.452183440288\n",
      "Training:: Epoch 172, Iteration 130, Current loss 0.4253469407558441 Accuracy 99.37574711117014\n",
      "Training:: Epoch 172, Iteration 140, Current loss 0.31208091974258423 Accuracy 99.19508946667439\n",
      "Training:: Epoch 172, Iteration 150, Current loss 0.3941287100315094 Accuracy 99.21964103487605\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 172, Probability Accuracy 70.92929168922048\n",
      "Starting Training\n",
      "Training:: Epoch 173, Iteration 0, Current loss 0.2557219862937927 Accuracy 99.5456165359943\n",
      "Training:: Epoch 173, Iteration 10, Current loss 0.40839117765426636 Accuracy 98.96882684223051\n",
      "Training:: Epoch 173, Iteration 20, Current loss 0.4062783718109131 Accuracy 99.3182486041728\n",
      "Training:: Epoch 173, Iteration 30, Current loss 0.4184715449810028 Accuracy 98.96432681242808\n",
      "Training:: Epoch 173, Iteration 40, Current loss 0.423730731010437 Accuracy 99.35089728904163\n",
      "Training:: Epoch 173, Iteration 50, Current loss 0.4017992615699768 Accuracy 99.60635836413354\n",
      "Training:: Epoch 173, Iteration 60, Current loss 0.5409926176071167 Accuracy 98.74438716115084\n",
      "Training:: Epoch 173, Iteration 70, Current loss 0.3637206256389618 Accuracy 98.99399290728812\n",
      "Training:: Epoch 173, Iteration 80, Current loss 0.4426865577697754 Accuracy 99.2811760886704\n",
      "Training:: Epoch 173, Iteration 90, Current loss 0.4461434483528137 Accuracy 98.77669902912622\n",
      "Training:: Epoch 173, Iteration 100, Current loss 0.37737607955932617 Accuracy 99.38728618840949\n",
      "Training:: Epoch 173, Iteration 110, Current loss 0.2778748571872711 Accuracy 99.43323269240956\n",
      "Training:: Epoch 173, Iteration 120, Current loss 0.29838457703590393 Accuracy 99.34793512790503\n",
      "Training:: Epoch 173, Iteration 130, Current loss 0.4608100652694702 Accuracy 99.09958776307225\n",
      "Training:: Epoch 173, Iteration 140, Current loss 0.31385695934295654 Accuracy 99.375\n",
      "Training:: Epoch 173, Iteration 150, Current loss 0.5109533667564392 Accuracy 98.42101485681475\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 173, Probability Accuracy 71.451498596548\n",
      "Starting Training\n",
      "Training:: Epoch 174, Iteration 0, Current loss 0.41406381130218506 Accuracy 99.36450295052201\n",
      "Training:: Epoch 174, Iteration 10, Current loss 0.8721632361412048 Accuracy 96.34433962264151\n",
      "Training:: Epoch 174, Iteration 20, Current loss 0.7214869260787964 Accuracy 95.34859302813943\n",
      "Training:: Epoch 174, Iteration 30, Current loss 0.7572417855262756 Accuracy 96.49685457012458\n",
      "Training:: Epoch 174, Iteration 40, Current loss 2.738880157470703 Accuracy 90.79652463451433\n",
      "Training:: Epoch 174, Iteration 50, Current loss 3.0955913066864014 Accuracy 77.98159546590286\n",
      "Training:: Epoch 174, Iteration 60, Current loss 5.128742218017578 Accuracy 72.43146162379166\n",
      "Training:: Epoch 174, Iteration 70, Current loss 4.211658477783203 Accuracy 80.47006745362563\n",
      "Training:: Epoch 174, Iteration 80, Current loss 2.5966341495513916 Accuracy 86.28342245989305\n",
      "Training:: Epoch 174, Iteration 90, Current loss 8.059126853942871 Accuracy 56.22482717162609\n",
      "Training:: Epoch 174, Iteration 100, Current loss 6.0127763748168945 Accuracy 62.97279362972794\n",
      "Training:: Epoch 174, Iteration 110, Current loss 2.629223346710205 Accuracy 88.53234316982606\n",
      "Training:: Epoch 174, Iteration 120, Current loss 2.539398193359375 Accuracy 83.2686822589846\n",
      "Training:: Epoch 174, Iteration 130, Current loss 3.4099087715148926 Accuracy 83.58122055787858\n",
      "Training:: Epoch 174, Iteration 140, Current loss 3.2535009384155273 Accuracy 79.24480228291887\n",
      "Training:: Epoch 174, Iteration 150, Current loss 1.3441542387008667 Accuracy 95.07422785962119\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 174, Probability Accuracy 69.80128459104581\n",
      "Starting Training\n",
      "Training:: Epoch 175, Iteration 0, Current loss 1.7481586933135986 Accuracy 91.869918699187\n",
      "Training:: Epoch 175, Iteration 10, Current loss 1.2168456315994263 Accuracy 95.15211970074813\n",
      "Training:: Epoch 175, Iteration 20, Current loss 0.7646252512931824 Accuracy 98.34030234802186\n",
      "Training:: Epoch 175, Iteration 30, Current loss 1.0279489755630493 Accuracy 95.93042702642435\n",
      "Training:: Epoch 175, Iteration 40, Current loss 0.9557874798774719 Accuracy 96.35159881673475\n",
      "Training:: Epoch 175, Iteration 50, Current loss 0.945385217666626 Accuracy 95.94667075319899\n",
      "Training:: Epoch 175, Iteration 60, Current loss 0.9153846502304077 Accuracy 98.0032283135051\n",
      "Training:: Epoch 175, Iteration 70, Current loss 1.609801173210144 Accuracy 94.40157904180872\n",
      "Training:: Epoch 175, Iteration 80, Current loss 0.8523313999176025 Accuracy 98.11498119928756\n",
      "Training:: Epoch 175, Iteration 90, Current loss 0.935978889465332 Accuracy 97.28672058439864\n",
      "Training:: Epoch 175, Iteration 100, Current loss 0.5726170539855957 Accuracy 98.99402960660832\n",
      "Training:: Epoch 175, Iteration 110, Current loss 0.7183794975280762 Accuracy 98.10946064024148\n",
      "Training:: Epoch 175, Iteration 120, Current loss 0.5659632086753845 Accuracy 98.45485676104569\n",
      "Training:: Epoch 175, Iteration 130, Current loss 0.6718317270278931 Accuracy 98.81993215804309\n",
      "Training:: Epoch 175, Iteration 140, Current loss 0.8049672842025757 Accuracy 98.07498601007275\n",
      "Training:: Epoch 175, Iteration 150, Current loss 0.9084381461143494 Accuracy 97.4080964906653\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 175, Probability Accuracy 71.26299518417866\n",
      "Starting Training\n",
      "Training:: Epoch 176, Iteration 0, Current loss 0.6393002271652222 Accuracy 98.01556420233463\n",
      "Training:: Epoch 176, Iteration 10, Current loss 0.49415919184684753 Accuracy 98.77783465574609\n",
      "Training:: Epoch 176, Iteration 20, Current loss 0.5868838429450989 Accuracy 98.45581747132232\n",
      "Training:: Epoch 176, Iteration 30, Current loss 0.5612620115280151 Accuracy 99.08376963350786\n",
      "Training:: Epoch 176, Iteration 40, Current loss 0.5133628249168396 Accuracy 99.04484848484849\n",
      "Training:: Epoch 176, Iteration 50, Current loss 0.4698101878166199 Accuracy 99.29312678598285\n",
      "Training:: Epoch 176, Iteration 60, Current loss 0.5181689858436584 Accuracy 98.84150675195451\n",
      "Training:: Epoch 176, Iteration 70, Current loss 0.4690459370613098 Accuracy 99.25170445097278\n",
      "Training:: Epoch 176, Iteration 80, Current loss 0.5216264724731445 Accuracy 98.94170862848853\n",
      "Training:: Epoch 176, Iteration 90, Current loss 0.6375658512115479 Accuracy 98.45962003960977\n",
      "Training:: Epoch 176, Iteration 100, Current loss 0.786431074142456 Accuracy 98.56690551574738\n",
      "Training:: Epoch 176, Iteration 110, Current loss 0.6678372621536255 Accuracy 98.78752512481358\n",
      "Training:: Epoch 176, Iteration 120, Current loss 0.5387813448905945 Accuracy 99.07171652875026\n",
      "Training:: Epoch 176, Iteration 130, Current loss 0.7542505860328674 Accuracy 98.65117244241544\n",
      "Training:: Epoch 176, Iteration 140, Current loss 0.387990266084671 Accuracy 99.23779174920334\n",
      "Training:: Epoch 176, Iteration 150, Current loss 0.472445011138916 Accuracy 99.22454672245468\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 176, Probability Accuracy 73.41365282559808\n",
      "Starting Training\n",
      "Training:: Epoch 177, Iteration 0, Current loss 0.5277767777442932 Accuracy 99.25309009187653\n",
      "Training:: Epoch 177, Iteration 10, Current loss 0.6750323176383972 Accuracy 99.31426332288402\n",
      "Training:: Epoch 177, Iteration 20, Current loss 0.42095935344696045 Accuracy 99.58888092633802\n",
      "Training:: Epoch 177, Iteration 30, Current loss 0.6363818645477295 Accuracy 98.82701162025872\n",
      "Training:: Epoch 177, Iteration 40, Current loss 0.3997264802455902 Accuracy 99.5396163469558\n",
      "Training:: Epoch 177, Iteration 50, Current loss 0.43393999338150024 Accuracy 98.80014326647564\n",
      "Training:: Epoch 177, Iteration 60, Current loss 0.47204264998435974 Accuracy 99.34481339187705\n",
      "Training:: Epoch 177, Iteration 70, Current loss 0.5822065472602844 Accuracy 99.28085025138421\n",
      "Training:: Epoch 177, Iteration 80, Current loss 0.538138747215271 Accuracy 99.29341379132498\n",
      "Training:: Epoch 177, Iteration 90, Current loss 0.47564756870269775 Accuracy 99.1951404707669\n",
      "Training:: Epoch 177, Iteration 100, Current loss 0.5957090854644775 Accuracy 99.13276709879302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 177, Iteration 110, Current loss 0.3778083920478821 Accuracy 99.14903583417909\n",
      "Training:: Epoch 177, Iteration 120, Current loss 0.4313381016254425 Accuracy 99.15611814345992\n",
      "Training:: Epoch 177, Iteration 130, Current loss 0.5668166875839233 Accuracy 98.79720003943606\n",
      "Training:: Epoch 177, Iteration 140, Current loss 0.5075043439865112 Accuracy 99.12109951041114\n",
      "Training:: Epoch 177, Iteration 150, Current loss 0.3889157176017761 Accuracy 99.52609024451726\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 177, Probability Accuracy 73.41577334947908\n",
      "Starting Training\n",
      "Training:: Epoch 178, Iteration 0, Current loss 0.46800294518470764 Accuracy 99.36475853605717\n",
      "Training:: Epoch 178, Iteration 10, Current loss 0.7055591344833374 Accuracy 99.09401867430896\n",
      "Training:: Epoch 178, Iteration 20, Current loss 0.5195679068565369 Accuracy 98.95917659303805\n",
      "Training:: Epoch 178, Iteration 30, Current loss 0.49303364753723145 Accuracy 99.0340909090909\n",
      "Training:: Epoch 178, Iteration 40, Current loss 0.5461888909339905 Accuracy 99.34438583270536\n",
      "Training:: Epoch 178, Iteration 50, Current loss 0.46493247151374817 Accuracy 99.31215198165738\n",
      "Training:: Epoch 178, Iteration 60, Current loss 0.4613155126571655 Accuracy 99.43038384361448\n",
      "Training:: Epoch 178, Iteration 70, Current loss 0.44449543952941895 Accuracy 99.51642615904207\n",
      "Training:: Epoch 178, Iteration 80, Current loss 0.48056530952453613 Accuracy 99.4598697848238\n",
      "Training:: Epoch 178, Iteration 90, Current loss 0.3977771997451782 Accuracy 99.38528466048798\n",
      "Training:: Epoch 178, Iteration 100, Current loss 0.5466907024383545 Accuracy 99.24115845168477\n",
      "Training:: Epoch 178, Iteration 110, Current loss 0.5355093479156494 Accuracy 99.1524835012157\n",
      "Training:: Epoch 178, Iteration 120, Current loss 0.32583749294281006 Accuracy 99.47013218807518\n",
      "Training:: Epoch 178, Iteration 130, Current loss 0.48436200618743896 Accuracy 99.39037825927286\n",
      "Training:: Epoch 178, Iteration 140, Current loss 0.4102737307548523 Accuracy 99.27722913983112\n",
      "Training:: Epoch 178, Iteration 150, Current loss 0.3504014313220978 Accuracy 99.34062311115996\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 178, Probability Accuracy 73.04412363770291\n",
      "Starting Training\n",
      "Training:: Epoch 179, Iteration 0, Current loss 0.4332234859466553 Accuracy 99.59014713717775\n",
      "Training:: Epoch 179, Iteration 10, Current loss 0.4009711742401123 Accuracy 99.42996742671009\n",
      "Training:: Epoch 179, Iteration 20, Current loss 0.3363663852214813 Accuracy 99.41137998691956\n",
      "Training:: Epoch 179, Iteration 30, Current loss 0.4702413082122803 Accuracy 99.50060024009603\n",
      "Training:: Epoch 179, Iteration 40, Current loss 0.3320385813713074 Accuracy 99.35825892857143\n",
      "Training:: Epoch 179, Iteration 50, Current loss 0.48994773626327515 Accuracy 99.3368969483727\n",
      "Training:: Epoch 179, Iteration 60, Current loss 0.5488633513450623 Accuracy 99.18283963227783\n",
      "Training:: Epoch 179, Iteration 70, Current loss 0.5100107192993164 Accuracy 99.22660479505026\n",
      "Training:: Epoch 179, Iteration 80, Current loss 0.46782851219177246 Accuracy 99.33206968137918\n",
      "Training:: Epoch 179, Iteration 90, Current loss 0.5562723278999329 Accuracy 99.16480110211813\n",
      "Training:: Epoch 179, Iteration 100, Current loss 0.3813714385032654 Accuracy 99.68895800933126\n",
      "Training:: Epoch 179, Iteration 110, Current loss 0.3939179480075836 Accuracy 99.47895791583166\n",
      "Training:: Epoch 179, Iteration 120, Current loss 0.47901755571365356 Accuracy 99.33207169097183\n",
      "Training:: Epoch 179, Iteration 130, Current loss 0.49672698974609375 Accuracy 99.20126857344218\n",
      "Training:: Epoch 179, Iteration 140, Current loss 0.34749463200569153 Accuracy 99.39296758675982\n",
      "Training:: Epoch 179, Iteration 150, Current loss 0.3958715796470642 Accuracy 99.468617971166\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 179, Probability Accuracy 72.89445929431197\n",
      "Starting Training\n",
      "Training:: Epoch 180, Iteration 0, Current loss 0.44778257608413696 Accuracy 99.38546627746197\n",
      "Training:: Epoch 180, Iteration 10, Current loss 0.4695006310939789 Accuracy 99.33189231676164\n",
      "Training:: Epoch 180, Iteration 20, Current loss 0.4815542697906494 Accuracy 99.47623666343355\n",
      "Training:: Epoch 180, Iteration 30, Current loss 0.5633111000061035 Accuracy 99.44507361268403\n",
      "Training:: Epoch 180, Iteration 40, Current loss 0.4543434977531433 Accuracy 99.52631578947368\n",
      "Training:: Epoch 180, Iteration 50, Current loss 0.3164232075214386 Accuracy 99.60131565832752\n",
      "Training:: Epoch 180, Iteration 60, Current loss 0.4459569454193115 Accuracy 99.2770503654912\n",
      "Training:: Epoch 180, Iteration 70, Current loss 0.2569628953933716 Accuracy 99.5495936551454\n",
      "Training:: Epoch 180, Iteration 80, Current loss 0.40686172246932983 Accuracy 99.4261994872421\n",
      "Training:: Epoch 180, Iteration 90, Current loss 0.4166908860206604 Accuracy 99.34802521687591\n",
      "Training:: Epoch 180, Iteration 100, Current loss 0.2966760993003845 Accuracy 99.63712168004942\n",
      "Training:: Epoch 180, Iteration 110, Current loss 0.32525092363357544 Accuracy 99.3688151912276\n",
      "Training:: Epoch 180, Iteration 120, Current loss 0.35270053148269653 Accuracy 99.55585300559747\n",
      "Training:: Epoch 180, Iteration 130, Current loss 0.41795486211776733 Accuracy 99.35461956521739\n",
      "Training:: Epoch 180, Iteration 140, Current loss 0.34786999225616455 Accuracy 99.60750308064442\n",
      "Training:: Epoch 180, Iteration 150, Current loss 0.3630369007587433 Accuracy 99.43406904357668\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 180, Probability Accuracy 72.74758511392236\n",
      "Starting Training\n",
      "Training:: Epoch 181, Iteration 0, Current loss 0.3278425633907318 Accuracy 99.4109246697608\n",
      "Training:: Epoch 181, Iteration 10, Current loss 0.41456708312034607 Accuracy 99.4635552873332\n",
      "Training:: Epoch 181, Iteration 20, Current loss 0.4081839621067047 Accuracy 99.64586448414259\n",
      "Training:: Epoch 181, Iteration 30, Current loss 0.4798501133918762 Accuracy 99.30555555555556\n",
      "Training:: Epoch 181, Iteration 40, Current loss 0.44097164273262024 Accuracy 99.50804162724693\n",
      "Training:: Epoch 181, Iteration 50, Current loss 0.3352918028831482 Accuracy 99.53075247192895\n",
      "Training:: Epoch 181, Iteration 60, Current loss 0.27520519495010376 Accuracy 99.56942314498053\n",
      "Training:: Epoch 181, Iteration 70, Current loss 0.49614185094833374 Accuracy 99.57674943566592\n",
      "Training:: Epoch 181, Iteration 80, Current loss 0.4214808940887451 Accuracy 99.38639234795163\n",
      "Training:: Epoch 181, Iteration 90, Current loss 0.4259625971317291 Accuracy 99.24219150025601\n",
      "Training:: Epoch 181, Iteration 100, Current loss 0.47123363614082336 Accuracy 99.46426238076572\n",
      "Training:: Epoch 181, Iteration 110, Current loss 0.4064328074455261 Accuracy 99.25646432138497\n",
      "Training:: Epoch 181, Iteration 120, Current loss 0.3725554049015045 Accuracy 99.40645324240703\n",
      "Training:: Epoch 181, Iteration 130, Current loss 0.2877385914325714 Accuracy 99.44998281196287\n",
      "Training:: Epoch 181, Iteration 140, Current loss 0.2899041473865509 Accuracy 99.54093343534812\n",
      "Training:: Epoch 181, Iteration 150, Current loss 0.36525532603263855 Accuracy 99.55073847363397\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 181, Probability Accuracy 72.27738684493949\n",
      "Starting Training\n",
      "Training:: Epoch 182, Iteration 0, Current loss 0.3992377519607544 Accuracy 99.29947460595447\n",
      "Training:: Epoch 182, Iteration 10, Current loss 0.40214765071868896 Accuracy 99.58055576361322\n",
      "Training:: Epoch 182, Iteration 20, Current loss 0.4322407841682434 Accuracy 99.58035288507392\n",
      "Training:: Epoch 182, Iteration 30, Current loss 0.3997850716114044 Accuracy 99.6235741444867\n",
      "Training:: Epoch 182, Iteration 40, Current loss 0.4072139263153076 Accuracy 99.35976718806839\n",
      "Training:: Epoch 182, Iteration 50, Current loss 0.5244196653366089 Accuracy 99.41588785046729\n",
      "Training:: Epoch 182, Iteration 60, Current loss 0.4437634348869324 Accuracy 99.47687453292369\n",
      "Training:: Epoch 182, Iteration 70, Current loss 0.3205665051937103 Accuracy 99.57602620928888\n",
      "Training:: Epoch 182, Iteration 80, Current loss 0.8031626343727112 Accuracy 99.2436098069901\n",
      "Training:: Epoch 182, Iteration 90, Current loss 0.5159896612167358 Accuracy 99.55253737628975\n",
      "Training:: Epoch 182, Iteration 100, Current loss 0.5016636848449707 Accuracy 99.1848617176128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 182, Iteration 110, Current loss 0.38164934515953064 Accuracy 99.44311853619729\n",
      "Training:: Epoch 182, Iteration 120, Current loss 0.4920888841152191 Accuracy 99.47584519961562\n",
      "Training:: Epoch 182, Iteration 130, Current loss 0.4006948471069336 Accuracy 99.44397817876626\n",
      "Training:: Epoch 182, Iteration 140, Current loss 0.3306970000267029 Accuracy 99.36086921786371\n",
      "Training:: Epoch 182, Iteration 150, Current loss 0.3696425259113312 Accuracy 99.35961459373715\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 182, Probability Accuracy 72.29490906858778\n",
      "Starting Training\n",
      "Training:: Epoch 183, Iteration 0, Current loss 0.44171375036239624 Accuracy 99.29093138561603\n",
      "Training:: Epoch 183, Iteration 10, Current loss 0.4500977396965027 Accuracy 99.57981040225468\n",
      "Training:: Epoch 183, Iteration 20, Current loss 0.39740481972694397 Accuracy 99.3627582506037\n",
      "Training:: Epoch 183, Iteration 30, Current loss 0.2559404671192169 Accuracy 99.52308944980807\n",
      "Training:: Epoch 183, Iteration 40, Current loss 0.45139214396476746 Accuracy 99.49851006613852\n",
      "Training:: Epoch 183, Iteration 50, Current loss 0.24520182609558105 Accuracy 99.69258715852722\n",
      "Training:: Epoch 183, Iteration 60, Current loss 0.33653607964515686 Accuracy 99.42632850241546\n",
      "Training:: Epoch 183, Iteration 70, Current loss 0.36863917112350464 Accuracy 99.56287691747278\n",
      "Training:: Epoch 183, Iteration 80, Current loss 0.3804595470428467 Accuracy 99.36373987006898\n",
      "Training:: Epoch 183, Iteration 90, Current loss 0.40429848432540894 Accuracy 99.49191909416433\n",
      "Training:: Epoch 183, Iteration 100, Current loss 0.3712724447250366 Accuracy 99.53648325358851\n",
      "Training:: Epoch 183, Iteration 110, Current loss 0.43275365233421326 Accuracy 99.41418962898676\n",
      "Training:: Epoch 183, Iteration 120, Current loss 0.5482487678527832 Accuracy 99.48215072719259\n",
      "Training:: Epoch 183, Iteration 130, Current loss 0.3099035620689392 Accuracy 99.58875237752532\n",
      "Training:: Epoch 183, Iteration 140, Current loss 0.5144611597061157 Accuracy 99.01577137436263\n",
      "Training:: Epoch 183, Iteration 150, Current loss 0.3677472174167633 Accuracy 99.53525277714805\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 183, Probability Accuracy 72.15562413156177\n",
      "Starting Training\n",
      "Training:: Epoch 184, Iteration 0, Current loss 0.4218430519104004 Accuracy 99.47202463885019\n",
      "Training:: Epoch 184, Iteration 10, Current loss 0.480487585067749 Accuracy 99.14888788016341\n",
      "Training:: Epoch 184, Iteration 20, Current loss 0.42759546637535095 Accuracy 99.33691000060281\n",
      "Training:: Epoch 184, Iteration 30, Current loss 0.4262182414531708 Accuracy 99.29568363918325\n",
      "Training:: Epoch 184, Iteration 40, Current loss 0.4006096124649048 Accuracy 99.26523947750363\n",
      "Training:: Epoch 184, Iteration 50, Current loss 0.2948508560657501 Accuracy 99.3879523247074\n",
      "Training:: Epoch 184, Iteration 60, Current loss 0.39094963669776917 Accuracy 99.65882098898726\n",
      "Training:: Epoch 184, Iteration 70, Current loss 0.38960063457489014 Accuracy 99.62033462033462\n",
      "Training:: Epoch 184, Iteration 80, Current loss 0.2637154161930084 Accuracy 99.56327226246272\n",
      "Training:: Epoch 184, Iteration 90, Current loss 0.46881672739982605 Accuracy 99.17555237990547\n",
      "Training:: Epoch 184, Iteration 100, Current loss 0.4117150902748108 Accuracy 99.3030891911814\n",
      "Training:: Epoch 184, Iteration 110, Current loss 0.33284634351730347 Accuracy 99.54632792485056\n",
      "Training:: Epoch 184, Iteration 120, Current loss 0.41815248131752014 Accuracy 99.42453570494376\n",
      "Training:: Epoch 184, Iteration 130, Current loss 0.35219281911849976 Accuracy 99.59470529901263\n",
      "Training:: Epoch 184, Iteration 140, Current loss 0.3911769390106201 Accuracy 99.47170697346795\n",
      "Training:: Epoch 184, Iteration 150, Current loss 0.3474489152431488 Accuracy 99.41696219270301\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 184, Probability Accuracy 72.00874995117215\n",
      "Starting Training\n",
      "Training:: Epoch 185, Iteration 0, Current loss 0.2319423407316208 Accuracy 99.69023122026773\n",
      "Training:: Epoch 185, Iteration 10, Current loss 0.2947998344898224 Accuracy 99.58853783982366\n",
      "Training:: Epoch 185, Iteration 20, Current loss 0.26612770557403564 Accuracy 99.56146064000694\n",
      "Training:: Epoch 185, Iteration 30, Current loss 0.3697262406349182 Accuracy 99.55500437700613\n",
      "Training:: Epoch 185, Iteration 40, Current loss 0.381448894739151 Accuracy 99.48205975078817\n",
      "Training:: Epoch 185, Iteration 50, Current loss 0.4128677546977997 Accuracy 99.50621488166185\n",
      "Training:: Epoch 185, Iteration 60, Current loss 0.30894899368286133 Accuracy 99.62324525185797\n",
      "Training:: Epoch 185, Iteration 70, Current loss 0.2792108654975891 Accuracy 99.37227434914762\n",
      "Training:: Epoch 185, Iteration 80, Current loss 0.35476261377334595 Accuracy 99.47891222927862\n",
      "Training:: Epoch 185, Iteration 90, Current loss 0.31881019473075867 Accuracy 99.52737020316027\n",
      "Training:: Epoch 185, Iteration 100, Current loss 0.6233760118484497 Accuracy 99.17290271760535\n",
      "Training:: Epoch 185, Iteration 110, Current loss 0.3575071692466736 Accuracy 99.51129086619481\n",
      "Training:: Epoch 185, Iteration 120, Current loss 0.29888302087783813 Accuracy 99.48824994949835\n",
      "Training:: Epoch 185, Iteration 130, Current loss 0.2831840515136719 Accuracy 99.58262639972854\n",
      "Training:: Epoch 185, Iteration 140, Current loss 0.44726571440696716 Accuracy 99.41690962099125\n",
      "Training:: Epoch 185, Iteration 150, Current loss 0.29380619525909424 Accuracy 99.6461852793143\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 185, Probability Accuracy 71.50328402185255\n",
      "Starting Training\n",
      "Training:: Epoch 186, Iteration 0, Current loss 0.3925095200538635 Accuracy 99.47063452321339\n",
      "Training:: Epoch 186, Iteration 10, Current loss 0.37069210410118103 Accuracy 99.52159842408892\n",
      "Training:: Epoch 186, Iteration 20, Current loss 0.3014070987701416 Accuracy 99.51578635308068\n",
      "Training:: Epoch 186, Iteration 30, Current loss 0.5398019552230835 Accuracy 99.35166380253085\n",
      "Training:: Epoch 186, Iteration 40, Current loss 0.6088369488716125 Accuracy 99.10230614455966\n",
      "Training:: Epoch 186, Iteration 50, Current loss 0.3746727705001831 Accuracy 99.58294717330863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 187, Iteration 110, Current loss 0.4555128216743469 Accuracy 99.39260563380282\n",
      "Training:: Epoch 187, Iteration 120, Current loss 0.38715529441833496 Accuracy 99.50753708485976\n",
      "Training:: Epoch 187, Iteration 130, Current loss 0.46430540084838867 Accuracy 99.40633245382585\n",
      "Training:: Epoch 187, Iteration 140, Current loss 0.3018034100532532 Accuracy 99.46796553016111\n",
      "Training:: Epoch 187, Iteration 150, Current loss 0.41861453652381897 Accuracy 99.35201851375675\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 187, Probability Accuracy 71.61544857450572\n",
      "Starting Training\n",
      "Training:: Epoch 188, Iteration 0, Current loss 0.4354614019393921 Accuracy 99.5981220657277\n",
      "Training:: Epoch 188, Iteration 10, Current loss 0.33277326822280884 Accuracy 99.48073701842546\n",
      "Training:: Epoch 188, Iteration 20, Current loss 0.5041451454162598 Accuracy 99.33454325468844\n",
      "Training:: Epoch 188, Iteration 30, Current loss 0.2775978744029999 Accuracy 99.60788973384031\n",
      "Training:: Epoch 188, Iteration 40, Current loss 0.5271046161651611 Accuracy 99.39896184318368\n",
      "Training:: Epoch 188, Iteration 50, Current loss 0.3924112617969513 Accuracy 99.5595459935626\n",
      "Training:: Epoch 188, Iteration 60, Current loss 0.3960474133491516 Accuracy 99.34131736526946\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-76d53e4fb509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmiddle_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_stages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mfinal_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_dilated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 259\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initialize_epoch = 15\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0\n",
    "for epoch in range(1000):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        \n",
    "        loss = 0\n",
    "        for p in predictions:\n",
    "            loss += ce_criterion(p, item_2)\n",
    "            loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(p[:, :, 1:], dim=1), \n",
    "                                                                F.log_softmax(p.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                        max=16) * src_mask_mse[:, :, 1:])\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(predictions[-1], dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "    # Calculating Expectation Step\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Calculating Validation Data Accuracy\")\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, item in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            item_0 = item[0].to(device)\n",
    "            item_1 = item[1].to(device)\n",
    "            item_2 = item[2].to(device)\n",
    "            src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "            src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "            middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "            pred = torch.argmax(predictions[-1], dim=1)\n",
    "            correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "            total += float(torch.sum(src_mask).item())\n",
    "    val_acc = correct * 100.0 / total\n",
    "    if val_acc > best_val_acc:\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-best-model.wt\")\n",
    "    torch.save(model.state_dict(), config.output_dir + \"ms-tcn-last-model.wt\")\n",
    "    print(f\"Validation:: Epoch {epoch}, Probability Accuracy {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\n",
    "\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-speed/final-em-maximized.wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split3/ms-tcn-initial-15-epochs.wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 57.770101729343025\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "\n",
    "print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_labels(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            select_item = np.random.randint(start, i, 1)[0]\n",
    "            unique_ids.append(select_item)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    select_item = np.random.randint(start, len(labels_arr), 1)[0]\n",
    "    unique_ids.append(select_item)\n",
    "    return unique_ids\n",
    "# get_selected_labels(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            unique_ids.append(i - 1)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    unique_ids.append(len(labels_arr) - 1)\n",
    "    return unique_ids\n",
    "# get_boundary(np.array([2, 2, 2, 2, 3, 3, 4, 4, 4, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "boundary_dict = {}\n",
    "for file in glob.glob(\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/*txt\"):\n",
    "    video_id = file.split(\"/\")[-1].split(\".txt\")[0]\n",
    "    data = open(file).read().split(\"\\n\")[0:-1]\n",
    "    data = np.array(data)\n",
    "    boundary = get_boundary(data)\n",
    "    boundary_dict[video_id] = boundary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[5]\n",
    "    for i, count in enumerate(count_all):\n",
    "        video_id = video_ids[i]\n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_selected_labels(labels)\n",
    "\n",
    "        loaded_vidid_selected_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_boundary_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[4]\n",
    "    for i, count in enumerate(count_all):\n",
    "        \n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_boundary(labels)\n",
    "        video_id = video_ids[i]\n",
    "        video_id_boundary_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in video_id_boundary_frames.keys():\n",
    "    if len(video_id_boundary_frames[ele]) != len(loaded_vidid_selected_frames[ele + \".txt\"]):\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(video_id_boundary_frames, open(\"dump_dir/video_id_boundary_frames_dict.pkl\", \"wb\"))\n",
    "# pickle.dump(loaded_vidid_selected_frames, open(\"dump_dir/loaded_vidid_selected_frames_dict.pkl\", \"wb\"))\n",
    "pickle.dump(boundary_dict, open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "    \n",
    "    weights = [1, 1, 1, 1, 0, 0]\n",
    "    ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "    for i, outp_ele in enumerate(outp[1]):\n",
    "        upped_logit = F.upsample(outp_ele, size=outp[0].shape[-1], mode='linear', align_corners=True)\n",
    "        ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i + 1] / sum(weights)\n",
    "    \n",
    "    return ensemble_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/results/c2f-tcn-model/split2_c2ftcn_model.wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration done\n",
      "11 iteration done\n",
      "21 iteration done\n",
      "31 iteration done\n",
      "41 iteration done\n",
      "51 iteration done\n",
      "61 iteration done\n",
      "71 iteration done\n",
      "81 iteration done\n",
      "91 iteration done\n",
      "101 iteration done\n",
      "111 iteration done\n",
      "121 iteration done\n",
      "131 iteration done\n",
      "141 iteration done\n",
      "151 iteration done\n",
      "Train Boundary avergage error = 107.269\n",
      "Train From boundary avergage accuracy = 87.407\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        calculate_element_probb(prob, item_1, item[4], item_2)\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f'{i+1} iteration done')\n",
    "get_boundary_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    idx = 4\n",
    "\n",
    "    cur_vid_feat = torch.softmax(predictions[-1], dim=1).permute(0, 2, 1)[idx]\n",
    "    cur_vidid = item[4][idx]\n",
    "\n",
    "    selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "    \n",
    "    bound_list = video_id_boundary_frames[cur_vidid]\n",
    "    \n",
    "    cumsum_feat = torch.cumsum(cur_vid_feat, dim=0)\n",
    "    prob_each_segment = prob_vals_per_segment(selected_frames, cur_vid_feat, item_2[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min prob 0 = 7.953912266787591e-36\n",
      "Min prob 1 = 2.7495868628582206e-249\n",
      "Min prob 2 = 8.185175464823537e-201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 442)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ymax = 1\n",
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "\n",
    "plt.plot([0, item[1][idx].item()], [0, 0])\n",
    "\n",
    "for pivot in loaded_vidid_selected_frames[cur_vidid + \".txt\"]:\n",
    "    plt.plot([pivot, pivot], [0, ymax], '--')\n",
    "    \n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax])\n",
    "    \n",
    "all_prob = []\n",
    "for i, segment in enumerate(prob_each_segment):\n",
    "    segment = segment.cpu().numpy()\n",
    "    print(f\"Min prob {i} = {np.min(segment).item()}\")\n",
    "    xs = np.arange(loaded_vidid_selected_frames[cur_vidid + \".txt\"][i], \n",
    "                   loaded_vidid_selected_frames[cur_vidid + \".txt\"][i+1])\n",
    "    plt.plot(xs, segment, '*')\n",
    "\n",
    "    \n",
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "\n",
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))\n",
    "loss_arr = -loss_arr[0].detach().cpu().numpy()\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr/np.max(loss_arr), 'k+')\n",
    "\n",
    "plt.ylim([0, ymax])\n",
    "plt.xlim([0, item[1][idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1895, 48])\n"
     ]
    }
   ],
   "source": [
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "print(loss_prob.shape)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1387])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf5b8ecb00>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3987, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([75, 345, 1305, 1415, 1440, 1519],\n",
       " [73, 282, 455, 1312, 1434, 1458],\n",
       " tensor(1520, device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid], loaded_vidid_selected_frames[cur_vidid + \".txt\"], item_1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_features_shortened = cur_vid_feat[:item[1][idx]]\n",
    "cur_features_shortened[:, item[2][idx][:item[1][idx]]].shape\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 47, 47, 47, 47, 47, 47,  4,  4,  4,  4,  4,  4,  4,  4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[2][idx][video_id_boundary_frames[cur_vidid][1]-6:video_id_boundary_frames[cur_vidid][1]+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid][0]-6,video_id_boundary_frames[cur_vidid][0]+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0794, 13.3350],\n",
       "        [ 1.0831, 13.5102],\n",
       "        [ 1.1767, 13.7662],\n",
       "        [ 1.3750, 14.0177],\n",
       "        [ 1.7298, 14.1727],\n",
       "        [ 2.1706, 14.0367],\n",
       "        [ 2.7338, 13.4563],\n",
       "        [ 4.5600, 12.1745],\n",
       "        [ 7.8450, 10.3246],\n",
       "        [11.0151,  8.3563],\n",
       "        [13.4775,  6.6546],\n",
       "        [14.1836,  5.2898],\n",
       "        [11.2142,  4.2791],\n",
       "        [ 5.9829,  3.5694],\n",
       "        [ 2.3440,  2.9389],\n",
       "        [ 0.7406,  2.3515]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_vid_feat[video_id_boundary_frames[cur_vidid][1]-7:video_id_boundary_frames[cur_vidid][1]+9][:,[47,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frames_and_get_classification_value(model_output, count_list, video_ids_list, labels_all, p_criterion):\n",
    "    new_selected_frames = {}\n",
    "#     new_selected_frames_probs_values = {}\n",
    "#     new_selected_frames_labels = {}\n",
    "    \n",
    "    labels_arr = []\n",
    "    probs_arr = []\n",
    "    \n",
    "    for i, video_id in enumerate(video_ids_list):\n",
    "        current_boundary_frames = boundaries_dict[video_id]\n",
    "        current_video_logits = model_output[i].T\n",
    "        current_video_count = count_list[i]\n",
    "        current_selected_frames = loaded_vidid_selected_frames[video_id]\n",
    "        labels = labels_all[i]\n",
    "        cur_seg_start = 0\n",
    "        for seg_num, ele in enumerate(current_boundary_frames):\n",
    "            cur_seg_end = ele + 1\n",
    "            total_seg_len = cur_seg_end - cur_seg_start\n",
    "            per_frames_far_from_boundary = int(0.3 * total_seg_len)\n",
    "            new_frames_start = cur_seg_start + per_frames_far_from_boundary\n",
    "            new_frames_end = cur_seg_end - per_frames_far_from_boundary\n",
    "            \n",
    "            frames_added = list(range(new_frames_start, new_frames_end))\n",
    "            frames_added.append(current_selected_frames[seg_num])\n",
    "            frames_added = list(set(frames_added))\n",
    "            frames_added.sort()\n",
    "            new_selected_frames[video_id] = frames_added\n",
    "\n",
    "            \n",
    "            labels_seg = torch.stack([labels[current_selected_frames[seg_num]]] * len(frames_added))\n",
    "            frames_added = torch.tensor(np.array(frames_added), dtype=torch.long, device=current_video_logits.device)\n",
    "            prob_seg = current_video_logits[frames_added, :]\n",
    "            \n",
    "            labels_arr.append(labels_seg)\n",
    "            probs_arr.append(prob_seg)\n",
    "            \n",
    "            cur_seg_start = cur_seg_end\n",
    "            \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    probs_arr = torch.cat(probs_arr)\n",
    "    loss = p_criterion(probs_arr, labels_arr)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
