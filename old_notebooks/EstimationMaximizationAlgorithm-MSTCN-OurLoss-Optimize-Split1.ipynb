{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mstcn_model import *\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 8, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 1, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split1/', 'project_name': 'breakfast-split-1', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split1.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split1.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split1_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=1,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split1/\",\n",
    "    project_name=\"breakfast-split-1\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1460\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 252\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", allow_pickle=True).item()\n",
    "# loaded_vidid_selected_frames\n",
    "video_id_boundary_frames = pickle.load(open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"rb\"))\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_var_actions = pickle.load(open(\"dump_dir/mean_var_actions.pkl\", \"rb\"))\n",
    "def get_possion_prob(cur_len, cur_class):\n",
    "    mean_class, std_class = loaded_mean_var_actions[cur_class]\n",
    "    mean_class = mean_class * 10\n",
    "    prob = cur_len * torch.log(torch.tensor(mean_class, device=device) + 1e-5)\n",
    "    prob = prob - mean_class\n",
    "    factorials = torch.cumsum(torch.log(torch.arange(1, torch.max(cur_len)+1, 1).type(torch.float).to(device)), \n",
    "                              dim=0)[min(cur_len)-1:]\n",
    "    prob = prob - factorials\n",
    "    return prob\n",
    "\n",
    "# get_possion_prob(torch.arange(10, 21), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_video_each_segment_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vals_per_segment(selected_frames, cur_vid_feat, labels):\n",
    "    prob_each_segment = []\n",
    "    \n",
    "    log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "    cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "    cur_boundary = 0\n",
    "    for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "\n",
    "        next_ele = selected_frames[i + 1]\n",
    "        label_cur_ele = labels[cur_ele]\n",
    "        label_next_ele = labels[next_ele]\n",
    "\n",
    "        indices = torch.arange(cur_ele, next_ele).to(cumsum_feat.device)\n",
    "        strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "        end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "        left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "        right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "\n",
    "        cur_boundary_len = indices - cur_boundary\n",
    "        len_prob = get_possion_prob(cur_boundary_len, label_cur_ele.item())\n",
    "\n",
    "        prob = torch.softmax((left_sum + right_sum + len_prob), dim=0)\n",
    "#         prob = torch.softmax((left_sum + right_sum), dim=0)\n",
    "\n",
    "        cur_boundary = round(np.sum(np.arange(cur_ele, next_ele, 1) * prob.detach().cpu().numpy()).item())\n",
    "        prob_each_segment.append(prob)\n",
    "    \n",
    "    return prob_each_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_element_probb(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_video = prob_vals_per_segment(selected_frames, cur_vid_feat, labels)\n",
    "        prob_video_each_segment_dict[cur_vidid] = prob_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_loss(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    loss_arr = []\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_each_video = prob_video_each_segment_dict[cur_vidid]\n",
    "        \n",
    "        log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "        cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "        \n",
    "        \n",
    "        current_vid_prob_arr = []\n",
    "        # Start segment log probability\n",
    "        start_ele_idx = selected_frames[0]\n",
    "        if start_ele_idx > 0:\n",
    "            start_ele_label_idx = labels[selected_frames[0]]\n",
    "            first_segment_prob = (cumsum_feat[start_ele_idx - 1, :])[start_ele_label_idx]\n",
    "            current_vid_prob_arr.append(first_segment_prob.unsqueeze(0))\n",
    "        \n",
    "        for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "            next_ele = selected_frames[i + 1]\n",
    "            label_cur_ele = labels[cur_ele]\n",
    "            label_next_ele = labels[next_ele]\n",
    "\n",
    "            indices = torch.arange(cur_ele, next_ele)\n",
    "            strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "            end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "            left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "            right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "            current_vid_prob_arr.append((left_sum + right_sum)  * (prob_each_video[i]))\n",
    "        \n",
    "        # End segment log probability\n",
    "        end_index = cur_vid_count.item() - 1\n",
    "        last_ele_label_idx = labels[selected_frames[-1]]\n",
    "        last_segment_sum_prob = (cumsum_feat[end_index, :] - cumsum_feat[selected_frames[-1] - 1, :])[last_ele_label_idx]\n",
    "        current_vid_prob_arr.append(last_segment_sum_prob.unsqueeze(0))\n",
    "            \n",
    "        loss_arr.append(torch.cat(current_vid_prob_arr))\n",
    "    return -torch.mean(torch.cat(loss_arr)), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_estimated_boundaries():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames, video_id_boundary_frames\n",
    "    estimated_boundary_dict = {}\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        selected_ele_list = loaded_vidid_selected_frames[ele + \".txt\"]\n",
    "        boundary_list = []\n",
    "        for i, segment in enumerate(estimated_boundary_probs):\n",
    "            estimated_boundary = np.sum(np.arange(selected_ele_list[i], selected_ele_list[i + 1], 1) \\\n",
    "                                        * segment.detach().cpu().numpy())\n",
    "            estimated_boundary = round(estimated_boundary.item())\n",
    "            \n",
    "            if (estimated_boundary < selected_ele_list[i]) or (estimated_boundary > selected_ele_list[i + 1]):\n",
    "                print(\"Estimated value wrong\")\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            boundary_list.append(estimated_boundary)\n",
    "\n",
    "        estimated_boundary_dict[ele] = boundary_list\n",
    "    return estimated_boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_boundary_err():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    err_list = []\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    estimated_boundar_dict = get_estimated_boundaries()\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if ele + \".txt\" not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        estimated_boundary_list = estimated_boundar_dict[ele]\n",
    "        \n",
    "        estimated = np.array(estimated_boundary_list) #.detach().cpu().numpy()\n",
    "        actual = np.array(video_id_boundary_frames[ele][:-1])\n",
    "        if len(actual) != len(estimated):\n",
    "            print(ele)\n",
    "            continue\n",
    "        \n",
    "        mse_err = (actual - estimated)**2\n",
    "        err_list.append(mse_err)\n",
    "        \n",
    "        estimated_labels = []\n",
    "        actual_labels = []\n",
    "        start_v1 = 0\n",
    "        start_v2 = 0\n",
    "        for i, v1 in enumerate(estimated):\n",
    "            estimated_labels.extend([i] * (v1 - start_v1))\n",
    "            start_v1 = v1\n",
    "            v2 = actual[i]\n",
    "            actual_labels.extend([i] * (v2 - start_v2))\n",
    "            start_v2 = v2\n",
    "            \n",
    "        last_ele = video_id_boundary_frames[ele][-1]\n",
    "        estimated_labels.extend([i+1] * (last_ele - start_v1))\n",
    "        actual_labels.extend([i+1] * (last_ele - start_v2))\n",
    "        \n",
    "        correct += np.sum(np.array(actual_labels) == np.array(estimated_labels))\n",
    "        total += len(actual_labels)\n",
    "        \n",
    "    print(f\"Train Boundary avergage error = {np.sqrt(np.mean(np.concatenate(err_list))):.3f}\")\n",
    "    print(f\"Train From boundary avergage accuracy = {correct * 100.0 / total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = MultiStageModel(num_stages=4, num_layers=10, num_f_maps=64, dim=2048, num_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(labels_all, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((labels_all.shape[0], labels_all.shape[1]), dtype=torch.long, device=labels_all.device) * (-100)\n",
    "    for iter_num, labels in enumerate(labels_all):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(loaded_vidid_selected_frames[cur_vidid + \".txt\"]))\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = labels[frame_idx_tensor]\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split1/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Training:: Epoch 15, Iteration 0, Current loss 1.4878519773483276 Accuracy 81.80546589197331\n",
      "Training:: Epoch 15, Iteration 10, Current loss 1.5497926473617554 Accuracy 69.16089400078421\n",
      "Training:: Epoch 15, Iteration 20, Current loss 1.2525343894958496 Accuracy 77.62193845509734\n",
      "Training:: Epoch 15, Iteration 30, Current loss 1.6481863260269165 Accuracy 66.33730834752981\n",
      "Training:: Epoch 15, Iteration 40, Current loss 1.8469901084899902 Accuracy 80.26038437693738\n",
      "Training:: Epoch 15, Iteration 50, Current loss 1.8984049558639526 Accuracy 76.39811880175851\n",
      "Training:: Epoch 15, Iteration 60, Current loss 1.526989221572876 Accuracy 77.93367346938776\n",
      "Training:: Epoch 15, Iteration 70, Current loss 1.6769968271255493 Accuracy 66.9547926304683\n",
      "Training:: Epoch 15, Iteration 80, Current loss 1.2930707931518555 Accuracy 63.7310146658208\n",
      "Training:: Epoch 15, Iteration 90, Current loss 1.3917895555496216 Accuracy 73.70881649180932\n",
      "Training:: Epoch 15, Iteration 100, Current loss 1.0147767066955566 Accuracy 60.770306723948444\n",
      "Training:: Epoch 15, Iteration 110, Current loss 1.9100109338760376 Accuracy 86.91873098652759\n",
      "Training:: Epoch 15, Iteration 120, Current loss 0.7607787251472473 Accuracy 71.7467845081555\n",
      "Training:: Epoch 15, Iteration 130, Current loss 1.2818576097488403 Accuracy 84.7590709976031\n",
      "Training:: Epoch 15, Iteration 140, Current loss 1.1327120065689087 Accuracy 74.69768395162943\n",
      "Training:: Epoch 15, Iteration 150, Current loss 1.141944408416748 Accuracy 84.78933587241133\n",
      "Training:: Epoch 15, Iteration 160, Current loss 1.2431458234786987 Accuracy 72.92005130397605\n",
      "Training:: Epoch 15, Iteration 170, Current loss 1.1415585279464722 Accuracy 75.77487765089722\n",
      "Training:: Epoch 15, Iteration 180, Current loss 1.3202824592590332 Accuracy 53.6767302259887\n",
      "Calculating Expectation\n",
      "Epoch 15 iter 0\n",
      "Epoch 15 iter 10\n",
      "Epoch 15 iter 20\n",
      "Epoch 15 iter 30\n",
      "Epoch 15 iter 40\n",
      "Epoch 15 iter 50\n",
      "Epoch 15 iter 60\n",
      "Epoch 15 iter 70\n",
      "Epoch 15 iter 80\n",
      "Epoch 15 iter 90\n",
      "Epoch 15 iter 100\n",
      "Epoch 15 iter 110\n",
      "Epoch 15 iter 120\n",
      "Epoch 15 iter 130\n",
      "Epoch 15 iter 140\n",
      "Epoch 15 iter 150\n",
      "Epoch 15 iter 160\n",
      "Epoch 15 iter 170\n",
      "Epoch 15 iter 180\n",
      "Train Boundary avergage error = 88.565\n",
      "Train From boundary avergage accuracy = 89.226\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 59.70179374859028\n",
      "Starting Training\n",
      "Training:: Epoch 16, Iteration 0, Current loss 2.7878717766960666 Accuracy 77.23806252147028\n",
      "Training:: Epoch 16, Iteration 10, Current loss 2.7087951331288913 Accuracy 79.68655207280081\n",
      "Training:: Epoch 16, Iteration 20, Current loss 3.41822273345554 Accuracy 74.70990776554596\n",
      "Training:: Epoch 16, Iteration 30, Current loss 5.324740929513146 Accuracy 57.60549558390579\n",
      "Training:: Epoch 16, Iteration 40, Current loss 12.175282928622272 Accuracy 38.80252100840336\n",
      "Training:: Epoch 16, Iteration 50, Current loss 6.88718954286179 Accuracy 65.51203682740925\n",
      "Training:: Epoch 16, Iteration 60, Current loss 2.859626070242321 Accuracy 82.01958041958042\n",
      "Training:: Epoch 16, Iteration 70, Current loss 3.5351195373952318 Accuracy 82.04661739624787\n",
      "Training:: Epoch 16, Iteration 80, Current loss 4.90070325674732 Accuracy 71.13847483257723\n",
      "Training:: Epoch 16, Iteration 90, Current loss 5.421831000648129 Accuracy 69.88903115663679\n",
      "Training:: Epoch 16, Iteration 100, Current loss 6.706898943555224 Accuracy 46.44698984854242\n",
      "Training:: Epoch 16, Iteration 110, Current loss 3.5102510430135214 Accuracy 72.3334695545566\n",
      "Training:: Epoch 16, Iteration 120, Current loss 2.697551709071072 Accuracy 67.31987331749802\n",
      "Training:: Epoch 16, Iteration 130, Current loss 3.4348222936155253 Accuracy 75.6247352816603\n",
      "Validation:: Epoch 16, Probability Accuracy 62.42209480394601\n",
      "Starting Training\n",
      "Training:: Epoch 17, Iteration 0, Current loss 2.47044664748456 Accuracy 84.83120780195048\n",
      "Training:: Epoch 17, Iteration 10, Current loss 2.487809898438754 Accuracy 79.69991697307556\n",
      "Training:: Epoch 17, Iteration 20, Current loss 2.978639148474638 Accuracy 69.896792534454\n",
      "Training:: Epoch 17, Iteration 30, Current loss 2.0641263188666077 Accuracy 82.61315953932684\n",
      "Training:: Epoch 17, Iteration 40, Current loss 2.0659848322151717 Accuracy 84.72448646886208\n",
      "Training:: Epoch 17, Iteration 50, Current loss 6.48058409717158 Accuracy 50.260361317747076\n",
      "Training:: Epoch 17, Iteration 60, Current loss 2.388817906663412 Accuracy 74.20979237682057\n",
      "Training:: Epoch 17, Iteration 70, Current loss 2.1553603003078905 Accuracy 83.96315096822711\n",
      "Training:: Epoch 17, Iteration 80, Current loss 2.270663971579111 Accuracy 79.66016991504247\n",
      "Training:: Epoch 17, Iteration 90, Current loss 3.137041678014991 Accuracy 75.53114086146682\n",
      "Training:: Epoch 17, Iteration 100, Current loss 2.0183927350077995 Accuracy 81.54617499595666\n",
      "Training:: Epoch 17, Iteration 110, Current loss 1.6999804738425888 Accuracy 85.53112336411347\n",
      "Training:: Epoch 17, Iteration 120, Current loss 5.018821146637828 Accuracy 68.58790976879426\n",
      "Training:: Epoch 17, Iteration 130, Current loss 3.7974299764850263 Accuracy 76.77107571147306\n",
      "Training:: Epoch 17, Iteration 140, Current loss 1.7550013735363161 Accuracy 88.68293956402559\n",
      "Training:: Epoch 17, Iteration 150, Current loss 2.8661666426326047 Accuracy 74.09937259664035\n",
      "Training:: Epoch 17, Iteration 160, Current loss 1.7573482465988428 Accuracy 83.87164761051689\n",
      "Training:: Epoch 17, Iteration 170, Current loss 2.156229353834445 Accuracy 85.26471833968657\n",
      "Training:: Epoch 17, Iteration 180, Current loss 2.686319256363668 Accuracy 80.73354437288505\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 17, Probability Accuracy 66.2133820846738\n",
      "Starting Training\n",
      "Training:: Epoch 18, Iteration 0, Current loss 2.585275117471192 Accuracy 79.05187009388949\n",
      "Training:: Epoch 18, Iteration 10, Current loss 2.354419112918441 Accuracy 84.02911582102334\n",
      "Training:: Epoch 18, Iteration 20, Current loss 3.8089806662800694 Accuracy 71.8483690861005\n",
      "Training:: Epoch 18, Iteration 30, Current loss 1.5037981108421943 Accuracy 88.1155486622722\n",
      "Training:: Epoch 18, Iteration 40, Current loss 2.555539970586925 Accuracy 75.29280830992633\n",
      "Training:: Epoch 18, Iteration 50, Current loss 2.894892136881915 Accuracy 77.67168946600931\n",
      "Training:: Epoch 18, Iteration 60, Current loss 2.2086173084377547 Accuracy 83.2515401482719\n",
      "Training:: Epoch 18, Iteration 70, Current loss 2.2491033895858217 Accuracy 84.76775279786075\n",
      "Training:: Epoch 18, Iteration 80, Current loss 2.461341352559104 Accuracy 80.62860808210391\n",
      "Training:: Epoch 18, Iteration 90, Current loss 1.5652589476842425 Accuracy 85.8334202022308\n",
      "Training:: Epoch 18, Iteration 100, Current loss 2.839017719172155 Accuracy 77.01642559450846\n",
      "Training:: Epoch 18, Iteration 110, Current loss 1.4895177386164282 Accuracy 86.72456575682382\n",
      "Training:: Epoch 18, Iteration 120, Current loss 1.328786215543622 Accuracy 87.0340616966581\n",
      "Training:: Epoch 18, Iteration 130, Current loss 1.9729453370311127 Accuracy 79.28129829984545\n",
      "Training:: Epoch 18, Iteration 140, Current loss 1.3301432321612618 Accuracy 84.97489839827875\n",
      "Training:: Epoch 18, Iteration 150, Current loss 6.412598780349526 Accuracy 61.40550713153724\n",
      "Training:: Epoch 18, Iteration 160, Current loss 2.4344157485203035 Accuracy 80.71155173495639\n",
      "Training:: Epoch 18, Iteration 170, Current loss 2.696219641240521 Accuracy 81.59080778564707\n",
      "Training:: Epoch 18, Iteration 180, Current loss 4.468673781903502 Accuracy 70.70038692312006\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 18, Probability Accuracy 59.76807499475686\n",
      "Starting Training\n",
      "Training:: Epoch 19, Iteration 0, Current loss 3.025560225584197 Accuracy 70.27193306263074\n",
      "Training:: Epoch 19, Iteration 10, Current loss 3.785348071112105 Accuracy 72.95727913728743\n",
      "Training:: Epoch 19, Iteration 20, Current loss 3.700468965780382 Accuracy 77.01321871705458\n",
      "Training:: Epoch 19, Iteration 30, Current loss 1.9689499479903745 Accuracy 80.77693231878254\n",
      "Training:: Epoch 19, Iteration 40, Current loss 4.2891880200668515 Accuracy 75.45599216550373\n",
      "Training:: Epoch 19, Iteration 50, Current loss 1.5139627179399977 Accuracy 91.42137660884164\n",
      "Training:: Epoch 19, Iteration 60, Current loss 1.6468727965392276 Accuracy 84.93068475064514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 19, Iteration 70, Current loss 3.0524833589722236 Accuracy 74.71530787060365\n",
      "Training:: Epoch 19, Iteration 80, Current loss 2.0651702590364884 Accuracy 85.42883211678833\n",
      "Training:: Epoch 19, Iteration 90, Current loss 1.7124813506378262 Accuracy 87.35917201229044\n",
      "Training:: Epoch 19, Iteration 100, Current loss 3.3362358321085064 Accuracy 76.81374425727412\n",
      "Training:: Epoch 19, Iteration 110, Current loss 1.7950045622553805 Accuracy 81.42073535002444\n",
      "Training:: Epoch 19, Iteration 120, Current loss 2.126823425286961 Accuracy 83.85610879578854\n",
      "Training:: Epoch 19, Iteration 130, Current loss 2.9768558594280914 Accuracy 73.15580286168522\n",
      "Training:: Epoch 19, Iteration 140, Current loss 2.817849183660167 Accuracy 79.15938410320433\n",
      "Training:: Epoch 19, Iteration 150, Current loss 4.321754579686055 Accuracy 68.10666968633224\n",
      "Training:: Epoch 19, Iteration 160, Current loss 2.4505387251346438 Accuracy 74.4894366197183\n",
      "Training:: Epoch 19, Iteration 170, Current loss 1.8177532324611425 Accuracy 85.89160957742658\n",
      "Training:: Epoch 19, Iteration 180, Current loss 2.807147390705682 Accuracy 78.93739638430567\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 19, Probability Accuracy 63.77462793467637\n",
      "Starting Training\n",
      "Training:: Epoch 20, Iteration 0, Current loss 4.6129824758312 Accuracy 68.12021213906894\n",
      "Training:: Epoch 20, Iteration 10, Current loss 1.3567329505017007 Accuracy 87.92406932800486\n",
      "Training:: Epoch 20, Iteration 20, Current loss 1.7312462319099824 Accuracy 84.48638254103126\n",
      "Training:: Epoch 20, Iteration 30, Current loss 3.6902401412695336 Accuracy 72.41084318023846\n",
      "Training:: Epoch 20, Iteration 40, Current loss 2.469621871147151 Accuracy 82.13096559378468\n",
      "Training:: Epoch 20, Iteration 50, Current loss 1.5437802674691423 Accuracy 83.92857142857143\n",
      "Training:: Epoch 20, Iteration 60, Current loss 1.789797643329139 Accuracy 84.88930438108189\n",
      "Training:: Epoch 20, Iteration 70, Current loss 1.8599596847973607 Accuracy 80.8414409676571\n",
      "Training:: Epoch 20, Iteration 80, Current loss 1.6289255315263804 Accuracy 87.1736882915482\n",
      "Training:: Epoch 20, Iteration 90, Current loss 1.9875021238075226 Accuracy 83.2337565347274\n",
      "Training:: Epoch 20, Iteration 100, Current loss 1.635856125614712 Accuracy 88.0970331278549\n",
      "Training:: Epoch 20, Iteration 110, Current loss 1.5105848025368112 Accuracy 85.1881042454813\n",
      "Training:: Epoch 20, Iteration 120, Current loss 3.462207228162604 Accuracy 80.5078791528656\n",
      "Training:: Epoch 20, Iteration 130, Current loss 1.9942178717773384 Accuracy 86.26831835478909\n",
      "Training:: Epoch 20, Iteration 140, Current loss 1.1558521348670807 Accuracy 87.37627778679214\n",
      "Training:: Epoch 20, Iteration 150, Current loss 1.7635744236783573 Accuracy 81.80642180642181\n",
      "Training:: Epoch 20, Iteration 160, Current loss 2.8234475793635294 Accuracy 70.44555887742308\n",
      "Training:: Epoch 20, Iteration 170, Current loss 2.684338420032857 Accuracy 73.06241822519819\n",
      "Training:: Epoch 20, Iteration 180, Current loss 2.1274234561878647 Accuracy 86.63166397415186\n",
      "Calculating Expectation\n",
      "Epoch 20 iter 0\n",
      "Epoch 20 iter 10\n",
      "Epoch 20 iter 20\n",
      "Epoch 20 iter 30\n",
      "Epoch 20 iter 40\n",
      "Epoch 20 iter 50\n",
      "Epoch 20 iter 60\n",
      "Epoch 20 iter 70\n",
      "Epoch 20 iter 80\n",
      "Epoch 20 iter 90\n",
      "Epoch 20 iter 100\n",
      "Epoch 20 iter 110\n",
      "Epoch 20 iter 120\n",
      "Epoch 20 iter 130\n",
      "Epoch 20 iter 140\n",
      "Epoch 20 iter 150\n",
      "Epoch 20 iter 160\n",
      "Epoch 20 iter 170\n",
      "Epoch 20 iter 180\n",
      "Train Boundary avergage error = 92.886\n",
      "Train From boundary avergage accuracy = 88.112\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 20, Probability Accuracy 66.9664161829125\n",
      "Starting Training\n",
      "Training:: Epoch 21, Iteration 0, Current loss 1.210377070557985 Accuracy 83.50408237557855\n",
      "Training:: Epoch 21, Iteration 10, Current loss 1.3271836175078036 Accuracy 87.57237386269644\n",
      "Training:: Epoch 21, Iteration 20, Current loss 1.5001296923489462 Accuracy 83.95546226156794\n",
      "Training:: Epoch 21, Iteration 30, Current loss 1.6011630597948563 Accuracy 85.66003202928391\n",
      "Training:: Epoch 21, Iteration 40, Current loss 1.1131408349010923 Accuracy 84.49880464480874\n",
      "Training:: Epoch 21, Iteration 50, Current loss 1.2845587454958376 Accuracy 85.41518807665011\n",
      "Training:: Epoch 21, Iteration 60, Current loss 0.9935810422869156 Accuracy 88.88570883419195\n",
      "Training:: Epoch 21, Iteration 70, Current loss 1.1376247252731695 Accuracy 88.10293386374939\n",
      "Training:: Epoch 21, Iteration 80, Current loss 1.6474758169663388 Accuracy 81.1841599384852\n",
      "Training:: Epoch 21, Iteration 90, Current loss 1.7575963178433547 Accuracy 80.26671315261919\n",
      "Training:: Epoch 21, Iteration 100, Current loss 1.5324974196212202 Accuracy 84.41558441558442\n",
      "Training:: Epoch 21, Iteration 110, Current loss 4.0400174952799395 Accuracy 65.15139215845441\n",
      "Training:: Epoch 21, Iteration 120, Current loss 2.2345935147490823 Accuracy 69.79121119370355\n",
      "Training:: Epoch 21, Iteration 130, Current loss 1.4228882132552951 Accuracy 86.91409212721848\n",
      "Training:: Epoch 21, Iteration 140, Current loss 2.402503355024601 Accuracy 71.70216741405082\n",
      "Training:: Epoch 21, Iteration 150, Current loss 0.8440938435180833 Accuracy 89.00335409678965\n",
      "Training:: Epoch 21, Iteration 160, Current loss 2.2721244790034754 Accuracy 81.20611317637339\n",
      "Training:: Epoch 21, Iteration 170, Current loss 2.708921074395799 Accuracy 69.97731691510046\n",
      "Training:: Epoch 21, Iteration 180, Current loss 1.1400261804726408 Accuracy 87.98763589852909\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 21, Probability Accuracy 67.06197989007205\n",
      "Starting Training\n",
      "Training:: Epoch 22, Iteration 0, Current loss 1.2603668450030967 Accuracy 87.58277368149157\n",
      "Training:: Epoch 22, Iteration 10, Current loss 1.2335463232075463 Accuracy 86.72396592740388\n",
      "Training:: Epoch 22, Iteration 20, Current loss 1.7502886982301833 Accuracy 82.88253477588871\n",
      "Training:: Epoch 22, Iteration 30, Current loss 1.3117996964027658 Accuracy 76.5807804152296\n",
      "Training:: Epoch 22, Iteration 40, Current loss 1.8246964430556465 Accuracy 80.01882352941176\n",
      "Training:: Epoch 22, Iteration 50, Current loss 1.2038177277098858 Accuracy 89.44211157768446\n",
      "Training:: Epoch 22, Iteration 60, Current loss 0.9976403157324876 Accuracy 88.72038491990986\n",
      "Training:: Epoch 22, Iteration 70, Current loss 1.030324914016646 Accuracy 86.48216482164821\n",
      "Training:: Epoch 22, Iteration 80, Current loss 1.1021642362830952 Accuracy 90.21991701244814\n",
      "Training:: Epoch 22, Iteration 90, Current loss 1.2052200455143003 Accuracy 85.75459675951211\n",
      "Training:: Epoch 22, Iteration 100, Current loss 0.6711952186029553 Accuracy 88.541567695962\n",
      "Training:: Epoch 22, Iteration 110, Current loss 1.4929704494194458 Accuracy 86.29435977305596\n",
      "Training:: Epoch 22, Iteration 120, Current loss 1.2222206592900855 Accuracy 87.56825627957772\n",
      "Training:: Epoch 22, Iteration 130, Current loss 0.9074765281323088 Accuracy 91.73597128600193\n",
      "Training:: Epoch 22, Iteration 140, Current loss 1.3865446656570741 Accuracy 83.25473221784353\n",
      "Training:: Epoch 22, Iteration 150, Current loss 1.376599162989287 Accuracy 83.22223361706492\n",
      "Training:: Epoch 22, Iteration 160, Current loss 2.3876543731400934 Accuracy 78.26638631090488\n",
      "Training:: Epoch 22, Iteration 170, Current loss 1.0808897215045778 Accuracy 86.30867709815078\n",
      "Training:: Epoch 22, Iteration 180, Current loss 1.2901881729537532 Accuracy 88.15514469453376\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 22, Probability Accuracy 67.52950999362909\n",
      "Starting Training\n",
      "Training:: Epoch 23, Iteration 0, Current loss 1.3631660341022978 Accuracy 78.08651246620607\n",
      "Training:: Epoch 23, Iteration 10, Current loss 0.8830768106417654 Accuracy 89.10795763740634\n",
      "Training:: Epoch 23, Iteration 20, Current loss 0.7275627590591989 Accuracy 87.14246424642464\n",
      "Training:: Epoch 23, Iteration 30, Current loss 1.4750065756338113 Accuracy 75.73473427744784\n",
      "Training:: Epoch 23, Iteration 40, Current loss 1.2572068745528076 Accuracy 88.38681646080317\n",
      "Training:: Epoch 23, Iteration 50, Current loss 0.8772070810521027 Accuracy 85.21096128751631\n",
      "Training:: Epoch 23, Iteration 60, Current loss 1.3111484580508357 Accuracy 89.94932432432432\n",
      "Training:: Epoch 23, Iteration 70, Current loss 1.874691187493838 Accuracy 75.31260811545495\n",
      "Training:: Epoch 23, Iteration 80, Current loss 1.4361803400397573 Accuracy 87.48666144981148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 23, Iteration 90, Current loss 0.9813635593828701 Accuracy 85.28294598596537\n",
      "Training:: Epoch 23, Iteration 100, Current loss 1.3784209664106215 Accuracy 85.81620128873888\n",
      "Training:: Epoch 23, Iteration 110, Current loss 0.8432603545047005 Accuracy 88.58033858033858\n",
      "Training:: Epoch 23, Iteration 120, Current loss 1.1098801494359118 Accuracy 81.44754599048228\n",
      "Training:: Epoch 23, Iteration 130, Current loss 1.22628467445049 Accuracy 86.55637012086387\n",
      "Training:: Epoch 23, Iteration 140, Current loss 1.3827306751383857 Accuracy 79.01209123385546\n",
      "Training:: Epoch 23, Iteration 150, Current loss 1.5066568255454902 Accuracy 86.07273231366388\n",
      "Training:: Epoch 23, Iteration 160, Current loss 0.9847208572662747 Accuracy 84.22584040116386\n",
      "Training:: Epoch 23, Iteration 170, Current loss 1.3256787428221037 Accuracy 79.63215258855585\n",
      "Training:: Epoch 23, Iteration 180, Current loss 0.7728877073305895 Accuracy 91.27404718693285\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 23, Probability Accuracy 69.26133013600516\n",
      "Starting Training\n",
      "Training:: Epoch 24, Iteration 0, Current loss 1.1020306716612298 Accuracy 86.59239842726082\n",
      "Training:: Epoch 24, Iteration 10, Current loss 0.9203668016499907 Accuracy 82.02024903991621\n",
      "Training:: Epoch 24, Iteration 20, Current loss 0.7578221080824195 Accuracy 92.68593512313448\n",
      "Training:: Epoch 24, Iteration 30, Current loss 1.297049462904942 Accuracy 85.70192141249784\n",
      "Training:: Epoch 24, Iteration 40, Current loss 1.3120591883035135 Accuracy 90.15303203661327\n",
      "Training:: Epoch 24, Iteration 50, Current loss 1.7192698399961333 Accuracy 74.06704617330803\n",
      "Training:: Epoch 24, Iteration 60, Current loss 1.0538635748737712 Accuracy 84.78242210969016\n",
      "Training:: Epoch 24, Iteration 70, Current loss 2.50762018489759 Accuracy 80.15774378585085\n",
      "Training:: Epoch 24, Iteration 80, Current loss 0.8727780823135372 Accuracy 87.4265460311538\n",
      "Training:: Epoch 24, Iteration 90, Current loss 5.937859342582656 Accuracy 70.17173480573261\n",
      "Training:: Epoch 24, Iteration 100, Current loss 1.4809309285310959 Accuracy 81.24632569077013\n",
      "Training:: Epoch 24, Iteration 110, Current loss 1.5362115580266997 Accuracy 86.36073932441045\n",
      "Training:: Epoch 24, Iteration 120, Current loss 1.2020694823690652 Accuracy 86.24370594159114\n",
      "Training:: Epoch 24, Iteration 130, Current loss 2.370205341548533 Accuracy 80.1897179639197\n",
      "Training:: Epoch 24, Iteration 140, Current loss 1.1468774608786738 Accuracy 80.0120454716555\n",
      "Training:: Epoch 24, Iteration 150, Current loss 1.2455710602765617 Accuracy 84.47314519799727\n",
      "Training:: Epoch 24, Iteration 160, Current loss 1.4323092856293065 Accuracy 89.19193975167921\n",
      "Training:: Epoch 24, Iteration 170, Current loss 2.626358710191429 Accuracy 74.46279666452854\n",
      "Training:: Epoch 24, Iteration 180, Current loss 3.083324677328377 Accuracy 77.76951119708355\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 24, Probability Accuracy 67.2663635536245\n",
      "Starting Training\n",
      "Training:: Epoch 25, Iteration 0, Current loss 1.0012597329348034 Accuracy 85.13997868836579\n",
      "Training:: Epoch 25, Iteration 10, Current loss 1.2748494634440544 Accuracy 88.44723526184235\n",
      "Training:: Epoch 25, Iteration 20, Current loss 0.9676747976030631 Accuracy 88.54340567612688\n",
      "Training:: Epoch 25, Iteration 30, Current loss 3.1802724343732676 Accuracy 78.3195082652026\n",
      "Training:: Epoch 25, Iteration 40, Current loss 0.9330137088338519 Accuracy 89.10432138762897\n",
      "Training:: Epoch 25, Iteration 50, Current loss 1.2431215369972706 Accuracy 74.78279995601012\n",
      "Training:: Epoch 25, Iteration 60, Current loss 1.0831555351696485 Accuracy 87.00466833134296\n",
      "Training:: Epoch 25, Iteration 70, Current loss 0.8249376671684874 Accuracy 86.21527160713228\n",
      "Training:: Epoch 25, Iteration 80, Current loss 0.9979155207280817 Accuracy 87.97495374982212\n",
      "Training:: Epoch 25, Iteration 90, Current loss 1.0968680811803126 Accuracy 88.4719222462203\n",
      "Training:: Epoch 25, Iteration 100, Current loss 0.8983113659229078 Accuracy 90.49329501915709\n",
      "Training:: Epoch 25, Iteration 110, Current loss 1.1775901271675862 Accuracy 86.94933092768005\n",
      "Training:: Epoch 25, Iteration 120, Current loss 0.9379937332542464 Accuracy 87.0250420768093\n",
      "Training:: Epoch 25, Iteration 130, Current loss 0.8719957064027958 Accuracy 89.92992473397352\n",
      "Training:: Epoch 25, Iteration 140, Current loss 1.696811629951609 Accuracy 84.03449481412423\n",
      "Training:: Epoch 25, Iteration 150, Current loss 0.8943352203654789 Accuracy 84.2071669554117\n",
      "Training:: Epoch 25, Iteration 160, Current loss 1.3410519322235162 Accuracy 84.77669209833579\n",
      "Training:: Epoch 25, Iteration 170, Current loss 0.7960405368347605 Accuracy 78.06859205776173\n",
      "Training:: Epoch 25, Iteration 180, Current loss 0.9817920734607939 Accuracy 85.6944315760215\n",
      "Calculating Expectation\n",
      "Epoch 25 iter 0\n",
      "Epoch 25 iter 10\n",
      "Epoch 25 iter 20\n",
      "Epoch 25 iter 30\n",
      "Epoch 25 iter 40\n",
      "Epoch 25 iter 50\n",
      "Epoch 25 iter 60\n",
      "Epoch 25 iter 70\n",
      "Epoch 25 iter 80\n",
      "Epoch 25 iter 90\n",
      "Epoch 25 iter 100\n",
      "Epoch 25 iter 110\n",
      "Epoch 25 iter 120\n",
      "Epoch 25 iter 130\n",
      "Epoch 25 iter 140\n",
      "Epoch 25 iter 150\n",
      "Epoch 25 iter 160\n",
      "Epoch 25 iter 170\n",
      "Epoch 25 iter 180\n",
      "Train Boundary avergage error = 89.790\n",
      "Train From boundary avergage accuracy = 87.980\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 25, Probability Accuracy 68.105860053579\n",
      "Starting Training\n",
      "Training:: Epoch 26, Iteration 0, Current loss 0.6505122137919903 Accuracy 89.08877049747457\n",
      "Training:: Epoch 26, Iteration 10, Current loss 0.9654020469498213 Accuracy 86.25481723694966\n",
      "Training:: Epoch 26, Iteration 20, Current loss 0.8420972994027252 Accuracy 89.9809976247031\n",
      "Training:: Epoch 26, Iteration 30, Current loss 0.9487007317841591 Accuracy 79.47932618683002\n",
      "Training:: Epoch 26, Iteration 40, Current loss 0.6284042324137723 Accuracy 93.07612814330004\n",
      "Training:: Epoch 26, Iteration 50, Current loss 1.4613067476932948 Accuracy 83.18911592994162\n",
      "Training:: Epoch 26, Iteration 60, Current loss 0.8748427739120939 Accuracy 87.12746716993001\n",
      "Training:: Epoch 26, Iteration 70, Current loss 0.7842344093682343 Accuracy 82.49964064970533\n",
      "Training:: Epoch 26, Iteration 80, Current loss 1.2336765400516096 Accuracy 82.49802905732628\n",
      "Training:: Epoch 26, Iteration 90, Current loss 1.2577524250934504 Accuracy 87.00938284657063\n",
      "Training:: Epoch 26, Iteration 100, Current loss 0.6550991927794216 Accuracy 86.12557427258805\n",
      "Training:: Epoch 26, Iteration 110, Current loss 1.6317597824318089 Accuracy 86.04232713563533\n",
      "Training:: Epoch 26, Iteration 120, Current loss 1.3301475983228752 Accuracy 85.97789267249064\n",
      "Training:: Epoch 26, Iteration 130, Current loss 1.5592904075902698 Accuracy 88.96404237330283\n",
      "Training:: Epoch 26, Iteration 140, Current loss 1.082110683905981 Accuracy 87.79188493296635\n",
      "Training:: Epoch 26, Iteration 150, Current loss 0.9975308591782531 Accuracy 88.26148683531234\n",
      "Training:: Epoch 26, Iteration 160, Current loss 1.1013694115545234 Accuracy 80.74353055521807\n",
      "Training:: Epoch 26, Iteration 170, Current loss 1.238564492720745 Accuracy 87.34066101824288\n",
      "Training:: Epoch 26, Iteration 180, Current loss 1.2309228517759434 Accuracy 85.84883307664968\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 26, Probability Accuracy 67.97646323270455\n",
      "Starting Training\n",
      "Training:: Epoch 27, Iteration 0, Current loss 0.9362042558975724 Accuracy 88.85670651429776\n",
      "Training:: Epoch 27, Iteration 10, Current loss 1.3600382893182739 Accuracy 84.94351249572064\n",
      "Training:: Epoch 27, Iteration 20, Current loss 0.7455674953071079 Accuracy 87.49330954504906\n",
      "Training:: Epoch 27, Iteration 30, Current loss 0.5785900896824744 Accuracy 91.59595489945367\n",
      "Training:: Epoch 27, Iteration 40, Current loss 1.370385063239053 Accuracy 85.52259017604334\n",
      "Training:: Epoch 27, Iteration 50, Current loss 1.4004231981546647 Accuracy 81.86598677396223\n",
      "Training:: Epoch 27, Iteration 60, Current loss 1.4956640117788127 Accuracy 85.86399276236429\n",
      "Training:: Epoch 27, Iteration 70, Current loss 1.0274896100517448 Accuracy 81.2174236783321\n",
      "Training:: Epoch 27, Iteration 80, Current loss 2.4857343311785867 Accuracy 78.26900921658986\n",
      "Training:: Epoch 27, Iteration 90, Current loss 0.9021111137827177 Accuracy 89.84907497565726\n",
      "Training:: Epoch 27, Iteration 100, Current loss 1.131305941217645 Accuracy 90.3215070225654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 27, Iteration 110, Current loss 1.54628488227036 Accuracy 88.15872020075282\n",
      "Training:: Epoch 27, Iteration 120, Current loss 1.9131388090097228 Accuracy 80.04300634855622\n",
      "Training:: Epoch 27, Iteration 130, Current loss 0.8063170503230674 Accuracy 90.29196269982238\n",
      "Training:: Epoch 27, Iteration 140, Current loss 1.636999387428095 Accuracy 84.534127843987\n",
      "Training:: Epoch 27, Iteration 150, Current loss 0.8357684648851651 Accuracy 82.26182316655243\n",
      "Training:: Epoch 27, Iteration 160, Current loss 2.4304928888586663 Accuracy 80.09852216748769\n",
      "Training:: Epoch 27, Iteration 170, Current loss 1.410142293435329 Accuracy 79.54310980103169\n",
      "Training:: Epoch 27, Iteration 180, Current loss 1.6574402690598231 Accuracy 88.4428960920933\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 27, Probability Accuracy 66.10752994527346\n",
      "Starting Training\n",
      "Training:: Epoch 28, Iteration 0, Current loss 1.5421890881160867 Accuracy 82.9865556978233\n",
      "Training:: Epoch 28, Iteration 10, Current loss 1.1609698312439016 Accuracy 87.33932733932734\n",
      "Training:: Epoch 28, Iteration 20, Current loss 0.876845444369163 Accuracy 89.82800703865584\n",
      "Training:: Epoch 28, Iteration 30, Current loss 0.8285999938878925 Accuracy 75.74128380579994\n",
      "Training:: Epoch 28, Iteration 40, Current loss 0.6993838802616086 Accuracy 90.30513317685478\n",
      "Training:: Epoch 28, Iteration 50, Current loss 0.804528391730063 Accuracy 89.03832833063957\n",
      "Training:: Epoch 28, Iteration 60, Current loss 1.2843034815122385 Accuracy 82.4383434155421\n",
      "Training:: Epoch 28, Iteration 70, Current loss 0.9443799729423922 Accuracy 86.34495714453446\n",
      "Training:: Epoch 28, Iteration 80, Current loss 0.9715682077467241 Accuracy 86.53986451276707\n",
      "Training:: Epoch 28, Iteration 90, Current loss 0.7475502637018827 Accuracy 86.47011846133368\n",
      "Training:: Epoch 28, Iteration 100, Current loss 0.960002229490842 Accuracy 87.97210159231477\n",
      "Training:: Epoch 28, Iteration 110, Current loss 1.0615396099620162 Accuracy 80.33046627433228\n",
      "Training:: Epoch 28, Iteration 120, Current loss 0.6396354156480464 Accuracy 89.34992063199084\n",
      "Training:: Epoch 28, Iteration 130, Current loss 1.1253709897429398 Accuracy 89.71688903351773\n",
      "Training:: Epoch 28, Iteration 140, Current loss 0.9417935648192035 Accuracy 89.34202059202059\n",
      "Training:: Epoch 28, Iteration 150, Current loss 1.232643808514446 Accuracy 82.81223304288399\n",
      "Training:: Epoch 28, Iteration 160, Current loss 1.0316943789342508 Accuracy 87.72857655073503\n",
      "Training:: Epoch 28, Iteration 170, Current loss 1.0880682216676412 Accuracy 80.03494467093768\n",
      "Training:: Epoch 28, Iteration 180, Current loss 0.923036650329145 Accuracy 88.55981941309255\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 28, Probability Accuracy 59.57239692771585\n",
      "Starting Training\n",
      "Training:: Epoch 29, Iteration 0, Current loss 0.8317484180698891 Accuracy 86.1842105263158\n",
      "Training:: Epoch 29, Iteration 10, Current loss 0.7810541400980008 Accuracy 86.24172967863895\n",
      "Training:: Epoch 29, Iteration 20, Current loss 1.1509121885301046 Accuracy 88.66969832831614\n",
      "Training:: Epoch 29, Iteration 30, Current loss 1.002469716457246 Accuracy 84.94239818270323\n",
      "Training:: Epoch 29, Iteration 40, Current loss 0.9939679284460374 Accuracy 87.44476358815731\n",
      "Training:: Epoch 29, Iteration 50, Current loss 1.053821822561176 Accuracy 89.59970465692737\n",
      "Training:: Epoch 29, Iteration 60, Current loss 0.6852491793221839 Accuracy 88.9496187243796\n",
      "Training:: Epoch 29, Iteration 70, Current loss 1.298441327132126 Accuracy 85.6886053928675\n",
      "Training:: Epoch 29, Iteration 80, Current loss 1.0580738408371735 Accuracy 88.90281009486307\n",
      "Training:: Epoch 29, Iteration 90, Current loss 1.1812006091310119 Accuracy 87.23874820302997\n",
      "Training:: Epoch 29, Iteration 100, Current loss 0.7255646035980112 Accuracy 80.31934373397787\n",
      "Training:: Epoch 29, Iteration 110, Current loss 0.808822574139095 Accuracy 90.85246967071058\n",
      "Training:: Epoch 29, Iteration 120, Current loss 0.9780471839732616 Accuracy 80.43643153042768\n",
      "Training:: Epoch 29, Iteration 130, Current loss 1.3114909316573349 Accuracy 77.0519356826713\n",
      "Training:: Epoch 29, Iteration 140, Current loss 0.7684017323053345 Accuracy 86.64358863406895\n",
      "Training:: Epoch 29, Iteration 150, Current loss 1.2061956388513966 Accuracy 84.26126931426383\n",
      "Training:: Epoch 29, Iteration 160, Current loss 1.1413115875464912 Accuracy 84.36060928090684\n",
      "Training:: Epoch 29, Iteration 170, Current loss 0.8880756600920501 Accuracy 83.59832635983264\n",
      "Training:: Epoch 29, Iteration 180, Current loss 0.8455904743111393 Accuracy 88.01541816706396\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 29, Probability Accuracy 68.48851059114958\n",
      "Starting Training\n",
      "Training:: Epoch 30, Iteration 0, Current loss 1.4004669715145757 Accuracy 80.63181647235803\n",
      "Training:: Epoch 30, Iteration 10, Current loss 0.7325211750171621 Accuracy 87.04554152315346\n",
      "Training:: Epoch 30, Iteration 20, Current loss 0.7258398052409434 Accuracy 87.37935316362815\n",
      "Training:: Epoch 30, Iteration 30, Current loss 0.5016048462940902 Accuracy 85.75877000924439\n",
      "Training:: Epoch 30, Iteration 40, Current loss 0.9360675426131684 Accuracy 83.34379413149223\n",
      "Training:: Epoch 30, Iteration 50, Current loss 1.064062424230621 Accuracy 77.90234036405663\n",
      "Training:: Epoch 30, Iteration 60, Current loss 0.6626865964614462 Accuracy 89.38705442795113\n",
      "Training:: Epoch 30, Iteration 70, Current loss 1.037254359755249 Accuracy 75.08793393485243\n",
      "Training:: Epoch 30, Iteration 80, Current loss 0.9669279902857358 Accuracy 87.92586535840829\n",
      "Training:: Epoch 30, Iteration 90, Current loss 1.5854260463928591 Accuracy 82.90995510958543\n",
      "Training:: Epoch 30, Iteration 100, Current loss 0.7365377782635116 Accuracy 89.98238402818555\n",
      "Training:: Epoch 30, Iteration 110, Current loss 0.6171559864679096 Accuracy 84.54575056984696\n",
      "Training:: Epoch 30, Iteration 120, Current loss 0.877529288568359 Accuracy 85.67481696366902\n",
      "Training:: Epoch 30, Iteration 130, Current loss 0.4644467114474199 Accuracy 88.90293225480283\n",
      "Training:: Epoch 30, Iteration 140, Current loss 0.9847826002715157 Accuracy 85.84236085169967\n",
      "Training:: Epoch 30, Iteration 150, Current loss 0.6369593264515452 Accuracy 89.60384833050368\n",
      "Training:: Epoch 30, Iteration 160, Current loss 0.5856274720134212 Accuracy 87.14075795372996\n",
      "Training:: Epoch 30, Iteration 170, Current loss 0.7834784800291629 Accuracy 89.70212426130011\n",
      "Training:: Epoch 30, Iteration 180, Current loss 0.7468891780916915 Accuracy 89.58956723683623\n",
      "Calculating Expectation\n",
      "Epoch 30 iter 0\n",
      "Epoch 30 iter 10\n",
      "Epoch 30 iter 20\n",
      "Epoch 30 iter 30\n",
      "Epoch 30 iter 40\n",
      "Epoch 30 iter 50\n",
      "Epoch 30 iter 60\n",
      "Epoch 30 iter 70\n",
      "Epoch 30 iter 80\n",
      "Epoch 30 iter 90\n",
      "Epoch 30 iter 100\n",
      "Epoch 30 iter 110\n",
      "Epoch 30 iter 120\n",
      "Epoch 30 iter 130\n",
      "Epoch 30 iter 140\n",
      "Epoch 30 iter 150\n",
      "Epoch 30 iter 160\n",
      "Epoch 30 iter 170\n",
      "Epoch 30 iter 180\n",
      "Train Boundary avergage error = 87.963\n",
      "Train From boundary avergage accuracy = 88.476\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 30, Probability Accuracy 70.4931720423725\n",
      "Starting Training\n",
      "Training:: Epoch 31, Iteration 0, Current loss 0.6446846173853669 Accuracy 87.77044652447445\n",
      "Training:: Epoch 31, Iteration 10, Current loss 0.7564324445889614 Accuracy 88.7685855890202\n",
      "Training:: Epoch 31, Iteration 20, Current loss 0.6729577450000575 Accuracy 90.11763562123275\n",
      "Training:: Epoch 31, Iteration 30, Current loss 2.117080606104514 Accuracy 85.34817890167956\n",
      "Training:: Epoch 31, Iteration 40, Current loss 0.6803378928494893 Accuracy 87.52283998955886\n",
      "Training:: Epoch 31, Iteration 50, Current loss 0.6056076348803895 Accuracy 89.77300463753966\n",
      "Training:: Epoch 31, Iteration 60, Current loss 0.9998365366889918 Accuracy 81.87078109932497\n",
      "Training:: Epoch 31, Iteration 70, Current loss 0.8376857312760256 Accuracy 84.13084241961248\n",
      "Training:: Epoch 31, Iteration 80, Current loss 0.42695568953592994 Accuracy 91.31466540104414\n",
      "Training:: Epoch 31, Iteration 90, Current loss 0.5008588490760864 Accuracy 86.93129300605936\n",
      "Training:: Epoch 31, Iteration 100, Current loss 0.6857911733209947 Accuracy 88.49903784477229\n",
      "Training:: Epoch 31, Iteration 110, Current loss 0.5524800275003001 Accuracy 90.62096565969533\n",
      "Training:: Epoch 31, Iteration 120, Current loss 0.657221231846187 Accuracy 89.60523705613966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 31, Iteration 130, Current loss 0.653428008154144 Accuracy 88.40050432589888\n",
      "Training:: Epoch 31, Iteration 140, Current loss 0.7939558728330544 Accuracy 88.6160159164387\n",
      "Training:: Epoch 31, Iteration 150, Current loss 0.7824096340927554 Accuracy 89.68853432911823\n",
      "Training:: Epoch 31, Iteration 160, Current loss 1.3958498270019144 Accuracy 80.05887300252313\n",
      "Training:: Epoch 31, Iteration 170, Current loss 1.503193521057518 Accuracy 84.25298483381737\n",
      "Training:: Epoch 31, Iteration 180, Current loss 1.2887007245285662 Accuracy 88.85792407193185\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 31, Probability Accuracy 65.29949230543981\n",
      "Starting Training\n",
      "Training:: Epoch 32, Iteration 0, Current loss 1.382227008173126 Accuracy 73.31301432490095\n",
      "Training:: Epoch 32, Iteration 10, Current loss 1.0217974162036216 Accuracy 87.56088422630198\n",
      "Training:: Epoch 32, Iteration 20, Current loss 1.299990798872436 Accuracy 85.96603839645212\n",
      "Training:: Epoch 32, Iteration 30, Current loss 1.5855318798231974 Accuracy 86.29388131462821\n",
      "Training:: Epoch 32, Iteration 40, Current loss 0.6682660002951293 Accuracy 82.97882649353167\n",
      "Training:: Epoch 32, Iteration 50, Current loss 1.2614345585537787 Accuracy 84.9109131403118\n",
      "Training:: Epoch 32, Iteration 60, Current loss 0.6847745793961609 Accuracy 89.81867444768653\n",
      "Training:: Epoch 32, Iteration 70, Current loss 1.0579553460031113 Accuracy 89.83530194643154\n",
      "Training:: Epoch 32, Iteration 80, Current loss 1.4715015236560576 Accuracy 84.701966452065\n",
      "Training:: Epoch 32, Iteration 90, Current loss 0.8581318435644818 Accuracy 83.65817091454272\n",
      "Training:: Epoch 32, Iteration 100, Current loss 1.0114937067156635 Accuracy 79.94347211653441\n",
      "Training:: Epoch 32, Iteration 110, Current loss 0.9285688034979526 Accuracy 85.58391558534151\n",
      "Training:: Epoch 32, Iteration 120, Current loss 1.2218454093363624 Accuracy 82.73555641721597\n",
      "Training:: Epoch 32, Iteration 130, Current loss 0.6496607097724421 Accuracy 83.49753694581281\n",
      "Training:: Epoch 32, Iteration 140, Current loss 0.7197776843303421 Accuracy 85.47091625313347\n",
      "Training:: Epoch 32, Iteration 150, Current loss 1.082214983206282 Accuracy 84.68072763435958\n",
      "Training:: Epoch 32, Iteration 160, Current loss 0.5093433779789314 Accuracy 89.46079563831262\n",
      "Training:: Epoch 32, Iteration 170, Current loss 0.6489115494400135 Accuracy 92.30941704035874\n",
      "Training:: Epoch 32, Iteration 180, Current loss 0.7696157531243957 Accuracy 87.02301635372501\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 32, Probability Accuracy 70.64650925365338\n",
      "Starting Training\n",
      "Training:: Epoch 33, Iteration 0, Current loss 0.6992199433366514 Accuracy 90.99763095551461\n",
      "Training:: Epoch 33, Iteration 10, Current loss 0.5450284346068744 Accuracy 90.91704820899542\n",
      "Training:: Epoch 33, Iteration 20, Current loss 0.7411417925055659 Accuracy 88.03070761014686\n",
      "Training:: Epoch 33, Iteration 30, Current loss 0.7346194620469771 Accuracy 88.15637773818258\n",
      "Training:: Epoch 33, Iteration 40, Current loss 0.6032633662073988 Accuracy 89.79937304075236\n",
      "Training:: Epoch 33, Iteration 50, Current loss 0.5021600534399261 Accuracy 90.08981885694214\n",
      "Training:: Epoch 33, Iteration 60, Current loss 0.3776227748708009 Accuracy 89.54674034602986\n",
      "Training:: Epoch 33, Iteration 70, Current loss 0.7338215977155386 Accuracy 87.81767821565674\n",
      "Training:: Epoch 33, Iteration 80, Current loss 0.5253918345219932 Accuracy 90.52152317880795\n",
      "Training:: Epoch 33, Iteration 90, Current loss 0.6003841776411604 Accuracy 92.24930083899321\n",
      "Training:: Epoch 33, Iteration 100, Current loss 0.5364760538818747 Accuracy 85.3899822971988\n",
      "Training:: Epoch 33, Iteration 110, Current loss 0.5302001229119825 Accuracy 85.99567619505164\n",
      "Training:: Epoch 33, Iteration 120, Current loss 0.6645282268205098 Accuracy 88.67902484617181\n",
      "Training:: Epoch 33, Iteration 130, Current loss 0.880707720148539 Accuracy 82.41497710922171\n",
      "Training:: Epoch 33, Iteration 140, Current loss 1.0765055617068242 Accuracy 86.17718816712413\n",
      "Training:: Epoch 33, Iteration 150, Current loss 1.9423639968635404 Accuracy 79.09662255377854\n",
      "Training:: Epoch 33, Iteration 160, Current loss 1.304381871336803 Accuracy 85.53747740230844\n",
      "Training:: Epoch 33, Iteration 170, Current loss 1.7369505916170098 Accuracy 79.91741153102295\n",
      "Training:: Epoch 33, Iteration 180, Current loss 0.8839213981059912 Accuracy 89.48528061692882\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 33, Probability Accuracy 63.642263296809396\n",
      "Starting Training\n",
      "Training:: Epoch 34, Iteration 0, Current loss 0.897589136471512 Accuracy 83.1459040232671\n",
      "Training:: Epoch 34, Iteration 10, Current loss 1.0550269977950588 Accuracy 85.56334691556903\n",
      "Training:: Epoch 34, Iteration 20, Current loss 1.388210101758149 Accuracy 87.52649600521768\n",
      "Training:: Epoch 34, Iteration 30, Current loss 0.8181250740206893 Accuracy 89.52814889180631\n",
      "Training:: Epoch 34, Iteration 40, Current loss 0.8475563204685194 Accuracy 88.83912181511124\n",
      "Training:: Epoch 34, Iteration 50, Current loss 0.6420191631213289 Accuracy 89.55145426936106\n",
      "Training:: Epoch 34, Iteration 60, Current loss 1.305186830630283 Accuracy 86.29656953153818\n",
      "Training:: Epoch 34, Iteration 70, Current loss 0.6537493905890446 Accuracy 91.84759341936272\n",
      "Training:: Epoch 34, Iteration 80, Current loss 0.8048103495503438 Accuracy 88.44777090790758\n",
      "Training:: Epoch 34, Iteration 90, Current loss 0.6457465339220637 Accuracy 87.59604070049146\n",
      "Training:: Epoch 34, Iteration 100, Current loss 0.5725517983411732 Accuracy 89.66572289706822\n",
      "Training:: Epoch 34, Iteration 110, Current loss 0.7804416604972584 Accuracy 87.41251093613299\n",
      "Training:: Epoch 34, Iteration 120, Current loss 0.8396194859586091 Accuracy 81.15886660299267\n",
      "Training:: Epoch 34, Iteration 130, Current loss 1.3825547339528632 Accuracy 81.19466666666666\n",
      "Training:: Epoch 34, Iteration 140, Current loss 0.749010618616598 Accuracy 84.98781582659997\n",
      "Training:: Epoch 34, Iteration 150, Current loss 0.7643387539082198 Accuracy 89.79246914550743\n",
      "Training:: Epoch 34, Iteration 160, Current loss 0.8020692765050771 Accuracy 86.17661408359083\n",
      "Training:: Epoch 34, Iteration 170, Current loss 0.4523427181258275 Accuracy 89.27633366542885\n",
      "Training:: Epoch 34, Iteration 180, Current loss 0.6907571721168793 Accuracy 87.52057177787182\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 34, Probability Accuracy 68.20201732413706\n",
      "Starting Training\n",
      "Training:: Epoch 35, Iteration 0, Current loss 0.5000735960260344 Accuracy 91.19357518692883\n",
      "Training:: Epoch 35, Iteration 10, Current loss 0.9021573832908139 Accuracy 85.48369505943941\n",
      "Training:: Epoch 35, Iteration 20, Current loss 0.6250846714597504 Accuracy 90.58728881737731\n",
      "Training:: Epoch 35, Iteration 30, Current loss 0.7252688391495512 Accuracy 88.6390377827167\n",
      "Training:: Epoch 35, Iteration 40, Current loss 0.9975329972783367 Accuracy 86.5703770571488\n",
      "Training:: Epoch 35, Iteration 50, Current loss 0.6960227938130512 Accuracy 91.27600902481825\n",
      "Training:: Epoch 35, Iteration 60, Current loss 0.5095703118507269 Accuracy 91.06183368869937\n",
      "Training:: Epoch 35, Iteration 70, Current loss 0.7104839508559264 Accuracy 83.70506161523576\n",
      "Training:: Epoch 35, Iteration 80, Current loss 0.42271318419655773 Accuracy 87.18365577412887\n",
      "Training:: Epoch 35, Iteration 90, Current loss 0.8376093825512231 Accuracy 72.16023119531187\n",
      "Training:: Epoch 35, Iteration 100, Current loss 0.7122747088828887 Accuracy 88.6164253250185\n",
      "Training:: Epoch 35, Iteration 110, Current loss 0.3189497827454972 Accuracy 87.16364167193811\n",
      "Training:: Epoch 35, Iteration 120, Current loss 0.3838081815761202 Accuracy 86.01964539445234\n",
      "Training:: Epoch 35, Iteration 130, Current loss 0.7851096522401233 Accuracy 83.73390303246099\n",
      "Training:: Epoch 35, Iteration 140, Current loss 0.47563633768002206 Accuracy 82.22993454289856\n",
      "Training:: Epoch 35, Iteration 150, Current loss 0.47417980245004887 Accuracy 91.90644455662124\n",
      "Training:: Epoch 35, Iteration 160, Current loss 0.5608233247497815 Accuracy 87.13356141410075\n",
      "Training:: Epoch 35, Iteration 170, Current loss 0.5317647137954596 Accuracy 88.19426164780205\n",
      "Training:: Epoch 35, Iteration 180, Current loss 0.8633695489324776 Accuracy 89.12375919445833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Expectation\n",
      "Epoch 35 iter 0\n",
      "Epoch 35 iter 10\n",
      "Epoch 35 iter 20\n",
      "Epoch 35 iter 30\n",
      "Epoch 35 iter 40\n",
      "Epoch 35 iter 50\n",
      "Epoch 35 iter 60\n",
      "Epoch 35 iter 70\n",
      "Epoch 35 iter 80\n",
      "Epoch 35 iter 90\n",
      "Epoch 35 iter 100\n",
      "Epoch 35 iter 110\n",
      "Epoch 35 iter 120\n",
      "Epoch 35 iter 130\n",
      "Epoch 35 iter 140\n",
      "Epoch 35 iter 150\n",
      "Epoch 35 iter 160\n",
      "Epoch 35 iter 170\n",
      "Epoch 35 iter 180\n",
      "Train Boundary avergage error = 87.284\n",
      "Train From boundary avergage accuracy = 88.449\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 35, Probability Accuracy 69.84974931839136\n",
      "Starting Training\n",
      "Training:: Epoch 36, Iteration 0, Current loss 0.5470055158866793 Accuracy 86.49942068409652\n",
      "Training:: Epoch 36, Iteration 10, Current loss 0.6344957356994132 Accuracy 87.79803646563815\n",
      "Training:: Epoch 36, Iteration 20, Current loss 0.4718931960310333 Accuracy 89.33670555600881\n",
      "Training:: Epoch 36, Iteration 30, Current loss 0.457473133968501 Accuracy 83.85975994946304\n",
      "Training:: Epoch 36, Iteration 40, Current loss 0.7829243487046926 Accuracy 86.5639066975856\n",
      "Training:: Epoch 36, Iteration 50, Current loss 0.4491010261131698 Accuracy 90.91637188813989\n",
      "Training:: Epoch 36, Iteration 60, Current loss 0.4287191135436593 Accuracy 88.40470968486667\n",
      "Training:: Epoch 36, Iteration 70, Current loss 0.37530392128292944 Accuracy 83.675\n",
      "Training:: Epoch 36, Iteration 80, Current loss 0.46898012268075817 Accuracy 87.18467082710733\n",
      "Training:: Epoch 36, Iteration 90, Current loss 0.6199939724898563 Accuracy 88.61467781835316\n",
      "Training:: Epoch 36, Iteration 100, Current loss 0.6461589501133227 Accuracy 87.95622746536783\n",
      "Training:: Epoch 36, Iteration 110, Current loss 0.45746516818777927 Accuracy 86.90924521296346\n",
      "Training:: Epoch 36, Iteration 120, Current loss 0.5017665706688111 Accuracy 89.9170468685193\n",
      "Training:: Epoch 36, Iteration 130, Current loss 0.45652881483372715 Accuracy 89.54168485377565\n",
      "Training:: Epoch 36, Iteration 140, Current loss 0.6491651536911296 Accuracy 91.45928106324146\n",
      "Training:: Epoch 36, Iteration 150, Current loss 0.6859023615902071 Accuracy 85.84273624823696\n",
      "Training:: Epoch 36, Iteration 160, Current loss 1.0145727481015723 Accuracy 86.34803274359652\n",
      "Training:: Epoch 36, Iteration 170, Current loss 0.4443373982722676 Accuracy 92.53641507162634\n",
      "Training:: Epoch 36, Iteration 180, Current loss 0.42961525226710867 Accuracy 81.90070250217066\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 36, Probability Accuracy 69.43089141351187\n",
      "Starting Training\n",
      "Training:: Epoch 37, Iteration 0, Current loss 0.4859992725302269 Accuracy 91.56801007556675\n",
      "Training:: Epoch 37, Iteration 10, Current loss 0.6002394437469482 Accuracy 83.05749128919861\n",
      "Training:: Epoch 37, Iteration 20, Current loss 0.438020916335256 Accuracy 91.82846827807863\n",
      "Training:: Epoch 37, Iteration 30, Current loss 0.4275311097304864 Accuracy 90.03296185842098\n",
      "Training:: Epoch 37, Iteration 40, Current loss 0.38940314396783493 Accuracy 91.22417455107164\n",
      "Training:: Epoch 37, Iteration 50, Current loss 0.38787489405770137 Accuracy 89.62133055410804\n",
      "Training:: Epoch 37, Iteration 60, Current loss 0.5349005445892627 Accuracy 88.26558399762341\n",
      "Training:: Epoch 37, Iteration 70, Current loss 0.895948441547645 Accuracy 89.07380241792706\n",
      "Training:: Epoch 37, Iteration 80, Current loss 0.5567391931963397 Accuracy 80.67532978932861\n",
      "Training:: Epoch 37, Iteration 90, Current loss 0.527696865764868 Accuracy 86.52678078409718\n",
      "Training:: Epoch 37, Iteration 100, Current loss 0.5065546474747874 Accuracy 90.1587428846308\n",
      "Training:: Epoch 37, Iteration 110, Current loss 0.5207006676938198 Accuracy 87.89601386481803\n",
      "Training:: Epoch 37, Iteration 120, Current loss 0.3996639494412707 Accuracy 93.39533687639293\n",
      "Training:: Epoch 37, Iteration 130, Current loss 0.49466861089118297 Accuracy 91.56135344476152\n",
      "Training:: Epoch 37, Iteration 140, Current loss 0.5755622771081281 Accuracy 82.66301822499871\n",
      "Training:: Epoch 37, Iteration 150, Current loss 0.3897124841416761 Accuracy 88.72610209578872\n",
      "Training:: Epoch 37, Iteration 160, Current loss 0.4552093848337654 Accuracy 87.86053515579752\n",
      "Training:: Epoch 37, Iteration 170, Current loss 0.368241386914585 Accuracy 89.81383681013995\n",
      "Training:: Epoch 37, Iteration 180, Current loss 0.39180717718760455 Accuracy 87.57434154630417\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 37, Probability Accuracy 68.84603361151672\n",
      "Starting Training\n",
      "Training:: Epoch 38, Iteration 0, Current loss 0.5063088069209988 Accuracy 83.90430087506982\n",
      "Training:: Epoch 38, Iteration 10, Current loss 0.40590917565359214 Accuracy 89.80487546930358\n",
      "Training:: Epoch 38, Iteration 20, Current loss 0.5840387803045576 Accuracy 87.5295287827925\n",
      "Training:: Epoch 38, Iteration 30, Current loss 0.6397993718844139 Accuracy 87.64374295377678\n",
      "Training:: Epoch 38, Iteration 40, Current loss 0.5859809836335093 Accuracy 81.79168452636091\n",
      "Training:: Epoch 38, Iteration 50, Current loss 0.3792375083978723 Accuracy 91.62689500280742\n",
      "Training:: Epoch 38, Iteration 60, Current loss 0.4684376067954175 Accuracy 91.9284467713787\n",
      "Training:: Epoch 38, Iteration 70, Current loss 0.4431791989297652 Accuracy 88.28950166583965\n",
      "Training:: Epoch 38, Iteration 80, Current loss 0.4800014198036455 Accuracy 88.54897772352761\n",
      "Training:: Epoch 38, Iteration 90, Current loss 0.44362164309061464 Accuracy 89.76195605342525\n",
      "Training:: Epoch 38, Iteration 100, Current loss 0.6296143525990017 Accuracy 79.7427652733119\n",
      "Training:: Epoch 38, Iteration 110, Current loss 0.35773428853051037 Accuracy 86.50811291398088\n",
      "Training:: Epoch 38, Iteration 120, Current loss 0.4873834013072213 Accuracy 85.41751400056314\n",
      "Training:: Epoch 38, Iteration 130, Current loss 0.6046221362606178 Accuracy 88.93930799773113\n",
      "Training:: Epoch 38, Iteration 140, Current loss 0.31254032803153814 Accuracy 89.39449007777858\n",
      "Training:: Epoch 38, Iteration 150, Current loss 0.3939075175399856 Accuracy 87.41294091215457\n",
      "Training:: Epoch 38, Iteration 160, Current loss 0.37278310846756885 Accuracy 93.0491118077325\n",
      "Training:: Epoch 38, Iteration 170, Current loss 0.529428347927293 Accuracy 92.14726840855107\n",
      "Training:: Epoch 38, Iteration 180, Current loss 0.4019964860299132 Accuracy 89.69793322734499\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 38, Probability Accuracy 69.30703451769017\n",
      "Starting Training\n",
      "Training:: Epoch 39, Iteration 0, Current loss 0.4693719289518975 Accuracy 89.42932396839333\n",
      "Training:: Epoch 39, Iteration 10, Current loss 0.36091889968982804 Accuracy 91.09289617486338\n",
      "Training:: Epoch 39, Iteration 20, Current loss 0.3706900952668408 Accuracy 93.26612240425182\n",
      "Training:: Epoch 39, Iteration 30, Current loss 0.37892715062318705 Accuracy 87.96685707111921\n",
      "Training:: Epoch 39, Iteration 40, Current loss 0.4175061650019449 Accuracy 86.00229313969042\n",
      "Training:: Epoch 39, Iteration 50, Current loss 0.4995484074067416 Accuracy 91.28567820156606\n",
      "Training:: Epoch 39, Iteration 60, Current loss 0.3387397947352478 Accuracy 88.3785989043012\n",
      "Training:: Epoch 39, Iteration 70, Current loss 0.38668829815760836 Accuracy 87.24821485665565\n",
      "Training:: Epoch 39, Iteration 80, Current loss 0.4751645196283261 Accuracy 89.48474619450991\n",
      "Training:: Epoch 39, Iteration 90, Current loss 0.41546529811259636 Accuracy 89.6037296037296\n",
      "Training:: Epoch 39, Iteration 100, Current loss 0.38013333319547227 Accuracy 87.40234880231776\n",
      "Training:: Epoch 39, Iteration 110, Current loss 0.3429991450274997 Accuracy 85.49029367844699\n",
      "Training:: Epoch 39, Iteration 120, Current loss 0.5204614905448717 Accuracy 87.69687649148857\n",
      "Training:: Epoch 39, Iteration 130, Current loss 0.3425160353964396 Accuracy 88.50904060863552\n",
      "Training:: Epoch 39, Iteration 140, Current loss 0.4149525857205929 Accuracy 91.38060979787599\n",
      "Training:: Epoch 39, Iteration 150, Current loss 0.41495773226324206 Accuracy 88.01873198847262\n",
      "Training:: Epoch 39, Iteration 160, Current loss 0.3916306740177404 Accuracy 91.08361340258682\n",
      "Training:: Epoch 39, Iteration 170, Current loss 0.36918885281815766 Accuracy 89.16997566286666\n",
      "Training:: Epoch 39, Iteration 180, Current loss 0.5003697752486392 Accuracy 82.91014710015928\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 39, Probability Accuracy 70.09627598323777\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 40, Iteration 0, Current loss 0.38844721809096927 Accuracy 84.01760848081933\n",
      "Training:: Epoch 40, Iteration 10, Current loss 0.4950499023689709 Accuracy 91.08992954828015\n",
      "Training:: Epoch 40, Iteration 20, Current loss 0.42555270409146273 Accuracy 86.22320854990363\n",
      "Training:: Epoch 40, Iteration 30, Current loss 0.6069050117704903 Accuracy 87.40069084628671\n",
      "Training:: Epoch 40, Iteration 40, Current loss 0.6432520444946698 Accuracy 89.03623747108712\n",
      "Training:: Epoch 40, Iteration 50, Current loss 0.32604875033670977 Accuracy 91.4488854727133\n",
      "Training:: Epoch 40, Iteration 60, Current loss 0.4267201335131895 Accuracy 85.42557326078507\n",
      "Training:: Epoch 40, Iteration 70, Current loss 0.3554524532316852 Accuracy 88.54574363992172\n",
      "Training:: Epoch 40, Iteration 80, Current loss 0.30897209077507815 Accuracy 92.61711933419839\n",
      "Training:: Epoch 40, Iteration 90, Current loss 0.44144711080918153 Accuracy 90.66567497210859\n",
      "Training:: Epoch 40, Iteration 100, Current loss 0.38970048790003475 Accuracy 89.93164603731756\n",
      "Training:: Epoch 40, Iteration 110, Current loss 0.3857298862931513 Accuracy 87.69451495419932\n",
      "Training:: Epoch 40, Iteration 120, Current loss 0.3452277516209161 Accuracy 90.11005808621216\n",
      "Training:: Epoch 40, Iteration 130, Current loss 0.33924410740109934 Accuracy 89.65295629820051\n",
      "Training:: Epoch 40, Iteration 140, Current loss 0.2777928430658274 Accuracy 81.27150336574421\n",
      "Training:: Epoch 40, Iteration 150, Current loss 0.2916651448786466 Accuracy 92.06338637725548\n",
      "Training:: Epoch 40, Iteration 160, Current loss 0.3878248567828313 Accuracy 91.42446502714787\n",
      "Training:: Epoch 40, Iteration 170, Current loss 0.36265880002956474 Accuracy 91.26005361930295\n",
      "Training:: Epoch 40, Iteration 180, Current loss 0.34355070896271617 Accuracy 88.4801407005716\n",
      "Calculating Expectation\n",
      "Epoch 40 iter 0\n",
      "Epoch 40 iter 10\n",
      "Epoch 40 iter 20\n",
      "Epoch 40 iter 30\n",
      "Epoch 40 iter 40\n",
      "Epoch 40 iter 50\n",
      "Epoch 40 iter 60\n",
      "Epoch 40 iter 70\n",
      "Epoch 40 iter 80\n",
      "Epoch 40 iter 90\n",
      "Epoch 40 iter 100\n",
      "Epoch 40 iter 110\n",
      "Epoch 40 iter 120\n",
      "Epoch 40 iter 130\n",
      "Epoch 40 iter 140\n",
      "Epoch 40 iter 150\n",
      "Epoch 40 iter 160\n",
      "Epoch 40 iter 170\n",
      "Epoch 40 iter 180\n",
      "Train Boundary avergage error = 87.651\n",
      "Train From boundary avergage accuracy = 88.364\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 40, Probability Accuracy 69.77219036765317\n",
      "Starting Training\n",
      "Training:: Epoch 41, Iteration 0, Current loss 0.339420657833371 Accuracy 87.3163064833006\n",
      "Training:: Epoch 41, Iteration 10, Current loss 0.23162873473284706 Accuracy 90.90255949708127\n",
      "Training:: Epoch 41, Iteration 20, Current loss 0.3253849915972924 Accuracy 86.9341815512601\n",
      "Training:: Epoch 41, Iteration 30, Current loss 0.456583347973156 Accuracy 86.10638539185663\n",
      "Training:: Epoch 41, Iteration 40, Current loss 0.32489642099972377 Accuracy 90.94187344380528\n",
      "Training:: Epoch 41, Iteration 50, Current loss 0.35973901127295077 Accuracy 90.93583253523053\n",
      "Training:: Epoch 41, Iteration 60, Current loss 0.372563113890396 Accuracy 83.30052786973249\n",
      "Training:: Epoch 41, Iteration 70, Current loss 0.3377211546230374 Accuracy 91.57409506536689\n",
      "Training:: Epoch 41, Iteration 80, Current loss 0.45664159432899015 Accuracy 85.97686669720568\n",
      "Training:: Epoch 41, Iteration 90, Current loss 0.50286110869126 Accuracy 90.57732210423532\n",
      "Training:: Epoch 41, Iteration 100, Current loss 0.4607339014427733 Accuracy 82.86778398510242\n",
      "Training:: Epoch 41, Iteration 110, Current loss 0.33827732393884813 Accuracy 84.47899107198602\n",
      "Training:: Epoch 41, Iteration 120, Current loss 0.395312979392475 Accuracy 88.08371013564883\n",
      "Training:: Epoch 41, Iteration 130, Current loss 0.36630339015172203 Accuracy 89.49988555733577\n",
      "Training:: Epoch 41, Iteration 140, Current loss 0.3991297421705145 Accuracy 87.12976022566995\n",
      "Training:: Epoch 41, Iteration 150, Current loss 0.3195897955850488 Accuracy 89.78988837820091\n",
      "Training:: Epoch 41, Iteration 160, Current loss 0.39122223423441116 Accuracy 90.89603828768945\n",
      "Training:: Epoch 41, Iteration 170, Current loss 0.3318426189366381 Accuracy 91.45211997620868\n",
      "Training:: Epoch 41, Iteration 180, Current loss 0.38705057590894576 Accuracy 88.01191362620997\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 41, Probability Accuracy 68.97780468598518\n",
      "Starting Training\n",
      "Training:: Epoch 42, Iteration 0, Current loss 0.2839271082567915 Accuracy 87.86616592693753\n",
      "Training:: Epoch 42, Iteration 10, Current loss 0.33161951399285805 Accuracy 88.80269814502529\n",
      "Training:: Epoch 42, Iteration 20, Current loss 0.24671563162239363 Accuracy 86.75776397515529\n",
      "Training:: Epoch 42, Iteration 30, Current loss 0.375651091612008 Accuracy 88.93752812666203\n",
      "Training:: Epoch 42, Iteration 40, Current loss 0.32725826140348 Accuracy 88.67890109088256\n",
      "Training:: Epoch 42, Iteration 50, Current loss 0.39178423023454395 Accuracy 89.32430753943609\n",
      "Training:: Epoch 42, Iteration 60, Current loss 0.32761216731984516 Accuracy 88.87525948292131\n",
      "Training:: Epoch 42, Iteration 70, Current loss 0.300745039631249 Accuracy 89.65030578641995\n",
      "Training:: Epoch 42, Iteration 80, Current loss 6.248128874522883 Accuracy 71.57746100053792\n",
      "Training:: Epoch 42, Iteration 90, Current loss 4.586973857175838 Accuracy 70.54016703719282\n",
      "Training:: Epoch 42, Iteration 100, Current loss 4.09776385765774 Accuracy 66.5114165625355\n",
      "Training:: Epoch 42, Iteration 110, Current loss 3.5127130074327164 Accuracy 74.19979849647369\n",
      "Training:: Epoch 42, Iteration 120, Current loss 4.160027565848982 Accuracy 73.74973398595446\n",
      "Training:: Epoch 42, Iteration 130, Current loss 2.841628987370955 Accuracy 82.49580033597312\n",
      "Training:: Epoch 42, Iteration 140, Current loss 2.5028410585577086 Accuracy 68.09763101220388\n",
      "Training:: Epoch 42, Iteration 150, Current loss 7.109782954807896 Accuracy 58.05869700964407\n",
      "Training:: Epoch 42, Iteration 160, Current loss 2.2335800189178636 Accuracy 76.45834608557263\n",
      "Training:: Epoch 42, Iteration 170, Current loss 1.822017606938613 Accuracy 80.17145856418033\n",
      "Training:: Epoch 42, Iteration 180, Current loss 1.9895256169341375 Accuracy 85.12678075441269\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 42, Probability Accuracy 67.147453019457\n",
      "Starting Training\n",
      "Training:: Epoch 43, Iteration 0, Current loss 1.2001709920509644 Accuracy 87.50142547610902\n",
      "Training:: Epoch 43, Iteration 10, Current loss 2.0034861377142286 Accuracy 82.28300357282254\n",
      "Training:: Epoch 43, Iteration 20, Current loss 1.5076189795120472 Accuracy 79.96357012750455\n",
      "Training:: Epoch 43, Iteration 30, Current loss 1.2358419296448946 Accuracy 86.97649572649573\n",
      "Training:: Epoch 43, Iteration 40, Current loss 6.897563848635747 Accuracy 61.12305854241338\n",
      "Training:: Epoch 43, Iteration 50, Current loss 2.2094844425491558 Accuracy 79.52037829317722\n",
      "Training:: Epoch 43, Iteration 60, Current loss 1.114620070058859 Accuracy 88.3706435360723\n",
      "Training:: Epoch 43, Iteration 70, Current loss 1.3912913007536079 Accuracy 87.29254057997447\n",
      "Training:: Epoch 43, Iteration 80, Current loss 1.9757932341708635 Accuracy 82.05650292695343\n",
      "Training:: Epoch 43, Iteration 90, Current loss 2.0179042508562364 Accuracy 81.3093663580618\n",
      "Training:: Epoch 43, Iteration 100, Current loss 0.941715560012389 Accuracy 89.43919108697688\n",
      "Training:: Epoch 43, Iteration 110, Current loss 1.0392679818452035 Accuracy 88.23334218210778\n",
      "Training:: Epoch 43, Iteration 120, Current loss 1.0822930411129665 Accuracy 86.5333008050744\n",
      "Training:: Epoch 43, Iteration 130, Current loss 0.8680073644824298 Accuracy 89.55121403158354\n",
      "Training:: Epoch 43, Iteration 140, Current loss 0.8134170129545046 Accuracy 87.82455860030359\n",
      "Training:: Epoch 43, Iteration 150, Current loss 1.279459129116797 Accuracy 84.37111264685556\n",
      "Training:: Epoch 43, Iteration 160, Current loss 1.1448922442174596 Accuracy 88.37380920434724\n",
      "Training:: Epoch 43, Iteration 170, Current loss 1.0704736925306577 Accuracy 88.11226245555004\n",
      "Training:: Epoch 43, Iteration 180, Current loss 2.7422558733661972 Accuracy 81.7196261682243\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 43, Probability Accuracy 66.14848581977041\n",
      "Starting Training\n",
      "Training:: Epoch 44, Iteration 0, Current loss 1.070569082249662 Accuracy 81.38155944263268\n",
      "Training:: Epoch 44, Iteration 10, Current loss 0.6937650436864198 Accuracy 86.65966386554622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 44, Iteration 20, Current loss 0.8142177910288522 Accuracy 90.90564607805987\n",
      "Training:: Epoch 44, Iteration 30, Current loss 0.6510584565994422 Accuracy 90.18166972851603\n",
      "Training:: Epoch 44, Iteration 40, Current loss 0.6945600274915411 Accuracy 86.49534737958632\n",
      "Training:: Epoch 44, Iteration 50, Current loss 0.6522237832314436 Accuracy 88.87076819255427\n",
      "Training:: Epoch 44, Iteration 60, Current loss 1.0075530413404026 Accuracy 86.79122875308161\n",
      "Training:: Epoch 44, Iteration 70, Current loss 1.271683025775059 Accuracy 83.23634735899732\n",
      "Training:: Epoch 44, Iteration 80, Current loss 1.2138941704840716 Accuracy 82.54241580146872\n",
      "Training:: Epoch 44, Iteration 90, Current loss 1.0049763862164849 Accuracy 84.83888844905391\n",
      "Training:: Epoch 44, Iteration 100, Current loss 1.442895216973335 Accuracy 85.12142787476836\n",
      "Training:: Epoch 44, Iteration 110, Current loss 1.139321742948964 Accuracy 87.79332537874679\n",
      "Training:: Epoch 44, Iteration 120, Current loss 1.2857023882624194 Accuracy 87.91903858317521\n",
      "Training:: Epoch 44, Iteration 130, Current loss 2.170844993701362 Accuracy 80.4384746024394\n",
      "Training:: Epoch 44, Iteration 140, Current loss 1.0603470124772507 Accuracy 87.05191169045762\n",
      "Training:: Epoch 44, Iteration 150, Current loss 1.211350542598308 Accuracy 86.3580823690074\n",
      "Training:: Epoch 44, Iteration 160, Current loss 0.8080117487641191 Accuracy 85.94496705065647\n",
      "Training:: Epoch 44, Iteration 170, Current loss 0.9632224338058137 Accuracy 85.80501201510471\n",
      "Training:: Epoch 44, Iteration 180, Current loss 0.32430698996580903 Accuracy 81.45695364238411\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 44, Probability Accuracy 69.27320140397529\n",
      "Starting Training\n",
      "Training:: Epoch 45, Iteration 0, Current loss 0.6861867768249994 Accuracy 86.43979057591623\n",
      "Training:: Epoch 45, Iteration 10, Current loss 0.6430416342882691 Accuracy 81.92923336141533\n",
      "Training:: Epoch 45, Iteration 20, Current loss 0.7812218242008708 Accuracy 86.8880067364369\n",
      "Training:: Epoch 45, Iteration 30, Current loss 3.7777822052884567 Accuracy 70.79286514755323\n",
      "Training:: Epoch 45, Iteration 40, Current loss 0.47736590292471565 Accuracy 85.93009643792854\n",
      "Training:: Epoch 45, Iteration 50, Current loss 0.8845568047756105 Accuracy 85.6071771867991\n",
      "Training:: Epoch 45, Iteration 60, Current loss 0.5490694568256669 Accuracy 85.99033816425121\n",
      "Training:: Epoch 45, Iteration 70, Current loss 0.38541099212869956 Accuracy 92.2539489671932\n",
      "Training:: Epoch 45, Iteration 80, Current loss 0.6055413428082155 Accuracy 87.53894080996885\n",
      "Training:: Epoch 45, Iteration 90, Current loss 0.36857680243588065 Accuracy 91.88985507246377\n",
      "Training:: Epoch 45, Iteration 100, Current loss 0.6983723546486673 Accuracy 86.50369528140989\n",
      "Training:: Epoch 45, Iteration 110, Current loss 0.48001138652162606 Accuracy 90.81820036727198\n",
      "Training:: Epoch 45, Iteration 120, Current loss 0.367345818596403 Accuracy 87.02274975272007\n",
      "Training:: Epoch 45, Iteration 130, Current loss 0.6341807836336866 Accuracy 86.41392336751214\n",
      "Training:: Epoch 45, Iteration 140, Current loss 0.4437343585724639 Accuracy 88.09392645641097\n",
      "Training:: Epoch 45, Iteration 150, Current loss 0.47338074727861645 Accuracy 87.53597697473617\n",
      "Training:: Epoch 45, Iteration 160, Current loss 0.6492079549959502 Accuracy 89.15456289394355\n",
      "Training:: Epoch 45, Iteration 170, Current loss 0.5026579183194698 Accuracy 90.22377518400118\n",
      "Training:: Epoch 45, Iteration 180, Current loss 0.4061275946301003 Accuracy 85.20118208683792\n",
      "Calculating Expectation\n",
      "Epoch 45 iter 0\n",
      "Epoch 45 iter 10\n",
      "Epoch 45 iter 20\n",
      "Epoch 45 iter 30\n",
      "Epoch 45 iter 40\n",
      "Epoch 45 iter 50\n",
      "Epoch 45 iter 60\n",
      "Epoch 45 iter 70\n",
      "Epoch 45 iter 80\n",
      "Epoch 45 iter 90\n",
      "Epoch 45 iter 100\n",
      "Epoch 45 iter 110\n",
      "Epoch 45 iter 120\n",
      "Epoch 45 iter 130\n",
      "Epoch 45 iter 140\n",
      "Epoch 45 iter 150\n",
      "Epoch 45 iter 160\n",
      "Epoch 45 iter 170\n",
      "Epoch 45 iter 180\n",
      "Train Boundary avergage error = 86.583\n",
      "Train From boundary avergage accuracy = 88.435\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 45, Probability Accuracy 70.1113129226666\n",
      "Starting Training\n",
      "Training:: Epoch 46, Iteration 0, Current loss 0.41298843106505184 Accuracy 91.16581282019243\n",
      "Training:: Epoch 46, Iteration 10, Current loss 0.34152928755784817 Accuracy 90.73824717522393\n",
      "Training:: Epoch 46, Iteration 20, Current loss 0.3631671257428497 Accuracy 87.9631920051452\n",
      "Training:: Epoch 46, Iteration 30, Current loss 0.2656878492841359 Accuracy 92.77933095536488\n",
      "Training:: Epoch 46, Iteration 40, Current loss 1.070901925469199 Accuracy 77.54303599374022\n",
      "Training:: Epoch 46, Iteration 50, Current loss 0.5963513364557623 Accuracy 90.15794223826715\n",
      "Training:: Epoch 46, Iteration 60, Current loss 0.3247607746192699 Accuracy 90.05694261936048\n",
      "Training:: Epoch 46, Iteration 70, Current loss 0.7386327450062212 Accuracy 87.55161121921124\n",
      "Training:: Epoch 46, Iteration 80, Current loss 0.6820271112049425 Accuracy 79.17910780337805\n",
      "Training:: Epoch 46, Iteration 90, Current loss 0.3396504872831482 Accuracy 90.4168034131933\n",
      "Training:: Epoch 46, Iteration 100, Current loss 0.44997878697480553 Accuracy 89.9001791656002\n",
      "Training:: Epoch 46, Iteration 110, Current loss 0.45896025258130513 Accuracy 90.83458685616473\n",
      "Training:: Epoch 46, Iteration 120, Current loss 0.4648710934890304 Accuracy 89.77379199571678\n",
      "Training:: Epoch 46, Iteration 130, Current loss 0.38830790766338996 Accuracy 91.26303058227265\n",
      "Training:: Epoch 46, Iteration 140, Current loss 0.42074244639819375 Accuracy 77.9059249208503\n",
      "Training:: Epoch 46, Iteration 150, Current loss 0.41857270420645615 Accuracy 90.76064849787902\n",
      "Training:: Epoch 46, Iteration 160, Current loss 0.4323510416885111 Accuracy 87.45319982585981\n",
      "Training:: Epoch 46, Iteration 170, Current loss 0.4448145867460639 Accuracy 85.79548559873203\n",
      "Training:: Epoch 46, Iteration 180, Current loss 0.35285452852536764 Accuracy 81.90102728377333\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 46, Probability Accuracy 67.29267819762495\n",
      "Starting Training\n",
      "Training:: Epoch 47, Iteration 0, Current loss 0.32395601056347295 Accuracy 89.3976586102719\n",
      "Training:: Epoch 47, Iteration 10, Current loss 0.34227471869801934 Accuracy 88.33986928104575\n",
      "Training:: Epoch 47, Iteration 20, Current loss 0.5611800299649954 Accuracy 85.57216791259344\n",
      "Training:: Epoch 47, Iteration 30, Current loss 0.3650109900941525 Accuracy 84.29115531104529\n",
      "Training:: Epoch 47, Iteration 40, Current loss 0.425728581694956 Accuracy 90.05437957844592\n",
      "Training:: Epoch 47, Iteration 50, Current loss 0.36769782297712483 Accuracy 86.23277812050175\n",
      "Training:: Epoch 47, Iteration 60, Current loss 0.3122659180280861 Accuracy 91.34436818732337\n",
      "Training:: Epoch 47, Iteration 70, Current loss 0.46012622351571963 Accuracy 76.67434924078091\n",
      "Training:: Epoch 47, Iteration 80, Current loss 0.4694010587587679 Accuracy 88.5344342278101\n",
      "Training:: Epoch 47, Iteration 90, Current loss 0.4444063163643814 Accuracy 87.55719069855195\n",
      "Training:: Epoch 47, Iteration 100, Current loss 0.3431167016728031 Accuracy 86.89641616982244\n",
      "Training:: Epoch 47, Iteration 110, Current loss 0.35785834165379915 Accuracy 87.83982782057092\n",
      "Training:: Epoch 47, Iteration 120, Current loss 0.2610412493860213 Accuracy 88.31155031346297\n",
      "Training:: Epoch 47, Iteration 130, Current loss 0.31642824253473256 Accuracy 89.12883177077846\n",
      "Training:: Epoch 47, Iteration 140, Current loss 0.35662258194832847 Accuracy 81.67392092854553\n",
      "Training:: Epoch 47, Iteration 150, Current loss 0.41346525580991406 Accuracy 89.06681019982507\n",
      "Training:: Epoch 47, Iteration 160, Current loss 0.34692767835850546 Accuracy 90.64468998265\n",
      "Training:: Epoch 47, Iteration 170, Current loss 0.29597090196317766 Accuracy 87.40591331726276\n",
      "Training:: Epoch 47, Iteration 180, Current loss 0.3221567728947456 Accuracy 86.7975277555225\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 47, Probability Accuracy 70.5198823953053\n",
      "Starting Training\n",
      "Training:: Epoch 48, Iteration 0, Current loss 0.28642695631998394 Accuracy 89.1891891891892\n",
      "Training:: Epoch 48, Iteration 10, Current loss 0.3603468336477332 Accuracy 90.62236286919831\n",
      "Training:: Epoch 48, Iteration 20, Current loss 0.3319561029742711 Accuracy 85.81908502772643\n",
      "Training:: Epoch 48, Iteration 30, Current loss 0.3204188760127417 Accuracy 91.54562965276693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 48, Iteration 40, Current loss 0.32904605945102955 Accuracy 88.42373842373843\n",
      "Training:: Epoch 48, Iteration 50, Current loss 0.3822170307084815 Accuracy 92.21944675733185\n",
      "Training:: Epoch 48, Iteration 60, Current loss 0.3680558093958309 Accuracy 82.41497710922171\n",
      "Training:: Epoch 48, Iteration 70, Current loss 0.3281565450393163 Accuracy 86.793953858393\n",
      "Training:: Epoch 48, Iteration 80, Current loss 0.47067040750216516 Accuracy 84.65929109999296\n",
      "Training:: Epoch 48, Iteration 90, Current loss 0.32274214335183593 Accuracy 88.62452772388441\n",
      "Training:: Epoch 48, Iteration 100, Current loss 0.2922967807875599 Accuracy 89.17901168780419\n",
      "Training:: Epoch 48, Iteration 110, Current loss 0.45147454699939227 Accuracy 77.97273197952022\n",
      "Training:: Epoch 48, Iteration 120, Current loss 0.3336560228709048 Accuracy 87.66271240782302\n",
      "Training:: Epoch 48, Iteration 130, Current loss 0.2715206465016316 Accuracy 88.1697687535251\n",
      "Training:: Epoch 48, Iteration 140, Current loss 0.2925702402033424 Accuracy 90.89526170488479\n",
      "Training:: Epoch 48, Iteration 150, Current loss 0.26594455827430585 Accuracy 86.87234737185766\n",
      "Training:: Epoch 48, Iteration 160, Current loss 0.3829180906051756 Accuracy 88.27510316368638\n",
      "Training:: Epoch 48, Iteration 170, Current loss 0.31176739964149003 Accuracy 89.37088348853055\n",
      "Training:: Epoch 48, Iteration 180, Current loss 0.2841189892568376 Accuracy 92.35561323815705\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 48, Probability Accuracy 69.88753952142962\n",
      "Starting Training\n",
      "Training:: Epoch 49, Iteration 0, Current loss 0.3047038492182186 Accuracy 89.63848444738584\n",
      "Training:: Epoch 49, Iteration 10, Current loss 0.2738003166600092 Accuracy 87.47179390748401\n",
      "Training:: Epoch 49, Iteration 20, Current loss 0.4810223452718959 Accuracy 84.79055744435179\n",
      "Training:: Epoch 49, Iteration 30, Current loss 0.38626349153244677 Accuracy 87.01210052745888\n",
      "Training:: Epoch 49, Iteration 40, Current loss 0.3057071046005799 Accuracy 88.0725705047602\n",
      "Training:: Epoch 49, Iteration 50, Current loss 0.28717971807929926 Accuracy 92.94405948960622\n",
      "Training:: Epoch 49, Iteration 60, Current loss 0.24629792062556216 Accuracy 89.23922476135378\n",
      "Training:: Epoch 49, Iteration 70, Current loss 0.32697809128721056 Accuracy 88.09983517777255\n",
      "Training:: Epoch 49, Iteration 80, Current loss 0.3583482622946832 Accuracy 89.73658051689861\n",
      "Training:: Epoch 49, Iteration 90, Current loss 0.8750160147475132 Accuracy 86.9128031931225\n",
      "Training:: Epoch 49, Iteration 100, Current loss 0.3683224226462029 Accuracy 89.01684180221679\n",
      "Training:: Epoch 49, Iteration 110, Current loss 0.48394433995483965 Accuracy 84.31779204722211\n",
      "Training:: Epoch 49, Iteration 120, Current loss 0.4231175720626023 Accuracy 85.50457021512325\n",
      "Training:: Epoch 49, Iteration 130, Current loss 0.41579861122632084 Accuracy 84.79986768111148\n",
      "Training:: Epoch 49, Iteration 140, Current loss 0.3432219443484465 Accuracy 83.2987012987013\n",
      "Training:: Epoch 49, Iteration 150, Current loss 0.309720890451 Accuracy 90.21451840924458\n",
      "Training:: Epoch 49, Iteration 160, Current loss 0.4152400149228523 Accuracy 87.7309248724415\n",
      "Training:: Epoch 49, Iteration 170, Current loss 0.2796989533959873 Accuracy 91.81503109718463\n",
      "Training:: Epoch 49, Iteration 180, Current loss 0.38720092082149477 Accuracy 86.95305491876806\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 49, Probability Accuracy 69.63112013327476\n",
      "Starting Training\n",
      "Training:: Epoch 50, Iteration 0, Current loss 0.25617899937928795 Accuracy 91.34872658804876\n",
      "Training:: Epoch 50, Iteration 10, Current loss 0.24003443122868937 Accuracy 91.0086004691165\n",
      "Training:: Epoch 50, Iteration 20, Current loss 0.566395330762536 Accuracy 87.51129586119646\n",
      "Training:: Epoch 50, Iteration 30, Current loss 0.6382889336434493 Accuracy 81.25125999596801\n",
      "Training:: Epoch 50, Iteration 40, Current loss 0.2764593298888693 Accuracy 89.34995398644692\n",
      "Training:: Epoch 50, Iteration 50, Current loss 0.3966743033173733 Accuracy 90.03016177680286\n",
      "Training:: Epoch 50, Iteration 60, Current loss 0.41000410647172925 Accuracy 89.82556476980268\n",
      "Training:: Epoch 50, Iteration 70, Current loss 0.3038238584104317 Accuracy 90.19767678826166\n",
      "Training:: Epoch 50, Iteration 80, Current loss 0.3912280331977578 Accuracy 89.152347124892\n",
      "Training:: Epoch 50, Iteration 90, Current loss 0.42377374160339426 Accuracy 90.16972635954278\n",
      "Training:: Epoch 50, Iteration 100, Current loss 0.3363742556299478 Accuracy 92.3365558645654\n",
      "Training:: Epoch 50, Iteration 110, Current loss 0.31269129353681674 Accuracy 93.0097298472577\n",
      "Training:: Epoch 50, Iteration 120, Current loss 0.31649216523447865 Accuracy 88.80986937590711\n",
      "Training:: Epoch 50, Iteration 130, Current loss 0.3995290255484326 Accuracy 89.07276995305165\n",
      "Training:: Epoch 50, Iteration 140, Current loss 0.36136569940356617 Accuracy 90.85595531887257\n",
      "Training:: Epoch 50, Iteration 150, Current loss 0.4821947266784786 Accuracy 85.91162706983441\n",
      "Training:: Epoch 50, Iteration 160, Current loss 0.3093988220435 Accuracy 88.01231260331757\n",
      "Training:: Epoch 50, Iteration 170, Current loss 0.38746435056435047 Accuracy 85.22600757069695\n",
      "Training:: Epoch 50, Iteration 180, Current loss 0.3875464698794149 Accuracy 85.65922300397675\n",
      "Calculating Expectation\n",
      "Epoch 50 iter 0\n",
      "Epoch 50 iter 10\n",
      "Epoch 50 iter 20\n",
      "Epoch 50 iter 30\n",
      "Epoch 50 iter 40\n",
      "Epoch 50 iter 50\n",
      "Epoch 50 iter 60\n",
      "Epoch 50 iter 70\n",
      "Epoch 50 iter 80\n",
      "Epoch 50 iter 90\n",
      "Epoch 50 iter 100\n",
      "Epoch 50 iter 110\n",
      "Epoch 50 iter 120\n",
      "Epoch 50 iter 130\n",
      "Epoch 50 iter 140\n",
      "Epoch 50 iter 150\n",
      "Epoch 50 iter 160\n",
      "Epoch 50 iter 170\n",
      "Epoch 50 iter 180\n",
      "Train Boundary avergage error = 86.703\n",
      "Train From boundary avergage accuracy = 88.508\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 50, Probability Accuracy 70.05175872834978\n",
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 0.3213834186547562 Accuracy 88.60604577163139\n",
      "Training:: Epoch 51, Iteration 10, Current loss 0.3061676360747738 Accuracy 88.9776046738072\n",
      "Training:: Epoch 51, Iteration 20, Current loss 0.25062780679959196 Accuracy 89.93649768593262\n",
      "Training:: Epoch 51, Iteration 30, Current loss 0.36184876663173055 Accuracy 89.7019748653501\n",
      "Training:: Epoch 51, Iteration 40, Current loss 0.26921897325157457 Accuracy 86.20689655172414\n",
      "Training:: Epoch 51, Iteration 50, Current loss 0.3505828089047467 Accuracy 88.03144281971093\n",
      "Training:: Epoch 51, Iteration 60, Current loss 0.2077719939722718 Accuracy 92.09361163820367\n",
      "Training:: Epoch 51, Iteration 70, Current loss 0.24157807161500575 Accuracy 91.10272049569174\n",
      "Training:: Epoch 51, Iteration 80, Current loss 0.4202510711986863 Accuracy 89.67497483867149\n",
      "Training:: Epoch 51, Iteration 90, Current loss 0.2156679690930301 Accuracy 93.25614233267439\n",
      "Training:: Epoch 51, Iteration 100, Current loss 0.44178175265965414 Accuracy 82.38138582318808\n",
      "Training:: Epoch 51, Iteration 110, Current loss 0.29869378404781755 Accuracy 89.75908816164407\n",
      "Training:: Epoch 51, Iteration 120, Current loss 0.3165671543390951 Accuracy 89.5718103287552\n",
      "Training:: Epoch 51, Iteration 130, Current loss 0.31329642080791575 Accuracy 88.214894134826\n",
      "Training:: Epoch 51, Iteration 140, Current loss 0.44048181944670567 Accuracy 82.58499730167296\n",
      "Training:: Epoch 51, Iteration 150, Current loss 0.47881609744200837 Accuracy 88.11459027315124\n",
      "Training:: Epoch 51, Iteration 160, Current loss 0.2793652659195446 Accuracy 91.82801325658468\n",
      "Training:: Epoch 51, Iteration 170, Current loss 0.3570524794720014 Accuracy 87.2172503699528\n",
      "Training:: Epoch 51, Iteration 180, Current loss 0.3151655487180826 Accuracy 90.85142234116364\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 51, Probability Accuracy 69.84856219159435\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 0.23239973127579205 Accuracy 92.54794966811907\n",
      "Training:: Epoch 52, Iteration 10, Current loss 0.2720202021394782 Accuracy 91.64030727519204\n",
      "Training:: Epoch 52, Iteration 20, Current loss 0.3018702731500932 Accuracy 90.31234656494316\n",
      "Training:: Epoch 52, Iteration 30, Current loss 0.2585431177297434 Accuracy 87.14723534669902\n",
      "Training:: Epoch 52, Iteration 40, Current loss 0.2859799359679568 Accuracy 89.43619582664526\n",
      "Training:: Epoch 52, Iteration 50, Current loss 0.21979972269145806 Accuracy 93.88359618925627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 52, Iteration 60, Current loss 0.36388140788087675 Accuracy 84.41659632491397\n",
      "Training:: Epoch 52, Iteration 70, Current loss 0.2960834931100997 Accuracy 90.50650429006366\n",
      "Training:: Epoch 52, Iteration 80, Current loss 0.3109966599349396 Accuracy 86.59976686432518\n",
      "Training:: Epoch 52, Iteration 90, Current loss 0.28919786023467026 Accuracy 88.55704369928634\n",
      "Training:: Epoch 52, Iteration 100, Current loss 0.3419214064116305 Accuracy 84.22462450971013\n",
      "Training:: Epoch 52, Iteration 110, Current loss 0.19450090442795698 Accuracy 93.24372125928546\n",
      "Training:: Epoch 52, Iteration 120, Current loss 0.3030670808129949 Accuracy 82.89667055186743\n",
      "Training:: Epoch 52, Iteration 130, Current loss 0.3690111290176363 Accuracy 87.22991689750693\n",
      "Training:: Epoch 52, Iteration 140, Current loss 0.3299887959490006 Accuracy 90.94807050976655\n",
      "Training:: Epoch 52, Iteration 150, Current loss 0.39458325745925443 Accuracy 91.34710503345941\n",
      "Training:: Epoch 52, Iteration 160, Current loss 0.2621346848141787 Accuracy 81.52785438499724\n",
      "Training:: Epoch 52, Iteration 170, Current loss 0.2964585281134925 Accuracy 90.21865100826702\n",
      "Training:: Epoch 52, Iteration 180, Current loss 0.30461043970085155 Accuracy 89.97329960652051\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 52, Probability Accuracy 70.15127952483272\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 0.2372604944996384 Accuracy 88.78185031066522\n",
      "Training:: Epoch 53, Iteration 10, Current loss 0.29555008548531014 Accuracy 85.18332999398918\n",
      "Training:: Epoch 53, Iteration 20, Current loss 0.23260996436744824 Accuracy 89.02606310013718\n",
      "Training:: Epoch 53, Iteration 30, Current loss 0.2870495330076831 Accuracy 91.78872161265629\n",
      "Training:: Epoch 53, Iteration 40, Current loss 0.2669476232482046 Accuracy 90.74903014025664\n",
      "Training:: Epoch 53, Iteration 50, Current loss 0.2729397656668996 Accuracy 86.83490942235387\n",
      "Training:: Epoch 53, Iteration 60, Current loss 0.310561200657082 Accuracy 88.3637236084453\n",
      "Training:: Epoch 53, Iteration 70, Current loss 0.39266278955202777 Accuracy 88.0757181896954\n",
      "Training:: Epoch 53, Iteration 80, Current loss 0.3827885832780735 Accuracy 91.30487954017366\n",
      "Training:: Epoch 53, Iteration 90, Current loss 0.33333069647539626 Accuracy 88.9627659574468\n",
      "Training:: Epoch 53, Iteration 100, Current loss 0.2848915839457643 Accuracy 87.45155832105235\n",
      "Training:: Epoch 53, Iteration 110, Current loss 0.2485149502088526 Accuracy 91.12468423632319\n",
      "Training:: Epoch 53, Iteration 120, Current loss 0.2118830306711326 Accuracy 83.24880314736693\n",
      "Training:: Epoch 53, Iteration 130, Current loss 0.3077113866708007 Accuracy 91.88584955286692\n",
      "Training:: Epoch 53, Iteration 140, Current loss 0.3438004656485295 Accuracy 87.82671610744325\n",
      "Training:: Epoch 53, Iteration 150, Current loss 0.27845163155521613 Accuracy 84.63827676610218\n",
      "Training:: Epoch 53, Iteration 160, Current loss 0.2966102897993162 Accuracy 89.39787339268051\n",
      "Training:: Epoch 53, Iteration 170, Current loss 0.2950888764070036 Accuracy 86.2882130168274\n",
      "Training:: Epoch 53, Iteration 180, Current loss 0.3153283467137819 Accuracy 90.4126838761376\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 53, Probability Accuracy 69.91009493057287\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 0.2003660509208848 Accuracy 89.46529310767721\n",
      "Training:: Epoch 54, Iteration 10, Current loss 0.22970602914118649 Accuracy 89.06455862977602\n",
      "Training:: Epoch 54, Iteration 20, Current loss 0.2768608297947658 Accuracy 89.38258532214711\n",
      "Training:: Epoch 54, Iteration 30, Current loss 0.23288948704471937 Accuracy 91.6811662355984\n",
      "Training:: Epoch 54, Iteration 40, Current loss 0.22601919144222024 Accuracy 88.28390237652937\n",
      "Training:: Epoch 54, Iteration 50, Current loss 0.20025409111718928 Accuracy 89.29601237780435\n",
      "Training:: Epoch 54, Iteration 60, Current loss 0.28995482714623055 Accuracy 88.27329192546584\n",
      "Training:: Epoch 54, Iteration 70, Current loss 0.23948817641990355 Accuracy 88.80014632361907\n",
      "Training:: Epoch 54, Iteration 80, Current loss 0.26570209819834045 Accuracy 86.80570564080398\n",
      "Training:: Epoch 54, Iteration 90, Current loss 0.2567349712851572 Accuracy 89.83852842018099\n",
      "Training:: Epoch 54, Iteration 100, Current loss 0.27402119733172786 Accuracy 92.42940935752284\n",
      "Training:: Epoch 54, Iteration 110, Current loss 0.33223164884912154 Accuracy 86.63366336633663\n",
      "Training:: Epoch 54, Iteration 120, Current loss 0.22085175888077052 Accuracy 92.07787380354985\n",
      "Training:: Epoch 54, Iteration 130, Current loss 0.3499528158715192 Accuracy 88.0481513327601\n",
      "Training:: Epoch 54, Iteration 140, Current loss 0.21953171775805158 Accuracy 88.0432986283259\n",
      "Training:: Epoch 54, Iteration 150, Current loss 0.23640001158397272 Accuracy 90.8916892646355\n",
      "Training:: Epoch 54, Iteration 160, Current loss 0.18068328835964337 Accuracy 92.95237313861426\n",
      "Training:: Epoch 54, Iteration 170, Current loss 0.2953737242338248 Accuracy 89.22870614699139\n",
      "Training:: Epoch 54, Iteration 180, Current loss 0.24575499638388895 Accuracy 92.8802888621424\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 54, Probability Accuracy 69.85251928091773\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 0.2242367130849061 Accuracy 88.45295515081708\n",
      "Training:: Epoch 55, Iteration 10, Current loss 0.28427570446416245 Accuracy 92.56611316113161\n",
      "Training:: Epoch 55, Iteration 20, Current loss 0.306579319148597 Accuracy 90.25081330674782\n",
      "Training:: Epoch 55, Iteration 30, Current loss 0.2558429803125022 Accuracy 89.9831386998064\n",
      "Training:: Epoch 55, Iteration 40, Current loss 0.2892433608711411 Accuracy 91.76570860362706\n",
      "Training:: Epoch 55, Iteration 50, Current loss 0.3035142349204581 Accuracy 88.77874435228269\n",
      "Training:: Epoch 55, Iteration 60, Current loss 0.19520741083829857 Accuracy 88.39570826414612\n",
      "Training:: Epoch 55, Iteration 70, Current loss 0.2536802977632233 Accuracy 89.51891196968671\n",
      "Training:: Epoch 55, Iteration 80, Current loss 0.316710192684441 Accuracy 86.5880439202928\n",
      "Training:: Epoch 55, Iteration 90, Current loss 0.33821971279111157 Accuracy 89.62641918311584\n",
      "Training:: Epoch 55, Iteration 100, Current loss 0.23665105756546967 Accuracy 93.03860374508011\n",
      "Training:: Epoch 55, Iteration 110, Current loss 0.22391378138622453 Accuracy 88.71952390094069\n",
      "Training:: Epoch 55, Iteration 120, Current loss 0.2706707602655568 Accuracy 91.16483516483517\n",
      "Training:: Epoch 55, Iteration 130, Current loss 0.21989388755773498 Accuracy 91.68372293439622\n",
      "Training:: Epoch 55, Iteration 140, Current loss 0.2926236851042357 Accuracy 91.2075769534333\n",
      "Training:: Epoch 55, Iteration 150, Current loss 0.2633754759763227 Accuracy 92.03481326429437\n",
      "Training:: Epoch 55, Iteration 160, Current loss 0.2738342691376522 Accuracy 90.77455118750953\n",
      "Training:: Epoch 55, Iteration 170, Current loss 0.4058445132416455 Accuracy 80.93117127148723\n",
      "Training:: Epoch 55, Iteration 180, Current loss 0.7905023115477822 Accuracy 86.04008951102223\n",
      "Calculating Expectation\n",
      "Epoch 55 iter 0\n",
      "Epoch 55 iter 10\n",
      "Epoch 55 iter 20\n",
      "Epoch 55 iter 30\n",
      "Epoch 55 iter 40\n",
      "Epoch 55 iter 50\n",
      "Epoch 55 iter 60\n",
      "Epoch 55 iter 70\n",
      "Epoch 55 iter 80\n",
      "Epoch 55 iter 90\n",
      "Epoch 55 iter 100\n",
      "Epoch 55 iter 110\n",
      "Epoch 55 iter 120\n",
      "Epoch 55 iter 130\n",
      "Epoch 55 iter 140\n",
      "Epoch 55 iter 150\n",
      "Epoch 55 iter 160\n",
      "Epoch 55 iter 170\n",
      "Epoch 55 iter 180\n",
      "Train Boundary avergage error = 87.884\n",
      "Train From boundary avergage accuracy = 88.327\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 68.81457475139587\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 0.1907599525655609 Accuracy 92.05764438183985\n",
      "Training:: Epoch 56, Iteration 10, Current loss 0.2846519835134418 Accuracy 88.66331380058136\n",
      "Training:: Epoch 56, Iteration 20, Current loss 0.3080670962629032 Accuracy 88.75615693823246\n",
      "Training:: Epoch 56, Iteration 30, Current loss 0.4728953322512285 Accuracy 86.86834733893558\n",
      "Training:: Epoch 56, Iteration 40, Current loss 0.32616568664071677 Accuracy 93.232275005968\n",
      "Training:: Epoch 56, Iteration 50, Current loss 0.5466686543004433 Accuracy 87.31629108399395\n",
      "Training:: Epoch 56, Iteration 60, Current loss 0.540178632885142 Accuracy 79.21472937000887\n",
      "Training:: Epoch 56, Iteration 70, Current loss 0.4099260170748692 Accuracy 82.16789667896678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 56, Iteration 80, Current loss 0.260750047631798 Accuracy 89.8688781799843\n",
      "Training:: Epoch 56, Iteration 90, Current loss 0.4149727628352917 Accuracy 86.4698031364698\n",
      "Training:: Epoch 56, Iteration 100, Current loss 0.3456870660479092 Accuracy 89.12954359114512\n",
      "Training:: Epoch 56, Iteration 110, Current loss 0.36604835553487713 Accuracy 89.54392139609914\n",
      "Training:: Epoch 56, Iteration 120, Current loss 0.3292075363072061 Accuracy 90.20017002859572\n",
      "Training:: Epoch 56, Iteration 130, Current loss 0.5691411575715967 Accuracy 89.21206961785849\n",
      "Training:: Epoch 56, Iteration 140, Current loss 0.35445836976880196 Accuracy 88.68499503146737\n",
      "Training:: Epoch 56, Iteration 150, Current loss 1.1664347129865222 Accuracy 84.61039344068897\n",
      "Training:: Epoch 56, Iteration 160, Current loss 0.9846860254152014 Accuracy 83.4349963583394\n",
      "Training:: Epoch 56, Iteration 170, Current loss 1.2025309027813211 Accuracy 84.10402830364347\n",
      "Training:: Epoch 56, Iteration 180, Current loss 0.5554025514342021 Accuracy 87.50783371631502\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 56, Probability Accuracy 51.527832187755976\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 4.825146782281602 Accuracy 76.52599569828674\n",
      "Training:: Epoch 57, Iteration 10, Current loss 1.829504553734524 Accuracy 80.09003278367666\n",
      "Training:: Epoch 57, Iteration 20, Current loss 2.8237850784772554 Accuracy 77.1824808866236\n",
      "Training:: Epoch 57, Iteration 30, Current loss 2.581642320060635 Accuracy 82.23998857877079\n",
      "Training:: Epoch 57, Iteration 40, Current loss 1.595066427474896 Accuracy 86.923828125\n",
      "Training:: Epoch 57, Iteration 50, Current loss 2.295497946097123 Accuracy 81.6783860156404\n",
      "Training:: Epoch 57, Iteration 60, Current loss 1.8640928042015417 Accuracy 74.70698805838124\n",
      "Training:: Epoch 57, Iteration 70, Current loss 1.8804522518417008 Accuracy 83.41545825707448\n",
      "Training:: Epoch 57, Iteration 80, Current loss 1.0374990159490651 Accuracy 87.21749696233293\n",
      "Training:: Epoch 57, Iteration 90, Current loss 1.006845307120592 Accuracy 86.20432513049963\n",
      "Training:: Epoch 57, Iteration 100, Current loss 1.0978532460046602 Accuracy 85.89618755149426\n",
      "Training:: Epoch 57, Iteration 110, Current loss 1.3928011058527066 Accuracy 86.51581426648721\n",
      "Training:: Epoch 57, Iteration 120, Current loss 1.6703151421716933 Accuracy 86.29201034100701\n",
      "Training:: Epoch 57, Iteration 130, Current loss 1.6576038445984698 Accuracy 84.04495683911709\n",
      "Training:: Epoch 57, Iteration 140, Current loss 2.068900612422026 Accuracy 82.53037884203002\n",
      "Training:: Epoch 57, Iteration 150, Current loss 2.404981478037155 Accuracy 80.35184685899164\n",
      "Training:: Epoch 57, Iteration 160, Current loss 3.3647033366284176 Accuracy 78.68455074337427\n",
      "Training:: Epoch 57, Iteration 170, Current loss 1.6104463523948696 Accuracy 82.88351279541322\n",
      "Training:: Epoch 57, Iteration 180, Current loss 1.8684928075262734 Accuracy 81.99754601226994\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 57, Probability Accuracy 60.92987642010043\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 2.1924892364497306 Accuracy 76.38357679369614\n",
      "Training:: Epoch 58, Iteration 10, Current loss 4.57949629566701 Accuracy 71.48485260154871\n",
      "Training:: Epoch 58, Iteration 20, Current loss 2.077157329377181 Accuracy 85.05188229986466\n",
      "Training:: Epoch 58, Iteration 30, Current loss 1.2232610594042033 Accuracy 87.1822358346095\n",
      "Training:: Epoch 58, Iteration 40, Current loss 1.4458347972366428 Accuracy 87.41156735467872\n",
      "Training:: Epoch 58, Iteration 50, Current loss 0.9439380843252656 Accuracy 86.53500897666068\n",
      "Training:: Epoch 58, Iteration 60, Current loss 0.8118171617793813 Accuracy 87.72423863162287\n",
      "Training:: Epoch 58, Iteration 70, Current loss 0.5057042815611532 Accuracy 92.73304308906701\n",
      "Training:: Epoch 58, Iteration 80, Current loss 0.9033230617941184 Accuracy 89.72394247926944\n",
      "Training:: Epoch 58, Iteration 90, Current loss 0.9273252868653558 Accuracy 84.05269544115097\n",
      "Training:: Epoch 58, Iteration 100, Current loss 1.7682026569095872 Accuracy 86.69450067476383\n",
      "Training:: Epoch 58, Iteration 110, Current loss 0.8301108871605334 Accuracy 90.29754994806623\n",
      "Training:: Epoch 58, Iteration 120, Current loss 0.7593975220615912 Accuracy 81.6\n",
      "Training:: Epoch 58, Iteration 130, Current loss 0.7382960469418981 Accuracy 87.67061193653991\n",
      "Training:: Epoch 58, Iteration 140, Current loss 0.6338604220931638 Accuracy 84.92903536203993\n",
      "Training:: Epoch 58, Iteration 150, Current loss 0.6494861832306126 Accuracy 88.59618311673105\n",
      "Training:: Epoch 58, Iteration 160, Current loss 0.7410025750026388 Accuracy 87.79520120914415\n",
      "Training:: Epoch 58, Iteration 170, Current loss 0.6162163411915569 Accuracy 87.49838896765047\n",
      "Training:: Epoch 58, Iteration 180, Current loss 0.4137527422612652 Accuracy 90.83475298126065\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 58, Probability Accuracy 68.68458436712292\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 0.8089992097931344 Accuracy 87.9163522690525\n",
      "Training:: Epoch 59, Iteration 10, Current loss 0.47632815672127293 Accuracy 91.48969578017665\n",
      "Training:: Epoch 59, Iteration 20, Current loss 0.34093614283442447 Accuracy 90.71366152053598\n",
      "Training:: Epoch 59, Iteration 30, Current loss 0.5307942798392821 Accuracy 85.63528968204098\n",
      "Training:: Epoch 59, Iteration 40, Current loss 0.4300821219080252 Accuracy 81.28422425032595\n",
      "Training:: Epoch 59, Iteration 50, Current loss 0.5552881158020296 Accuracy 86.03755416466056\n",
      "Training:: Epoch 59, Iteration 60, Current loss 0.3812421313259416 Accuracy 91.03589250285577\n",
      "Training:: Epoch 59, Iteration 70, Current loss 0.4423905219470311 Accuracy 88.74235867848067\n",
      "Training:: Epoch 59, Iteration 80, Current loss 0.6041834547968652 Accuracy 85.80851745116875\n",
      "Training:: Epoch 59, Iteration 90, Current loss 0.4537565634968754 Accuracy 91.99247334309011\n",
      "Training:: Epoch 59, Iteration 100, Current loss 1.969893855084631 Accuracy 81.08216755105322\n",
      "Training:: Epoch 59, Iteration 110, Current loss 0.7349060773975171 Accuracy 87.41460851287441\n",
      "Training:: Epoch 59, Iteration 120, Current loss 0.6012359611493097 Accuracy 81.43356790695715\n",
      "Training:: Epoch 59, Iteration 130, Current loss 1.294427136970285 Accuracy 87.63934045517883\n",
      "Training:: Epoch 59, Iteration 140, Current loss 0.4988494989087432 Accuracy 90.22878228782288\n",
      "Training:: Epoch 59, Iteration 150, Current loss 1.350308342342743 Accuracy 82.87272391163694\n",
      "Training:: Epoch 59, Iteration 160, Current loss 0.949609833297888 Accuracy 85.4917917777466\n",
      "Training:: Epoch 59, Iteration 170, Current loss 1.2697259951480153 Accuracy 80.06134969325153\n",
      "Training:: Epoch 59, Iteration 180, Current loss 0.9976863455192418 Accuracy 84.60975335060255\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 59, Probability Accuracy 66.2584929029603\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 0.6413311010090018 Accuracy 86.64472264191669\n",
      "Training:: Epoch 60, Iteration 10, Current loss 0.7029462965189928 Accuracy 90.90316876832162\n",
      "Training:: Epoch 60, Iteration 20, Current loss 0.6363981255391528 Accuracy 87.46937906060283\n",
      "Training:: Epoch 60, Iteration 30, Current loss 0.4874817280165965 Accuracy 86.64257151645647\n",
      "Training:: Epoch 60, Iteration 40, Current loss 0.5754294050525008 Accuracy 82.1259646502365\n",
      "Training:: Epoch 60, Iteration 50, Current loss 0.7836252283714423 Accuracy 87.34949879599037\n",
      "Training:: Epoch 60, Iteration 60, Current loss 0.3643769978645429 Accuracy 89.60714044222057\n",
      "Training:: Epoch 60, Iteration 70, Current loss 0.36421887756766247 Accuracy 90.24977430033103\n",
      "Training:: Epoch 60, Iteration 80, Current loss 0.46623279202070295 Accuracy 89.33333333333333\n",
      "Training:: Epoch 60, Iteration 90, Current loss 0.3323256951774696 Accuracy 90.44144741690619\n",
      "Training:: Epoch 60, Iteration 100, Current loss 0.39923605602146295 Accuracy 91.62024141132777\n",
      "Training:: Epoch 60, Iteration 110, Current loss 0.34554354852838953 Accuracy 86.49570000774773\n",
      "Training:: Epoch 60, Iteration 120, Current loss 0.46840014890687715 Accuracy 85.16860821581852\n",
      "Training:: Epoch 60, Iteration 130, Current loss 0.4764066125522736 Accuracy 90.08616860620059\n",
      "Training:: Epoch 60, Iteration 140, Current loss 0.4367954654950297 Accuracy 88.36234393511346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 60, Iteration 150, Current loss 0.48706702805187635 Accuracy 83.02875829566221\n",
      "Training:: Epoch 60, Iteration 160, Current loss 1.3029426116112797 Accuracy 83.54380077711056\n",
      "Training:: Epoch 60, Iteration 170, Current loss 0.6355849027435674 Accuracy 89.0217953878889\n",
      "Training:: Epoch 60, Iteration 180, Current loss 0.5018386831775036 Accuracy 91.2230114635347\n",
      "Calculating Expectation\n",
      "Epoch 60 iter 0\n",
      "Epoch 60 iter 10\n",
      "Epoch 60 iter 20\n",
      "Epoch 60 iter 30\n",
      "Epoch 60 iter 40\n",
      "Epoch 60 iter 50\n",
      "Epoch 60 iter 60\n",
      "Epoch 60 iter 70\n",
      "Epoch 60 iter 80\n",
      "Epoch 60 iter 90\n",
      "Epoch 60 iter 100\n",
      "Epoch 60 iter 110\n",
      "Epoch 60 iter 120\n",
      "Epoch 60 iter 130\n",
      "Epoch 60 iter 140\n",
      "Epoch 60 iter 150\n",
      "Epoch 60 iter 160\n",
      "Epoch 60 iter 170\n",
      "Epoch 60 iter 180\n",
      "Train Boundary avergage error = 88.940\n",
      "Train From boundary avergage accuracy = 88.319\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 68.60247476366284\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 0.9522292204399825 Accuracy 79.63407005838198\n",
      "Training:: Epoch 61, Iteration 10, Current loss 0.8411878297233489 Accuracy 87.47751243430103\n",
      "Training:: Epoch 61, Iteration 20, Current loss 0.36056253581164016 Accuracy 90.82042455536431\n",
      "Training:: Epoch 61, Iteration 30, Current loss 0.3567713513465595 Accuracy 89.99174387670855\n",
      "Training:: Epoch 61, Iteration 40, Current loss 0.6327049619961763 Accuracy 85.92839771506137\n",
      "Training:: Epoch 61, Iteration 50, Current loss 0.6691789332589129 Accuracy 85.42686726609098\n",
      "Training:: Epoch 61, Iteration 60, Current loss 0.7050673342788188 Accuracy 84.9865815990683\n",
      "Training:: Epoch 61, Iteration 70, Current loss 0.6004221043936417 Accuracy 84.59154020019373\n",
      "Training:: Epoch 61, Iteration 80, Current loss 0.35566570717170076 Accuracy 89.61479338095668\n",
      "Training:: Epoch 61, Iteration 90, Current loss 0.817317370987802 Accuracy 88.20928518791452\n",
      "Training:: Epoch 61, Iteration 100, Current loss 0.6259821749055046 Accuracy 88.9205993272857\n",
      "Training:: Epoch 61, Iteration 110, Current loss 0.46812673778447433 Accuracy 89.32475439324755\n",
      "Training:: Epoch 61, Iteration 120, Current loss 0.36569893794472974 Accuracy 93.57954545454545\n",
      "Training:: Epoch 61, Iteration 130, Current loss 0.38207670910412395 Accuracy 92.39985163204747\n",
      "Training:: Epoch 61, Iteration 140, Current loss 0.3912880320214341 Accuracy 93.20323514135194\n",
      "Training:: Epoch 61, Iteration 150, Current loss 0.43389036100771167 Accuracy 86.46573080099091\n",
      "Training:: Epoch 61, Iteration 160, Current loss 0.5214930210576098 Accuracy 86.95068468629732\n",
      "Training:: Epoch 61, Iteration 170, Current loss 0.399182948217476 Accuracy 83.98978649426849\n",
      "Training:: Epoch 61, Iteration 180, Current loss 1.403886922459329 Accuracy 82.15242093073749\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 61, Probability Accuracy 57.004048102377816\n",
      "Starting Training\n",
      "Training:: Epoch 62, Iteration 0, Current loss 0.8281081733710084 Accuracy 88.51839119972499\n",
      "Training:: Epoch 62, Iteration 10, Current loss 1.6249721894715272 Accuracy 79.67371400033295\n",
      "Training:: Epoch 62, Iteration 20, Current loss 0.9890412839197944 Accuracy 86.33129867396337\n",
      "Training:: Epoch 62, Iteration 30, Current loss 0.595807571651654 Accuracy 85.47378547378547\n",
      "Training:: Epoch 62, Iteration 40, Current loss 0.6805759033355004 Accuracy 88.28072076954342\n",
      "Training:: Epoch 62, Iteration 50, Current loss 0.623665072053307 Accuracy 85.40998437371495\n",
      "Training:: Epoch 62, Iteration 60, Current loss 0.4671653331843109 Accuracy 88.42610218533466\n",
      "Training:: Epoch 62, Iteration 70, Current loss 0.42471882949676304 Accuracy 93.40614886731392\n",
      "Training:: Epoch 62, Iteration 80, Current loss 7.100006674312935 Accuracy 72.95737949004281\n",
      "Training:: Epoch 62, Iteration 90, Current loss 0.7263635638773428 Accuracy 89.45694963157364\n",
      "Training:: Epoch 62, Iteration 100, Current loss 0.5729963420293764 Accuracy 88.5963848113551\n",
      "Training:: Epoch 62, Iteration 110, Current loss 0.753084557932997 Accuracy 85.57341907824222\n",
      "Training:: Epoch 62, Iteration 120, Current loss 1.139903863069366 Accuracy 83.93346564596919\n",
      "Training:: Epoch 62, Iteration 130, Current loss 0.7717878028477707 Accuracy 89.92195242814668\n",
      "Training:: Epoch 62, Iteration 140, Current loss 0.5671791213621655 Accuracy 85.16447607290684\n",
      "Training:: Epoch 62, Iteration 150, Current loss 0.4418317533948574 Accuracy 87.69656897784131\n",
      "Training:: Epoch 62, Iteration 160, Current loss 0.4465891665428817 Accuracy 91.68038634617496\n",
      "Training:: Epoch 62, Iteration 170, Current loss 0.4717198445772487 Accuracy 88.02301495972382\n",
      "Training:: Epoch 62, Iteration 180, Current loss 0.788060385267326 Accuracy 80.98513101176927\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 62, Probability Accuracy 68.12366695553419\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 0.4212248430557985 Accuracy 85.81590860099159\n",
      "Training:: Epoch 63, Iteration 10, Current loss 0.3724064118469649 Accuracy 88.90791133537574\n",
      "Training:: Epoch 63, Iteration 20, Current loss 0.27435947310414416 Accuracy 90.01956947162427\n",
      "Training:: Epoch 63, Iteration 30, Current loss 0.5323450319070921 Accuracy 89.1742195367573\n",
      "Training:: Epoch 63, Iteration 40, Current loss 0.3760606190824657 Accuracy 88.89168607212537\n",
      "Training:: Epoch 63, Iteration 50, Current loss 0.7720445613720729 Accuracy 87.8820303618003\n",
      "Training:: Epoch 63, Iteration 60, Current loss 0.442706923717532 Accuracy 83.43101604278075\n",
      "Training:: Epoch 63, Iteration 70, Current loss 0.4812737024326098 Accuracy 86.94642857142857\n",
      "Training:: Epoch 63, Iteration 80, Current loss 0.3611926877245068 Accuracy 85.95759971254043\n",
      "Training:: Epoch 63, Iteration 90, Current loss 0.34684951182326124 Accuracy 86.62545875989956\n",
      "Training:: Epoch 63, Iteration 100, Current loss 0.37214059451182846 Accuracy 83.78814706801374\n",
      "Training:: Epoch 63, Iteration 110, Current loss 0.5316276784967908 Accuracy 80.3559497054\n",
      "Training:: Epoch 63, Iteration 120, Current loss 0.35526736636726775 Accuracy 80.40147913365028\n",
      "Training:: Epoch 63, Iteration 130, Current loss 0.3673533610132357 Accuracy 87.70289276618391\n",
      "Training:: Epoch 63, Iteration 140, Current loss 0.7458078341553032 Accuracy 83.27600583698144\n",
      "Training:: Epoch 63, Iteration 150, Current loss 0.4420510608329803 Accuracy 81.94666971847104\n",
      "Training:: Epoch 63, Iteration 160, Current loss 0.35988805015749004 Accuracy 91.36591179976162\n",
      "Training:: Epoch 63, Iteration 170, Current loss 0.4128144483062861 Accuracy 88.94556724995157\n",
      "Training:: Epoch 63, Iteration 180, Current loss 0.43077992553902866 Accuracy 87.61965300628178\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 63, Probability Accuracy 68.89114442980322\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 0.3703029009140343 Accuracy 85.05169867060562\n",
      "Training:: Epoch 64, Iteration 10, Current loss 0.3169303740029067 Accuracy 88.70354609929078\n",
      "Training:: Epoch 64, Iteration 20, Current loss 0.3604722649704395 Accuracy 89.37654944449545\n",
      "Training:: Epoch 64, Iteration 30, Current loss 0.5271760958130706 Accuracy 90.35569504380803\n",
      "Training:: Epoch 64, Iteration 40, Current loss 0.30388527250772096 Accuracy 86.23363286264441\n",
      "Training:: Epoch 64, Iteration 50, Current loss 0.3215324025168836 Accuracy 88.59219465950387\n",
      "Training:: Epoch 64, Iteration 60, Current loss 0.47568387034311 Accuracy 80.24768353528154\n",
      "Training:: Epoch 64, Iteration 70, Current loss 0.3315466697206243 Accuracy 89.14963205233033\n",
      "Training:: Epoch 64, Iteration 80, Current loss 0.43810375305588917 Accuracy 90.54006353688669\n",
      "Training:: Epoch 64, Iteration 90, Current loss 0.2980580233304182 Accuracy 90.44189852700491\n",
      "Training:: Epoch 64, Iteration 100, Current loss 0.33467738286509746 Accuracy 90.3279697529236\n",
      "Training:: Epoch 64, Iteration 110, Current loss 0.30888343400776036 Accuracy 86.77720207253886\n",
      "Training:: Epoch 64, Iteration 120, Current loss 0.42506342684501114 Accuracy 88.08249232119351\n",
      "Training:: Epoch 64, Iteration 130, Current loss 0.3340674749460195 Accuracy 88.61952149057802\n",
      "Training:: Epoch 64, Iteration 140, Current loss 0.35824170410372974 Accuracy 89.3044275609117\n",
      "Training:: Epoch 64, Iteration 150, Current loss 0.2749798012701773 Accuracy 90.59188892948484\n",
      "Training:: Epoch 64, Iteration 160, Current loss 0.4072174113874401 Accuracy 87.3911666930505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 64, Iteration 170, Current loss 0.23397482774416734 Accuracy 91.10603148215492\n",
      "Training:: Epoch 64, Iteration 180, Current loss 0.3721333214400143 Accuracy 89.24207543828581\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 64, Probability Accuracy 68.41945938245664\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 0.455156181661027 Accuracy 90.21730045967405\n",
      "Training:: Epoch 65, Iteration 10, Current loss 0.2994140663188707 Accuracy 93.90678397440558\n",
      "Training:: Epoch 65, Iteration 20, Current loss 0.29591789867751955 Accuracy 87.15861118290687\n",
      "Training:: Epoch 65, Iteration 30, Current loss 0.43267273305107845 Accuracy 90.91863941181224\n",
      "Training:: Epoch 65, Iteration 40, Current loss 0.3707517699578055 Accuracy 91.2474645030426\n",
      "Training:: Epoch 65, Iteration 50, Current loss 0.28377862968563555 Accuracy 89.68958550174493\n",
      "Training:: Epoch 65, Iteration 60, Current loss 0.22364916674271598 Accuracy 92.71604938271605\n",
      "Training:: Epoch 65, Iteration 70, Current loss 0.3624797956462183 Accuracy 87.2190566635011\n",
      "Training:: Epoch 65, Iteration 80, Current loss 0.33162604874127305 Accuracy 91.00626146353804\n",
      "Training:: Epoch 65, Iteration 90, Current loss 0.2552891091008786 Accuracy 90.8955223880597\n",
      "Training:: Epoch 65, Iteration 100, Current loss 0.4296234671319563 Accuracy 90.48204464574572\n",
      "Training:: Epoch 65, Iteration 110, Current loss 0.35926661848686914 Accuracy 86.42733217669826\n",
      "Training:: Epoch 65, Iteration 120, Current loss 0.42953214152700586 Accuracy 91.96171608326082\n",
      "Training:: Epoch 65, Iteration 130, Current loss 0.34469865987401876 Accuracy 88.39262393564228\n",
      "Training:: Epoch 65, Iteration 140, Current loss 0.3766311581531693 Accuracy 83.08014190848722\n",
      "Training:: Epoch 65, Iteration 150, Current loss 0.3106048605545215 Accuracy 87.87179487179488\n",
      "Training:: Epoch 65, Iteration 160, Current loss 0.25974090372054115 Accuracy 89.45004083855159\n",
      "Training:: Epoch 65, Iteration 170, Current loss 0.36529662354566783 Accuracy 83.7813138372218\n",
      "Training:: Epoch 65, Iteration 180, Current loss 0.27278692439946 Accuracy 89.49791396921306\n",
      "Calculating Expectation\n",
      "Epoch 65 iter 0\n",
      "Epoch 65 iter 10\n",
      "Epoch 65 iter 20\n",
      "Epoch 65 iter 30\n",
      "Epoch 65 iter 40\n",
      "Epoch 65 iter 50\n",
      "Epoch 65 iter 60\n",
      "Epoch 65 iter 70\n",
      "Epoch 65 iter 80\n",
      "Epoch 65 iter 90\n",
      "Epoch 65 iter 100\n",
      "Epoch 65 iter 110\n",
      "Epoch 65 iter 120\n",
      "Epoch 65 iter 130\n",
      "Epoch 65 iter 140\n",
      "Epoch 65 iter 150\n",
      "Epoch 65 iter 160\n",
      "Epoch 65 iter 170\n",
      "Epoch 65 iter 180\n",
      "Train Boundary avergage error = 88.236\n",
      "Train From boundary avergage accuracy = 88.438\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 69.65862190407223\n",
      "Starting Training\n",
      "Training:: Epoch 66, Iteration 0, Current loss 0.2960382593187241 Accuracy 85.60229157244082\n",
      "Training:: Epoch 66, Iteration 10, Current loss 0.27202852645629877 Accuracy 91.43280423280423\n",
      "Training:: Epoch 66, Iteration 20, Current loss 0.24434351264238263 Accuracy 87.9486597001363\n",
      "Training:: Epoch 66, Iteration 30, Current loss 0.2750196052753252 Accuracy 82.6848885110835\n",
      "Training:: Epoch 66, Iteration 40, Current loss 0.41237791741251795 Accuracy 87.8511946454977\n",
      "Training:: Epoch 66, Iteration 50, Current loss 0.3058950714912169 Accuracy 87.35461712590742\n",
      "Training:: Epoch 66, Iteration 60, Current loss 0.2716383930028818 Accuracy 90.15569815450431\n",
      "Training:: Epoch 66, Iteration 70, Current loss 0.29015564580982384 Accuracy 89.98875140607424\n",
      "Training:: Epoch 66, Iteration 80, Current loss 0.23831898972066554 Accuracy 90.07260666905701\n",
      "Training:: Epoch 66, Iteration 90, Current loss 0.32579748776355166 Accuracy 87.04516938519448\n",
      "Training:: Epoch 66, Iteration 100, Current loss 0.2452935806336612 Accuracy 88.19597878921785\n",
      "Training:: Epoch 66, Iteration 110, Current loss 0.24259244039351258 Accuracy 85.52764415939188\n",
      "Training:: Epoch 66, Iteration 120, Current loss 0.28149457273684747 Accuracy 87.689271496138\n",
      "Training:: Epoch 66, Iteration 130, Current loss 0.2586862771913642 Accuracy 89.98741434764369\n",
      "Training:: Epoch 66, Iteration 140, Current loss 0.4216887272018936 Accuracy 84.33980478386785\n",
      "Training:: Epoch 66, Iteration 150, Current loss 0.33953696528019334 Accuracy 86.56547988489534\n",
      "Training:: Epoch 66, Iteration 160, Current loss 0.33239184048304027 Accuracy 87.32357324449899\n",
      "Training:: Epoch 66, Iteration 170, Current loss 0.26686446526711743 Accuracy 89.77770566282655\n",
      "Training:: Epoch 66, Iteration 180, Current loss 0.2930988270473316 Accuracy 89.23189667017817\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 66, Probability Accuracy 69.52902722873164\n",
      "Starting Training\n",
      "Training:: Epoch 67, Iteration 0, Current loss 0.2944206529570128 Accuracy 88.33616780045351\n",
      "Training:: Epoch 67, Iteration 10, Current loss 0.3215877133862393 Accuracy 90.52697095435684\n",
      "Training:: Epoch 67, Iteration 20, Current loss 0.2921643657868965 Accuracy 84.45398859395902\n",
      "Training:: Epoch 67, Iteration 30, Current loss 0.3056181335894379 Accuracy 89.27425600177011\n",
      "Training:: Epoch 67, Iteration 40, Current loss 0.311326446669109 Accuracy 89.57995458968537\n",
      "Training:: Epoch 67, Iteration 50, Current loss 0.2623588335010822 Accuracy 90.59307416744062\n",
      "Training:: Epoch 67, Iteration 60, Current loss 0.2911858884186544 Accuracy 92.2019878593823\n",
      "Training:: Epoch 67, Iteration 70, Current loss 0.2879978237339127 Accuracy 89.38372093023256\n",
      "Training:: Epoch 67, Iteration 80, Current loss 0.23173018714923962 Accuracy 91.66832768586805\n",
      "Training:: Epoch 67, Iteration 90, Current loss 0.27347021482785683 Accuracy 94.25616092282759\n",
      "Training:: Epoch 67, Iteration 100, Current loss 0.2550959301775646 Accuracy 91.88673406230659\n",
      "Training:: Epoch 67, Iteration 110, Current loss 0.254622651445789 Accuracy 89.73814920190964\n",
      "Training:: Epoch 67, Iteration 120, Current loss 0.20200953020293555 Accuracy 86.71611253196932\n",
      "Training:: Epoch 67, Iteration 130, Current loss 0.2432641519724282 Accuracy 90.37242858535635\n",
      "Training:: Epoch 67, Iteration 140, Current loss 0.24525026530244554 Accuracy 88.5395306447938\n",
      "Training:: Epoch 67, Iteration 150, Current loss 0.2933259282577538 Accuracy 88.4506416310205\n",
      "Training:: Epoch 67, Iteration 160, Current loss 0.30646045955476825 Accuracy 86.93409300893133\n",
      "Training:: Epoch 67, Iteration 170, Current loss 0.2537280885558708 Accuracy 87.66194084575898\n",
      "Training:: Epoch 67, Iteration 180, Current loss 0.2900470730217862 Accuracy 91.37413465661949\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 67, Probability Accuracy 68.90855562282607\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 0.3936919085103265 Accuracy 88.2217311941589\n",
      "Training:: Epoch 68, Iteration 10, Current loss 0.28454153109634217 Accuracy 89.93513287298597\n",
      "Training:: Epoch 68, Iteration 20, Current loss 0.32991815648005435 Accuracy 83.73039626481504\n",
      "Training:: Epoch 68, Iteration 30, Current loss 0.25661551888647643 Accuracy 86.9308952522393\n",
      "Training:: Epoch 68, Iteration 40, Current loss 0.47601902443422905 Accuracy 84.73737971076824\n",
      "Training:: Epoch 68, Iteration 50, Current loss 0.26244093894771064 Accuracy 89.61187214611873\n",
      "Training:: Epoch 68, Iteration 60, Current loss 0.2159706111231499 Accuracy 89.04975347377858\n",
      "Training:: Epoch 68, Iteration 70, Current loss 0.284410761053108 Accuracy 90.80051327608331\n",
      "Training:: Epoch 68, Iteration 80, Current loss 0.27494223263150797 Accuracy 89.53422590986234\n",
      "Training:: Epoch 68, Iteration 90, Current loss 0.352172361665801 Accuracy 88.715953307393\n",
      "Training:: Epoch 68, Iteration 100, Current loss 0.23214248971054058 Accuracy 88.86454183266932\n",
      "Training:: Epoch 68, Iteration 110, Current loss 0.2630862423167255 Accuracy 91.53517255420951\n",
      "Training:: Epoch 68, Iteration 120, Current loss 0.31298615634185023 Accuracy 83.94924172082946\n",
      "Training:: Epoch 68, Iteration 130, Current loss 0.2580110471399114 Accuracy 83.9750500539042\n",
      "Training:: Epoch 68, Iteration 140, Current loss 0.22713568588709857 Accuracy 89.13746630727763\n",
      "Training:: Epoch 68, Iteration 150, Current loss 0.29145556229411335 Accuracy 88.49600782141111\n",
      "Training:: Epoch 68, Iteration 160, Current loss 0.2423061479819133 Accuracy 90.35558204211362\n",
      "Training:: Epoch 68, Iteration 170, Current loss 0.25443216950104697 Accuracy 90.84373143196673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 68, Iteration 180, Current loss 0.25761419661479096 Accuracy 89.45401513079938\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 68, Probability Accuracy 68.71703249957461\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 0.2800892428015493 Accuracy 91.4149940599012\n",
      "Training:: Epoch 69, Iteration 10, Current loss 0.3408190945907643 Accuracy 85.34190693493743\n",
      "Training:: Epoch 69, Iteration 20, Current loss 0.3200412579849464 Accuracy 84.96539537623325\n",
      "Training:: Epoch 69, Iteration 30, Current loss 0.2643728373399598 Accuracy 81.46634242251773\n",
      "Training:: Epoch 69, Iteration 40, Current loss 0.232119777105271 Accuracy 90.24031387935263\n",
      "Training:: Epoch 69, Iteration 50, Current loss 0.2821073868030873 Accuracy 85.83308285742912\n",
      "Training:: Epoch 69, Iteration 60, Current loss 0.2535971134994726 Accuracy 88.74555160142349\n",
      "Training:: Epoch 69, Iteration 70, Current loss 0.21596326472332789 Accuracy 83.08946189409465\n",
      "Training:: Epoch 69, Iteration 80, Current loss 0.3016301982461445 Accuracy 91.36351013607332\n",
      "Training:: Epoch 69, Iteration 90, Current loss 0.3105701810533599 Accuracy 82.20190779014308\n",
      "Training:: Epoch 69, Iteration 100, Current loss 0.19433866442155798 Accuracy 86.2369080985396\n",
      "Training:: Epoch 69, Iteration 110, Current loss 0.3064531596223628 Accuracy 82.92252107587315\n",
      "Training:: Epoch 69, Iteration 120, Current loss 0.25678883084641735 Accuracy 86.47262469547378\n",
      "Training:: Epoch 69, Iteration 130, Current loss 0.2751000755563243 Accuracy 88.05780818194174\n",
      "Training:: Epoch 69, Iteration 140, Current loss 0.2309261340944033 Accuracy 91.9042406358574\n",
      "Training:: Epoch 69, Iteration 150, Current loss 0.17784394982934482 Accuracy 86.17587757845216\n",
      "Training:: Epoch 69, Iteration 160, Current loss 0.28070927770412246 Accuracy 89.43161112090755\n",
      "Training:: Epoch 69, Iteration 170, Current loss 0.24755286310308067 Accuracy 86.85298169651644\n",
      "Training:: Epoch 69, Iteration 180, Current loss 0.23230565664550137 Accuracy 84.1631926553937\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 69, Probability Accuracy 69.12837193473969\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 0.2442445802570636 Accuracy 88.70690999844018\n",
      "Training:: Epoch 70, Iteration 10, Current loss 0.27271504162873256 Accuracy 89.84375\n",
      "Training:: Epoch 70, Iteration 20, Current loss 0.22080054763231813 Accuracy 90.80956496686834\n",
      "Training:: Epoch 70, Iteration 30, Current loss 0.27171592058550004 Accuracy 89.00519700711364\n",
      "Training:: Epoch 70, Iteration 40, Current loss 0.2078115000327285 Accuracy 91.62787513691129\n",
      "Training:: Epoch 70, Iteration 50, Current loss 0.3292084707174674 Accuracy 83.3897616468039\n",
      "Training:: Epoch 70, Iteration 60, Current loss 0.27186573145408766 Accuracy 89.17336692726667\n",
      "Training:: Epoch 70, Iteration 70, Current loss 0.24391301149099426 Accuracy 89.40573045631412\n",
      "Training:: Epoch 70, Iteration 80, Current loss 0.26949876371721976 Accuracy 87.93379035981955\n",
      "Training:: Epoch 70, Iteration 90, Current loss 0.3115698916543084 Accuracy 87.78849144634526\n",
      "Training:: Epoch 70, Iteration 100, Current loss 0.26109545073131146 Accuracy 87.90552077404666\n",
      "Training:: Epoch 70, Iteration 110, Current loss 0.2547818491720498 Accuracy 90.33632803501497\n",
      "Training:: Epoch 70, Iteration 120, Current loss 0.3870655627246453 Accuracy 86.07991488355597\n",
      "Training:: Epoch 70, Iteration 130, Current loss 0.2596103864802051 Accuracy 90.28876716034354\n",
      "Training:: Epoch 70, Iteration 140, Current loss 0.26195898516998795 Accuracy 91.41507932296918\n",
      "Training:: Epoch 70, Iteration 150, Current loss 0.2893795502982513 Accuracy 83.01532686893962\n",
      "Training:: Epoch 70, Iteration 160, Current loss 0.3240085611953674 Accuracy 86.22961956521739\n",
      "Training:: Epoch 70, Iteration 170, Current loss 0.1880371101526881 Accuracy 92.9016059005074\n",
      "Training:: Epoch 70, Iteration 180, Current loss 0.2678345306953494 Accuracy 90.21432050657575\n",
      "Calculating Expectation\n",
      "Epoch 70 iter 0\n",
      "Epoch 70 iter 10\n",
      "Epoch 70 iter 20\n",
      "Epoch 70 iter 30\n",
      "Epoch 70 iter 40\n",
      "Epoch 70 iter 50\n",
      "Epoch 70 iter 60\n",
      "Epoch 70 iter 70\n",
      "Epoch 70 iter 80\n",
      "Epoch 70 iter 90\n",
      "Epoch 70 iter 100\n",
      "Epoch 70 iter 110\n",
      "Epoch 70 iter 120\n",
      "Epoch 70 iter 130\n",
      "Epoch 70 iter 140\n",
      "Epoch 70 iter 150\n",
      "Epoch 70 iter 160\n",
      "Epoch 70 iter 170\n",
      "Epoch 70 iter 180\n",
      "Train Boundary avergage error = 87.951\n",
      "Train From boundary avergage accuracy = 88.450\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 68.59495629394843\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 0.1666514761866263 Accuracy 86.1836352279825\n",
      "Training:: Epoch 71, Iteration 10, Current loss 0.22299115015924628 Accuracy 78.89114017314422\n",
      "Training:: Epoch 71, Iteration 20, Current loss 0.19907681706198405 Accuracy 91.36642441111283\n",
      "Training:: Epoch 71, Iteration 30, Current loss 0.21354962104327924 Accuracy 89.1699696899044\n",
      "Training:: Epoch 71, Iteration 40, Current loss 0.1907838243021014 Accuracy 89.73668297509758\n",
      "Training:: Epoch 71, Iteration 50, Current loss 0.23801736621227065 Accuracy 90.09510138166158\n",
      "Training:: Epoch 71, Iteration 60, Current loss 0.31098964649399524 Accuracy 91.99357413069389\n",
      "Training:: Epoch 71, Iteration 70, Current loss 0.280703784791077 Accuracy 84.09507739710953\n",
      "Training:: Epoch 71, Iteration 80, Current loss 0.26628098827868363 Accuracy 84.1448730915571\n",
      "Training:: Epoch 71, Iteration 90, Current loss 0.22990319929115963 Accuracy 85.48184305894534\n",
      "Training:: Epoch 71, Iteration 100, Current loss 0.34439743730541417 Accuracy 89.69509841759938\n",
      "Training:: Epoch 71, Iteration 110, Current loss 0.3710432470348105 Accuracy 80.76630939319774\n",
      "Training:: Epoch 71, Iteration 120, Current loss 0.3313453134058878 Accuracy 91.62800080693968\n",
      "Training:: Epoch 71, Iteration 130, Current loss 0.2269589834654435 Accuracy 81.15640319656168\n",
      "Training:: Epoch 71, Iteration 140, Current loss 0.28279865469351545 Accuracy 87.14899240171788\n",
      "Training:: Epoch 71, Iteration 150, Current loss 0.2319463023845566 Accuracy 92.02069023997286\n",
      "Training:: Epoch 71, Iteration 160, Current loss 0.3027275089443673 Accuracy 91.9118230116565\n",
      "Training:: Epoch 71, Iteration 170, Current loss 0.28885204153502925 Accuracy 84.02469006643302\n",
      "Training:: Epoch 71, Iteration 180, Current loss 0.3454194124111162 Accuracy 88.23712948517941\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 71, Probability Accuracy 68.9172612193375\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 0.22273765548300145 Accuracy 89.65670692943421\n",
      "Training:: Epoch 72, Iteration 10, Current loss 0.3155133775513983 Accuracy 87.16058356263953\n",
      "Training:: Epoch 72, Iteration 20, Current loss 0.4169037285486343 Accuracy 88.14625228519196\n",
      "Training:: Epoch 72, Iteration 30, Current loss 0.43704453335665366 Accuracy 82.12701991732432\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.28244979647254154 Accuracy 91.22425691463349\n",
      "Training:: Epoch 72, Iteration 50, Current loss 0.3324367332318582 Accuracy 85.52407395721588\n",
      "Training:: Epoch 72, Iteration 60, Current loss 0.27525441020884783 Accuracy 88.65536824218576\n",
      "Training:: Epoch 72, Iteration 70, Current loss 0.38834339122225064 Accuracy 92.66144083613288\n",
      "Training:: Epoch 72, Iteration 80, Current loss 0.3550019854791774 Accuracy 87.29240206006597\n",
      "Training:: Epoch 72, Iteration 90, Current loss 0.211674628419056 Accuracy 82.49279114929676\n",
      "Training:: Epoch 72, Iteration 100, Current loss 0.2503286200706306 Accuracy 80.2143684820394\n",
      "Training:: Epoch 72, Iteration 110, Current loss 0.25441316575296485 Accuracy 94.37212863705973\n",
      "Training:: Epoch 72, Iteration 120, Current loss 0.2966024508239207 Accuracy 90.8367390836739\n",
      "Training:: Epoch 72, Iteration 130, Current loss 0.35949559608021636 Accuracy 87.71410849754282\n",
      "Training:: Epoch 72, Iteration 140, Current loss 0.35277123847486047 Accuracy 88.7417570729632\n",
      "Training:: Epoch 72, Iteration 150, Current loss 0.25795544531456516 Accuracy 91.09104978075831\n",
      "Training:: Epoch 72, Iteration 160, Current loss 0.26843052520461996 Accuracy 86.7094813216982\n",
      "Training:: Epoch 72, Iteration 170, Current loss 0.30810858239159966 Accuracy 79.06238984843144\n",
      "Training:: Epoch 72, Iteration 180, Current loss 0.30999408499763614 Accuracy 92.6057345618955\n",
      "Calculating Validation Data Accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:: Epoch 72, Probability Accuracy 68.44359762732924\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 0.33375032646125274 Accuracy 87.46468926553672\n",
      "Training:: Epoch 73, Iteration 10, Current loss 0.23858975932689472 Accuracy 90.64949006977993\n",
      "Training:: Epoch 73, Iteration 20, Current loss 0.2552930787621751 Accuracy 86.59876279863481\n",
      "Training:: Epoch 73, Iteration 30, Current loss 0.2339653236384213 Accuracy 86.5845311430527\n",
      "Training:: Epoch 73, Iteration 40, Current loss 0.26291144336439143 Accuracy 90.83477720023102\n",
      "Training:: Epoch 73, Iteration 50, Current loss 0.2497612893028463 Accuracy 90.97879008338464\n",
      "Training:: Epoch 73, Iteration 60, Current loss 0.3341715536745971 Accuracy 84.18585269298886\n",
      "Training:: Epoch 73, Iteration 70, Current loss 0.4953446388499788 Accuracy 84.72331350319901\n",
      "Training:: Epoch 73, Iteration 80, Current loss 0.29405907597279957 Accuracy 86.60991979047307\n",
      "Training:: Epoch 73, Iteration 90, Current loss 0.251649731800533 Accuracy 88.51528384279476\n",
      "Training:: Epoch 73, Iteration 100, Current loss 0.2729982789127947 Accuracy 84.81327800829875\n",
      "Training:: Epoch 73, Iteration 110, Current loss 0.2974035765545077 Accuracy 89.8328530259366\n",
      "Training:: Epoch 73, Iteration 120, Current loss 0.2648155843889393 Accuracy 92.61904761904762\n",
      "Training:: Epoch 73, Iteration 130, Current loss 0.3118089965355653 Accuracy 89.57634228187919\n",
      "Training:: Epoch 73, Iteration 140, Current loss 0.21459835898153837 Accuracy 87.92143363369445\n",
      "Training:: Epoch 73, Iteration 150, Current loss 0.2441035348524173 Accuracy 92.1664820382972\n",
      "Training:: Epoch 73, Iteration 160, Current loss 0.5508332371042705 Accuracy 86.65220000923404\n",
      "Training:: Epoch 73, Iteration 170, Current loss 0.30558925144122695 Accuracy 85.98042080654588\n",
      "Training:: Epoch 73, Iteration 180, Current loss 0.20713335457588178 Accuracy 89.0558537070165\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 73, Probability Accuracy 68.44696115325411\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 0.247356680665197 Accuracy 88.96969024092373\n",
      "Training:: Epoch 74, Iteration 10, Current loss 0.20295871316685438 Accuracy 91.89645998309807\n",
      "Training:: Epoch 74, Iteration 20, Current loss 0.3148629947300813 Accuracy 92.76615969581749\n",
      "Training:: Epoch 74, Iteration 30, Current loss 0.263296031821206 Accuracy 90.4994047956465\n",
      "Training:: Epoch 74, Iteration 40, Current loss 0.26544940982668375 Accuracy 82.27693144722525\n",
      "Training:: Epoch 74, Iteration 50, Current loss 0.29650850023716224 Accuracy 83.2055140277643\n",
      "Training:: Epoch 74, Iteration 60, Current loss 0.27295497860170975 Accuracy 81.80935875216637\n",
      "Training:: Epoch 74, Iteration 70, Current loss 0.3016924539892301 Accuracy 90.24298124134458\n",
      "Training:: Epoch 74, Iteration 80, Current loss 0.39331936691178143 Accuracy 91.19658878919783\n",
      "Training:: Epoch 74, Iteration 90, Current loss 0.3188090850787181 Accuracy 90.03449898071193\n",
      "Training:: Epoch 74, Iteration 100, Current loss 0.3763816102670417 Accuracy 87.89210547536489\n",
      "Training:: Epoch 74, Iteration 110, Current loss 0.3748175645103759 Accuracy 86.56780265221731\n",
      "Training:: Epoch 74, Iteration 120, Current loss 0.2963069149956905 Accuracy 87.49177090190915\n",
      "Training:: Epoch 74, Iteration 130, Current loss 0.2218814106719487 Accuracy 84.39934559052665\n",
      "Training:: Epoch 74, Iteration 140, Current loss 0.31945503206756565 Accuracy 88.62973760932945\n",
      "Training:: Epoch 74, Iteration 150, Current loss 0.24487342859498895 Accuracy 83.79754894158842\n",
      "Training:: Epoch 74, Iteration 160, Current loss 0.2711297482573331 Accuracy 88.75810473815461\n",
      "Training:: Epoch 74, Iteration 170, Current loss 0.2596193732320523 Accuracy 90.52592774637503\n",
      "Training:: Epoch 74, Iteration 180, Current loss 0.6086825524863997 Accuracy 88.52240676903791\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 74, Probability Accuracy 68.56547597848926\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 0.2931602396231012 Accuracy 85.00806451612904\n",
      "Training:: Epoch 75, Iteration 10, Current loss 0.23682854174538748 Accuracy 83.95821804095185\n",
      "Training:: Epoch 75, Iteration 20, Current loss 0.3127842599155138 Accuracy 80.40441460214556\n",
      "Training:: Epoch 75, Iteration 30, Current loss 0.30004158936293784 Accuracy 87.48763600395648\n",
      "Training:: Epoch 75, Iteration 40, Current loss 0.3423760197594857 Accuracy 86.62713300254146\n",
      "Training:: Epoch 75, Iteration 50, Current loss 0.2494494486338652 Accuracy 89.99645264278112\n",
      "Training:: Epoch 75, Iteration 60, Current loss 0.4417682161227911 Accuracy 85.09839724081964\n",
      "Training:: Epoch 75, Iteration 70, Current loss 0.4490078646175416 Accuracy 87.98950835671587\n",
      "Training:: Epoch 75, Iteration 80, Current loss 0.35966683131602684 Accuracy 87.88781224997777\n",
      "Training:: Epoch 75, Iteration 90, Current loss 0.378229869318533 Accuracy 85.99163408202237\n",
      "Training:: Epoch 75, Iteration 100, Current loss 0.30431958383581753 Accuracy 78.5420663195396\n",
      "Training:: Epoch 75, Iteration 110, Current loss 0.3501493534104508 Accuracy 88.44898269055572\n",
      "Training:: Epoch 75, Iteration 120, Current loss 0.2413238214004151 Accuracy 91.42276172284183\n",
      "Training:: Epoch 75, Iteration 130, Current loss 0.3534275210336654 Accuracy 90.16378849919886\n",
      "Training:: Epoch 75, Iteration 140, Current loss 0.3049041744017942 Accuracy 89.34698833342132\n",
      "Training:: Epoch 75, Iteration 150, Current loss 0.2670163325797561 Accuracy 90.74584749567113\n",
      "Training:: Epoch 75, Iteration 160, Current loss 0.3812089297970759 Accuracy 88.59844606396435\n",
      "Training:: Epoch 75, Iteration 170, Current loss 0.22004905683204806 Accuracy 82.97164667393675\n",
      "Training:: Epoch 75, Iteration 180, Current loss 0.4255386303126302 Accuracy 89.18554072296385\n",
      "Calculating Expectation\n",
      "Epoch 75 iter 0\n",
      "Epoch 75 iter 10\n",
      "Epoch 75 iter 20\n",
      "Epoch 75 iter 30\n",
      "Epoch 75 iter 40\n",
      "Epoch 75 iter 50\n",
      "Epoch 75 iter 60\n",
      "Epoch 75 iter 70\n",
      "Epoch 75 iter 80\n",
      "Epoch 75 iter 90\n",
      "Epoch 75 iter 100\n",
      "Epoch 75 iter 110\n",
      "Epoch 75 iter 120\n",
      "Epoch 75 iter 130\n",
      "Epoch 75 iter 140\n",
      "Epoch 75 iter 150\n",
      "Epoch 75 iter 160\n",
      "Epoch 75 iter 170\n",
      "Epoch 75 iter 180\n",
      "Train Boundary avergage error = 87.414\n",
      "Train From boundary avergage accuracy = 88.446\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 68.0429423333373\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 0.2056848339977338 Accuracy 90.10598648963429\n",
      "Training:: Epoch 76, Iteration 10, Current loss 0.3785826919015825 Accuracy 88.81936608882963\n",
      "Training:: Epoch 76, Iteration 20, Current loss 0.31856792413602475 Accuracy 86.86035859180735\n",
      "Training:: Epoch 76, Iteration 30, Current loss 0.3898687478196071 Accuracy 89.10977348328845\n",
      "Training:: Epoch 76, Iteration 40, Current loss 0.32616067769574547 Accuracy 92.86020813217466\n",
      "Training:: Epoch 76, Iteration 50, Current loss 0.3988206479903007 Accuracy 83.59639233370913\n",
      "Training:: Epoch 76, Iteration 60, Current loss 0.30536853048698703 Accuracy 88.60428157427316\n",
      "Training:: Epoch 76, Iteration 70, Current loss 0.3410623625198622 Accuracy 87.00390288407928\n",
      "Training:: Epoch 76, Iteration 80, Current loss 0.2737532063260457 Accuracy 85.30930930930931\n",
      "Training:: Epoch 76, Iteration 90, Current loss 0.3930824420010287 Accuracy 76.95351137487636\n",
      "Training:: Epoch 76, Iteration 100, Current loss 0.31532294234308045 Accuracy 90.06519714374419\n",
      "Training:: Epoch 76, Iteration 110, Current loss 0.22763151615649777 Accuracy 85.88619818749072\n",
      "Training:: Epoch 76, Iteration 120, Current loss 0.2697503218374833 Accuracy 88.94316418596632\n",
      "Training:: Epoch 76, Iteration 130, Current loss 0.2435056834377362 Accuracy 93.76612209802235\n",
      "Training:: Epoch 76, Iteration 140, Current loss 0.36273556907487337 Accuracy 91.2631998406057\n",
      "Training:: Epoch 76, Iteration 150, Current loss 0.34795460321704486 Accuracy 91.24873401102818\n",
      "Training:: Epoch 76, Iteration 160, Current loss 0.2765522111536715 Accuracy 85.67423806785509\n",
      "Training:: Epoch 76, Iteration 170, Current loss 0.35135646028627154 Accuracy 87.48285134171103\n",
      "Training:: Epoch 76, Iteration 180, Current loss 0.4157311665472314 Accuracy 87.15237761186552\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 76, Probability Accuracy 68.55004333012809\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 0.35793265607153124 Accuracy 89.98018437149996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 77, Iteration 10, Current loss 0.2848568279125818 Accuracy 92.00833912439194\n",
      "Training:: Epoch 77, Iteration 20, Current loss 0.2440099448841129 Accuracy 91.60742153632738\n",
      "Training:: Epoch 77, Iteration 30, Current loss 0.30442674489165433 Accuracy 89.00119134545174\n",
      "Training:: Epoch 77, Iteration 40, Current loss 0.28991109495104966 Accuracy 89.49291914116034\n",
      "Training:: Epoch 77, Iteration 50, Current loss 0.502266382981954 Accuracy 88.27668326477593\n",
      "Training:: Epoch 77, Iteration 60, Current loss 0.2742633201476747 Accuracy 80.7991192850667\n",
      "Training:: Epoch 77, Iteration 70, Current loss 0.27715837565427337 Accuracy 89.06542932133483\n",
      "Training:: Epoch 77, Iteration 80, Current loss 0.32537599620986324 Accuracy 85.21859314759584\n",
      "Training:: Epoch 77, Iteration 90, Current loss 0.2849555644020786 Accuracy 87.44769874476988\n",
      "Training:: Epoch 77, Iteration 100, Current loss 0.22472298304816646 Accuracy 85.43556131392424\n",
      "Training:: Epoch 77, Iteration 110, Current loss 0.2450701890322732 Accuracy 91.89628615437276\n",
      "Training:: Epoch 77, Iteration 120, Current loss 0.8572284860840684 Accuracy 88.38082795786724\n",
      "Training:: Epoch 77, Iteration 130, Current loss 0.29102034323244946 Accuracy 91.37198872881692\n",
      "Training:: Epoch 77, Iteration 140, Current loss 0.5953307093795496 Accuracy 82.45299356754082\n",
      "Training:: Epoch 77, Iteration 150, Current loss 0.3640574562638834 Accuracy 88.84184777495531\n",
      "Training:: Epoch 77, Iteration 160, Current loss 0.5104811710066274 Accuracy 86.29618320610687\n",
      "Training:: Epoch 77, Iteration 170, Current loss 0.35424275582725706 Accuracy 89.17475974983475\n",
      "Training:: Epoch 77, Iteration 180, Current loss 0.37382789294311275 Accuracy 87.35870579695116\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 77, Probability Accuracy 68.73345442026663\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 0.268654377544291 Accuracy 90.15951595159515\n",
      "Training:: Epoch 78, Iteration 10, Current loss 0.4143945168826969 Accuracy 86.07025761124122\n",
      "Training:: Epoch 78, Iteration 20, Current loss 0.34207896892235695 Accuracy 90.59107869742198\n",
      "Training:: Epoch 78, Iteration 30, Current loss 0.31932476124643294 Accuracy 85.1645463147934\n",
      "Training:: Epoch 78, Iteration 40, Current loss 0.3332802898839758 Accuracy 87.93016194331983\n",
      "Training:: Epoch 78, Iteration 50, Current loss 0.4493265983376585 Accuracy 85.81493317838466\n",
      "Training:: Epoch 78, Iteration 60, Current loss 0.32951935055713 Accuracy 88.88467086781566\n",
      "Training:: Epoch 78, Iteration 70, Current loss 0.7746058050531137 Accuracy 87.21864951768488\n",
      "Training:: Epoch 78, Iteration 80, Current loss 0.43560327329608717 Accuracy 91.95070791819612\n",
      "Training:: Epoch 78, Iteration 90, Current loss 0.3958640837291659 Accuracy 88.1071242849714\n",
      "Training:: Epoch 78, Iteration 100, Current loss 0.3039215341983825 Accuracy 91.14031158230053\n",
      "Training:: Epoch 78, Iteration 110, Current loss 0.48403950796176676 Accuracy 89.87771132732304\n",
      "Training:: Epoch 78, Iteration 120, Current loss 0.42945603485869305 Accuracy 85.03557118325293\n",
      "Training:: Epoch 78, Iteration 130, Current loss 0.21795058041743448 Accuracy 84.14028553693358\n",
      "Training:: Epoch 78, Iteration 140, Current loss 0.28573064202937815 Accuracy 90.74987490170848\n",
      "Training:: Epoch 78, Iteration 150, Current loss 0.5371596970097275 Accuracy 88.09205959310087\n",
      "Training:: Epoch 78, Iteration 160, Current loss 0.8201184582425395 Accuracy 86.76290677758747\n",
      "Training:: Epoch 78, Iteration 170, Current loss 0.8856109595409436 Accuracy 84.53243032039593\n",
      "Training:: Epoch 78, Iteration 180, Current loss 0.5013762002518105 Accuracy 81.72447520878791\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 78, Probability Accuracy 65.27258409804085\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 1.5282827771850795 Accuracy 86.1299233648191\n",
      "Training:: Epoch 79, Iteration 10, Current loss 0.8626616249471553 Accuracy 86.7059753206492\n",
      "Training:: Epoch 79, Iteration 20, Current loss 0.7934729082564758 Accuracy 84.4703497212367\n",
      "Training:: Epoch 79, Iteration 30, Current loss 0.5187163353292505 Accuracy 88.5274913040825\n",
      "Training:: Epoch 79, Iteration 40, Current loss 0.5764372241606031 Accuracy 87.7301052933171\n",
      "Training:: Epoch 79, Iteration 50, Current loss 0.5410202996223406 Accuracy 90.04983824429483\n",
      "Training:: Epoch 79, Iteration 60, Current loss 0.4492246018658534 Accuracy 91.26331593622649\n",
      "Training:: Epoch 79, Iteration 70, Current loss 0.31019499258654293 Accuracy 92.06907329270199\n",
      "Training:: Epoch 79, Iteration 80, Current loss 0.44139376563076005 Accuracy 90.91382575757575\n",
      "Training:: Epoch 79, Iteration 90, Current loss 0.3657888835547805 Accuracy 83.52158706496188\n",
      "Training:: Epoch 79, Iteration 100, Current loss 0.30928276140013866 Accuracy 90.68489703476911\n",
      "Training:: Epoch 79, Iteration 110, Current loss 0.3994626414480145 Accuracy 90.28449777433714\n",
      "Training:: Epoch 79, Iteration 120, Current loss 0.5262902324475759 Accuracy 87.77112043139604\n",
      "Training:: Epoch 79, Iteration 130, Current loss 2.4201973652837774 Accuracy 80.17259824548891\n",
      "Training:: Epoch 79, Iteration 140, Current loss 1.6748987035362446 Accuracy 77.17050041647978\n",
      "Training:: Epoch 79, Iteration 150, Current loss 4.992405945406628 Accuracy 55.04285224545766\n",
      "Training:: Epoch 79, Iteration 160, Current loss 5.444034211978799 Accuracy 57.396130424728284\n",
      "Training:: Epoch 79, Iteration 170, Current loss 3.5943869171836034 Accuracy 69.72444232375045\n",
      "Training:: Epoch 79, Iteration 180, Current loss 1.8682832668710752 Accuracy 84.04262877442274\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 79, Probability Accuracy 61.56795707349502\n",
      "Starting Training\n",
      "Training:: Epoch 80, Iteration 0, Current loss 2.8006780690996744 Accuracy 78.80716134598792\n",
      "Training:: Epoch 80, Iteration 10, Current loss 1.3724225643867622 Accuracy 84.65088282504013\n",
      "Training:: Epoch 80, Iteration 20, Current loss 2.657224891832552 Accuracy 80.56537102473499\n",
      "Training:: Epoch 80, Iteration 30, Current loss 2.6489790335058343 Accuracy 81.31984294774992\n",
      "Training:: Epoch 80, Iteration 40, Current loss 2.283596534184862 Accuracy 74.91600890507995\n",
      "Training:: Epoch 80, Iteration 50, Current loss 1.2696402180777255 Accuracy 82.7041908840624\n",
      "Training:: Epoch 80, Iteration 60, Current loss 1.1022610487540678 Accuracy 90.4635761589404\n",
      "Training:: Epoch 80, Iteration 70, Current loss 1.0442099990728313 Accuracy 84.91832491832491\n",
      "Training:: Epoch 80, Iteration 80, Current loss 0.7308379709969459 Accuracy 87.29227116855711\n",
      "Training:: Epoch 80, Iteration 90, Current loss 0.7449983450573956 Accuracy 87.62719495722648\n",
      "Training:: Epoch 80, Iteration 100, Current loss 0.7638214786407475 Accuracy 89.68292513702907\n",
      "Training:: Epoch 80, Iteration 110, Current loss 0.6715819340619692 Accuracy 92.2781596383677\n",
      "Training:: Epoch 80, Iteration 120, Current loss 0.8862956592933112 Accuracy 86.00625918119691\n",
      "Training:: Epoch 80, Iteration 130, Current loss 0.7240052485613748 Accuracy 81.87400318979266\n",
      "Training:: Epoch 80, Iteration 140, Current loss 0.7586831255353651 Accuracy 89.08261365488023\n",
      "Training:: Epoch 80, Iteration 150, Current loss 0.8460652784638707 Accuracy 87.47994805591628\n",
      "Training:: Epoch 80, Iteration 160, Current loss 0.7773735216288484 Accuracy 88.24479250766996\n",
      "Training:: Epoch 80, Iteration 170, Current loss 0.5256009081166906 Accuracy 88.58249346771183\n",
      "Training:: Epoch 80, Iteration 180, Current loss 0.49007918172825893 Accuracy 89.42510677991065\n",
      "Calculating Expectation\n",
      "Epoch 80 iter 0\n",
      "Epoch 80 iter 10\n",
      "Epoch 80 iter 20\n",
      "Epoch 80 iter 30\n",
      "Epoch 80 iter 40\n",
      "Epoch 80 iter 50\n",
      "Epoch 80 iter 60\n",
      "Epoch 80 iter 70\n",
      "Epoch 80 iter 80\n",
      "Epoch 80 iter 90\n",
      "Epoch 80 iter 100\n",
      "Epoch 80 iter 110\n",
      "Epoch 80 iter 120\n",
      "Epoch 80 iter 130\n",
      "Epoch 80 iter 140\n",
      "Epoch 80 iter 150\n",
      "Epoch 80 iter 160\n",
      "Epoch 80 iter 170\n",
      "Epoch 80 iter 180\n",
      "Train Boundary avergage error = 87.923\n",
      "Train From boundary avergage accuracy = 88.283\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 80, Probability Accuracy 68.88065814309627\n",
      "Starting Training\n",
      "Training:: Epoch 81, Iteration 0, Current loss 0.3651211247055898 Accuracy 90.67173901714524\n",
      "Training:: Epoch 81, Iteration 10, Current loss 0.4276132316990126 Accuracy 90.42628522872114\n",
      "Training:: Epoch 81, Iteration 20, Current loss 0.4011424589096567 Accuracy 89.56773315180395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 81, Iteration 30, Current loss 0.33532072518053335 Accuracy 90.52961127054526\n",
      "Training:: Epoch 81, Iteration 40, Current loss 0.6324839188682932 Accuracy 79.16666666666667\n",
      "Training:: Epoch 81, Iteration 50, Current loss 0.356962048645716 Accuracy 88.82629107981221\n",
      "Training:: Epoch 81, Iteration 60, Current loss 0.5529402252015286 Accuracy 81.1490359562272\n",
      "Training:: Epoch 81, Iteration 70, Current loss 0.39677307172863946 Accuracy 82.10037844656695\n",
      "Training:: Epoch 81, Iteration 80, Current loss 0.330915121016031 Accuracy 87.76383342840845\n",
      "Training:: Epoch 81, Iteration 90, Current loss 0.4843992242081982 Accuracy 88.51067553377669\n",
      "Training:: Epoch 81, Iteration 100, Current loss 0.32337514423822844 Accuracy 92.52846424261504\n",
      "Training:: Epoch 81, Iteration 110, Current loss 0.4222734671016845 Accuracy 86.01883986117997\n",
      "Training:: Epoch 81, Iteration 120, Current loss 0.47003269079644683 Accuracy 88.73781426372499\n",
      "Training:: Epoch 81, Iteration 130, Current loss 0.3229017161592576 Accuracy 88.58887477057743\n",
      "Training:: Epoch 81, Iteration 140, Current loss 0.4436661260339829 Accuracy 86.10927517074245\n",
      "Training:: Epoch 81, Iteration 150, Current loss 0.4216031009554682 Accuracy 92.27799227799228\n",
      "Training:: Epoch 81, Iteration 160, Current loss 0.3067913059784019 Accuracy 92.83884847863948\n",
      "Training:: Epoch 81, Iteration 170, Current loss 0.36428156949067203 Accuracy 87.69181127204934\n",
      "Training:: Epoch 81, Iteration 180, Current loss 0.4830594039095158 Accuracy 86.3917525773196\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 81, Probability Accuracy 66.4231078188128\n",
      "Starting Training\n",
      "Training:: Epoch 82, Iteration 0, Current loss 0.29726278716609267 Accuracy 91.72291078659958\n",
      "Training:: Epoch 82, Iteration 10, Current loss 0.39886749542552064 Accuracy 91.12019963953972\n",
      "Training:: Epoch 82, Iteration 20, Current loss 0.4592439845327771 Accuracy 86.40229430379746\n",
      "Training:: Epoch 82, Iteration 30, Current loss 0.36794599603230277 Accuracy 86.49742930591259\n",
      "Training:: Epoch 82, Iteration 40, Current loss 0.2927585972598852 Accuracy 92.99333641221878\n",
      "Training:: Epoch 82, Iteration 50, Current loss 0.4077692522184016 Accuracy 87.98626019750967\n",
      "Training:: Epoch 82, Iteration 60, Current loss 0.3194209681561726 Accuracy 91.38420534214033\n",
      "Training:: Epoch 82, Iteration 70, Current loss 0.28361135288465145 Accuracy 90.41112651929195\n",
      "Training:: Epoch 82, Iteration 80, Current loss 0.2791697178135701 Accuracy 87.42372129146788\n",
      "Training:: Epoch 82, Iteration 90, Current loss 0.3513937447414349 Accuracy 84.52705957925164\n",
      "Training:: Epoch 82, Iteration 100, Current loss 0.28688188332938436 Accuracy 88.03846367628259\n",
      "Training:: Epoch 82, Iteration 110, Current loss 0.27031716139624445 Accuracy 90.62865415669025\n",
      "Training:: Epoch 82, Iteration 120, Current loss 0.3300503079855277 Accuracy 84.12479134288839\n",
      "Training:: Epoch 82, Iteration 130, Current loss 0.2061648768204681 Accuracy 90.66945270433175\n",
      "Training:: Epoch 82, Iteration 140, Current loss 0.27809249274976255 Accuracy 88.61026129695571\n",
      "Training:: Epoch 82, Iteration 150, Current loss 0.270561581087789 Accuracy 90.37823011404649\n",
      "Training:: Epoch 82, Iteration 160, Current loss 0.3027082458522786 Accuracy 88.45834945553037\n",
      "Training:: Epoch 82, Iteration 170, Current loss 0.3326451699805122 Accuracy 87.55968836391052\n",
      "Training:: Epoch 82, Iteration 180, Current loss 0.21829777278673587 Accuracy 90.61724738030881\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 82, Probability Accuracy 68.40145462603527\n",
      "Starting Training\n",
      "Training:: Epoch 83, Iteration 0, Current loss 0.34702696555113116 Accuracy 87.98242489015557\n",
      "Training:: Epoch 83, Iteration 10, Current loss 0.21380474146957498 Accuracy 88.1610797330008\n",
      "Training:: Epoch 83, Iteration 20, Current loss 0.37407937590241847 Accuracy 83.67830961621388\n",
      "Training:: Epoch 83, Iteration 30, Current loss 0.31772499420205624 Accuracy 87.75769108785283\n",
      "Training:: Epoch 83, Iteration 40, Current loss 0.5538206483762765 Accuracy 87.05608621111928\n",
      "Training:: Epoch 83, Iteration 50, Current loss 0.2928516396766324 Accuracy 86.43955461293743\n",
      "Training:: Epoch 83, Iteration 60, Current loss 0.3146134498031889 Accuracy 77.40106177606178\n",
      "Training:: Epoch 83, Iteration 70, Current loss 0.2920406806778787 Accuracy 83.44624887914715\n",
      "Training:: Epoch 83, Iteration 80, Current loss 0.2721581207264853 Accuracy 91.34576408593433\n",
      "Training:: Epoch 83, Iteration 90, Current loss 0.2532307548353735 Accuracy 84.49698862368949\n",
      "Training:: Epoch 83, Iteration 100, Current loss 0.25006730247011977 Accuracy 89.69783605264455\n",
      "Training:: Epoch 83, Iteration 110, Current loss 0.2220286292625717 Accuracy 86.98241768713966\n",
      "Training:: Epoch 83, Iteration 120, Current loss 0.2641062508794507 Accuracy 87.5523942588594\n",
      "Training:: Epoch 83, Iteration 130, Current loss 0.2454575638793668 Accuracy 91.80869647325433\n",
      "Training:: Epoch 83, Iteration 140, Current loss 0.19568938967561278 Accuracy 90.9950248756219\n",
      "Training:: Epoch 83, Iteration 150, Current loss 0.19585484081648255 Accuracy 93.94373580290058\n",
      "Training:: Epoch 83, Iteration 160, Current loss 0.2610632051378493 Accuracy 89.03925180671673\n",
      "Training:: Epoch 83, Iteration 170, Current loss 0.2425818149511749 Accuracy 92.28690302620817\n",
      "Training:: Epoch 83, Iteration 180, Current loss 0.27538823403659224 Accuracy 84.77899634430044\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 83, Probability Accuracy 68.49701833319483\n",
      "Starting Training\n",
      "Training:: Epoch 84, Iteration 0, Current loss 0.36372089156142823 Accuracy 85.53690344062153\n",
      "Training:: Epoch 84, Iteration 10, Current loss 0.2931156006104265 Accuracy 84.31799458632459\n",
      "Training:: Epoch 84, Iteration 20, Current loss 0.26152496510780715 Accuracy 90.88634515543879\n",
      "Training:: Epoch 84, Iteration 30, Current loss 0.25615521457854834 Accuracy 86.57976037977545\n",
      "Training:: Epoch 84, Iteration 40, Current loss 0.20380784374184235 Accuracy 84.20060958714325\n",
      "Training:: Epoch 84, Iteration 50, Current loss 0.30108875096426413 Accuracy 89.20225624496373\n",
      "Training:: Epoch 84, Iteration 60, Current loss 0.2662818497694783 Accuracy 88.95307374126372\n",
      "Training:: Epoch 84, Iteration 70, Current loss 0.251742715759959 Accuracy 89.36089692038412\n",
      "Training:: Epoch 84, Iteration 80, Current loss 0.23777969381502664 Accuracy 89.14044152184124\n",
      "Training:: Epoch 84, Iteration 90, Current loss 0.22390184564014126 Accuracy 87.48953974895397\n",
      "Training:: Epoch 84, Iteration 100, Current loss 0.22707793962482625 Accuracy 91.75870858113849\n",
      "Training:: Epoch 84, Iteration 110, Current loss 0.19545913229374948 Accuracy 84.8454066408482\n",
      "Training:: Epoch 84, Iteration 120, Current loss 0.3363627717816681 Accuracy 88.51080284209\n",
      "Training:: Epoch 84, Iteration 130, Current loss 0.26253862652509463 Accuracy 92.05534478910957\n",
      "Training:: Epoch 84, Iteration 140, Current loss 0.1991441529494788 Accuracy 93.25573433755564\n",
      "Training:: Epoch 84, Iteration 150, Current loss 0.4714194216256956 Accuracy 84.03138793526239\n",
      "Training:: Epoch 84, Iteration 160, Current loss 0.19865377035441695 Accuracy 90.51409618573798\n",
      "Training:: Epoch 84, Iteration 170, Current loss 0.2971617942489461 Accuracy 91.41706078122263\n",
      "Training:: Epoch 84, Iteration 180, Current loss 0.21506985747965782 Accuracy 89.76756979782698\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 84, Probability Accuracy 68.4394426835397\n",
      "Starting Training\n",
      "Training:: Epoch 85, Iteration 0, Current loss 0.23112191332285043 Accuracy 88.58072247902756\n",
      "Training:: Epoch 85, Iteration 10, Current loss 0.17126322060218208 Accuracy 93.04652326163081\n",
      "Training:: Epoch 85, Iteration 20, Current loss 0.21428784290960803 Accuracy 75.54064131245339\n",
      "Training:: Epoch 85, Iteration 30, Current loss 0.2808372531913443 Accuracy 91.90155959364716\n",
      "Training:: Epoch 85, Iteration 40, Current loss 0.2824906563785863 Accuracy 88.96103896103897\n",
      "Training:: Epoch 85, Iteration 50, Current loss 0.23132368904641984 Accuracy 89.60223307745987\n",
      "Training:: Epoch 85, Iteration 60, Current loss 0.3586558445593446 Accuracy 85.65260710727333\n",
      "Training:: Epoch 85, Iteration 70, Current loss 0.28080965629904464 Accuracy 84.1529367375173\n",
      "Training:: Epoch 85, Iteration 80, Current loss 0.2020679602680897 Accuracy 90.38366945400885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 85, Iteration 90, Current loss 0.2207269226369007 Accuracy 89.39687606886329\n",
      "Training:: Epoch 85, Iteration 100, Current loss 0.322969971453406 Accuracy 86.94623373208887\n",
      "Training:: Epoch 85, Iteration 110, Current loss 0.20773536286535166 Accuracy 90.28901734104046\n",
      "Training:: Epoch 85, Iteration 120, Current loss 0.337326681896871 Accuracy 82.08900331004045\n",
      "Training:: Epoch 85, Iteration 130, Current loss 0.35966883239319286 Accuracy 85.83136637817991\n",
      "Training:: Epoch 85, Iteration 140, Current loss 0.26404741819120037 Accuracy 88.3271375464684\n",
      "Training:: Epoch 85, Iteration 150, Current loss 0.2638936671319938 Accuracy 91.66525207944322\n",
      "Training:: Epoch 85, Iteration 160, Current loss 0.21824962204370246 Accuracy 88.05271585170587\n",
      "Training:: Epoch 85, Iteration 170, Current loss 0.27368075830247557 Accuracy 87.72404830237721\n",
      "Training:: Epoch 85, Iteration 180, Current loss 0.19582458665252428 Accuracy 87.98445541434724\n",
      "Calculating Expectation\n",
      "Epoch 85 iter 0\n",
      "Epoch 85 iter 10\n",
      "Epoch 85 iter 20\n",
      "Epoch 85 iter 30\n",
      "Epoch 85 iter 40\n",
      "Epoch 85 iter 50\n",
      "Epoch 85 iter 60\n",
      "Epoch 85 iter 70\n",
      "Epoch 85 iter 80\n",
      "Epoch 85 iter 90\n",
      "Epoch 85 iter 100\n",
      "Epoch 85 iter 110\n",
      "Epoch 85 iter 120\n",
      "Epoch 85 iter 130\n",
      "Epoch 85 iter 140\n",
      "Epoch 85 iter 150\n",
      "Epoch 85 iter 160\n",
      "Epoch 85 iter 170\n",
      "Epoch 85 iter 180\n",
      "Train Boundary avergage error = 88.009\n",
      "Train From boundary avergage accuracy = 88.245\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 85, Probability Accuracy 68.3529802818239\n",
      "Starting Training\n",
      "Training:: Epoch 86, Iteration 0, Current loss 0.19366721160062844 Accuracy 89.36170212765957\n",
      "Training:: Epoch 86, Iteration 10, Current loss 0.19176875850326996 Accuracy 87.73135165451487\n",
      "Training:: Epoch 86, Iteration 20, Current loss 0.2003187945678349 Accuracy 87.31631863882444\n",
      "Training:: Epoch 86, Iteration 30, Current loss 0.21776629735747485 Accuracy 87.88794153471376\n",
      "Training:: Epoch 86, Iteration 40, Current loss 0.30194604871355646 Accuracy 87.1807711567351\n",
      "Training:: Epoch 86, Iteration 50, Current loss 0.22721553842896888 Accuracy 87.83478260869565\n",
      "Training:: Epoch 86, Iteration 60, Current loss 0.2655921562420545 Accuracy 90.87859280079789\n",
      "Training:: Epoch 86, Iteration 70, Current loss 0.27877476627783954 Accuracy 86.86553873552982\n",
      "Training:: Epoch 86, Iteration 80, Current loss 0.19770726115462625 Accuracy 79.80445799883393\n",
      "Training:: Epoch 86, Iteration 90, Current loss 0.2650043854424071 Accuracy 80.71389940508382\n",
      "Training:: Epoch 86, Iteration 100, Current loss 0.27033159216689995 Accuracy 83.39064968863278\n",
      "Training:: Epoch 86, Iteration 110, Current loss 0.2594881127237093 Accuracy 88.12838927158141\n",
      "Training:: Epoch 86, Iteration 120, Current loss 0.2073431575092352 Accuracy 89.56585570469798\n",
      "Training:: Epoch 86, Iteration 130, Current loss 0.1964438579770106 Accuracy 92.65068849051701\n",
      "Training:: Epoch 86, Iteration 140, Current loss 0.16535647594579791 Accuracy 92.08781909642236\n",
      "Training:: Epoch 86, Iteration 150, Current loss 0.28349468558707325 Accuracy 86.62933366891085\n",
      "Training:: Epoch 86, Iteration 160, Current loss 0.22076116193921483 Accuracy 92.56736146415862\n",
      "Training:: Epoch 86, Iteration 170, Current loss 0.24146065886049065 Accuracy 91.55953772237372\n",
      "Training:: Epoch 86, Iteration 180, Current loss 0.25773096408202806 Accuracy 91.30598015553768\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 86, Probability Accuracy 68.26256079078473\n",
      "Starting Training\n",
      "Training:: Epoch 87, Iteration 0, Current loss 0.19829195081864595 Accuracy 88.90446288405472\n",
      "Training:: Epoch 87, Iteration 10, Current loss 0.251708275458153 Accuracy 89.35144053813936\n",
      "Training:: Epoch 87, Iteration 20, Current loss 0.16201332375266628 Accuracy 88.8462843569732\n",
      "Training:: Epoch 87, Iteration 30, Current loss 0.20327704362334997 Accuracy 90.00189597421475\n",
      "Training:: Epoch 87, Iteration 40, Current loss 0.23994785574387387 Accuracy 86.38593361788877\n",
      "Training:: Epoch 87, Iteration 50, Current loss 0.16743802764677498 Accuracy 92.6878374668253\n",
      "Training:: Epoch 87, Iteration 60, Current loss 0.13025635452025372 Accuracy 85.47108208955224\n",
      "Training:: Epoch 87, Iteration 70, Current loss 0.20052291966229338 Accuracy 87.55855893205091\n",
      "Training:: Epoch 87, Iteration 80, Current loss 0.2163768213296086 Accuracy 89.49416342412451\n",
      "Training:: Epoch 87, Iteration 90, Current loss 0.23571946387467718 Accuracy 83.13445314281371\n",
      "Training:: Epoch 87, Iteration 100, Current loss 0.32260661654075046 Accuracy 88.45257086759112\n",
      "Training:: Epoch 87, Iteration 110, Current loss 0.2718678117367742 Accuracy 85.50155614077089\n",
      "Training:: Epoch 87, Iteration 120, Current loss 0.3238949992827474 Accuracy 86.16002471424159\n",
      "Training:: Epoch 87, Iteration 130, Current loss 0.26174324744088723 Accuracy 81.12712975098296\n",
      "Training:: Epoch 87, Iteration 140, Current loss 0.21021637743527125 Accuracy 88.67579908675799\n",
      "Training:: Epoch 87, Iteration 150, Current loss 0.360101124673645 Accuracy 85.73139435414885\n",
      "Training:: Epoch 87, Iteration 160, Current loss 0.2966478813543986 Accuracy 90.01219087425984\n",
      "Training:: Epoch 87, Iteration 170, Current loss 0.19919703089867086 Accuracy 83.31489384120962\n",
      "Training:: Epoch 87, Iteration 180, Current loss 0.2634863124440459 Accuracy 91.40292323627575\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 87, Probability Accuracy 67.11361990574213\n",
      "Starting Training\n",
      "Training:: Epoch 88, Iteration 0, Current loss 0.22655008236491292 Accuracy 91.63464986424876\n",
      "Training:: Epoch 88, Iteration 10, Current loss 0.2405988818815336 Accuracy 88.88007254590796\n",
      "Training:: Epoch 88, Iteration 20, Current loss 0.19925646994425297 Accuracy 89.76544684547726\n",
      "Training:: Epoch 88, Iteration 30, Current loss 0.1866402810086275 Accuracy 91.16205810290515\n",
      "Training:: Epoch 88, Iteration 40, Current loss 0.19668112495101672 Accuracy 81.97101913377632\n",
      "Training:: Epoch 88, Iteration 50, Current loss 0.2291585612538289 Accuracy 88.51013811342932\n",
      "Training:: Epoch 88, Iteration 60, Current loss 0.21216009994886123 Accuracy 93.00258397932816\n",
      "Training:: Epoch 88, Iteration 70, Current loss 0.25446237371083574 Accuracy 84.43258626823194\n",
      "Training:: Epoch 88, Iteration 80, Current loss 0.17793613393773877 Accuracy 85.93578183417638\n",
      "Training:: Epoch 88, Iteration 90, Current loss 0.5850349354662505 Accuracy 89.79428799680447\n",
      "Training:: Epoch 88, Iteration 100, Current loss 0.21852344483198366 Accuracy 93.05084745762711\n",
      "Training:: Epoch 88, Iteration 110, Current loss 0.20078239794536257 Accuracy 89.45540206020921\n",
      "Training:: Epoch 88, Iteration 120, Current loss 0.1744964570060504 Accuracy 89.26395160229714\n",
      "Training:: Epoch 88, Iteration 130, Current loss 0.19689442257487416 Accuracy 91.12306385732195\n",
      "Training:: Epoch 88, Iteration 140, Current loss 0.2781030586677046 Accuracy 83.05020762551906\n",
      "Training:: Epoch 88, Iteration 150, Current loss 0.28241332992431767 Accuracy 80.72727272727273\n",
      "Training:: Epoch 88, Iteration 160, Current loss 0.1746914455386227 Accuracy 91.33815304869731\n",
      "Training:: Epoch 88, Iteration 170, Current loss 0.23022787277958814 Accuracy 88.4015692586259\n",
      "Training:: Epoch 88, Iteration 180, Current loss 0.2146927427776424 Accuracy 89.39901254185347\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 88, Probability Accuracy 68.61632457629466\n",
      "Starting Training\n",
      "Training:: Epoch 89, Iteration 0, Current loss 0.23924297347355944 Accuracy 87.12161288426498\n",
      "Training:: Epoch 89, Iteration 10, Current loss 0.21821453636841903 Accuracy 90.02490315439955\n",
      "Training:: Epoch 89, Iteration 20, Current loss 0.2130923549064308 Accuracy 89.07031142503325\n",
      "Training:: Epoch 89, Iteration 30, Current loss 0.2814311308941607 Accuracy 89.44289892289036\n",
      "Training:: Epoch 89, Iteration 40, Current loss 0.2476705648095757 Accuracy 91.58494729100269\n",
      "Training:: Epoch 89, Iteration 50, Current loss 0.2967672723107003 Accuracy 87.63482012386055\n",
      "Training:: Epoch 89, Iteration 60, Current loss 0.29404774898611796 Accuracy 90.91890918909189\n",
      "Training:: Epoch 89, Iteration 70, Current loss 0.18419275802818583 Accuracy 91.99264341798118\n",
      "Training:: Epoch 89, Iteration 80, Current loss 0.19236900283577432 Accuracy 93.32329014763484\n",
      "Training:: Epoch 89, Iteration 90, Current loss 0.2215878735843872 Accuracy 94.14418648369208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 89, Iteration 100, Current loss 0.26246849053994536 Accuracy 83.30153742565369\n",
      "Training:: Epoch 89, Iteration 110, Current loss 0.2803248356674421 Accuracy 89.18825561312607\n",
      "Training:: Epoch 89, Iteration 120, Current loss 0.2141897570260779 Accuracy 93.42998955067921\n",
      "Training:: Epoch 89, Iteration 130, Current loss 0.1698717150853087 Accuracy 91.09600490332646\n",
      "Training:: Epoch 89, Iteration 140, Current loss 0.266090544223852 Accuracy 80.10195009187363\n",
      "Training:: Epoch 89, Iteration 150, Current loss 0.16258954909130052 Accuracy 88.98895990703079\n",
      "Training:: Epoch 89, Iteration 160, Current loss 0.23869105514174302 Accuracy 90.52222338972365\n",
      "Training:: Epoch 89, Iteration 170, Current loss 0.15558147336357225 Accuracy 90.77932807049753\n",
      "Training:: Epoch 89, Iteration 180, Current loss 0.21520816084140387 Accuracy 87.93008532892927\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 89, Probability Accuracy 67.7499198689412\n",
      "Starting Training\n",
      "Training:: Epoch 90, Iteration 0, Current loss 0.2814612123264622 Accuracy 86.1799124129514\n",
      "Training:: Epoch 90, Iteration 10, Current loss 0.22687293796866626 Accuracy 88.22509605472777\n",
      "Training:: Epoch 90, Iteration 20, Current loss 0.7089498004471364 Accuracy 84.79151602795855\n",
      "Training:: Epoch 90, Iteration 30, Current loss 0.2503168094874789 Accuracy 89.82020095187731\n",
      "Training:: Epoch 90, Iteration 40, Current loss 0.3406202122792994 Accuracy 87.72665764546684\n",
      "Training:: Epoch 90, Iteration 50, Current loss 0.2750206759248659 Accuracy 87.6863354037267\n",
      "Training:: Epoch 90, Iteration 60, Current loss 0.19704782035987425 Accuracy 91.34263172724711\n",
      "Training:: Epoch 90, Iteration 70, Current loss 0.14337721446487345 Accuracy 90.68753891310149\n",
      "Training:: Epoch 90, Iteration 80, Current loss 0.2778064260056764 Accuracy 81.67446362316502\n",
      "Training:: Epoch 90, Iteration 90, Current loss 0.2554030460238741 Accuracy 85.94215065252233\n",
      "Training:: Epoch 90, Iteration 100, Current loss 0.20262648439939446 Accuracy 93.9812837339339\n",
      "Training:: Epoch 90, Iteration 110, Current loss 0.2847141149175816 Accuracy 87.66557297362985\n",
      "Training:: Epoch 90, Iteration 120, Current loss 0.20838260962008562 Accuracy 88.61243081745604\n",
      "Training:: Epoch 90, Iteration 130, Current loss 0.2716821923058859 Accuracy 89.95707519242156\n",
      "Training:: Epoch 90, Iteration 140, Current loss 0.18800199644846033 Accuracy 92.54826081087162\n",
      "Training:: Epoch 90, Iteration 150, Current loss 0.14431129964928 Accuracy 91.50006619886138\n",
      "Training:: Epoch 90, Iteration 160, Current loss 0.17945248350459123 Accuracy 93.8167891778368\n",
      "Training:: Epoch 90, Iteration 170, Current loss 0.2220527102423872 Accuracy 78.60669226549643\n",
      "Training:: Epoch 90, Iteration 180, Current loss 0.1778599260170942 Accuracy 89.39650115592974\n",
      "Calculating Expectation\n",
      "Epoch 90 iter 0\n",
      "Epoch 90 iter 10\n",
      "Epoch 90 iter 20\n",
      "Epoch 90 iter 30\n",
      "Epoch 90 iter 40\n",
      "Epoch 90 iter 50\n",
      "Epoch 90 iter 60\n",
      "Epoch 90 iter 70\n",
      "Epoch 90 iter 80\n",
      "Epoch 90 iter 90\n",
      "Epoch 90 iter 100\n",
      "Epoch 90 iter 110\n",
      "Epoch 90 iter 120\n",
      "Epoch 90 iter 130\n",
      "Epoch 90 iter 140\n",
      "Epoch 90 iter 150\n",
      "Epoch 90 iter 160\n",
      "Epoch 90 iter 170\n",
      "Epoch 90 iter 180\n",
      "Train Boundary avergage error = 88.067\n",
      "Train From boundary avergage accuracy = 88.180\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 90, Probability Accuracy 68.05560501917209\n",
      "Starting Training\n",
      "Training:: Epoch 91, Iteration 0, Current loss 0.1862733305850609 Accuracy 89.27887127678105\n",
      "Training:: Epoch 91, Iteration 10, Current loss 0.2598264918361732 Accuracy 84.95707726093032\n",
      "Training:: Epoch 91, Iteration 20, Current loss 0.25691397558095935 Accuracy 84.94729526236534\n",
      "Training:: Epoch 91, Iteration 30, Current loss 0.28516354420314427 Accuracy 88.45971339090823\n",
      "Training:: Epoch 91, Iteration 40, Current loss 0.21297437044593504 Accuracy 86.32924335378323\n",
      "Training:: Epoch 91, Iteration 50, Current loss 0.19201016771471244 Accuracy 87.86654697785757\n",
      "Training:: Epoch 91, Iteration 60, Current loss 0.21440394131786497 Accuracy 83.2178598922248\n",
      "Training:: Epoch 91, Iteration 70, Current loss 0.22050176673523528 Accuracy 86.92015209125475\n",
      "Training:: Epoch 91, Iteration 80, Current loss 0.16771082547178345 Accuracy 88.55585019217591\n",
      "Training:: Epoch 91, Iteration 90, Current loss 0.3637026327581182 Accuracy 89.83894317770539\n",
      "Training:: Epoch 91, Iteration 100, Current loss 0.2654372271627857 Accuracy 84.70655875458847\n",
      "Training:: Epoch 91, Iteration 110, Current loss 0.17398728773168606 Accuracy 89.53269069572507\n",
      "Training:: Epoch 91, Iteration 120, Current loss 0.25976758163990565 Accuracy 88.76412905157572\n",
      "Training:: Epoch 91, Iteration 130, Current loss 0.2091532826679179 Accuracy 88.39985080193958\n",
      "Training:: Epoch 91, Iteration 140, Current loss 0.3307222085346334 Accuracy 85.86450153068016\n",
      "Training:: Epoch 91, Iteration 150, Current loss 0.28890769808366595 Accuracy 92.68497757847534\n",
      "Training:: Epoch 91, Iteration 160, Current loss 0.23665976377553777 Accuracy 88.79249152141337\n",
      "Training:: Epoch 91, Iteration 170, Current loss 0.17521980929317993 Accuracy 87.83422923530432\n",
      "Training:: Epoch 91, Iteration 180, Current loss 0.25666850107742006 Accuracy 88.57845975680371\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 91, Probability Accuracy 66.84552710408332\n",
      "Starting Training\n",
      "Training:: Epoch 92, Iteration 0, Current loss 0.25191662858079455 Accuracy 92.38981998758535\n",
      "Training:: Epoch 92, Iteration 10, Current loss 0.24598148190250543 Accuracy 84.86694152923538\n",
      "Training:: Epoch 92, Iteration 20, Current loss 0.314809214069016 Accuracy 88.13714285714286\n",
      "Training:: Epoch 92, Iteration 30, Current loss 0.22257640529023937 Accuracy 71.83932709638658\n",
      "Training:: Epoch 92, Iteration 40, Current loss 0.19052113439175988 Accuracy 91.66132820841341\n",
      "Training:: Epoch 92, Iteration 50, Current loss 0.20420952084117217 Accuracy 91.5203605681044\n",
      "Training:: Epoch 92, Iteration 60, Current loss 0.2970589710459224 Accuracy 84.6834070242763\n",
      "Training:: Epoch 92, Iteration 70, Current loss 0.5304363753778427 Accuracy 90.95649340068438\n",
      "Training:: Epoch 92, Iteration 80, Current loss 0.6232336024112541 Accuracy 83.13609467455622\n",
      "Training:: Epoch 92, Iteration 90, Current loss 0.505012863174532 Accuracy 84.35179897201598\n",
      "Training:: Epoch 92, Iteration 100, Current loss 0.47144691762144597 Accuracy 85.74604891532961\n",
      "Training:: Epoch 92, Iteration 110, Current loss 0.39085603150252624 Accuracy 91.36552494675465\n",
      "Training:: Epoch 92, Iteration 120, Current loss 0.48187321210371614 Accuracy 84.90484429065744\n",
      "Training:: Epoch 92, Iteration 130, Current loss 0.6167637134102413 Accuracy 83.44704653021083\n",
      "Training:: Epoch 92, Iteration 140, Current loss 0.3346567933743472 Accuracy 92.44254915896707\n",
      "Training:: Epoch 92, Iteration 150, Current loss 0.44322617812189946 Accuracy 86.76305539050637\n",
      "Training:: Epoch 92, Iteration 160, Current loss 0.49379373162087364 Accuracy 82.27485435966001\n",
      "Training:: Epoch 92, Iteration 170, Current loss 0.38525317107684903 Accuracy 88.85297758217577\n",
      "Training:: Epoch 92, Iteration 180, Current loss 0.327006652877773 Accuracy 84.02604599612081\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 92, Probability Accuracy 67.56235383501311\n",
      "Starting Training\n",
      "Training:: Epoch 93, Iteration 0, Current loss 0.35528066180558665 Accuracy 90.16623741512527\n",
      "Training:: Epoch 93, Iteration 10, Current loss 0.19725761370931266 Accuracy 89.2675483214649\n",
      "Training:: Epoch 93, Iteration 20, Current loss 0.40659193425618045 Accuracy 87.27177882107459\n",
      "Training:: Epoch 93, Iteration 30, Current loss 0.2842869545349076 Accuracy 82.75159812185325\n",
      "Training:: Epoch 93, Iteration 40, Current loss 0.19974359293054547 Accuracy 85.04933615746242\n",
      "Training:: Epoch 93, Iteration 50, Current loss 0.24921080013093524 Accuracy 89.4853381208857\n",
      "Training:: Epoch 93, Iteration 60, Current loss 0.17081293875263887 Accuracy 93.06363515253786\n",
      "Training:: Epoch 93, Iteration 70, Current loss 0.23529480669298145 Accuracy 89.2560316307593\n",
      "Training:: Epoch 93, Iteration 80, Current loss 0.2350910146001797 Accuracy 86.28402616585778\n",
      "Training:: Epoch 93, Iteration 90, Current loss 0.30586603113967337 Accuracy 79.31894588439432\n",
      "Training:: Epoch 93, Iteration 100, Current loss 0.26310099052887675 Accuracy 87.6417033773862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 93, Iteration 110, Current loss 0.26614627373034483 Accuracy 89.7703927492447\n",
      "Training:: Epoch 93, Iteration 120, Current loss 0.23870546982377253 Accuracy 90.31843036346092\n",
      "Training:: Epoch 93, Iteration 130, Current loss 0.2375094175342599 Accuracy 94.33962264150944\n",
      "Training:: Epoch 93, Iteration 140, Current loss 0.23982301069110312 Accuracy 90.4054054054054\n",
      "Training:: Epoch 93, Iteration 150, Current loss 0.21467977925035406 Accuracy 85.25615646527832\n",
      "Training:: Epoch 93, Iteration 160, Current loss 0.41994486837889644 Accuracy 86.27287853577371\n",
      "Training:: Epoch 93, Iteration 170, Current loss 0.4650290638546063 Accuracy 85.56561804893549\n",
      "Training:: Epoch 93, Iteration 180, Current loss 0.37300857327168446 Accuracy 84.4092938772421\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 93, Probability Accuracy 68.07459904792431\n",
      "Starting Training\n",
      "Training:: Epoch 94, Iteration 0, Current loss 0.31149158350493195 Accuracy 88.0497339559127\n",
      "Training:: Epoch 94, Iteration 10, Current loss 0.38194607761738697 Accuracy 86.58794697560432\n",
      "Training:: Epoch 94, Iteration 20, Current loss 0.37440214320638615 Accuracy 93.32174416920179\n",
      "Training:: Epoch 94, Iteration 30, Current loss 0.32590079869103894 Accuracy 79.53291210109573\n",
      "Training:: Epoch 94, Iteration 40, Current loss 0.3977001149323727 Accuracy 92.35599658888286\n",
      "Training:: Epoch 94, Iteration 50, Current loss 0.2669933083946348 Accuracy 88.18877551020408\n",
      "Training:: Epoch 94, Iteration 60, Current loss 0.2949131828354572 Accuracy 90.76923076923077\n",
      "Training:: Epoch 94, Iteration 70, Current loss 0.2151413279164911 Accuracy 86.95782231528567\n",
      "Training:: Epoch 94, Iteration 80, Current loss 0.20500367305828573 Accuracy 87.76494845360824\n",
      "Training:: Epoch 94, Iteration 90, Current loss 0.28282323329407216 Accuracy 89.2012596621815\n",
      "Training:: Epoch 94, Iteration 100, Current loss 0.3214028722241955 Accuracy 91.44623840604604\n",
      "Training:: Epoch 94, Iteration 110, Current loss 0.15470843402264034 Accuracy 86.07463461681134\n",
      "Training:: Epoch 94, Iteration 120, Current loss 0.3315820741869905 Accuracy 88.81865651132661\n",
      "Training:: Epoch 94, Iteration 130, Current loss 0.23206691603282803 Accuracy 91.42667257421355\n",
      "Training:: Epoch 94, Iteration 140, Current loss 0.30867274249757537 Accuracy 80.37132311821944\n",
      "Training:: Epoch 94, Iteration 150, Current loss 0.2773046282097145 Accuracy 88.18489075070215\n",
      "Training:: Epoch 94, Iteration 160, Current loss 0.2750217732589474 Accuracy 91.86098543230746\n",
      "Training:: Epoch 94, Iteration 170, Current loss 0.2507362438725314 Accuracy 88.17544490382888\n",
      "Training:: Epoch 94, Iteration 180, Current loss 0.335434646978597 Accuracy 89.42977721071745\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 94, Probability Accuracy 67.76297826370835\n",
      "Starting Training\n",
      "Training:: Epoch 95, Iteration 0, Current loss 0.24698004668686585 Accuracy 77.87763567231248\n",
      "Training:: Epoch 95, Iteration 10, Current loss 0.2680003462270413 Accuracy 87.62997490139836\n",
      "Training:: Epoch 95, Iteration 20, Current loss 0.1795626645966046 Accuracy 89.47892300801274\n",
      "Training:: Epoch 95, Iteration 30, Current loss 0.1680456050678026 Accuracy 88.82720333102013\n",
      "Training:: Epoch 95, Iteration 40, Current loss 0.24880029895275907 Accuracy 89.28937128814253\n",
      "Training:: Epoch 95, Iteration 50, Current loss 0.19505975993771113 Accuracy 87.47782467731082\n",
      "Training:: Epoch 95, Iteration 60, Current loss 0.19770355825615046 Accuracy 91.81932471264368\n",
      "Training:: Epoch 95, Iteration 70, Current loss 0.1956144607020741 Accuracy 84.32176835868512\n",
      "Training:: Epoch 95, Iteration 80, Current loss 0.15795473063116325 Accuracy 83.90287769784173\n",
      "Training:: Epoch 95, Iteration 90, Current loss 0.20314858277642117 Accuracy 91.82284927161163\n",
      "Training:: Epoch 95, Iteration 100, Current loss 0.2488132994869139 Accuracy 82.41812059993879\n",
      "Training:: Epoch 95, Iteration 110, Current loss 0.2428530251125472 Accuracy 91.11948064819862\n",
      "Training:: Epoch 95, Iteration 120, Current loss 0.29498359021830023 Accuracy 88.56621559981754\n",
      "Training:: Epoch 95, Iteration 130, Current loss 0.24631100886661172 Accuracy 88.86168142148553\n",
      "Training:: Epoch 95, Iteration 140, Current loss 0.21345196751777937 Accuracy 88.21989528795811\n",
      "Training:: Epoch 95, Iteration 150, Current loss 0.2467042910500768 Accuracy 88.89376325217519\n",
      "Training:: Epoch 95, Iteration 160, Current loss 0.17888787311160806 Accuracy 93.82760845024995\n",
      "Training:: Epoch 95, Iteration 170, Current loss 0.19516424460400483 Accuracy 87.2444011684518\n",
      "Training:: Epoch 95, Iteration 180, Current loss 0.28404329112571264 Accuracy 84.12806539509536\n",
      "Calculating Expectation\n",
      "Epoch 95 iter 0\n",
      "Epoch 95 iter 10\n",
      "Epoch 95 iter 20\n",
      "Epoch 95 iter 30\n",
      "Epoch 95 iter 40\n",
      "Epoch 95 iter 50\n",
      "Epoch 95 iter 60\n",
      "Epoch 95 iter 70\n",
      "Epoch 95 iter 80\n",
      "Epoch 95 iter 90\n",
      "Epoch 95 iter 100\n",
      "Epoch 95 iter 110\n",
      "Epoch 95 iter 120\n",
      "Epoch 95 iter 130\n",
      "Epoch 95 iter 140\n",
      "Epoch 95 iter 150\n",
      "Epoch 95 iter 160\n",
      "Epoch 95 iter 170\n",
      "Epoch 95 iter 180\n",
      "Train Boundary avergage error = 87.063\n",
      "Train From boundary avergage accuracy = 88.131\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 95, Probability Accuracy 67.70441334172236\n",
      "Starting Training\n",
      "Training:: Epoch 96, Iteration 0, Current loss 0.17549308102807265 Accuracy 84.86731609002351\n",
      "Training:: Epoch 96, Iteration 10, Current loss 0.18054424687900977 Accuracy 86.20382732532265\n",
      "Training:: Epoch 96, Iteration 20, Current loss 0.23429840295024115 Accuracy 88.42117402837043\n",
      "Training:: Epoch 96, Iteration 30, Current loss 0.21497020020222005 Accuracy 85.99861143253877\n",
      "Training:: Epoch 96, Iteration 40, Current loss 0.17598761486993 Accuracy 88.52596314907872\n",
      "Training:: Epoch 96, Iteration 50, Current loss 0.14197700008445577 Accuracy 84.06816932380428\n",
      "Training:: Epoch 96, Iteration 60, Current loss 0.21103356087041275 Accuracy 89.92388546574846\n",
      "Training:: Epoch 96, Iteration 70, Current loss 0.25618725678550974 Accuracy 83.63282591278471\n",
      "Training:: Epoch 96, Iteration 80, Current loss 0.20998720557836206 Accuracy 88.03038117117539\n",
      "Training:: Epoch 96, Iteration 90, Current loss 0.20003086257717673 Accuracy 93.20145520691223\n",
      "Training:: Epoch 96, Iteration 100, Current loss 0.1726802107569204 Accuracy 87.94613805876759\n",
      "Training:: Epoch 96, Iteration 110, Current loss 0.24294831087755747 Accuracy 91.13199836534532\n",
      "Training:: Epoch 96, Iteration 120, Current loss 0.8958843170482718 Accuracy 82.53247439011511\n",
      "Training:: Epoch 96, Iteration 130, Current loss 1.6152458719623235 Accuracy 80.5863280194542\n",
      "Training:: Epoch 96, Iteration 140, Current loss 0.850353159517701 Accuracy 81.3050193050193\n",
      "Training:: Epoch 96, Iteration 150, Current loss 0.823437645520626 Accuracy 90.0281425891182\n",
      "Training:: Epoch 96, Iteration 160, Current loss 1.1941566450853993 Accuracy 79.4876278744426\n",
      "Training:: Epoch 96, Iteration 170, Current loss 0.860641477483651 Accuracy 87.92183493867368\n",
      "Training:: Epoch 96, Iteration 180, Current loss 0.7903473262578545 Accuracy 88.28430245920478\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 96, Probability Accuracy 37.90060583037541\n",
      "Starting Training\n",
      "Training:: Epoch 97, Iteration 0, Current loss 7.9556019011046395 Accuracy 76.89555290509456\n",
      "Training:: Epoch 97, Iteration 10, Current loss 10.806054550456722 Accuracy 53.36911993219381\n",
      "Training:: Epoch 97, Iteration 20, Current loss 2.721679260318007 Accuracy 83.41841557950463\n",
      "Training:: Epoch 97, Iteration 30, Current loss 2.3773220220949876 Accuracy 76.13125718465879\n",
      "Training:: Epoch 97, Iteration 40, Current loss 4.655801358308441 Accuracy 59.0615531813108\n",
      "Training:: Epoch 97, Iteration 50, Current loss 1.5885654399517168 Accuracy 84.97033269760543\n",
      "Training:: Epoch 97, Iteration 60, Current loss 1.3590083135450994 Accuracy 81.3975155279503\n",
      "Training:: Epoch 97, Iteration 70, Current loss 1.2191569566360747 Accuracy 80.79937304075236\n",
      "Training:: Epoch 97, Iteration 80, Current loss 0.7975216269177416 Accuracy 88.92348754448399\n",
      "Training:: Epoch 97, Iteration 90, Current loss 0.5713687225381443 Accuracy 84.20054072001139\n",
      "Training:: Epoch 97, Iteration 100, Current loss 0.5751989941027038 Accuracy 83.03877775619861\n",
      "Training:: Epoch 97, Iteration 110, Current loss 0.6746719991456851 Accuracy 89.25485703374862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 97, Iteration 120, Current loss 4.93080714689357 Accuracy 75.52576235541535\n",
      "Training:: Epoch 97, Iteration 130, Current loss 1.4530563181925178 Accuracy 85.286385724528\n",
      "Training:: Epoch 97, Iteration 140, Current loss 1.037256622524619 Accuracy 85.17393042782886\n",
      "Training:: Epoch 97, Iteration 150, Current loss 0.5619587483551571 Accuracy 90.01313197636244\n",
      "Training:: Epoch 97, Iteration 160, Current loss 0.48504768662527736 Accuracy 90.58358633318876\n",
      "Training:: Epoch 97, Iteration 170, Current loss 0.6425421742754419 Accuracy 80.45119621577368\n",
      "Training:: Epoch 97, Iteration 180, Current loss 0.821203252233637 Accuracy 85.57301967315499\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 97, Probability Accuracy 66.2165477561325\n",
      "Starting Training\n",
      "Training:: Epoch 98, Iteration 0, Current loss 1.0360097052924104 Accuracy 89.01671516545984\n",
      "Training:: Epoch 98, Iteration 10, Current loss 0.5133824485992367 Accuracy 84.83462583681315\n",
      "Training:: Epoch 98, Iteration 20, Current loss 0.9139699296869873 Accuracy 87.40767647414941\n",
      "Training:: Epoch 98, Iteration 30, Current loss 0.5008077225787887 Accuracy 89.10826542491269\n",
      "Training:: Epoch 98, Iteration 40, Current loss 0.61988884793006 Accuracy 90.91712986356745\n",
      "Training:: Epoch 98, Iteration 50, Current loss 0.40860313995014474 Accuracy 91.22204100652377\n",
      "Training:: Epoch 98, Iteration 60, Current loss 0.42737934679343664 Accuracy 88.38863876382636\n",
      "Training:: Epoch 98, Iteration 70, Current loss 0.6385332640769835 Accuracy 86.55983067503213\n",
      "Training:: Epoch 98, Iteration 80, Current loss 0.6764012367168686 Accuracy 83.40738582562459\n",
      "Training:: Epoch 98, Iteration 90, Current loss 0.7952425394212365 Accuracy 85.97592433361994\n",
      "Training:: Epoch 98, Iteration 100, Current loss 0.9830660151062848 Accuracy 88.60891174940761\n",
      "Training:: Epoch 98, Iteration 110, Current loss 0.520443690972055 Accuracy 82.67163449792328\n",
      "Training:: Epoch 98, Iteration 120, Current loss 0.541851932737859 Accuracy 86.84074091530377\n",
      "Training:: Epoch 98, Iteration 130, Current loss 0.3610210684701409 Accuracy 92.22613596336738\n",
      "Training:: Epoch 98, Iteration 140, Current loss 0.4109845325033933 Accuracy 87.88742182070882\n",
      "Training:: Epoch 98, Iteration 150, Current loss 0.2367241463555666 Accuracy 88.57094307561597\n",
      "Training:: Epoch 98, Iteration 160, Current loss 0.6981476252901844 Accuracy 85.45678697445054\n",
      "Training:: Epoch 98, Iteration 170, Current loss 0.4285284091372183 Accuracy 86.28896838723809\n",
      "Training:: Epoch 98, Iteration 180, Current loss 0.31141849728143556 Accuracy 81.39984947855069\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 98, Probability Accuracy 68.67350451701746\n",
      "Starting Training\n",
      "Training:: Epoch 99, Iteration 0, Current loss 0.4852907718509006 Accuracy 86.0597439544808\n",
      "Training:: Epoch 99, Iteration 10, Current loss 0.2844014102132461 Accuracy 90.65491320101671\n",
      "Training:: Epoch 99, Iteration 20, Current loss 0.32257806407743356 Accuracy 85.61170573093077\n",
      "Training:: Epoch 99, Iteration 30, Current loss 0.3276296939024259 Accuracy 86.62068965517241\n",
      "Training:: Epoch 99, Iteration 40, Current loss 0.38218186348443844 Accuracy 86.57202505219206\n",
      "Training:: Epoch 99, Iteration 50, Current loss 0.3608027957976092 Accuracy 89.65277777777777\n",
      "Training:: Epoch 99, Iteration 60, Current loss 0.531372315839126 Accuracy 88.37975159954836\n",
      "Training:: Epoch 99, Iteration 70, Current loss 0.4783448755886953 Accuracy 89.36596833298762\n",
      "Training:: Epoch 99, Iteration 80, Current loss 0.4679085403846575 Accuracy 89.28041543026706\n",
      "Training:: Epoch 99, Iteration 90, Current loss 0.4762680525831971 Accuracy 90.3798773849768\n",
      "Training:: Epoch 99, Iteration 100, Current loss 0.35410304513343915 Accuracy 90.28082307303086\n",
      "Training:: Epoch 99, Iteration 110, Current loss 0.4756143796652983 Accuracy 89.10459587955626\n",
      "Training:: Epoch 99, Iteration 120, Current loss 0.41121742942863965 Accuracy 83.06969459671105\n",
      "Training:: Epoch 99, Iteration 130, Current loss 0.4663739503619264 Accuracy 85.44145585441456\n",
      "Training:: Epoch 99, Iteration 140, Current loss 0.5016841089413266 Accuracy 89.44292948967667\n",
      "Training:: Epoch 99, Iteration 150, Current loss 0.5319524476080985 Accuracy 86.86229148910265\n",
      "Training:: Epoch 99, Iteration 160, Current loss 0.35711217596211403 Accuracy 88.42651817292497\n",
      "Training:: Epoch 99, Iteration 170, Current loss 0.2794769174735198 Accuracy 92.52259230570617\n",
      "Training:: Epoch 99, Iteration 180, Current loss 0.3180822564323786 Accuracy 92.39864864864865\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 99, Probability Accuracy 67.18286896890123\n",
      "Starting Training\n",
      "Training:: Epoch 100, Iteration 0, Current loss 0.3729449877837822 Accuracy 90.14084507042253\n",
      "Training:: Epoch 100, Iteration 10, Current loss 0.33087759603191297 Accuracy 89.64271894750419\n",
      "Training:: Epoch 100, Iteration 20, Current loss 0.3057998960771158 Accuracy 84.09888600640927\n",
      "Training:: Epoch 100, Iteration 30, Current loss 0.7504980929720548 Accuracy 84.86024844720497\n",
      "Training:: Epoch 100, Iteration 40, Current loss 0.2752860385813153 Accuracy 89.5672791938352\n",
      "Training:: Epoch 100, Iteration 50, Current loss 0.28870366502068806 Accuracy 90.25299078250637\n",
      "Training:: Epoch 100, Iteration 60, Current loss 0.2232849086316148 Accuracy 86.4635231316726\n",
      "Training:: Epoch 100, Iteration 70, Current loss 0.2760169873467404 Accuracy 92.39282712244326\n",
      "Training:: Epoch 100, Iteration 80, Current loss 0.24635763510133638 Accuracy 92.0646166926456\n",
      "Training:: Epoch 100, Iteration 90, Current loss 0.3215341251886687 Accuracy 80.83750955576319\n",
      "Training:: Epoch 100, Iteration 100, Current loss 0.2634696060752791 Accuracy 88.85518332893696\n",
      "Training:: Epoch 100, Iteration 110, Current loss 0.2884243565758769 Accuracy 88.62909672262191\n",
      "Training:: Epoch 100, Iteration 120, Current loss 0.31309827416650365 Accuracy 82.44165637391926\n",
      "Training:: Epoch 100, Iteration 130, Current loss 0.43633801876702966 Accuracy 88.03290083410565\n",
      "Training:: Epoch 100, Iteration 140, Current loss 0.3998032767520669 Accuracy 87.29493387294934\n",
      "Training:: Epoch 100, Iteration 150, Current loss 0.3468060357351266 Accuracy 86.76597725610162\n",
      "Training:: Epoch 100, Iteration 160, Current loss 0.25778606047642205 Accuracy 88.68040602891418\n",
      "Training:: Epoch 100, Iteration 170, Current loss 0.24218917285838718 Accuracy 89.67902914823637\n",
      "Training:: Epoch 100, Iteration 180, Current loss 0.246591724963947 Accuracy 89.59687044158599\n",
      "Calculating Expectation\n",
      "Epoch 100 iter 0\n",
      "Epoch 100 iter 10\n",
      "Epoch 100 iter 20\n",
      "Epoch 100 iter 30\n",
      "Epoch 100 iter 40\n",
      "Epoch 100 iter 50\n",
      "Epoch 100 iter 60\n",
      "Epoch 100 iter 70\n",
      "Epoch 100 iter 80\n",
      "Epoch 100 iter 90\n",
      "Epoch 100 iter 100\n",
      "Epoch 100 iter 110\n",
      "Epoch 100 iter 120\n",
      "Epoch 100 iter 130\n",
      "Epoch 100 iter 140\n",
      "Epoch 100 iter 150\n",
      "Epoch 100 iter 160\n",
      "Epoch 100 iter 170\n",
      "Epoch 100 iter 180\n",
      "Train Boundary avergage error = 87.233\n",
      "Train From boundary avergage accuracy = 88.104\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 100, Probability Accuracy 69.38043852463882\n",
      "Starting Training\n",
      "Training:: Epoch 101, Iteration 0, Current loss 0.24081276378294955 Accuracy 88.43460248569738\n",
      "Training:: Epoch 101, Iteration 10, Current loss 0.2296527418937274 Accuracy 83.32679921590591\n",
      "Training:: Epoch 101, Iteration 20, Current loss 0.32425313769790637 Accuracy 91.09892224788298\n",
      "Training:: Epoch 101, Iteration 30, Current loss 0.20521425648964403 Accuracy 91.60374056108417\n",
      "Training:: Epoch 101, Iteration 40, Current loss 0.4473524383375883 Accuracy 83.70589322002586\n",
      "Training:: Epoch 101, Iteration 50, Current loss 0.22595838048240321 Accuracy 90.16665132124113\n",
      "Training:: Epoch 101, Iteration 60, Current loss 0.204992146796206 Accuracy 89.26915289661028\n",
      "Training:: Epoch 101, Iteration 70, Current loss 0.20173548151412263 Accuracy 86.86292819580282\n",
      "Training:: Epoch 101, Iteration 80, Current loss 0.23263789418682002 Accuracy 87.26444995101711\n",
      "Training:: Epoch 101, Iteration 90, Current loss 0.20275365817804186 Accuracy 90.64625257210616\n",
      "Training:: Epoch 101, Iteration 100, Current loss 0.18467984375129276 Accuracy 87.82635094110503\n",
      "Training:: Epoch 101, Iteration 110, Current loss 0.2445981580676109 Accuracy 87.50262549884478\n",
      "Training:: Epoch 101, Iteration 120, Current loss 0.26996283031889723 Accuracy 86.14668218859138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 101, Iteration 130, Current loss 0.21282848678511135 Accuracy 88.9908256880734\n",
      "Training:: Epoch 101, Iteration 140, Current loss 0.2834039917393604 Accuracy 83.06105610561056\n",
      "Training:: Epoch 101, Iteration 150, Current loss 0.23417795455272045 Accuracy 89.08306364617044\n",
      "Training:: Epoch 101, Iteration 160, Current loss 0.16873023222771769 Accuracy 92.41991589221743\n",
      "Training:: Epoch 101, Iteration 170, Current loss 0.1726938123658797 Accuracy 92.34268800701601\n",
      "Training:: Epoch 101, Iteration 180, Current loss 0.35328757429238306 Accuracy 90.20622209623698\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 101, Probability Accuracy 69.27359711290762\n",
      "Starting Training\n",
      "Training:: Epoch 102, Iteration 0, Current loss 0.32832379242564985 Accuracy 86.397511502819\n",
      "Training:: Epoch 102, Iteration 10, Current loss 0.21357199553465456 Accuracy 91.16036677881127\n",
      "Training:: Epoch 102, Iteration 20, Current loss 0.16247557039755622 Accuracy 92.54109467978554\n",
      "Training:: Epoch 102, Iteration 30, Current loss 0.2836892991971184 Accuracy 87.3416059352636\n",
      "Training:: Epoch 102, Iteration 40, Current loss 0.21062684622050307 Accuracy 85.88779989176838\n",
      "Training:: Epoch 102, Iteration 50, Current loss 0.24619854267817565 Accuracy 79.1044776119403\n",
      "Training:: Epoch 102, Iteration 60, Current loss 0.20933383724598287 Accuracy 89.00814791601378\n",
      "Training:: Epoch 102, Iteration 70, Current loss 0.14363944780219579 Accuracy 90.80330513861047\n",
      "Training:: Epoch 102, Iteration 80, Current loss 0.22217311410195023 Accuracy 89.04807621298279\n",
      "Training:: Epoch 102, Iteration 90, Current loss 0.24903071465637444 Accuracy 84.2434284853506\n",
      "Training:: Epoch 102, Iteration 100, Current loss 0.23742473710539774 Accuracy 88.77081415781106\n",
      "Training:: Epoch 102, Iteration 110, Current loss 0.19495698275023834 Accuracy 90.38249908293594\n",
      "Training:: Epoch 102, Iteration 120, Current loss 0.21692374645282558 Accuracy 88.79284649776453\n",
      "Training:: Epoch 102, Iteration 130, Current loss 0.215002114927325 Accuracy 88.6376233233566\n",
      "Training:: Epoch 102, Iteration 140, Current loss 0.18932537862589083 Accuracy 85.36928487690504\n",
      "Training:: Epoch 102, Iteration 150, Current loss 0.17849149654873492 Accuracy 89.46414056149565\n",
      "Training:: Epoch 102, Iteration 160, Current loss 0.20907691470722883 Accuracy 87.959127312897\n",
      "Training:: Epoch 102, Iteration 170, Current loss 0.24977144546245042 Accuracy 89.63470319634703\n",
      "Training:: Epoch 102, Iteration 180, Current loss 0.2735194140709001 Accuracy 80.8585297385812\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 102, Probability Accuracy 69.57354448361963\n",
      "Starting Training\n",
      "Training:: Epoch 103, Iteration 0, Current loss 0.19509298601670985 Accuracy 85.48416029849675\n",
      "Training:: Epoch 103, Iteration 10, Current loss 0.2696550128651375 Accuracy 86.08761202780802\n",
      "Training:: Epoch 103, Iteration 20, Current loss 0.32958040390575144 Accuracy 86.56909169177787\n",
      "Training:: Epoch 103, Iteration 30, Current loss 0.20585454357245475 Accuracy 91.64840721388842\n",
      "Training:: Epoch 103, Iteration 40, Current loss 0.23989645952055624 Accuracy 88.00281141868513\n",
      "Training:: Epoch 103, Iteration 50, Current loss 0.20674483591412546 Accuracy 88.90329751759911\n",
      "Training:: Epoch 103, Iteration 60, Current loss 0.23046917227926167 Accuracy 91.81554024967714\n",
      "Training:: Epoch 103, Iteration 70, Current loss 0.24147977697393194 Accuracy 83.51075385618076\n",
      "Training:: Epoch 103, Iteration 80, Current loss 0.26177705285183506 Accuracy 86.17005707595493\n",
      "Training:: Epoch 103, Iteration 90, Current loss 0.1725570687373014 Accuracy 92.01719947653767\n",
      "Training:: Epoch 103, Iteration 100, Current loss 0.21590154550430796 Accuracy 86.90024548091944\n",
      "Training:: Epoch 103, Iteration 110, Current loss 0.1570435237970301 Accuracy 91.63380831544018\n",
      "Training:: Epoch 103, Iteration 120, Current loss 0.16057807235094843 Accuracy 91.17647058823529\n",
      "Training:: Epoch 103, Iteration 130, Current loss 0.20418281512362024 Accuracy 87.47351470463599\n",
      "Training:: Epoch 103, Iteration 140, Current loss 0.23765073062980752 Accuracy 81.72811765907466\n",
      "Training:: Epoch 103, Iteration 150, Current loss 0.20861344093185008 Accuracy 89.6453791832975\n",
      "Training:: Epoch 103, Iteration 160, Current loss 0.23456630976574805 Accuracy 91.4831025328263\n",
      "Training:: Epoch 103, Iteration 170, Current loss 0.18990900887900958 Accuracy 81.54466645032683\n",
      "Training:: Epoch 103, Iteration 180, Current loss 0.23471314493847556 Accuracy 87.94782608695652\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 103, Probability Accuracy 69.2617258449375\n",
      "Starting Training\n",
      "Training:: Epoch 104, Iteration 0, Current loss 0.16590552292621435 Accuracy 89.67067380543489\n",
      "Training:: Epoch 104, Iteration 10, Current loss 0.19047208939347107 Accuracy 88.12329196720577\n",
      "Training:: Epoch 104, Iteration 20, Current loss 0.1628490579912454 Accuracy 89.98304006783972\n",
      "Training:: Epoch 104, Iteration 30, Current loss 0.20507052659606878 Accuracy 89.207779886148\n",
      "Training:: Epoch 104, Iteration 40, Current loss 0.18630176958535177 Accuracy 89.34960190259538\n",
      "Training:: Epoch 104, Iteration 50, Current loss 0.2689493143317341 Accuracy 89.54993570510072\n",
      "Training:: Epoch 104, Iteration 60, Current loss 0.2198010738000091 Accuracy 92.32085679584851\n",
      "Training:: Epoch 104, Iteration 70, Current loss 0.19550745513744344 Accuracy 91.53876621447668\n",
      "Training:: Epoch 104, Iteration 80, Current loss 0.18717811635909945 Accuracy 88.53310502283105\n",
      "Training:: Epoch 104, Iteration 90, Current loss 0.16590153167926908 Accuracy 89.1327155097045\n",
      "Training:: Epoch 104, Iteration 100, Current loss 0.218505364826162 Accuracy 82.32498394348106\n",
      "Training:: Epoch 104, Iteration 110, Current loss 0.18922435078112787 Accuracy 93.3084652862363\n",
      "Training:: Epoch 104, Iteration 120, Current loss 0.2216846156132415 Accuracy 87.58813957693003\n",
      "Training:: Epoch 104, Iteration 130, Current loss 0.20877810735242336 Accuracy 90.4629886332132\n",
      "Training:: Epoch 104, Iteration 140, Current loss 0.24387460135534755 Accuracy 86.70490851002033\n",
      "Training:: Epoch 104, Iteration 150, Current loss 0.2303995129304159 Accuracy 81.47416653499764\n",
      "Training:: Epoch 104, Iteration 160, Current loss 0.2748142389219729 Accuracy 88.07885684043397\n",
      "Training:: Epoch 104, Iteration 170, Current loss 0.21378293190038733 Accuracy 89.60197469916693\n",
      "Training:: Epoch 104, Iteration 180, Current loss 0.2522279198728564 Accuracy 86.32725870215023\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 104, Probability Accuracy 68.78628156273372\n",
      "Starting Training\n",
      "Training:: Epoch 105, Iteration 0, Current loss 0.18138022554201613 Accuracy 90.56325987841946\n",
      "Training:: Epoch 105, Iteration 10, Current loss 0.2179810072453663 Accuracy 89.60319901568748\n",
      "Training:: Epoch 105, Iteration 20, Current loss 0.23467638172198751 Accuracy 85.328237108563\n",
      "Training:: Epoch 105, Iteration 30, Current loss 0.13132575931629661 Accuracy 91.1768300134425\n",
      "Training:: Epoch 105, Iteration 40, Current loss 0.21870781515142584 Accuracy 81.14296125349205\n",
      "Training:: Epoch 105, Iteration 50, Current loss 0.13706567577301593 Accuracy 91.83540524401498\n",
      "Training:: Epoch 105, Iteration 60, Current loss 0.20697898745968069 Accuracy 86.28937069019072\n",
      "Training:: Epoch 105, Iteration 70, Current loss 0.15085734461027003 Accuracy 85.75078911769127\n",
      "Training:: Epoch 105, Iteration 80, Current loss 0.1887499056687925 Accuracy 81.9708474200521\n",
      "Training:: Epoch 105, Iteration 90, Current loss 0.19487828381056752 Accuracy 89.17684228503117\n",
      "Training:: Epoch 105, Iteration 100, Current loss 0.162406300775482 Accuracy 89.96899594562366\n",
      "Training:: Epoch 105, Iteration 110, Current loss 0.16848445734277212 Accuracy 89.89691728955982\n",
      "Training:: Epoch 105, Iteration 120, Current loss 0.16667866211848595 Accuracy 90.26596101024988\n",
      "Training:: Epoch 105, Iteration 130, Current loss 0.22071419160501327 Accuracy 90.64002023779408\n",
      "Training:: Epoch 105, Iteration 140, Current loss 0.17605973212681303 Accuracy 87.25701943844493\n",
      "Training:: Epoch 105, Iteration 150, Current loss 0.25563651094655815 Accuracy 81.52492668621701\n",
      "Training:: Epoch 105, Iteration 160, Current loss 0.23918703530550028 Accuracy 85.39611360239162\n",
      "Training:: Epoch 105, Iteration 170, Current loss 0.19475555334035421 Accuracy 91.5479990179229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 105, Iteration 180, Current loss 0.16508740777022254 Accuracy 90.2987437981632\n",
      "Calculating Expectation\n",
      "Epoch 105 iter 0\n",
      "Epoch 105 iter 10\n",
      "Epoch 105 iter 20\n",
      "Epoch 105 iter 30\n",
      "Epoch 105 iter 40\n",
      "Epoch 105 iter 50\n",
      "Epoch 105 iter 60\n",
      "Epoch 105 iter 70\n",
      "Epoch 105 iter 80\n",
      "Epoch 105 iter 90\n",
      "Epoch 105 iter 100\n",
      "Epoch 105 iter 110\n",
      "Epoch 105 iter 120\n",
      "Epoch 105 iter 130\n",
      "Epoch 105 iter 140\n",
      "Epoch 105 iter 150\n",
      "Epoch 105 iter 160\n",
      "Epoch 105 iter 170\n",
      "Epoch 105 iter 180\n",
      "Train Boundary avergage error = 87.288\n",
      "Train From boundary avergage accuracy = 88.078\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 105, Probability Accuracy 68.63492289611453\n",
      "Starting Training\n",
      "Training:: Epoch 106, Iteration 0, Current loss 0.20892429192457468 Accuracy 83.09891279555995\n",
      "Training:: Epoch 106, Iteration 10, Current loss 0.2008716518672698 Accuracy 83.17344687836659\n",
      "Training:: Epoch 106, Iteration 20, Current loss 0.15093867939685993 Accuracy 90.61487174738402\n",
      "Training:: Epoch 106, Iteration 30, Current loss 0.17395278456184482 Accuracy 91.92155895411939\n",
      "Training:: Epoch 106, Iteration 40, Current loss 0.2225378958173428 Accuracy 86.28533750927224\n",
      "Training:: Epoch 106, Iteration 50, Current loss 0.17280171272392242 Accuracy 84.41686125107533\n",
      "Training:: Epoch 106, Iteration 60, Current loss 0.19207062143008155 Accuracy 75.6872797180391\n",
      "Training:: Epoch 106, Iteration 70, Current loss 0.15010313138198084 Accuracy 88.4594882729211\n",
      "Training:: Epoch 106, Iteration 80, Current loss 0.2144872603838694 Accuracy 87.56801432605552\n",
      "Training:: Epoch 106, Iteration 90, Current loss 0.13158811598209633 Accuracy 90.02656630506974\n",
      "Training:: Epoch 106, Iteration 100, Current loss 0.15958752794749667 Accuracy 89.21233804954736\n",
      "Training:: Epoch 106, Iteration 110, Current loss 0.1761296920712189 Accuracy 80.26997149007971\n",
      "Training:: Epoch 106, Iteration 120, Current loss 0.17397265238422105 Accuracy 87.7360448225773\n",
      "Training:: Epoch 106, Iteration 130, Current loss 0.2079066933802909 Accuracy 85.44200187090739\n",
      "Training:: Epoch 106, Iteration 140, Current loss 0.19301456824636692 Accuracy 88.79174128305067\n",
      "Training:: Epoch 106, Iteration 150, Current loss 0.1526023351844897 Accuracy 89.22167585556052\n",
      "Training:: Epoch 106, Iteration 160, Current loss 0.1763567000109975 Accuracy 91.21671722278396\n",
      "Training:: Epoch 106, Iteration 170, Current loss 0.2283851826906932 Accuracy 88.4340991535671\n",
      "Training:: Epoch 106, Iteration 180, Current loss 0.17787196540367015 Accuracy 86.86942269167729\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 106, Probability Accuracy 68.96435058228569\n",
      "Starting Training\n",
      "Training:: Epoch 107, Iteration 0, Current loss 0.1923572868827113 Accuracy 84.73715454125717\n",
      "Training:: Epoch 107, Iteration 10, Current loss 0.21149674735869867 Accuracy 89.29011281343908\n",
      "Training:: Epoch 107, Iteration 20, Current loss 0.17701276326110288 Accuracy 91.18602761982129\n",
      "Training:: Epoch 107, Iteration 30, Current loss 0.1782440554782223 Accuracy 92.21528111484864\n",
      "Training:: Epoch 107, Iteration 40, Current loss 0.1143848494326593 Accuracy 88.7914715552456\n",
      "Training:: Epoch 107, Iteration 50, Current loss 0.21903059927587787 Accuracy 81.11826048369204\n",
      "Training:: Epoch 107, Iteration 60, Current loss 0.2192697013188124 Accuracy 80.37570537570538\n",
      "Training:: Epoch 107, Iteration 70, Current loss 0.1509324386620255 Accuracy 84.39555454751644\n",
      "Training:: Epoch 107, Iteration 80, Current loss 0.16545500770410526 Accuracy 87.34873487348734\n",
      "Training:: Epoch 107, Iteration 90, Current loss 0.21682274257653633 Accuracy 89.96189816418428\n",
      "Training:: Epoch 107, Iteration 100, Current loss 0.2717365063940259 Accuracy 88.44231791600212\n",
      "Training:: Epoch 107, Iteration 110, Current loss 0.17764391243565508 Accuracy 88.29470646586708\n",
      "Training:: Epoch 107, Iteration 120, Current loss 0.18469249268113838 Accuracy 88.02691198681862\n",
      "Training:: Epoch 107, Iteration 130, Current loss 0.23892509416698363 Accuracy 82.97697368421052\n",
      "Training:: Epoch 107, Iteration 140, Current loss 0.19769858349936012 Accuracy 88.08693683495585\n",
      "Training:: Epoch 107, Iteration 150, Current loss 0.21981050135664637 Accuracy 81.89137464528476\n",
      "Training:: Epoch 107, Iteration 160, Current loss 0.19961307172344633 Accuracy 85.84536263222489\n",
      "Training:: Epoch 107, Iteration 170, Current loss 0.2328843637839619 Accuracy 85.26283240568955\n",
      "Training:: Epoch 107, Iteration 180, Current loss 0.19762455422130631 Accuracy 85.78825536062378\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 107, Probability Accuracy 68.49543549746548\n",
      "Starting Training\n",
      "Training:: Epoch 108, Iteration 0, Current loss 0.25180979843017337 Accuracy 84.63140735374026\n",
      "Training:: Epoch 108, Iteration 10, Current loss 0.16386414765743668 Accuracy 90.62708102108768\n",
      "Training:: Epoch 108, Iteration 20, Current loss 0.20969358979396424 Accuracy 84.09005829188726\n",
      "Training:: Epoch 108, Iteration 30, Current loss 0.2223347820317988 Accuracy 87.89560213727908\n",
      "Training:: Epoch 108, Iteration 40, Current loss 0.19265649107553093 Accuracy 88.67190077192056\n",
      "Training:: Epoch 108, Iteration 50, Current loss 0.23486277561311406 Accuracy 88.94011872514552\n",
      "Training:: Epoch 108, Iteration 60, Current loss 0.23015065946920954 Accuracy 88.57230627447969\n",
      "Training:: Epoch 108, Iteration 70, Current loss 0.13949542035205728 Accuracy 84.0560572499574\n",
      "Training:: Epoch 108, Iteration 80, Current loss 0.20548812566290725 Accuracy 88.50014909054767\n",
      "Training:: Epoch 108, Iteration 90, Current loss 0.16871184716262355 Accuracy 90.31683054453453\n",
      "Training:: Epoch 108, Iteration 100, Current loss 0.18805203887943825 Accuracy 90.79948391689283\n",
      "Training:: Epoch 108, Iteration 110, Current loss 0.21038552007151629 Accuracy 90.49314117034207\n",
      "Training:: Epoch 108, Iteration 120, Current loss 0.17025812569001772 Accuracy 85.77024122283258\n",
      "Training:: Epoch 108, Iteration 130, Current loss 0.19788976174977396 Accuracy 90.56501464049452\n",
      "Training:: Epoch 108, Iteration 140, Current loss 0.1474525430110556 Accuracy 93.25217178514622\n",
      "Training:: Epoch 108, Iteration 150, Current loss 0.17643981623048227 Accuracy 90.03368034292713\n",
      "Training:: Epoch 108, Iteration 160, Current loss 0.15647851126391732 Accuracy 91.26656528351076\n",
      "Training:: Epoch 108, Iteration 170, Current loss 0.14092029669272488 Accuracy 90.25549613784908\n",
      "Training:: Epoch 108, Iteration 180, Current loss 0.18872914399905485 Accuracy 90.00368414589218\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 108, Probability Accuracy 68.6313615157235\n",
      "Starting Training\n",
      "Training:: Epoch 109, Iteration 0, Current loss 0.15826612134677837 Accuracy 81.71093347086126\n",
      "Training:: Epoch 109, Iteration 10, Current loss 0.21433920775845092 Accuracy 85.90934684684684\n",
      "Training:: Epoch 109, Iteration 20, Current loss 0.2223979243999598 Accuracy 90.93087557603687\n",
      "Training:: Epoch 109, Iteration 30, Current loss 0.22248426045629818 Accuracy 89.87463837994214\n",
      "Training:: Epoch 109, Iteration 40, Current loss 0.14537770352768492 Accuracy 92.26860074995066\n",
      "Training:: Epoch 109, Iteration 50, Current loss 0.20632390633400943 Accuracy 86.1026453736438\n",
      "Training:: Epoch 109, Iteration 60, Current loss 0.17390230043062038 Accuracy 89.25353925353926\n",
      "Training:: Epoch 109, Iteration 70, Current loss 0.1927006925694879 Accuracy 89.04425739885963\n",
      "Training:: Epoch 109, Iteration 80, Current loss 0.20159069706699148 Accuracy 82.79529411764706\n",
      "Training:: Epoch 109, Iteration 90, Current loss 0.1544984602030828 Accuracy 90.64474173169825\n",
      "Training:: Epoch 109, Iteration 100, Current loss 0.2183672172707976 Accuracy 82.43785769197916\n",
      "Training:: Epoch 109, Iteration 110, Current loss 0.17491419681028236 Accuracy 90.57231061505581\n",
      "Training:: Epoch 109, Iteration 120, Current loss 0.1938213891383914 Accuracy 87.71772668458246\n",
      "Training:: Epoch 109, Iteration 130, Current loss 0.19321169241056962 Accuracy 86.59433221500464\n",
      "Training:: Epoch 109, Iteration 140, Current loss 0.21055468154512347 Accuracy 85.66975158304919\n",
      "Training:: Epoch 109, Iteration 150, Current loss 0.23391423531031486 Accuracy 87.03163881976538\n",
      "Training:: Epoch 109, Iteration 160, Current loss 0.12796106100229737 Accuracy 92.71426975081015\n",
      "Training:: Epoch 109, Iteration 170, Current loss 0.2101899530702697 Accuracy 82.35503696446068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 109, Iteration 180, Current loss 0.20581638551844322 Accuracy 89.36032272006915\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 109, Probability Accuracy 68.54786693100023\n",
      "Starting Training\n",
      "Training:: Epoch 110, Iteration 0, Current loss 0.172335214031516 Accuracy 81.3769521282025\n",
      "Training:: Epoch 110, Iteration 10, Current loss 0.2155070755516088 Accuracy 84.82649498035792\n",
      "Training:: Epoch 110, Iteration 20, Current loss 0.20367888026404032 Accuracy 84.13036078965283\n",
      "Training:: Epoch 110, Iteration 30, Current loss 0.14795036226274752 Accuracy 89.99367612005643\n",
      "Training:: Epoch 110, Iteration 40, Current loss 0.1632001364956464 Accuracy 85.52838427947599\n",
      "Training:: Epoch 110, Iteration 50, Current loss 0.16344968717693667 Accuracy 85.84827258696427\n",
      "Training:: Epoch 110, Iteration 60, Current loss 0.14421560000990796 Accuracy 89.55103359173127\n",
      "Training:: Epoch 110, Iteration 70, Current loss 0.20551466087290354 Accuracy 87.61680202602393\n",
      "Training:: Epoch 110, Iteration 80, Current loss 0.13653296790381947 Accuracy 91.99505917041877\n",
      "Training:: Epoch 110, Iteration 90, Current loss 0.17211991534171478 Accuracy 91.24737097557029\n",
      "Training:: Epoch 110, Iteration 100, Current loss 0.1840544363022522 Accuracy 91.52178579094226\n",
      "Training:: Epoch 110, Iteration 110, Current loss 0.17934597872270464 Accuracy 82.56493506493507\n",
      "Training:: Epoch 110, Iteration 120, Current loss 0.1907299537655368 Accuracy 88.88574126534466\n",
      "Training:: Epoch 110, Iteration 130, Current loss 0.14385376580073034 Accuracy 92.4829257545715\n",
      "Training:: Epoch 110, Iteration 140, Current loss 0.21269600430894425 Accuracy 82.88585251741188\n",
      "Training:: Epoch 110, Iteration 150, Current loss 0.13014234783700038 Accuracy 89.0438802757961\n",
      "Training:: Epoch 110, Iteration 160, Current loss 0.1895801344044995 Accuracy 88.24004975124379\n",
      "Training:: Epoch 110, Iteration 170, Current loss 0.13401258293361978 Accuracy 92.57138580142959\n",
      "Training:: Epoch 110, Iteration 180, Current loss 0.1604636530906266 Accuracy 89.20238255183841\n",
      "Calculating Expectation\n",
      "Epoch 110 iter 0\n",
      "Epoch 110 iter 10\n",
      "Epoch 110 iter 20\n",
      "Epoch 110 iter 30\n",
      "Epoch 110 iter 40\n",
      "Epoch 110 iter 50\n",
      "Epoch 110 iter 60\n",
      "Epoch 110 iter 70\n",
      "Epoch 110 iter 80\n",
      "Epoch 110 iter 90\n",
      "Epoch 110 iter 100\n",
      "Epoch 110 iter 110\n",
      "Epoch 110 iter 120\n",
      "Epoch 110 iter 130\n",
      "Epoch 110 iter 140\n",
      "Epoch 110 iter 150\n",
      "Epoch 110 iter 160\n",
      "Epoch 110 iter 170\n",
      "Epoch 110 iter 180\n",
      "Train Boundary avergage error = 87.507\n",
      "Train From boundary avergage accuracy = 88.023\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 110, Probability Accuracy 68.64521132835532\n",
      "Starting Training\n",
      "Training:: Epoch 111, Iteration 0, Current loss 0.14575330323273125 Accuracy 89.4907908992416\n",
      "Training:: Epoch 111, Iteration 10, Current loss 0.1772164700520151 Accuracy 89.69055240909464\n",
      "Training:: Epoch 111, Iteration 20, Current loss 0.2205913110864215 Accuracy 86.06803980560055\n",
      "Training:: Epoch 111, Iteration 30, Current loss 0.13133524761304985 Accuracy 85.87770382695507\n",
      "Training:: Epoch 111, Iteration 40, Current loss 0.1632939626900554 Accuracy 86.41522988505747\n",
      "Training:: Epoch 111, Iteration 50, Current loss 0.16039630538573482 Accuracy 89.48664034053029\n",
      "Training:: Epoch 111, Iteration 60, Current loss 0.17723245047079764 Accuracy 80.35697149985606\n",
      "Training:: Epoch 111, Iteration 70, Current loss 0.17065649329417026 Accuracy 90.84116856950973\n",
      "Training:: Epoch 111, Iteration 80, Current loss 0.23886544446246005 Accuracy 89.31895093062606\n",
      "Training:: Epoch 111, Iteration 90, Current loss 0.2440684109940017 Accuracy 85.12601260126013\n",
      "Training:: Epoch 111, Iteration 100, Current loss 0.14771670178810622 Accuracy 86.95467019967825\n",
      "Training:: Epoch 111, Iteration 110, Current loss 0.24344714442818707 Accuracy 85.06307339449542\n",
      "Training:: Epoch 111, Iteration 120, Current loss 0.15585257273689557 Accuracy 87.45982657206962\n",
      "Training:: Epoch 111, Iteration 130, Current loss 0.2076956411662488 Accuracy 84.97827816190419\n",
      "Training:: Epoch 111, Iteration 140, Current loss 0.1616549133167919 Accuracy 85.66176470588235\n",
      "Training:: Epoch 111, Iteration 150, Current loss 0.11534234603206919 Accuracy 92.15463539758153\n",
      "Training:: Epoch 111, Iteration 160, Current loss 0.1469643431432304 Accuracy 84.74739374498797\n",
      "Training:: Epoch 111, Iteration 170, Current loss 0.2019128774941257 Accuracy 84.14813203545597\n",
      "Training:: Epoch 111, Iteration 180, Current loss 0.16299907361612354 Accuracy 92.84432154465628\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 111, Probability Accuracy 68.48277281163068\n",
      "Starting Training\n",
      "Training:: Epoch 112, Iteration 0, Current loss 0.44789642037536076 Accuracy 82.00824781816438\n",
      "Training:: Epoch 112, Iteration 10, Current loss 0.22954515554640842 Accuracy 88.057051547612\n",
      "Training:: Epoch 112, Iteration 20, Current loss 0.19223322361643916 Accuracy 88.37932008663716\n",
      "Training:: Epoch 112, Iteration 30, Current loss 0.15388530966747488 Accuracy 90.52689961175818\n",
      "Training:: Epoch 112, Iteration 40, Current loss 0.2051195412712157 Accuracy 88.37363873408661\n",
      "Training:: Epoch 112, Iteration 50, Current loss 0.18515352401540344 Accuracy 87.13486289871292\n",
      "Training:: Epoch 112, Iteration 60, Current loss 0.22998874192970284 Accuracy 87.92235801581596\n",
      "Training:: Epoch 112, Iteration 70, Current loss 0.21956824926662635 Accuracy 89.64566609908375\n",
      "Training:: Epoch 112, Iteration 80, Current loss 0.19640318981919147 Accuracy 87.92693169092945\n",
      "Training:: Epoch 112, Iteration 90, Current loss 0.2706212647165812 Accuracy 87.98863321319656\n",
      "Training:: Epoch 112, Iteration 100, Current loss 0.14979854929758202 Accuracy 85.1921809795739\n",
      "Training:: Epoch 112, Iteration 110, Current loss 0.17348201739652613 Accuracy 87.89630259243519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3bc684dbca6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmiddle_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mboundary_target_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_single_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_stages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mfinal_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_dilated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 259\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initialize_epoch = 15\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0\n",
    "for epoch in range(15, 1000):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        boundary_target_tensor = get_single_random(item_2, item[4])\n",
    "        \n",
    "        loss = 0\n",
    "        for p in predictions:\n",
    "            if epoch <= initialize_epoch:\n",
    "                loss += ce_criterion(p, boundary_target_tensor)\n",
    "                loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(p[:, :, 1:], dim=1), \n",
    "                                                                    F.log_softmax(p.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                            max=16) * src_mask_mse[:, :, 1:])\n",
    "            else:\n",
    "                prob = torch.softmax(p, dim=1)\n",
    "                prob = prob.permute(0, 2, 1)\n",
    "                es_loss, _ = get_estimated_loss(prob, item_1, item[4], item_2)\n",
    "                loss += es_loss\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(predictions[-1], dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "    if epoch == initialize_epoch:\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-initial-15-epochs.wt\")\n",
    "    # Calculating Expectation Step\n",
    "    model.eval()\n",
    "    if (epoch >= initialize_epoch) and (epoch % expectation_cal_gap == 0):\n",
    "        print(\"Calculating Expectation\")\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        for i, item in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                item_0 = item[0].to(device)\n",
    "                item_1 = item[1].to(device)\n",
    "                item_2 = item[2].to(device)\n",
    "                src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "                src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "                middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "                prob = torch.softmax(predictions[-1], dim=1)\n",
    "                prob = prob.permute(0, 2, 1)\n",
    "                calculate_element_probb(prob, item_1, item[4], item_2)\n",
    "                \n",
    "                if i % 10 == 0:\n",
    "#                     pred = torch.argmax(prob, dim=2)\n",
    "#                     correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "#                     total += float(torch.sum(src_mask).item())\n",
    "                    print(f\"Epoch {epoch} iter {i}\")\n",
    "                    \n",
    "#         print(f\"Epoch {epoch} After Expectation}, train acc. {correct * 100.0 / total: .3f}\")\n",
    "        get_boundary_err()\n",
    "\n",
    "    print(\"Calculating Validation Data Accuracy\")\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, item in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            item_0 = item[0].to(device)\n",
    "            item_1 = item[1].to(device)\n",
    "            item_2 = item[2].to(device)\n",
    "            src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "            src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "            middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "            pred = torch.argmax(predictions[-1], dim=1)\n",
    "            correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "            total += float(torch.sum(src_mask).item())\n",
    "    val_acc = correct * 100.0 / total\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), config.output_dir + \"ms-tcn-emmax-best-model.wt\")\n",
    "    torch.save(model.state_dict(), config.output_dir + \"ms-tcn-emmax-last-model.wt\")\n",
    "    print(f\"Validation:: Epoch {epoch}, Probability Accuracy {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\n",
    "\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-speed/final-em-maximized.wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-split1/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(config.output_dir + \"ms-tcn-emmax-last-model.wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Probability Accuracy 68.77104676883872\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "\n",
    "print(f\"Validation:: Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration done\n",
      "11 iteration done\n",
      "21 iteration done\n",
      "31 iteration done\n",
      "41 iteration done\n",
      "51 iteration done\n",
      "61 iteration done\n",
      "71 iteration done\n",
      "81 iteration done\n",
      "91 iteration done\n",
      "101 iteration done\n",
      "111 iteration done\n",
      "121 iteration done\n",
      "131 iteration done\n",
      "141 iteration done\n",
      "151 iteration done\n",
      "161 iteration done\n",
      "171 iteration done\n",
      "181 iteration done\n",
      "Train Boundary avergage error = 87.745\n",
      "Train From boundary avergage accuracy = 87.956\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        calculate_element_probb(prob, item_1, item[4], item_2)\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f'{i+1} iteration done')\n",
    "get_boundary_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    idx = 1\n",
    "\n",
    "    cur_vid_feat = torch.softmax(predictions[-1], dim=1).permute(0, 2, 1)[idx]\n",
    "    cur_vidid = item[4][idx]\n",
    "\n",
    "    selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "    \n",
    "    bound_list = video_id_boundary_frames[cur_vidid]\n",
    "    \n",
    "    cumsum_feat = torch.cumsum(cur_vid_feat, dim=0)\n",
    "    prob_each_segment = prob_vals_per_segment(selected_frames, cur_vid_feat, item_2[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min prob 0 = 5.513831629761545e-121\n",
      "Min prob 1 = 0.0\n",
      "Min prob 2 = 0.0\n",
      "Min prob 3 = 3.129120345015135e-27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1622)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ymax = 1\n",
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "\n",
    "plt.plot([0, item[1][idx].item()], [0, 0])\n",
    "\n",
    "for pivot in loaded_vidid_selected_frames[cur_vidid + \".txt\"]:\n",
    "    plt.plot([pivot, pivot], [0, ymax], '--')\n",
    "    \n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax])\n",
    "    \n",
    "all_prob = []\n",
    "for i, segment in enumerate(prob_each_segment):\n",
    "    segment = segment.cpu().numpy()\n",
    "    print(f\"Min prob {i} = {np.min(segment).item()}\")\n",
    "    xs = np.arange(loaded_vidid_selected_frames[cur_vidid + \".txt\"][i], \n",
    "                   loaded_vidid_selected_frames[cur_vidid + \".txt\"][i+1])\n",
    "    plt.plot(xs, segment, '*')\n",
    "\n",
    "    \n",
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "\n",
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))\n",
    "loss_arr = -loss_arr[0].detach().cpu().numpy()\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr/np.max(loss_arr), 'k+')\n",
    "\n",
    "plt.ylim([0, ymax])\n",
    "plt.xlim([0, item[1][idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1895, 48])\n"
     ]
    }
   ],
   "source": [
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "print(loss_prob.shape)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1387])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf5b8ecb00>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3987, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([75, 345, 1305, 1415, 1440, 1519],\n",
       " [73, 282, 455, 1312, 1434, 1458],\n",
       " tensor(1520, device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid], loaded_vidid_selected_frames[cur_vidid + \".txt\"], item_1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_features_shortened = cur_vid_feat[:item[1][idx]]\n",
    "cur_features_shortened[:, item[2][idx][:item[1][idx]]].shape\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 47, 47, 47, 47, 47, 47,  4,  4,  4,  4,  4,  4,  4,  4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[2][idx][video_id_boundary_frames[cur_vidid][1]-6:video_id_boundary_frames[cur_vidid][1]+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid][0]-6,video_id_boundary_frames[cur_vidid][0]+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0794, 13.3350],\n",
       "        [ 1.0831, 13.5102],\n",
       "        [ 1.1767, 13.7662],\n",
       "        [ 1.3750, 14.0177],\n",
       "        [ 1.7298, 14.1727],\n",
       "        [ 2.1706, 14.0367],\n",
       "        [ 2.7338, 13.4563],\n",
       "        [ 4.5600, 12.1745],\n",
       "        [ 7.8450, 10.3246],\n",
       "        [11.0151,  8.3563],\n",
       "        [13.4775,  6.6546],\n",
       "        [14.1836,  5.2898],\n",
       "        [11.2142,  4.2791],\n",
       "        [ 5.9829,  3.5694],\n",
       "        [ 2.3440,  2.9389],\n",
       "        [ 0.7406,  2.3515]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_vid_feat[video_id_boundary_frames[cur_vidid][1]-7:video_id_boundary_frames[cur_vidid][1]+9][:,[47,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frames_and_get_classification_value(model_output, count_list, video_ids_list, labels_all, p_criterion):\n",
    "    new_selected_frames = {}\n",
    "#     new_selected_frames_probs_values = {}\n",
    "#     new_selected_frames_labels = {}\n",
    "    \n",
    "    labels_arr = []\n",
    "    probs_arr = []\n",
    "    \n",
    "    for i, video_id in enumerate(video_ids_list):\n",
    "        current_boundary_frames = boundaries_dict[video_id]\n",
    "        current_video_logits = model_output[i].T\n",
    "        current_video_count = count_list[i]\n",
    "        current_selected_frames = loaded_vidid_selected_frames[video_id]\n",
    "        labels = labels_all[i]\n",
    "        cur_seg_start = 0\n",
    "        for seg_num, ele in enumerate(current_boundary_frames):\n",
    "            cur_seg_end = ele + 1\n",
    "            total_seg_len = cur_seg_end - cur_seg_start\n",
    "            per_frames_far_from_boundary = int(0.3 * total_seg_len)\n",
    "            new_frames_start = cur_seg_start + per_frames_far_from_boundary\n",
    "            new_frames_end = cur_seg_end - per_frames_far_from_boundary\n",
    "            \n",
    "            frames_added = list(range(new_frames_start, new_frames_end))\n",
    "            frames_added.append(current_selected_frames[seg_num])\n",
    "            frames_added = list(set(frames_added))\n",
    "            frames_added.sort()\n",
    "            new_selected_frames[video_id] = frames_added\n",
    "\n",
    "            \n",
    "            labels_seg = torch.stack([labels[current_selected_frames[seg_num]]] * len(frames_added))\n",
    "            frames_added = torch.tensor(np.array(frames_added), dtype=torch.long, device=current_video_logits.device)\n",
    "            prob_seg = current_video_logits[frames_added, :]\n",
    "            \n",
    "            labels_arr.append(labels_seg)\n",
    "            probs_arr.append(prob_seg)\n",
    "            \n",
    "            cur_seg_start = cur_seg_end\n",
    "            \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    probs_arr = torch.cat(probs_arr)\n",
    "    loss = p_criterion(probs_arr, labels_arr)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
