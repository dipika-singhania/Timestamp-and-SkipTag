{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mstcn_model import *\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 8, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 2, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize/', 'project_name': 'breakfast-split-2', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split2.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split2.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split2_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=2,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize/\",\n",
    "    project_name=\"breakfast-split-2\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1261\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 451\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, collate_fn=collate_fn_override,\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4, collate_fn=collate_fn_override,\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", allow_pickle=True).item()\n",
    "# loaded_vidid_selected_frames\n",
    "video_id_boundary_frames = pickle.load(open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"rb\"))\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_loss(features, labels_all, video_ids):\n",
    "    global ce_criterion\n",
    "    labels_arr = []\n",
    "    selected_probs_arr = []\n",
    "    for iter_num in range(len(features)):\n",
    "        cur_vid_feat = features[iter_num].T\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        selected_frames = torch.tensor(loaded_vidid_selected_frames[cur_vidid + \".txt\"], dtype=torch.long, \n",
    "                                       device=cur_vid_feat.device)\n",
    "        selected_labels = labels[selected_frames]\n",
    "        selected_probs = cur_vid_feat[selected_frames, :]\n",
    "        \n",
    "        labels_arr.append(selected_labels)\n",
    "        selected_probs_arr.append(selected_probs)\n",
    "        \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    selected_probs_arr = torch.cat(selected_probs_arr)\n",
    "    \n",
    "    return ce_criterion(selected_probs_arr, labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = MultiStageModel(num_stages=4, num_layers=10, num_f_maps=64, dim=2048, num_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(labels_all, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((labels_all.shape[0], labels_all.shape[1]), dtype=torch.long, device=labels_all.device) * (-100)\n",
    "    for iter_num, labels in enumerate(labels_all):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(loaded_vidid_selected_frames[cur_vidid + \".txt\"]))\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = labels[frame_idx_tensor]\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Training:: Epoch 0, Iteration 0, Current loss 15.664618492126465 Accuracy 2.481057792304264\n",
      "Training:: Epoch 0, Iteration 10, Current loss 13.902303695678711 Accuracy 4.390785456563974\n",
      "Training:: Epoch 0, Iteration 20, Current loss 12.930048942565918 Accuracy 19.981893429901707\n",
      "Training:: Epoch 0, Iteration 30, Current loss 12.422285079956055 Accuracy 14.079311404857055\n",
      "Training:: Epoch 0, Iteration 40, Current loss 11.979695320129395 Accuracy 7.010842779694431\n",
      "Training:: Epoch 0, Iteration 50, Current loss 11.644539833068848 Accuracy 5.0834597875569045\n",
      "Training:: Epoch 0, Iteration 60, Current loss 11.07874584197998 Accuracy 5.0526003852422585\n",
      "Training:: Epoch 0, Iteration 70, Current loss 11.063798904418945 Accuracy 6.820054509729353\n",
      "Training:: Epoch 0, Iteration 80, Current loss 10.39537525177002 Accuracy 10.014086310667178\n",
      "Training:: Epoch 0, Iteration 90, Current loss 10.942529678344727 Accuracy 7.298772169167804\n",
      "Training:: Epoch 0, Iteration 100, Current loss 10.188672065734863 Accuracy 14.219924812030076\n",
      "Training:: Epoch 0, Iteration 110, Current loss 9.837943077087402 Accuracy 12.469547956329514\n",
      "Training:: Epoch 0, Iteration 120, Current loss 10.15499496459961 Accuracy 10.994059405940595\n",
      "Training:: Epoch 0, Iteration 130, Current loss 10.588399887084961 Accuracy 12.161342970389368\n",
      "Training:: Epoch 0, Iteration 140, Current loss 10.23259449005127 Accuracy 14.867602373028506\n",
      "Training:: Epoch 0, Iteration 150, Current loss 9.429178237915039 Accuracy 12.685383826135622\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 0, Probability Accuracy 13.594585076852963\n",
      "Starting Training\n",
      "Training:: Epoch 1, Iteration 0, Current loss 9.348538398742676 Accuracy 19.09875620199823\n",
      "Training:: Epoch 1, Iteration 10, Current loss 9.473282814025879 Accuracy 19.346929677922656\n",
      "Training:: Epoch 1, Iteration 20, Current loss 8.3357572555542 Accuracy 14.125463801881095\n",
      "Training:: Epoch 1, Iteration 30, Current loss 9.632308006286621 Accuracy 17.753833772969106\n",
      "Training:: Epoch 1, Iteration 40, Current loss 8.809183120727539 Accuracy 12.719298245614034\n",
      "Training:: Epoch 1, Iteration 50, Current loss 9.923053741455078 Accuracy 10.857469710363477\n",
      "Training:: Epoch 1, Iteration 60, Current loss 9.644932746887207 Accuracy 17.99878897971541\n",
      "Training:: Epoch 1, Iteration 70, Current loss 8.342902183532715 Accuracy 11.866740088105727\n",
      "Training:: Epoch 1, Iteration 80, Current loss 10.490781784057617 Accuracy 11.211105792245093\n",
      "Training:: Epoch 1, Iteration 90, Current loss 9.997740745544434 Accuracy 11.534920426147574\n",
      "Training:: Epoch 1, Iteration 100, Current loss 8.05335807800293 Accuracy 17.59421013226853\n",
      "Training:: Epoch 1, Iteration 110, Current loss 7.703002452850342 Accuracy 16.60006148170919\n",
      "Training:: Epoch 1, Iteration 120, Current loss 8.063846588134766 Accuracy 16.63681146439767\n",
      "Training:: Epoch 1, Iteration 130, Current loss 8.12143611907959 Accuracy 11.581686320101866\n",
      "Training:: Epoch 1, Iteration 140, Current loss 8.553008079528809 Accuracy 16.068002320289143\n",
      "Training:: Epoch 1, Iteration 150, Current loss 9.582049369812012 Accuracy 9.907373101148574\n",
      "Starting Training\n",
      "Training:: Epoch 2, Iteration 0, Current loss 8.949381828308105 Accuracy 16.681542306319173\n",
      "Training:: Epoch 2, Iteration 10, Current loss 7.833477973937988 Accuracy 15.640206438928484\n",
      "Training:: Epoch 2, Iteration 20, Current loss 7.767874717712402 Accuracy 13.908127928294968\n",
      "Training:: Epoch 2, Iteration 30, Current loss 7.546760559082031 Accuracy 22.014223820812557\n",
      "Training:: Epoch 2, Iteration 40, Current loss 8.704032897949219 Accuracy 12.767336357744654\n",
      "Training:: Epoch 2, Iteration 50, Current loss 8.459221839904785 Accuracy 17.354644768678057\n",
      "Training:: Epoch 2, Iteration 60, Current loss 8.008007049560547 Accuracy 15.161522757295545\n",
      "Training:: Epoch 2, Iteration 70, Current loss 7.683829307556152 Accuracy 9.131989469134346\n",
      "Training:: Epoch 2, Iteration 80, Current loss 7.747324466705322 Accuracy 14.441121657348416\n",
      "Training:: Epoch 2, Iteration 90, Current loss 6.884059906005859 Accuracy 21.605778289542553\n",
      "Training:: Epoch 2, Iteration 100, Current loss 7.412039279937744 Accuracy 20.86948231480599\n",
      "Training:: Epoch 2, Iteration 110, Current loss 6.2701215744018555 Accuracy 18.07718933887158\n",
      "Training:: Epoch 2, Iteration 120, Current loss 8.03615951538086 Accuracy 17.59923895182911\n",
      "Training:: Epoch 2, Iteration 130, Current loss 7.207816123962402 Accuracy 11.174749163879598\n",
      "Training:: Epoch 2, Iteration 140, Current loss 7.620447158813477 Accuracy 15.49953314659197\n",
      "Training:: Epoch 2, Iteration 150, Current loss 7.4942307472229 Accuracy 22.60206591244466\n",
      "Starting Training\n",
      "Training:: Epoch 3, Iteration 0, Current loss 7.367825508117676 Accuracy 17.05358263270172\n",
      "Training:: Epoch 3, Iteration 10, Current loss 6.92625093460083 Accuracy 16.481570899726382\n",
      "Training:: Epoch 3, Iteration 20, Current loss 6.360539436340332 Accuracy 24.36794347856203\n",
      "Training:: Epoch 3, Iteration 30, Current loss 6.183032512664795 Accuracy 25.254928619986405\n",
      "Training:: Epoch 3, Iteration 40, Current loss 6.394251346588135 Accuracy 26.120538986299824\n",
      "Training:: Epoch 3, Iteration 50, Current loss 7.061509609222412 Accuracy 20.76192774961783\n",
      "Training:: Epoch 3, Iteration 60, Current loss 5.978770732879639 Accuracy 25.87981805123294\n",
      "Training:: Epoch 3, Iteration 70, Current loss 6.704959869384766 Accuracy 20.897014712183974\n",
      "Training:: Epoch 3, Iteration 80, Current loss 5.176036357879639 Accuracy 26.324214202561116\n",
      "Training:: Epoch 3, Iteration 90, Current loss 8.640941619873047 Accuracy 25.107758620689655\n",
      "Training:: Epoch 3, Iteration 100, Current loss 6.16516637802124 Accuracy 32.73581406848807\n",
      "Training:: Epoch 3, Iteration 110, Current loss 7.083735466003418 Accuracy 21.64482373260185\n",
      "Training:: Epoch 3, Iteration 120, Current loss 6.56320858001709 Accuracy 35.20969055374593\n",
      "Training:: Epoch 3, Iteration 130, Current loss 5.334532737731934 Accuracy 32.63417997931272\n",
      "Training:: Epoch 3, Iteration 140, Current loss 7.1684746742248535 Accuracy 27.172867628448493\n",
      "Training:: Epoch 3, Iteration 150, Current loss 5.874478816986084 Accuracy 38.16292403248925\n",
      "Starting Training\n",
      "Training:: Epoch 4, Iteration 0, Current loss 6.673820972442627 Accuracy 27.311871025884944\n",
      "Training:: Epoch 4, Iteration 10, Current loss 6.2496724128723145 Accuracy 25.86032165675259\n",
      "Training:: Epoch 4, Iteration 20, Current loss 5.95979642868042 Accuracy 23.652920364248015\n",
      "Training:: Epoch 4, Iteration 30, Current loss 5.652010917663574 Accuracy 40.49427290836653\n",
      "Training:: Epoch 4, Iteration 40, Current loss 4.978234767913818 Accuracy 39.65468440418908\n",
      "Training:: Epoch 4, Iteration 50, Current loss 6.139676094055176 Accuracy 26.6071668232452\n",
      "Training:: Epoch 4, Iteration 60, Current loss 6.104470729827881 Accuracy 25.6508346581876\n",
      "Training:: Epoch 4, Iteration 70, Current loss 5.951351165771484 Accuracy 38.426097548059104\n",
      "Training:: Epoch 4, Iteration 80, Current loss 5.45802640914917 Accuracy 29.275885404917663\n",
      "Training:: Epoch 4, Iteration 90, Current loss 5.2850823402404785 Accuracy 29.549780683605064\n",
      "Training:: Epoch 4, Iteration 100, Current loss 6.180172920227051 Accuracy 30.643162098884453\n",
      "Training:: Epoch 4, Iteration 110, Current loss 6.624989986419678 Accuracy 31.561660197073753\n",
      "Training:: Epoch 4, Iteration 120, Current loss 6.825196743011475 Accuracy 31.200800243552386\n",
      "Training:: Epoch 4, Iteration 130, Current loss 5.994378566741943 Accuracy 40.444418317559226\n",
      "Training:: Epoch 4, Iteration 140, Current loss 6.953896999359131 Accuracy 20.430023455824863\n",
      "Training:: Epoch 4, Iteration 150, Current loss 5.258980751037598 Accuracy 41.41287284144427\n",
      "Starting Training\n",
      "Training:: Epoch 5, Iteration 0, Current loss 5.007814884185791 Accuracy 35.51326053042121\n",
      "Training:: Epoch 5, Iteration 10, Current loss 4.887813568115234 Accuracy 35.30730677921393\n",
      "Training:: Epoch 5, Iteration 20, Current loss 5.804327964782715 Accuracy 32.84448025785657\n",
      "Training:: Epoch 5, Iteration 30, Current loss 4.384613037109375 Accuracy 40.83394833948339\n",
      "Training:: Epoch 5, Iteration 40, Current loss 5.875267505645752 Accuracy 29.755934470076898\n",
      "Training:: Epoch 5, Iteration 50, Current loss 4.194594860076904 Accuracy 43.77905554196232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 5, Iteration 60, Current loss 5.629953384399414 Accuracy 24.757574681206265\n",
      "Training:: Epoch 5, Iteration 70, Current loss 3.899505138397217 Accuracy 48.69525959367946\n",
      "Training:: Epoch 5, Iteration 80, Current loss 4.743191719055176 Accuracy 33.640492331995\n",
      "Training:: Epoch 5, Iteration 90, Current loss 5.967964172363281 Accuracy 45.02266241396676\n",
      "Training:: Epoch 5, Iteration 100, Current loss 5.064793109893799 Accuracy 41.04717835227844\n",
      "Training:: Epoch 5, Iteration 110, Current loss 4.966911315917969 Accuracy 41.03070002367611\n",
      "Training:: Epoch 5, Iteration 120, Current loss 5.202577114105225 Accuracy 45.81308713961775\n",
      "Training:: Epoch 5, Iteration 130, Current loss 4.803657531738281 Accuracy 41.328596802841915\n",
      "Training:: Epoch 5, Iteration 140, Current loss 4.6027750968933105 Accuracy 37.438042131350684\n",
      "Training:: Epoch 5, Iteration 150, Current loss 4.8541741371154785 Accuracy 47.042988824537076\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 5, Probability Accuracy 36.44860587479803\n",
      "Starting Training\n",
      "Training:: Epoch 6, Iteration 0, Current loss 4.066253662109375 Accuracy 50.25248661055853\n",
      "Training:: Epoch 6, Iteration 10, Current loss 4.833798408508301 Accuracy 51.582089552238806\n",
      "Training:: Epoch 6, Iteration 20, Current loss 4.676763534545898 Accuracy 40.94255291179177\n",
      "Training:: Epoch 6, Iteration 30, Current loss 4.957111358642578 Accuracy 46.694986462515175\n",
      "Training:: Epoch 6, Iteration 40, Current loss 4.092615127563477 Accuracy 46.875960651706116\n",
      "Training:: Epoch 6, Iteration 50, Current loss 4.079898834228516 Accuracy 51.87938235181424\n",
      "Training:: Epoch 6, Iteration 60, Current loss 4.177460193634033 Accuracy 43.760267376649864\n",
      "Training:: Epoch 6, Iteration 70, Current loss 4.5955023765563965 Accuracy 40.694040306067464\n",
      "Training:: Epoch 6, Iteration 80, Current loss 4.689939022064209 Accuracy 59.774716067771365\n",
      "Training:: Epoch 6, Iteration 90, Current loss 4.182360649108887 Accuracy 51.421608448415924\n",
      "Training:: Epoch 6, Iteration 100, Current loss 5.275055408477783 Accuracy 50.281941961215786\n",
      "Training:: Epoch 6, Iteration 110, Current loss 4.227292537689209 Accuracy 50.493072781172486\n",
      "Training:: Epoch 6, Iteration 120, Current loss 3.5502631664276123 Accuracy 46.31568451490677\n",
      "Training:: Epoch 6, Iteration 130, Current loss 4.810209274291992 Accuracy 54.24325381306218\n",
      "Training:: Epoch 6, Iteration 140, Current loss 4.058440208435059 Accuracy 52.49556737588652\n",
      "Training:: Epoch 6, Iteration 150, Current loss 3.7919538021087646 Accuracy 62.05789592250507\n",
      "Starting Training\n",
      "Training:: Epoch 7, Iteration 0, Current loss 3.629155397415161 Accuracy 55.761525722527914\n",
      "Training:: Epoch 7, Iteration 10, Current loss 5.421952724456787 Accuracy 50.941284623686\n",
      "Training:: Epoch 7, Iteration 20, Current loss 3.796764373779297 Accuracy 59.946403790017705\n",
      "Training:: Epoch 7, Iteration 30, Current loss 4.423155307769775 Accuracy 52.8135565931412\n",
      "Training:: Epoch 7, Iteration 40, Current loss 4.140315055847168 Accuracy 42.763536866359445\n",
      "Training:: Epoch 7, Iteration 50, Current loss 3.8808631896972656 Accuracy 56.424167694204684\n",
      "Training:: Epoch 7, Iteration 60, Current loss 4.130268573760986 Accuracy 50.788245680413674\n",
      "Training:: Epoch 7, Iteration 70, Current loss 3.9689791202545166 Accuracy 40.953375086986775\n",
      "Training:: Epoch 7, Iteration 80, Current loss 5.2983808517456055 Accuracy 32.57992385326645\n",
      "Training:: Epoch 7, Iteration 90, Current loss 3.6787922382354736 Accuracy 59.0975788701394\n",
      "Training:: Epoch 7, Iteration 100, Current loss 3.528428077697754 Accuracy 50.27438196142353\n",
      "Training:: Epoch 7, Iteration 110, Current loss 3.885584592819214 Accuracy 69.92264795826587\n",
      "Training:: Epoch 7, Iteration 120, Current loss 4.03244686126709 Accuracy 55.316669579910275\n",
      "Training:: Epoch 7, Iteration 130, Current loss 3.428389072418213 Accuracy 57.37807419758233\n",
      "Training:: Epoch 7, Iteration 140, Current loss 5.06784725189209 Accuracy 47.77200026327914\n",
      "Training:: Epoch 7, Iteration 150, Current loss 3.7477879524230957 Accuracy 48.6235252055774\n",
      "Starting Training\n",
      "Training:: Epoch 8, Iteration 0, Current loss 5.457237720489502 Accuracy 38.26924099781927\n",
      "Training:: Epoch 8, Iteration 10, Current loss 5.118294715881348 Accuracy 49.210564087842684\n",
      "Training:: Epoch 8, Iteration 20, Current loss 2.690946102142334 Accuracy 62.87716670233255\n",
      "Training:: Epoch 8, Iteration 30, Current loss 4.176037311553955 Accuracy 54.110811181772426\n",
      "Training:: Epoch 8, Iteration 40, Current loss 4.125854969024658 Accuracy 62.206346423562415\n",
      "Training:: Epoch 8, Iteration 50, Current loss 2.9286201000213623 Accuracy 72.34651182648791\n",
      "Training:: Epoch 8, Iteration 60, Current loss 3.4939615726470947 Accuracy 53.17443120260022\n",
      "Training:: Epoch 8, Iteration 70, Current loss 3.9722249507904053 Accuracy 56.107277512982854\n",
      "Training:: Epoch 8, Iteration 80, Current loss 5.003778457641602 Accuracy 55.507419040398574\n",
      "Training:: Epoch 8, Iteration 90, Current loss 3.9543895721435547 Accuracy 55.1366876955429\n",
      "Training:: Epoch 8, Iteration 100, Current loss 2.517970085144043 Accuracy 63.57565350124022\n",
      "Training:: Epoch 8, Iteration 110, Current loss 2.414628267288208 Accuracy 70.18273659188827\n",
      "Training:: Epoch 8, Iteration 120, Current loss 2.808901309967041 Accuracy 75.5618979664645\n",
      "Training:: Epoch 8, Iteration 130, Current loss 3.1207892894744873 Accuracy 72.31836111464563\n",
      "Training:: Epoch 8, Iteration 140, Current loss 3.939128875732422 Accuracy 66.94661831138258\n",
      "Training:: Epoch 8, Iteration 150, Current loss 3.90189266204834 Accuracy 51.228972369911965\n",
      "Starting Training\n",
      "Training:: Epoch 9, Iteration 0, Current loss 3.638380289077759 Accuracy 57.3303044687055\n",
      "Training:: Epoch 9, Iteration 10, Current loss 3.6277687549591064 Accuracy 57.28825881497637\n",
      "Training:: Epoch 9, Iteration 20, Current loss 2.9184365272521973 Accuracy 72.36373596274993\n",
      "Training:: Epoch 9, Iteration 30, Current loss 2.6504149436950684 Accuracy 55.77512162892666\n",
      "Training:: Epoch 9, Iteration 40, Current loss 2.6741509437561035 Accuracy 78.99063107654234\n",
      "Training:: Epoch 9, Iteration 50, Current loss 3.2898364067077637 Accuracy 60.83823626627884\n",
      "Training:: Epoch 9, Iteration 60, Current loss 3.376911163330078 Accuracy 60.38440714672442\n",
      "Training:: Epoch 9, Iteration 70, Current loss 3.336435556411743 Accuracy 68.33276275248203\n",
      "Training:: Epoch 9, Iteration 80, Current loss 2.6008427143096924 Accuracy 61.61363960622057\n",
      "Training:: Epoch 9, Iteration 90, Current loss 3.0464978218078613 Accuracy 59.28890543559196\n",
      "Training:: Epoch 9, Iteration 100, Current loss 4.020175933837891 Accuracy 66.29851831651668\n",
      "Training:: Epoch 9, Iteration 110, Current loss 4.274287700653076 Accuracy 53.94215811059256\n",
      "Training:: Epoch 9, Iteration 120, Current loss 2.7120251655578613 Accuracy 66.63225191597506\n",
      "Training:: Epoch 9, Iteration 130, Current loss 2.4970643520355225 Accuracy 69.42857142857143\n",
      "Training:: Epoch 9, Iteration 140, Current loss 3.108748435974121 Accuracy 65.94112510914307\n",
      "Training:: Epoch 9, Iteration 150, Current loss 3.736097574234009 Accuracy 61.57825944886178\n",
      "Starting Training\n",
      "Training:: Epoch 10, Iteration 0, Current loss 3.4953737258911133 Accuracy 71.89383402874363\n",
      "Training:: Epoch 10, Iteration 10, Current loss 2.668963670730591 Accuracy 62.31768137762304\n",
      "Training:: Epoch 10, Iteration 20, Current loss 2.0214755535125732 Accuracy 69.79217306757414\n",
      "Training:: Epoch 10, Iteration 30, Current loss 2.290348768234253 Accuracy 67.39297848244621\n",
      "Training:: Epoch 10, Iteration 40, Current loss 2.969637393951416 Accuracy 74.53530458254095\n",
      "Training:: Epoch 10, Iteration 50, Current loss 2.710163116455078 Accuracy 57.627291242362524\n",
      "Training:: Epoch 10, Iteration 60, Current loss 2.848214626312256 Accuracy 62.90006038647343\n",
      "Training:: Epoch 10, Iteration 70, Current loss 3.0800185203552246 Accuracy 54.266412101630195\n",
      "Training:: Epoch 10, Iteration 80, Current loss 3.245115280151367 Accuracy 60.25071502464553\n",
      "Training:: Epoch 10, Iteration 90, Current loss 4.2422919273376465 Accuracy 55.25888439895766\n",
      "Training:: Epoch 10, Iteration 100, Current loss 3.1702077388763428 Accuracy 64.99812757458494\n",
      "Training:: Epoch 10, Iteration 110, Current loss 2.7087388038635254 Accuracy 65.39039791520497\n",
      "Training:: Epoch 10, Iteration 120, Current loss 2.1306393146514893 Accuracy 57.17834078489816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 10, Iteration 130, Current loss 3.9755606651306152 Accuracy 69.03184961527224\n",
      "Training:: Epoch 10, Iteration 140, Current loss 2.3381810188293457 Accuracy 66.64745990588688\n",
      "Training:: Epoch 10, Iteration 150, Current loss 3.2484538555145264 Accuracy 77.97752808988764\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 10, Probability Accuracy 50.0379086050462\n",
      "Starting Training\n",
      "Training:: Epoch 11, Iteration 0, Current loss 1.8433260917663574 Accuracy 62.32648528595225\n",
      "Training:: Epoch 11, Iteration 10, Current loss 2.206193685531616 Accuracy 66.68054977092878\n",
      "Training:: Epoch 11, Iteration 20, Current loss 1.902950644493103 Accuracy 76.40745622293353\n",
      "Training:: Epoch 11, Iteration 30, Current loss 3.211918592453003 Accuracy 68.82549092256392\n",
      "Training:: Epoch 11, Iteration 40, Current loss 2.6332545280456543 Accuracy 73.51932668329177\n",
      "Training:: Epoch 11, Iteration 50, Current loss 3.0342042446136475 Accuracy 70.60726916999022\n",
      "Training:: Epoch 11, Iteration 60, Current loss 2.967571973800659 Accuracy 73.9034661483641\n",
      "Training:: Epoch 11, Iteration 70, Current loss 2.2705838680267334 Accuracy 58.3321514678769\n",
      "Training:: Epoch 11, Iteration 80, Current loss 2.8690545558929443 Accuracy 73.09822996892312\n",
      "Training:: Epoch 11, Iteration 90, Current loss 2.3893985748291016 Accuracy 69.6229172756504\n",
      "Training:: Epoch 11, Iteration 100, Current loss 2.830611228942871 Accuracy 56.07461797975789\n",
      "Training:: Epoch 11, Iteration 110, Current loss 1.9314075708389282 Accuracy 67.99041202384038\n",
      "Training:: Epoch 11, Iteration 120, Current loss 3.271603584289551 Accuracy 70.61114439784302\n",
      "Training:: Epoch 11, Iteration 130, Current loss 3.687926769256592 Accuracy 67.97549770290965\n",
      "Training:: Epoch 11, Iteration 140, Current loss 2.0395429134368896 Accuracy 76.08191463113742\n",
      "Training:: Epoch 11, Iteration 150, Current loss 3.530550956726074 Accuracy 62.3728813559322\n",
      "Starting Training\n",
      "Training:: Epoch 12, Iteration 0, Current loss 3.9609644412994385 Accuracy 67.03783895246933\n",
      "Training:: Epoch 12, Iteration 10, Current loss 2.020953416824341 Accuracy 59.07899663047548\n",
      "Training:: Epoch 12, Iteration 20, Current loss 2.4081873893737793 Accuracy 80.29948525970987\n",
      "Training:: Epoch 12, Iteration 30, Current loss 1.4785857200622559 Accuracy 67.48436748436748\n",
      "Training:: Epoch 12, Iteration 40, Current loss 2.9095981121063232 Accuracy 61.726525313717005\n",
      "Training:: Epoch 12, Iteration 50, Current loss 2.780118942260742 Accuracy 68.96825396825396\n",
      "Training:: Epoch 12, Iteration 60, Current loss 2.9911670684814453 Accuracy 57.751030870388945\n",
      "Training:: Epoch 12, Iteration 70, Current loss 2.827179431915283 Accuracy 75.13548666811185\n",
      "Training:: Epoch 12, Iteration 80, Current loss 2.1280245780944824 Accuracy 80.75615582506431\n",
      "Training:: Epoch 12, Iteration 90, Current loss 2.2688374519348145 Accuracy 76.7079670519831\n",
      "Training:: Epoch 12, Iteration 100, Current loss 2.8946430683135986 Accuracy 60.879938919641155\n",
      "Training:: Epoch 12, Iteration 110, Current loss 2.7219722270965576 Accuracy 71.87675070028011\n",
      "Training:: Epoch 12, Iteration 120, Current loss 2.265960454940796 Accuracy 63.6184494119096\n",
      "Training:: Epoch 12, Iteration 130, Current loss 2.2602083683013916 Accuracy 77.65228228696058\n",
      "Training:: Epoch 12, Iteration 140, Current loss 2.5705173015594482 Accuracy 78.26725403817915\n",
      "Training:: Epoch 12, Iteration 150, Current loss 2.700155735015869 Accuracy 65.67465914433474\n",
      "Starting Training\n",
      "Training:: Epoch 13, Iteration 0, Current loss 1.2850887775421143 Accuracy 80.81597222222223\n",
      "Training:: Epoch 13, Iteration 10, Current loss 2.65838623046875 Accuracy 77.34697386519944\n",
      "Training:: Epoch 13, Iteration 20, Current loss 1.8251591920852661 Accuracy 68.58302583025831\n",
      "Training:: Epoch 13, Iteration 30, Current loss 1.8487091064453125 Accuracy 74.29890202112036\n",
      "Training:: Epoch 13, Iteration 40, Current loss 2.3757946491241455 Accuracy 76.60550458715596\n",
      "Training:: Epoch 13, Iteration 50, Current loss 1.848341464996338 Accuracy 78.33708814418473\n",
      "Training:: Epoch 13, Iteration 60, Current loss 1.1560652256011963 Accuracy 66.54397834912044\n",
      "Training:: Epoch 13, Iteration 70, Current loss 2.041778326034546 Accuracy 73.92500167706447\n",
      "Training:: Epoch 13, Iteration 80, Current loss 1.721450686454773 Accuracy 69.68886246531906\n",
      "Training:: Epoch 13, Iteration 90, Current loss 2.8351523876190186 Accuracy 68.75508957654723\n",
      "Training:: Epoch 13, Iteration 100, Current loss 2.4427669048309326 Accuracy 78.70118924992977\n",
      "Training:: Epoch 13, Iteration 110, Current loss 2.021526336669922 Accuracy 62.25937520815293\n",
      "Training:: Epoch 13, Iteration 120, Current loss 1.51164972782135 Accuracy 81.88785800726099\n",
      "Training:: Epoch 13, Iteration 130, Current loss 2.450920820236206 Accuracy 70.48007657044985\n",
      "Training:: Epoch 13, Iteration 140, Current loss 2.2681703567504883 Accuracy 71.3629867036251\n",
      "Training:: Epoch 13, Iteration 150, Current loss 1.5073330402374268 Accuracy 57.50465864814501\n",
      "Starting Training\n",
      "Training:: Epoch 14, Iteration 0, Current loss 1.6769979000091553 Accuracy 80.2871556072953\n",
      "Training:: Epoch 14, Iteration 10, Current loss 2.459005117416382 Accuracy 77.52708788513844\n",
      "Training:: Epoch 14, Iteration 20, Current loss 1.6058634519577026 Accuracy 69.02070818842871\n",
      "Training:: Epoch 14, Iteration 30, Current loss 1.2409346103668213 Accuracy 86.74154316475045\n",
      "Training:: Epoch 14, Iteration 40, Current loss 1.571044683456421 Accuracy 74.74992855101458\n",
      "Training:: Epoch 14, Iteration 50, Current loss 2.167219400405884 Accuracy 69.90589055420007\n",
      "Training:: Epoch 14, Iteration 60, Current loss 2.896505832672119 Accuracy 65.97466035719738\n",
      "Training:: Epoch 14, Iteration 70, Current loss 1.9905784130096436 Accuracy 76.16703715465228\n",
      "Training:: Epoch 14, Iteration 80, Current loss 1.9811900854110718 Accuracy 79.65379494007989\n",
      "Training:: Epoch 14, Iteration 90, Current loss 2.1541285514831543 Accuracy 71.00980543149032\n",
      "Training:: Epoch 14, Iteration 100, Current loss 2.470656156539917 Accuracy 74.40089585666293\n",
      "Training:: Epoch 14, Iteration 110, Current loss 1.4040577411651611 Accuracy 76.1162680398401\n",
      "Training:: Epoch 14, Iteration 120, Current loss 2.248903751373291 Accuracy 78.78488863828588\n",
      "Training:: Epoch 14, Iteration 130, Current loss 2.2606842517852783 Accuracy 75.52269914674656\n",
      "Training:: Epoch 14, Iteration 140, Current loss 1.2485308647155762 Accuracy 70.53343542623787\n",
      "Training:: Epoch 14, Iteration 150, Current loss 2.2161686420440674 Accuracy 71.53864676137732\n",
      "Starting Training\n",
      "Training:: Epoch 15, Iteration 0, Current loss 2.2921643257141113 Accuracy 73.03004930691228\n",
      "Training:: Epoch 15, Iteration 10, Current loss 1.0992103815078735 Accuracy 83.4471977767485\n",
      "Training:: Epoch 15, Iteration 20, Current loss 1.7635862827301025 Accuracy 63.229287479199236\n",
      "Training:: Epoch 15, Iteration 30, Current loss 2.3453922271728516 Accuracy 65.94476837316589\n",
      "Training:: Epoch 15, Iteration 40, Current loss 2.393446922302246 Accuracy 80.38116591928251\n",
      "Training:: Epoch 15, Iteration 50, Current loss 1.6458888053894043 Accuracy 79.0654410155395\n",
      "Training:: Epoch 15, Iteration 60, Current loss 1.671815037727356 Accuracy 63.60129123691048\n",
      "Training:: Epoch 15, Iteration 70, Current loss 3.115405797958374 Accuracy 52.480705622932746\n",
      "Training:: Epoch 15, Iteration 80, Current loss 2.0049469470977783 Accuracy 76.168876482903\n",
      "Training:: Epoch 15, Iteration 90, Current loss 1.9633327722549438 Accuracy 73.10000631751848\n",
      "Training:: Epoch 15, Iteration 100, Current loss 2.4884819984436035 Accuracy 71.00128842260261\n",
      "Training:: Epoch 15, Iteration 110, Current loss 2.0129575729370117 Accuracy 80.7086952782317\n",
      "Training:: Epoch 15, Iteration 120, Current loss 2.136072874069214 Accuracy 85.89830508474576\n",
      "Training:: Epoch 15, Iteration 130, Current loss 1.7977042198181152 Accuracy 74.51477531036895\n",
      "Training:: Epoch 15, Iteration 140, Current loss 1.7212016582489014 Accuracy 69.81511639045885\n",
      "Training:: Epoch 15, Iteration 150, Current loss 1.9691615104675293 Accuracy 76.84488928071359\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 15, Probability Accuracy 55.3140406844264\n",
      "Starting Training\n",
      "Training:: Epoch 16, Iteration 0, Current loss 2.0974559783935547 Accuracy 72.67650578380534\n",
      "Training:: Epoch 16, Iteration 10, Current loss 1.666767954826355 Accuracy 70.81204253556139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 16, Iteration 20, Current loss 2.973477840423584 Accuracy 78.07204719467805\n",
      "Training:: Epoch 16, Iteration 30, Current loss 1.7825002670288086 Accuracy 71.37199624110619\n",
      "Training:: Epoch 16, Iteration 40, Current loss 1.134177565574646 Accuracy 81.43336383043611\n",
      "Training:: Epoch 16, Iteration 50, Current loss 1.0133440494537354 Accuracy 74.81119121180913\n",
      "Training:: Epoch 16, Iteration 60, Current loss 1.2976866960525513 Accuracy 71.76873123385238\n",
      "Training:: Epoch 16, Iteration 70, Current loss 1.879880428314209 Accuracy 72.59939548942107\n",
      "Training:: Epoch 16, Iteration 80, Current loss 1.859548807144165 Accuracy 66.03391079985256\n",
      "Training:: Epoch 16, Iteration 90, Current loss 1.7421222925186157 Accuracy 83.781140621783\n",
      "Training:: Epoch 16, Iteration 100, Current loss 1.3329195976257324 Accuracy 82.43452791178498\n",
      "Training:: Epoch 16, Iteration 110, Current loss 1.6094855070114136 Accuracy 79.38454043520623\n",
      "Training:: Epoch 16, Iteration 120, Current loss 1.398886799812317 Accuracy 73.18360428384314\n",
      "Training:: Epoch 16, Iteration 130, Current loss 1.4071768522262573 Accuracy 71.5852853843616\n",
      "Training:: Epoch 16, Iteration 140, Current loss 2.349872350692749 Accuracy 80.32640571209996\n",
      "Training:: Epoch 16, Iteration 150, Current loss 2.8229503631591797 Accuracy 52.525579887957775\n",
      "Starting Training\n",
      "Training:: Epoch 17, Iteration 0, Current loss 1.351532220840454 Accuracy 71.62613575628006\n",
      "Training:: Epoch 17, Iteration 10, Current loss 1.7847166061401367 Accuracy 69.09415231890802\n",
      "Training:: Epoch 17, Iteration 20, Current loss 2.4819602966308594 Accuracy 68.59075180874488\n",
      "Training:: Epoch 17, Iteration 30, Current loss 1.2654138803482056 Accuracy 85.49881235154395\n",
      "Training:: Epoch 17, Iteration 40, Current loss 2.1866230964660645 Accuracy 76.92493508062081\n",
      "Training:: Epoch 17, Iteration 50, Current loss 1.6285803318023682 Accuracy 79.94013967409379\n",
      "Training:: Epoch 17, Iteration 60, Current loss 1.6893010139465332 Accuracy 81.025534565082\n",
      "Training:: Epoch 17, Iteration 70, Current loss 1.6203479766845703 Accuracy 82.91457286432161\n",
      "Training:: Epoch 17, Iteration 80, Current loss 1.4096485376358032 Accuracy 80.86300487625357\n",
      "Training:: Epoch 17, Iteration 90, Current loss 1.128298044204712 Accuracy 72.21358216692587\n",
      "Training:: Epoch 17, Iteration 100, Current loss 1.42789626121521 Accuracy 78.02074286006233\n",
      "Training:: Epoch 17, Iteration 110, Current loss 1.7876752614974976 Accuracy 73.52746837832736\n",
      "Training:: Epoch 17, Iteration 120, Current loss 1.6867976188659668 Accuracy 67.02659994732684\n",
      "Training:: Epoch 17, Iteration 130, Current loss 1.3958152532577515 Accuracy 76.8936678614098\n",
      "Training:: Epoch 17, Iteration 140, Current loss 1.9683059453964233 Accuracy 77.4322446143155\n",
      "Training:: Epoch 17, Iteration 150, Current loss 1.7897909879684448 Accuracy 85.71428571428571\n",
      "Starting Training\n",
      "Training:: Epoch 18, Iteration 0, Current loss 1.2075780630111694 Accuracy 80.3909952606635\n",
      "Training:: Epoch 18, Iteration 10, Current loss 1.249982237815857 Accuracy 67.7992316519483\n",
      "Training:: Epoch 18, Iteration 20, Current loss 2.104670763015747 Accuracy 66.56594651760085\n",
      "Training:: Epoch 18, Iteration 30, Current loss 1.484000563621521 Accuracy 81.98963509109366\n",
      "Training:: Epoch 18, Iteration 40, Current loss 1.4323967695236206 Accuracy 81.61557354680251\n",
      "Training:: Epoch 18, Iteration 50, Current loss 1.5404659509658813 Accuracy 82.2426039583477\n",
      "Training:: Epoch 18, Iteration 60, Current loss 1.4716219902038574 Accuracy 67.78562259306804\n",
      "Training:: Epoch 18, Iteration 70, Current loss 1.696081519126892 Accuracy 79.43674564148414\n",
      "Training:: Epoch 18, Iteration 80, Current loss 1.1437931060791016 Accuracy 73.75057313159101\n",
      "Training:: Epoch 18, Iteration 90, Current loss 1.337641954421997 Accuracy 84.736877032318\n",
      "Training:: Epoch 18, Iteration 100, Current loss 1.8188773393630981 Accuracy 86.35238291726841\n",
      "Training:: Epoch 18, Iteration 110, Current loss 1.943914532661438 Accuracy 64.96452576549665\n",
      "Training:: Epoch 18, Iteration 120, Current loss 2.120028018951416 Accuracy 80.26792577138465\n",
      "Training:: Epoch 18, Iteration 130, Current loss 1.8549624681472778 Accuracy 57.479410074698336\n",
      "Training:: Epoch 18, Iteration 140, Current loss 1.413102149963379 Accuracy 80.9106239460371\n",
      "Training:: Epoch 18, Iteration 150, Current loss 1.8384618759155273 Accuracy 69.08682269018419\n",
      "Starting Training\n",
      "Training:: Epoch 19, Iteration 0, Current loss 0.7565672993659973 Accuracy 83.2059056156077\n",
      "Training:: Epoch 19, Iteration 10, Current loss 1.1456501483917236 Accuracy 77.63157894736842\n",
      "Training:: Epoch 19, Iteration 20, Current loss 1.3041884899139404 Accuracy 79.62246634689772\n",
      "Training:: Epoch 19, Iteration 30, Current loss 0.8920434713363647 Accuracy 86.71208596432923\n",
      "Training:: Epoch 19, Iteration 40, Current loss 1.3463023900985718 Accuracy 81.07662318188633\n",
      "Training:: Epoch 19, Iteration 50, Current loss 1.4511128664016724 Accuracy 78.11505832449629\n",
      "Training:: Epoch 19, Iteration 60, Current loss 1.452622652053833 Accuracy 81.20714023975292\n",
      "Training:: Epoch 19, Iteration 70, Current loss 1.1361457109451294 Accuracy 86.78555766700775\n",
      "Training:: Epoch 19, Iteration 80, Current loss 0.9367218613624573 Accuracy 85.13600618795927\n",
      "Training:: Epoch 19, Iteration 90, Current loss 0.925330400466919 Accuracy 85.80234335201223\n",
      "Training:: Epoch 19, Iteration 100, Current loss 0.865405261516571 Accuracy 80.83185316755477\n",
      "Training:: Epoch 19, Iteration 110, Current loss 1.222388505935669 Accuracy 75.55211907408302\n",
      "Training:: Epoch 19, Iteration 120, Current loss 1.3384946584701538 Accuracy 79.58588370313696\n",
      "Training:: Epoch 19, Iteration 130, Current loss 1.7192445993423462 Accuracy 69.44835917052546\n",
      "Training:: Epoch 19, Iteration 140, Current loss 1.257203459739685 Accuracy 78.62134226233805\n",
      "Training:: Epoch 19, Iteration 150, Current loss 1.019447684288025 Accuracy 75.00327068161005\n",
      "Starting Training\n",
      "Training:: Epoch 20, Iteration 0, Current loss 1.4513493776321411 Accuracy 76.61264286175502\n",
      "Training:: Epoch 20, Iteration 10, Current loss 1.9326192140579224 Accuracy 61.581366840330986\n",
      "Training:: Epoch 20, Iteration 20, Current loss 1.9051027297973633 Accuracy 81.73674781325745\n",
      "Training:: Epoch 20, Iteration 30, Current loss 1.3604410886764526 Accuracy 81.82486521810162\n",
      "Training:: Epoch 20, Iteration 40, Current loss 1.9711567163467407 Accuracy 72.54435309389875\n",
      "Training:: Epoch 20, Iteration 50, Current loss 0.9799091219902039 Accuracy 78.46161918253843\n",
      "Training:: Epoch 20, Iteration 60, Current loss 1.1542778015136719 Accuracy 81.22763726095003\n",
      "Training:: Epoch 20, Iteration 70, Current loss 1.1762311458587646 Accuracy 70.94508301404854\n",
      "Training:: Epoch 20, Iteration 80, Current loss 1.7378135919570923 Accuracy 73.34070307318152\n",
      "Training:: Epoch 20, Iteration 90, Current loss 1.5902035236358643 Accuracy 62.09451029320025\n",
      "Training:: Epoch 20, Iteration 100, Current loss 1.6039094924926758 Accuracy 74.08702119747117\n",
      "Training:: Epoch 20, Iteration 110, Current loss 1.2630836963653564 Accuracy 82.60589749590451\n",
      "Training:: Epoch 20, Iteration 120, Current loss 1.3783659934997559 Accuracy 77.89627866526297\n",
      "Training:: Epoch 20, Iteration 130, Current loss 2.1042916774749756 Accuracy 62.27673255537032\n",
      "Training:: Epoch 20, Iteration 140, Current loss 1.372952938079834 Accuracy 80.50976690145959\n",
      "Training:: Epoch 20, Iteration 150, Current loss 1.694374442100525 Accuracy 71.4632864836464\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 20, Probability Accuracy 54.017897833202134\n",
      "Starting Training\n",
      "Training:: Epoch 21, Iteration 0, Current loss 0.8495872616767883 Accuracy 76.62585804260138\n",
      "Training:: Epoch 21, Iteration 10, Current loss 1.4019380807876587 Accuracy 81.29950760008563\n",
      "Training:: Epoch 21, Iteration 20, Current loss 1.2397019863128662 Accuracy 77.97015496460685\n",
      "Training:: Epoch 21, Iteration 30, Current loss 1.6357890367507935 Accuracy 67.53288078065337\n",
      "Training:: Epoch 21, Iteration 40, Current loss 1.2012120485305786 Accuracy 81.0607020656444\n",
      "Training:: Epoch 21, Iteration 50, Current loss 1.1500684022903442 Accuracy 74.37669551737501\n",
      "Training:: Epoch 21, Iteration 60, Current loss 1.4041520357131958 Accuracy 78.96758478545999\n",
      "Training:: Epoch 21, Iteration 70, Current loss 1.1935412883758545 Accuracy 89.84709480122324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 21, Iteration 80, Current loss 1.1772027015686035 Accuracy 83.54126359330175\n",
      "Training:: Epoch 21, Iteration 90, Current loss 1.2263929843902588 Accuracy 80.29252437703141\n",
      "Training:: Epoch 21, Iteration 100, Current loss 1.685544490814209 Accuracy 78.43795798729849\n",
      "Training:: Epoch 21, Iteration 110, Current loss 1.1901023387908936 Accuracy 80.76119143006444\n",
      "Training:: Epoch 21, Iteration 120, Current loss 1.2744698524475098 Accuracy 82.1379781420765\n",
      "Training:: Epoch 21, Iteration 130, Current loss 1.8735169172286987 Accuracy 73.08874986745838\n",
      "Training:: Epoch 21, Iteration 140, Current loss 2.187990188598633 Accuracy 70.18147086914995\n",
      "Training:: Epoch 21, Iteration 150, Current loss 1.4091788530349731 Accuracy 73.55765759260596\n",
      "Starting Training\n",
      "Training:: Epoch 22, Iteration 0, Current loss 1.0283654928207397 Accuracy 67.85255683964046\n",
      "Training:: Epoch 22, Iteration 10, Current loss 1.302449107170105 Accuracy 80.24832245401014\n",
      "Training:: Epoch 22, Iteration 20, Current loss 1.5650453567504883 Accuracy 54.48173322005098\n",
      "Training:: Epoch 22, Iteration 30, Current loss 1.0447803735733032 Accuracy 79.24198250728863\n",
      "Training:: Epoch 22, Iteration 40, Current loss 0.9605531096458435 Accuracy 77.35879161791298\n",
      "Training:: Epoch 22, Iteration 50, Current loss 1.3953831195831299 Accuracy 80.75746540422432\n",
      "Training:: Epoch 22, Iteration 60, Current loss 1.604770302772522 Accuracy 80.75027817517088\n",
      "Training:: Epoch 22, Iteration 70, Current loss 1.3205361366271973 Accuracy 71.78919397697076\n",
      "Training:: Epoch 22, Iteration 80, Current loss 1.3071568012237549 Accuracy 84.97652582159624\n",
      "Training:: Epoch 22, Iteration 90, Current loss 1.1610642671585083 Accuracy 80.15111597156634\n",
      "Training:: Epoch 22, Iteration 100, Current loss 0.819171667098999 Accuracy 61.07790158289895\n",
      "Training:: Epoch 22, Iteration 110, Current loss 1.1908962726593018 Accuracy 78.56889204545455\n",
      "Training:: Epoch 22, Iteration 120, Current loss 0.9755793809890747 Accuracy 87.93984962406014\n",
      "Training:: Epoch 22, Iteration 130, Current loss 1.1081691980361938 Accuracy 81.7313713212273\n",
      "Training:: Epoch 22, Iteration 140, Current loss 1.8035485744476318 Accuracy 79.59428800213533\n",
      "Training:: Epoch 22, Iteration 150, Current loss 1.2019479274749756 Accuracy 82.99856527977045\n",
      "Starting Training\n",
      "Training:: Epoch 23, Iteration 0, Current loss 1.1398077011108398 Accuracy 81.19911176905995\n",
      "Training:: Epoch 23, Iteration 10, Current loss 1.1418522596359253 Accuracy 81.32131598810105\n",
      "Training:: Epoch 23, Iteration 20, Current loss 0.9645540118217468 Accuracy 79.37028533945556\n",
      "Training:: Epoch 23, Iteration 30, Current loss 0.6907233595848083 Accuracy 72.76706793501137\n",
      "Training:: Epoch 23, Iteration 40, Current loss 0.871235728263855 Accuracy 82.51972240346403\n",
      "Training:: Epoch 23, Iteration 50, Current loss 0.7490841150283813 Accuracy 81.90820584144646\n",
      "Training:: Epoch 23, Iteration 60, Current loss 1.0801968574523926 Accuracy 84.3159252818758\n",
      "Training:: Epoch 23, Iteration 70, Current loss 0.9578712582588196 Accuracy 80.63655298099454\n",
      "Training:: Epoch 23, Iteration 80, Current loss 1.2449181079864502 Accuracy 79.85826771653544\n",
      "Training:: Epoch 23, Iteration 90, Current loss 1.418036699295044 Accuracy 83.77112604357565\n",
      "Training:: Epoch 23, Iteration 100, Current loss 1.1129266023635864 Accuracy 74.2533535813718\n",
      "Training:: Epoch 23, Iteration 110, Current loss 1.145186185836792 Accuracy 80.72385871841408\n",
      "Training:: Epoch 23, Iteration 120, Current loss 1.2474915981292725 Accuracy 79.62711050652156\n",
      "Training:: Epoch 23, Iteration 130, Current loss 0.8842100501060486 Accuracy 65.74593190209217\n",
      "Training:: Epoch 23, Iteration 140, Current loss 1.172044277191162 Accuracy 77.61276831818375\n",
      "Training:: Epoch 23, Iteration 150, Current loss 0.8335244059562683 Accuracy 85.27551942186089\n",
      "Starting Training\n",
      "Training:: Epoch 24, Iteration 0, Current loss 1.0851175785064697 Accuracy 85.9510752356892\n",
      "Training:: Epoch 24, Iteration 10, Current loss 1.2527291774749756 Accuracy 78.99728997289972\n",
      "Training:: Epoch 24, Iteration 20, Current loss 1.3425239324569702 Accuracy 77.18736477715275\n",
      "Training:: Epoch 24, Iteration 30, Current loss 1.4016436338424683 Accuracy 84.86832090120262\n",
      "Training:: Epoch 24, Iteration 40, Current loss 2.288788318634033 Accuracy 55.28665931642779\n",
      "Training:: Epoch 24, Iteration 50, Current loss 0.907244861125946 Accuracy 75.55378697439978\n",
      "Training:: Epoch 24, Iteration 60, Current loss 1.0297988653182983 Accuracy 73.64014062000639\n",
      "Training:: Epoch 24, Iteration 70, Current loss 1.5357130765914917 Accuracy 74.75560667050028\n",
      "Training:: Epoch 24, Iteration 80, Current loss 1.2014744281768799 Accuracy 78.46337859705903\n",
      "Training:: Epoch 24, Iteration 90, Current loss 0.895370364189148 Accuracy 91.05701103971\n",
      "Training:: Epoch 24, Iteration 100, Current loss 1.282594919204712 Accuracy 76.05241795198887\n",
      "Training:: Epoch 24, Iteration 110, Current loss 1.134613275527954 Accuracy 77.70610446821901\n",
      "Training:: Epoch 24, Iteration 120, Current loss 1.4551312923431396 Accuracy 82.52180232558139\n",
      "Training:: Epoch 24, Iteration 130, Current loss 1.4969308376312256 Accuracy 74.9001996007984\n",
      "Training:: Epoch 24, Iteration 140, Current loss 1.3743736743927002 Accuracy 80.16518315645327\n",
      "Training:: Epoch 24, Iteration 150, Current loss 1.348600149154663 Accuracy 82.56944984826083\n",
      "Starting Training\n",
      "Training:: Epoch 25, Iteration 0, Current loss 1.1518070697784424 Accuracy 68.50393700787401\n",
      "Training:: Epoch 25, Iteration 10, Current loss 1.7392386198043823 Accuracy 77.74401144082945\n",
      "Training:: Epoch 25, Iteration 20, Current loss 2.067906141281128 Accuracy 68.83282364933741\n",
      "Training:: Epoch 25, Iteration 30, Current loss 1.5526412725448608 Accuracy 77.61598374534371\n",
      "Training:: Epoch 25, Iteration 40, Current loss 1.0673513412475586 Accuracy 85.14712751050911\n",
      "Training:: Epoch 25, Iteration 50, Current loss 1.6170803308486938 Accuracy 81.85493366826282\n",
      "Training:: Epoch 25, Iteration 60, Current loss 1.2789117097854614 Accuracy 84.13471502590673\n",
      "Training:: Epoch 25, Iteration 70, Current loss 1.5487231016159058 Accuracy 78.35738881969375\n",
      "Training:: Epoch 25, Iteration 80, Current loss 1.4744845628738403 Accuracy 76.71699069548532\n",
      "Training:: Epoch 25, Iteration 90, Current loss 0.8453316688537598 Accuracy 73.70317543727948\n",
      "Training:: Epoch 25, Iteration 100, Current loss 1.296962022781372 Accuracy 57.221942627076\n",
      "Training:: Epoch 25, Iteration 110, Current loss 1.753359079360962 Accuracy 78.45804988662131\n",
      "Training:: Epoch 25, Iteration 120, Current loss 0.8396706581115723 Accuracy 75.88573959255979\n",
      "Training:: Epoch 25, Iteration 130, Current loss 1.2190090417861938 Accuracy 82.14739042783543\n",
      "Training:: Epoch 25, Iteration 140, Current loss 0.8966108560562134 Accuracy 77.91417266916912\n",
      "Training:: Epoch 25, Iteration 150, Current loss 1.3178170919418335 Accuracy 62.8217939566976\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 25, Probability Accuracy 57.57664581348138\n",
      "Starting Training\n",
      "Training:: Epoch 26, Iteration 0, Current loss 0.6535000801086426 Accuracy 85.71882230186453\n",
      "Training:: Epoch 26, Iteration 10, Current loss 1.1072049140930176 Accuracy 84.50258175559381\n",
      "Training:: Epoch 26, Iteration 20, Current loss 0.8044896721839905 Accuracy 78.28409167737138\n",
      "Training:: Epoch 26, Iteration 30, Current loss 1.1262129545211792 Accuracy 77.41518856552436\n",
      "Training:: Epoch 26, Iteration 40, Current loss 0.7239953875541687 Accuracy 78.97188668002816\n",
      "Training:: Epoch 26, Iteration 50, Current loss 1.1096611022949219 Accuracy 69.60997992543734\n",
      "Training:: Epoch 26, Iteration 60, Current loss 1.1694246530532837 Accuracy 77.1937202130642\n",
      "Training:: Epoch 26, Iteration 70, Current loss 1.028029441833496 Accuracy 83.36897392609266\n",
      "Training:: Epoch 26, Iteration 80, Current loss 1.1548264026641846 Accuracy 85.27321190581964\n",
      "Training:: Epoch 26, Iteration 90, Current loss 1.52191162109375 Accuracy 62.92508700164845\n",
      "Training:: Epoch 26, Iteration 100, Current loss 1.506704330444336 Accuracy 52.3977480838364\n",
      "Training:: Epoch 26, Iteration 110, Current loss 1.3358263969421387 Accuracy 55.552965311450954\n",
      "Training:: Epoch 26, Iteration 120, Current loss 1.7238185405731201 Accuracy 76.01296596434359\n",
      "Training:: Epoch 26, Iteration 130, Current loss 1.9689369201660156 Accuracy 71.6358120721842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 26, Iteration 140, Current loss 1.2385082244873047 Accuracy 78.51010101010101\n",
      "Training:: Epoch 26, Iteration 150, Current loss 1.196557879447937 Accuracy 74.32491863840268\n",
      "Starting Training\n",
      "Training:: Epoch 27, Iteration 0, Current loss 1.3132156133651733 Accuracy 76.42770352369381\n",
      "Training:: Epoch 27, Iteration 10, Current loss 1.0855109691619873 Accuracy 74.00946922501869\n",
      "Training:: Epoch 27, Iteration 20, Current loss 1.4276865720748901 Accuracy 86.35492525871982\n",
      "Training:: Epoch 27, Iteration 30, Current loss 1.0541410446166992 Accuracy 72.3202170963365\n",
      "Training:: Epoch 27, Iteration 40, Current loss 0.6892832517623901 Accuracy 61.679618013294636\n",
      "Training:: Epoch 27, Iteration 50, Current loss 1.0381993055343628 Accuracy 75.19019442096365\n",
      "Training:: Epoch 27, Iteration 60, Current loss 0.7496564984321594 Accuracy 76.36982350905525\n",
      "Training:: Epoch 27, Iteration 70, Current loss 1.3295836448669434 Accuracy 76.32323053812848\n",
      "Training:: Epoch 27, Iteration 80, Current loss 1.04994535446167 Accuracy 84.78554816377306\n",
      "Training:: Epoch 27, Iteration 90, Current loss 1.2551037073135376 Accuracy 88.77416742585282\n",
      "Training:: Epoch 27, Iteration 100, Current loss 0.984929621219635 Accuracy 87.69962208947946\n",
      "Training:: Epoch 27, Iteration 110, Current loss 0.8248403668403625 Accuracy 75.73796713683292\n",
      "Training:: Epoch 27, Iteration 120, Current loss 1.0572807788848877 Accuracy 71.98353214398817\n",
      "Training:: Epoch 27, Iteration 130, Current loss 0.8248370885848999 Accuracy 70.45333143406575\n",
      "Training:: Epoch 27, Iteration 140, Current loss 1.0854392051696777 Accuracy 82.55364238410596\n",
      "Training:: Epoch 27, Iteration 150, Current loss 0.6678171157836914 Accuracy 80.8957708957709\n",
      "Starting Training\n",
      "Training:: Epoch 28, Iteration 0, Current loss 1.164239764213562 Accuracy 78.37802711952519\n",
      "Training:: Epoch 28, Iteration 10, Current loss 0.8434992432594299 Accuracy 83.123642059522\n",
      "Training:: Epoch 28, Iteration 20, Current loss 0.6325308680534363 Accuracy 67.32066639129836\n",
      "Training:: Epoch 28, Iteration 30, Current loss 1.1850109100341797 Accuracy 78.43801074013543\n",
      "Training:: Epoch 28, Iteration 40, Current loss 0.5973371267318726 Accuracy 85.67193675889328\n",
      "Training:: Epoch 28, Iteration 50, Current loss 0.5980679988861084 Accuracy 78.02824313656697\n",
      "Training:: Epoch 28, Iteration 60, Current loss 0.8967382907867432 Accuracy 85.5521064301552\n",
      "Training:: Epoch 28, Iteration 70, Current loss 0.684884786605835 Accuracy 82.38477801268499\n",
      "Training:: Epoch 28, Iteration 80, Current loss 0.8820921182632446 Accuracy 78.91161835558013\n",
      "Training:: Epoch 28, Iteration 90, Current loss 0.9071504473686218 Accuracy 79.23284387174108\n",
      "Training:: Epoch 28, Iteration 100, Current loss 0.6594489812850952 Accuracy 69.30179911831289\n",
      "Training:: Epoch 28, Iteration 110, Current loss 0.8320446014404297 Accuracy 81.30406147091108\n",
      "Training:: Epoch 28, Iteration 120, Current loss 0.6238715052604675 Accuracy 89.07677116761293\n",
      "Training:: Epoch 28, Iteration 130, Current loss 0.8148216605186462 Accuracy 86.26590115951818\n",
      "Training:: Epoch 28, Iteration 140, Current loss 0.744832456111908 Accuracy 85.04626773596546\n",
      "Training:: Epoch 28, Iteration 150, Current loss 0.7156901955604553 Accuracy 76.5825653421967\n",
      "Starting Training\n",
      "Training:: Epoch 29, Iteration 0, Current loss 0.9809694290161133 Accuracy 79.53865336658355\n",
      "Training:: Epoch 29, Iteration 10, Current loss 0.6743364930152893 Accuracy 76.78192485332667\n",
      "Training:: Epoch 29, Iteration 20, Current loss 0.7428588271141052 Accuracy 86.9296577946768\n",
      "Training:: Epoch 29, Iteration 30, Current loss 0.6665676236152649 Accuracy 85.03084304318025\n",
      "Training:: Epoch 29, Iteration 40, Current loss 0.9628424048423767 Accuracy 85.25967257342572\n",
      "Training:: Epoch 29, Iteration 50, Current loss 0.6136022806167603 Accuracy 81.1311704168847\n",
      "Training:: Epoch 29, Iteration 60, Current loss 0.4339276850223541 Accuracy 82.48223590715301\n",
      "Training:: Epoch 29, Iteration 70, Current loss 0.5160972476005554 Accuracy 63.35325732080968\n",
      "Training:: Epoch 29, Iteration 80, Current loss 0.6643323302268982 Accuracy 81.66502705361535\n",
      "Training:: Epoch 29, Iteration 90, Current loss 0.8770918250083923 Accuracy 79.31319215076743\n",
      "Training:: Epoch 29, Iteration 100, Current loss 0.6641117334365845 Accuracy 84.97793072491957\n",
      "Training:: Epoch 29, Iteration 110, Current loss 0.8760003447532654 Accuracy 82.61085648020321\n",
      "Training:: Epoch 29, Iteration 120, Current loss 1.074082374572754 Accuracy 83.5881028557886\n",
      "Training:: Epoch 29, Iteration 130, Current loss 0.4507567882537842 Accuracy 81.76737226598601\n",
      "Training:: Epoch 29, Iteration 140, Current loss 0.5999975204467773 Accuracy 83.14757481940144\n",
      "Training:: Epoch 29, Iteration 150, Current loss 0.8153891563415527 Accuracy 84.40421871493754\n",
      "Starting Training\n",
      "Training:: Epoch 30, Iteration 0, Current loss 0.769451916217804 Accuracy 84.62105740866802\n",
      "Training:: Epoch 30, Iteration 10, Current loss 0.6789231300354004 Accuracy 76.21110224552605\n",
      "Training:: Epoch 30, Iteration 20, Current loss 0.8493953347206116 Accuracy 86.81446440025657\n",
      "Training:: Epoch 30, Iteration 30, Current loss 0.9214017987251282 Accuracy 72.2626788036411\n",
      "Training:: Epoch 30, Iteration 40, Current loss 0.8671129941940308 Accuracy 82.77628689924371\n",
      "Training:: Epoch 30, Iteration 50, Current loss 0.6878724098205566 Accuracy 66.26183145170486\n",
      "Training:: Epoch 30, Iteration 60, Current loss 1.02411949634552 Accuracy 88.92156862745098\n",
      "Training:: Epoch 30, Iteration 70, Current loss 0.5831144452095032 Accuracy 80.92816962353959\n",
      "Training:: Epoch 30, Iteration 80, Current loss 0.7683406472206116 Accuracy 67.91546589817483\n",
      "Training:: Epoch 30, Iteration 90, Current loss 0.9287392497062683 Accuracy 76.53906057873596\n",
      "Training:: Epoch 30, Iteration 100, Current loss 0.8845124840736389 Accuracy 76.19425778863776\n",
      "Training:: Epoch 30, Iteration 110, Current loss 0.4045387804508209 Accuracy 75.30417814508723\n",
      "Training:: Epoch 30, Iteration 120, Current loss 0.45607927441596985 Accuracy 90.081979212414\n",
      "Training:: Epoch 30, Iteration 130, Current loss 0.5265456438064575 Accuracy 82.18007638034824\n",
      "Training:: Epoch 30, Iteration 140, Current loss 0.5294776558876038 Accuracy 88.56601003303561\n",
      "Training:: Epoch 30, Iteration 150, Current loss 0.6089655756950378 Accuracy 80.33401952320871\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 30, Probability Accuracy 59.92356133736587\n",
      "Starting Training\n",
      "Training:: Epoch 31, Iteration 0, Current loss 0.5989128351211548 Accuracy 85.03052377776638\n",
      "Training:: Epoch 31, Iteration 10, Current loss 0.5953944325447083 Accuracy 85.31761125903385\n",
      "Training:: Epoch 31, Iteration 20, Current loss 0.8064886331558228 Accuracy 85.61869666408069\n",
      "Training:: Epoch 31, Iteration 30, Current loss 0.7748808860778809 Accuracy 63.53893427265233\n",
      "Training:: Epoch 31, Iteration 40, Current loss 0.6237836480140686 Accuracy 81.4398857439626\n",
      "Training:: Epoch 31, Iteration 50, Current loss 0.6900482773780823 Accuracy 77.47521480502313\n",
      "Training:: Epoch 31, Iteration 60, Current loss 0.7244841456413269 Accuracy 78.53425235616824\n",
      "Training:: Epoch 31, Iteration 70, Current loss 0.7038103342056274 Accuracy 78.66729579532348\n",
      "Training:: Epoch 31, Iteration 80, Current loss 0.5237489342689514 Accuracy 87.8849840255591\n",
      "Training:: Epoch 31, Iteration 90, Current loss 0.5817359089851379 Accuracy 80.620429346544\n",
      "Training:: Epoch 31, Iteration 100, Current loss 0.6903942227363586 Accuracy 78.12963879267689\n",
      "Training:: Epoch 31, Iteration 110, Current loss 0.7490582466125488 Accuracy 84.11706675435713\n",
      "Training:: Epoch 31, Iteration 120, Current loss 0.5683595538139343 Accuracy 86.77364864864865\n",
      "Training:: Epoch 31, Iteration 130, Current loss 0.7405021786689758 Accuracy 83.08518253400143\n",
      "Training:: Epoch 31, Iteration 140, Current loss 0.6035561561584473 Accuracy 86.35270824494414\n",
      "Training:: Epoch 31, Iteration 150, Current loss 0.8820013403892517 Accuracy 80.88455772113943\n",
      "Starting Training\n",
      "Training:: Epoch 32, Iteration 0, Current loss 0.7279051542282104 Accuracy 78.89202609288287\n",
      "Training:: Epoch 32, Iteration 10, Current loss 0.7241353392601013 Accuracy 78.00174619443496\n",
      "Training:: Epoch 32, Iteration 20, Current loss 0.9886249899864197 Accuracy 83.70477568740955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 32, Iteration 30, Current loss 0.784907877445221 Accuracy 78.9171974522293\n",
      "Training:: Epoch 32, Iteration 40, Current loss 0.8452825546264648 Accuracy 74.84003475787976\n",
      "Training:: Epoch 32, Iteration 50, Current loss 1.067298412322998 Accuracy 77.9367816091954\n",
      "Training:: Epoch 32, Iteration 60, Current loss 0.5532417297363281 Accuracy 77.98313422347154\n",
      "Training:: Epoch 32, Iteration 70, Current loss 1.3030799627304077 Accuracy 84.78758270978913\n",
      "Training:: Epoch 32, Iteration 80, Current loss 0.8031848669052124 Accuracy 78.90020012181328\n",
      "Training:: Epoch 32, Iteration 90, Current loss 0.4707181751728058 Accuracy 79.08041300035603\n",
      "Training:: Epoch 32, Iteration 100, Current loss 0.8900812864303589 Accuracy 85.09776759550323\n",
      "Training:: Epoch 32, Iteration 110, Current loss 0.7653201818466187 Accuracy 74.43090582790214\n",
      "Training:: Epoch 32, Iteration 120, Current loss 1.2139354944229126 Accuracy 74.08104850858692\n",
      "Training:: Epoch 32, Iteration 130, Current loss 0.7876498103141785 Accuracy 75.28528528528528\n",
      "Training:: Epoch 32, Iteration 140, Current loss 0.8853800296783447 Accuracy 81.95698017714045\n",
      "Training:: Epoch 32, Iteration 150, Current loss 0.7819178104400635 Accuracy 83.80440822374514\n",
      "Starting Training\n",
      "Training:: Epoch 33, Iteration 0, Current loss 0.9762780070304871 Accuracy 79.44597298649325\n",
      "Training:: Epoch 33, Iteration 10, Current loss 0.9982892870903015 Accuracy 73.33540372670808\n",
      "Training:: Epoch 33, Iteration 20, Current loss 0.848839521408081 Accuracy 79.7965526985024\n",
      "Training:: Epoch 33, Iteration 30, Current loss 1.2955710887908936 Accuracy 80.67153516431179\n",
      "Training:: Epoch 33, Iteration 40, Current loss 2.071110248565674 Accuracy 63.24229914258495\n",
      "Training:: Epoch 33, Iteration 50, Current loss 1.1077967882156372 Accuracy 80.6949806949807\n",
      "Training:: Epoch 33, Iteration 60, Current loss 0.9869298934936523 Accuracy 68.44520265897994\n",
      "Training:: Epoch 33, Iteration 70, Current loss 1.966548204421997 Accuracy 40.274788942228106\n",
      "Training:: Epoch 33, Iteration 80, Current loss 1.1425596475601196 Accuracy 84.2640136927685\n",
      "Training:: Epoch 33, Iteration 90, Current loss 1.0108801126480103 Accuracy 71.23624894222633\n",
      "Training:: Epoch 33, Iteration 100, Current loss 1.3747152090072632 Accuracy 73.96707381837493\n",
      "Training:: Epoch 33, Iteration 110, Current loss 1.3815335035324097 Accuracy 72.89634650355431\n",
      "Training:: Epoch 33, Iteration 120, Current loss 2.4769370555877686 Accuracy 76.91274566862157\n",
      "Training:: Epoch 33, Iteration 130, Current loss 0.9285805225372314 Accuracy 62.88117095718046\n",
      "Training:: Epoch 33, Iteration 140, Current loss 1.2643076181411743 Accuracy 82.09679987263175\n",
      "Training:: Epoch 33, Iteration 150, Current loss 0.6346482634544373 Accuracy 71.34428564162555\n",
      "Starting Training\n",
      "Training:: Epoch 34, Iteration 0, Current loss 1.280414342880249 Accuracy 76.46239554317549\n",
      "Training:: Epoch 34, Iteration 10, Current loss 0.9709932208061218 Accuracy 68.06317843639468\n",
      "Training:: Epoch 34, Iteration 20, Current loss 1.6328803300857544 Accuracy 86.11340327714252\n",
      "Training:: Epoch 34, Iteration 30, Current loss 0.6531726121902466 Accuracy 80.61164566111036\n",
      "Training:: Epoch 34, Iteration 40, Current loss 0.983818531036377 Accuracy 78.1775420336269\n",
      "Training:: Epoch 34, Iteration 50, Current loss 0.9238390922546387 Accuracy 80.53386790462123\n",
      "Training:: Epoch 34, Iteration 60, Current loss 1.0977791547775269 Accuracy 80.31570426335672\n",
      "Training:: Epoch 34, Iteration 70, Current loss 0.7157254219055176 Accuracy 77.5363075958625\n",
      "Training:: Epoch 34, Iteration 80, Current loss 0.8718034029006958 Accuracy 69.17348309119765\n",
      "Training:: Epoch 34, Iteration 90, Current loss 1.4287054538726807 Accuracy 72.13773625896155\n",
      "Training:: Epoch 34, Iteration 100, Current loss 0.6260654926300049 Accuracy 83.16267942583733\n",
      "Training:: Epoch 34, Iteration 110, Current loss 0.8280180096626282 Accuracy 83.92015706806282\n",
      "Training:: Epoch 34, Iteration 120, Current loss 1.5619909763336182 Accuracy 68.11004619733848\n",
      "Training:: Epoch 34, Iteration 130, Current loss 1.0635963678359985 Accuracy 79.51040730239956\n",
      "Training:: Epoch 34, Iteration 140, Current loss 1.0000721216201782 Accuracy 79.44840933833314\n",
      "Training:: Epoch 34, Iteration 150, Current loss 0.5743697881698608 Accuracy 80.53852306051719\n",
      "Starting Training\n",
      "Training:: Epoch 35, Iteration 0, Current loss 0.9977097511291504 Accuracy 77.88970032737346\n",
      "Training:: Epoch 35, Iteration 10, Current loss 0.7792626023292542 Accuracy 77.45146348674486\n",
      "Training:: Epoch 35, Iteration 20, Current loss 0.6213498115539551 Accuracy 62.62693156732892\n",
      "Training:: Epoch 35, Iteration 30, Current loss 1.3408474922180176 Accuracy 71.0834849403679\n",
      "Training:: Epoch 35, Iteration 40, Current loss 0.9582732319831848 Accuracy 77.95627575609416\n",
      "Training:: Epoch 35, Iteration 50, Current loss 0.5292272567749023 Accuracy 80.18767973361587\n",
      "Training:: Epoch 35, Iteration 60, Current loss 0.9363698959350586 Accuracy 72.96497584541063\n",
      "Training:: Epoch 35, Iteration 70, Current loss 0.9198998212814331 Accuracy 76.69460986989341\n",
      "Training:: Epoch 35, Iteration 80, Current loss 0.856112003326416 Accuracy 86.48273565004996\n",
      "Training:: Epoch 35, Iteration 90, Current loss 0.8791326880455017 Accuracy 70.87446659448443\n",
      "Training:: Epoch 35, Iteration 100, Current loss 0.7294314503669739 Accuracy 83.39572609577289\n",
      "Training:: Epoch 35, Iteration 110, Current loss 0.5080599784851074 Accuracy 76.49313130122044\n",
      "Training:: Epoch 35, Iteration 120, Current loss 0.9391291737556458 Accuracy 79.2551087673039\n",
      "Training:: Epoch 35, Iteration 130, Current loss 1.082115650177002 Accuracy 58.110169491525426\n",
      "Training:: Epoch 35, Iteration 140, Current loss 1.3690602779388428 Accuracy 72.34748289037242\n",
      "Training:: Epoch 35, Iteration 150, Current loss 0.74076908826828 Accuracy 71.83387576057733\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 35, Probability Accuracy 53.41798897957493\n",
      "Starting Training\n",
      "Training:: Epoch 36, Iteration 0, Current loss 1.2291910648345947 Accuracy 75.4249461335887\n",
      "Training:: Epoch 36, Iteration 10, Current loss 1.0309537649154663 Accuracy 76.44769480797194\n",
      "Training:: Epoch 36, Iteration 20, Current loss 1.9116657972335815 Accuracy 64.2773497688752\n",
      "Training:: Epoch 36, Iteration 30, Current loss 1.2726287841796875 Accuracy 84.24475710114149\n",
      "Training:: Epoch 36, Iteration 40, Current loss 0.7894599437713623 Accuracy 77.6440292250569\n",
      "Training:: Epoch 36, Iteration 50, Current loss 0.7658354640007019 Accuracy 77.3216995447648\n",
      "Training:: Epoch 36, Iteration 60, Current loss 1.3218826055526733 Accuracy 86.02857393286003\n",
      "Training:: Epoch 36, Iteration 70, Current loss 1.8464181423187256 Accuracy 72.43041517648888\n",
      "Training:: Epoch 36, Iteration 80, Current loss 0.7407961487770081 Accuracy 74.26349281168984\n",
      "Training:: Epoch 36, Iteration 90, Current loss 0.7057511210441589 Accuracy 60.825954078833945\n",
      "Training:: Epoch 36, Iteration 100, Current loss 1.5132111310958862 Accuracy 74.17442305240341\n",
      "Training:: Epoch 36, Iteration 110, Current loss 1.3161413669586182 Accuracy 73.63823227132579\n",
      "Training:: Epoch 36, Iteration 120, Current loss 0.7226640582084656 Accuracy 66.93261148311726\n",
      "Training:: Epoch 36, Iteration 130, Current loss 1.3115711212158203 Accuracy 76.51470370917689\n",
      "Training:: Epoch 36, Iteration 140, Current loss 0.7650043964385986 Accuracy 82.55760486384305\n",
      "Training:: Epoch 36, Iteration 150, Current loss 0.8457702994346619 Accuracy 83.87621090956375\n",
      "Starting Training\n",
      "Training:: Epoch 37, Iteration 0, Current loss 1.0146043300628662 Accuracy 79.31712364200725\n",
      "Training:: Epoch 37, Iteration 10, Current loss 0.60308837890625 Accuracy 76.88636725171006\n",
      "Training:: Epoch 37, Iteration 20, Current loss 0.9095584154129028 Accuracy 72.28097259745272\n",
      "Training:: Epoch 37, Iteration 30, Current loss 0.8851519823074341 Accuracy 77.8252798559937\n",
      "Training:: Epoch 37, Iteration 40, Current loss 0.7938175201416016 Accuracy 73.69905651201245\n",
      "Training:: Epoch 37, Iteration 50, Current loss 0.6575314402580261 Accuracy 86.79432541304222\n",
      "Training:: Epoch 37, Iteration 60, Current loss 1.8161380290985107 Accuracy 85.51381998582565\n",
      "Training:: Epoch 37, Iteration 70, Current loss 0.6076732873916626 Accuracy 81.29336931380108\n",
      "Training:: Epoch 37, Iteration 80, Current loss 1.0624593496322632 Accuracy 81.76814351726013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 37, Iteration 90, Current loss 1.3018125295639038 Accuracy 69.76047904191617\n",
      "Training:: Epoch 37, Iteration 100, Current loss 0.8037773966789246 Accuracy 72.94731582745794\n",
      "Training:: Epoch 37, Iteration 110, Current loss 0.757719099521637 Accuracy 84.81998255886383\n",
      "Training:: Epoch 37, Iteration 120, Current loss 0.9489984512329102 Accuracy 74.18881631797701\n",
      "Training:: Epoch 37, Iteration 130, Current loss 0.9445183873176575 Accuracy 77.76825668856517\n",
      "Training:: Epoch 37, Iteration 140, Current loss 0.8850764632225037 Accuracy 80.80357142857143\n",
      "Training:: Epoch 37, Iteration 150, Current loss 0.8780725598335266 Accuracy 66.89419795221843\n",
      "Starting Training\n",
      "Training:: Epoch 38, Iteration 0, Current loss 0.8750118017196655 Accuracy 83.30613781850131\n",
      "Training:: Epoch 38, Iteration 10, Current loss 0.707131564617157 Accuracy 82.2709432940858\n",
      "Training:: Epoch 38, Iteration 20, Current loss 0.6256869435310364 Accuracy 83.26224783861672\n",
      "Training:: Epoch 38, Iteration 30, Current loss 0.7651240825653076 Accuracy 73.17791772548154\n",
      "Training:: Epoch 38, Iteration 40, Current loss 0.6608113050460815 Accuracy 83.10355883295928\n",
      "Training:: Epoch 38, Iteration 50, Current loss 0.5995767712593079 Accuracy 75.7404181184669\n",
      "Training:: Epoch 38, Iteration 60, Current loss 0.86540687084198 Accuracy 81.2590799031477\n",
      "Training:: Epoch 38, Iteration 70, Current loss 1.0486059188842773 Accuracy 71.25228185469149\n",
      "Training:: Epoch 38, Iteration 80, Current loss 0.8377272486686707 Accuracy 84.36682696363789\n",
      "Training:: Epoch 38, Iteration 90, Current loss 1.3558491468429565 Accuracy 70.8382928073457\n",
      "Training:: Epoch 38, Iteration 100, Current loss 0.891740620136261 Accuracy 76.76645091693635\n",
      "Training:: Epoch 38, Iteration 110, Current loss 0.7363771796226501 Accuracy 80.0050154643484\n",
      "Training:: Epoch 38, Iteration 120, Current loss 0.7270922064781189 Accuracy 73.22745959339979\n",
      "Training:: Epoch 38, Iteration 130, Current loss 1.1521670818328857 Accuracy 75.74102964118565\n",
      "Training:: Epoch 38, Iteration 140, Current loss 1.1476919651031494 Accuracy 78.39829224624707\n",
      "Training:: Epoch 38, Iteration 150, Current loss 1.0451445579528809 Accuracy 82.90245632132036\n",
      "Starting Training\n",
      "Training:: Epoch 39, Iteration 0, Current loss 0.7605993747711182 Accuracy 68.53865290703031\n",
      "Training:: Epoch 39, Iteration 10, Current loss 0.822517991065979 Accuracy 65.27308082516063\n",
      "Training:: Epoch 39, Iteration 20, Current loss 0.4695693850517273 Accuracy 79.17156552410238\n",
      "Training:: Epoch 39, Iteration 30, Current loss 0.8769283890724182 Accuracy 82.01406995621588\n",
      "Training:: Epoch 39, Iteration 40, Current loss 0.7591856718063354 Accuracy 72.80559694241109\n",
      "Training:: Epoch 39, Iteration 50, Current loss 0.677778422832489 Accuracy 84.4075617743376\n",
      "Training:: Epoch 39, Iteration 60, Current loss 0.9649497270584106 Accuracy 80.35921205098494\n",
      "Training:: Epoch 39, Iteration 70, Current loss 0.8341574668884277 Accuracy 82.17096336499321\n",
      "Training:: Epoch 39, Iteration 80, Current loss 0.5679008960723877 Accuracy 69.72736511864257\n",
      "Training:: Epoch 39, Iteration 90, Current loss 0.8232306241989136 Accuracy 78.56159918881727\n",
      "Training:: Epoch 39, Iteration 100, Current loss 0.8712028861045837 Accuracy 72.42753623188406\n",
      "Training:: Epoch 39, Iteration 110, Current loss 0.7830399870872498 Accuracy 76.97954084023114\n",
      "Training:: Epoch 39, Iteration 120, Current loss 0.5011492371559143 Accuracy 78.26679384112276\n",
      "Training:: Epoch 39, Iteration 130, Current loss 0.8281437158584595 Accuracy 80.96044328151454\n",
      "Training:: Epoch 39, Iteration 140, Current loss 0.7026030421257019 Accuracy 82.5391714960292\n",
      "Training:: Epoch 39, Iteration 150, Current loss 0.6245354413986206 Accuracy 86.1083808400498\n",
      "Starting Training\n",
      "Training:: Epoch 40, Iteration 0, Current loss 0.4743809103965759 Accuracy 84.55198733303561\n",
      "Training:: Epoch 40, Iteration 10, Current loss 1.1257487535476685 Accuracy 85.18278874150761\n",
      "Training:: Epoch 40, Iteration 20, Current loss 0.7356249690055847 Accuracy 84.11573664009448\n",
      "Training:: Epoch 40, Iteration 30, Current loss 0.5857949256896973 Accuracy 75.4728370221328\n",
      "Training:: Epoch 40, Iteration 40, Current loss 0.7331566214561462 Accuracy 89.05268245529241\n",
      "Training:: Epoch 40, Iteration 50, Current loss 0.8571027517318726 Accuracy 76.96752341103806\n",
      "Training:: Epoch 40, Iteration 60, Current loss 0.4815874695777893 Accuracy 85.92294507787466\n",
      "Training:: Epoch 40, Iteration 70, Current loss 0.5464493632316589 Accuracy 86.44341801385681\n",
      "Training:: Epoch 40, Iteration 80, Current loss 0.8070601224899292 Accuracy 74.61054933041815\n",
      "Training:: Epoch 40, Iteration 90, Current loss 0.9868509769439697 Accuracy 82.80754924920788\n",
      "Training:: Epoch 40, Iteration 100, Current loss 0.8822215795516968 Accuracy 85.10729351348047\n",
      "Training:: Epoch 40, Iteration 110, Current loss 0.4421268701553345 Accuracy 81.51353138951661\n",
      "Training:: Epoch 40, Iteration 120, Current loss 0.7624417543411255 Accuracy 75.90991944493445\n",
      "Training:: Epoch 40, Iteration 130, Current loss 0.6874493360519409 Accuracy 88.15491313602966\n",
      "Training:: Epoch 40, Iteration 140, Current loss 0.6285524368286133 Accuracy 83.60613010842368\n",
      "Training:: Epoch 40, Iteration 150, Current loss 0.6084856390953064 Accuracy 74.0280751843921\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 40, Probability Accuracy 59.589737747027385\n",
      "Starting Training\n",
      "Training:: Epoch 41, Iteration 0, Current loss 0.6070464253425598 Accuracy 82.75011420740064\n",
      "Training:: Epoch 41, Iteration 10, Current loss 0.4863009452819824 Accuracy 81.6814533164343\n",
      "Training:: Epoch 41, Iteration 20, Current loss 0.630569577217102 Accuracy 73.08568598315998\n",
      "Training:: Epoch 41, Iteration 30, Current loss 0.48430967330932617 Accuracy 79.9341021416804\n",
      "Training:: Epoch 41, Iteration 40, Current loss 0.4225336015224457 Accuracy 80.54017094017094\n",
      "Training:: Epoch 41, Iteration 50, Current loss 0.6928666830062866 Accuracy 78.86888769478774\n",
      "Training:: Epoch 41, Iteration 60, Current loss 0.8159584999084473 Accuracy 85.56664709336465\n",
      "Training:: Epoch 41, Iteration 70, Current loss 0.4759712517261505 Accuracy 80.17155110793423\n",
      "Training:: Epoch 41, Iteration 80, Current loss 0.5253186821937561 Accuracy 82.32149329467198\n",
      "Training:: Epoch 41, Iteration 90, Current loss 0.487586110830307 Accuracy 72.0497389502212\n",
      "Training:: Epoch 41, Iteration 100, Current loss 0.4581359028816223 Accuracy 84.07602764641688\n",
      "Training:: Epoch 41, Iteration 110, Current loss 0.5498061180114746 Accuracy 79.21085858585859\n",
      "Training:: Epoch 41, Iteration 120, Current loss 0.5820920467376709 Accuracy 83.4466761558332\n",
      "Training:: Epoch 41, Iteration 130, Current loss 0.5343809723854065 Accuracy 78.6249203314213\n",
      "Training:: Epoch 41, Iteration 140, Current loss 0.7138776183128357 Accuracy 82.29802826731809\n",
      "Training:: Epoch 41, Iteration 150, Current loss 0.8174318075180054 Accuracy 84.02929261452165\n",
      "Starting Training\n",
      "Training:: Epoch 42, Iteration 0, Current loss 1.2198165655136108 Accuracy 85.55012927319736\n",
      "Training:: Epoch 42, Iteration 10, Current loss 0.6068732738494873 Accuracy 77.35140931372548\n",
      "Training:: Epoch 42, Iteration 20, Current loss 0.5707465410232544 Accuracy 84.15857962064402\n",
      "Training:: Epoch 42, Iteration 30, Current loss 0.6239626407623291 Accuracy 75.28635322351914\n",
      "Training:: Epoch 42, Iteration 40, Current loss 0.43550318479537964 Accuracy 74.94155682666003\n",
      "Training:: Epoch 42, Iteration 50, Current loss 0.6333461403846741 Accuracy 88.06137724550898\n",
      "Training:: Epoch 42, Iteration 60, Current loss 0.6229228973388672 Accuracy 74.64010713090057\n",
      "Training:: Epoch 42, Iteration 70, Current loss 0.6088994145393372 Accuracy 83.67196785330239\n",
      "Training:: Epoch 42, Iteration 80, Current loss 0.37555912137031555 Accuracy 81.70818047242068\n",
      "Training:: Epoch 42, Iteration 90, Current loss 0.5458738207817078 Accuracy 75.19056141048586\n",
      "Training:: Epoch 42, Iteration 100, Current loss 0.7772144675254822 Accuracy 85.76766677656886\n",
      "Training:: Epoch 42, Iteration 110, Current loss 0.5155536532402039 Accuracy 84.0522799202907\n",
      "Training:: Epoch 42, Iteration 120, Current loss 0.7611713409423828 Accuracy 86.4362690449647\n",
      "Training:: Epoch 42, Iteration 130, Current loss 0.49396011233329773 Accuracy 88.47291190090945\n",
      "Training:: Epoch 42, Iteration 140, Current loss 0.6267666816711426 Accuracy 85.551959726717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 42, Iteration 150, Current loss 0.46576911211013794 Accuracy 86.50741888146095\n",
      "Starting Training\n",
      "Training:: Epoch 43, Iteration 0, Current loss 0.5141934156417847 Accuracy 81.02478650281192\n",
      "Training:: Epoch 43, Iteration 10, Current loss 0.4533786475658417 Accuracy 86.88180255871683\n",
      "Training:: Epoch 43, Iteration 20, Current loss 0.5493927001953125 Accuracy 82.02539550374688\n",
      "Training:: Epoch 43, Iteration 30, Current loss 0.4359167218208313 Accuracy 79.73033207042567\n",
      "Training:: Epoch 43, Iteration 40, Current loss 0.5482295155525208 Accuracy 84.49412799095647\n",
      "Training:: Epoch 43, Iteration 50, Current loss 0.4471777081489563 Accuracy 76.82570593962998\n",
      "Training:: Epoch 43, Iteration 60, Current loss 0.4254418909549713 Accuracy 86.37742010730115\n",
      "Training:: Epoch 43, Iteration 70, Current loss 0.6132215857505798 Accuracy 69.78183962264151\n",
      "Training:: Epoch 43, Iteration 80, Current loss 0.6845492124557495 Accuracy 78.9347290640394\n",
      "Training:: Epoch 43, Iteration 90, Current loss 0.5345273613929749 Accuracy 83.02033080634045\n",
      "Training:: Epoch 43, Iteration 100, Current loss 0.45741069316864014 Accuracy 85.03318085174924\n",
      "Training:: Epoch 43, Iteration 110, Current loss 0.5305076241493225 Accuracy 79.35591675527874\n",
      "Training:: Epoch 43, Iteration 120, Current loss 0.8492234349250793 Accuracy 89.27053417576106\n",
      "Training:: Epoch 43, Iteration 130, Current loss 0.3965676426887512 Accuracy 85.49589455488332\n",
      "Training:: Epoch 43, Iteration 140, Current loss 0.4874098598957062 Accuracy 69.15270115540751\n",
      "Training:: Epoch 43, Iteration 150, Current loss 0.5839509963989258 Accuracy 88.90898131404461\n",
      "Starting Training\n",
      "Training:: Epoch 44, Iteration 0, Current loss 0.4224131405353546 Accuracy 89.85834709307377\n",
      "Training:: Epoch 44, Iteration 10, Current loss 0.35769346356391907 Accuracy 77.0069746758414\n",
      "Training:: Epoch 44, Iteration 20, Current loss 0.6251351833343506 Accuracy 83.76248722429197\n",
      "Training:: Epoch 44, Iteration 30, Current loss 0.5577462911605835 Accuracy 79.07421576128539\n",
      "Training:: Epoch 44, Iteration 40, Current loss 0.3959100842475891 Accuracy 85.33243369491878\n",
      "Training:: Epoch 44, Iteration 50, Current loss 0.3803381025791168 Accuracy 69.82218566095045\n",
      "Training:: Epoch 44, Iteration 60, Current loss 0.9192554950714111 Accuracy 86.58032515844586\n",
      "Training:: Epoch 44, Iteration 70, Current loss 0.5453484058380127 Accuracy 83.75148456057008\n",
      "Training:: Epoch 44, Iteration 80, Current loss 0.5136709213256836 Accuracy 83.6734693877551\n",
      "Training:: Epoch 44, Iteration 90, Current loss 0.2875570058822632 Accuracy 83.26523464861079\n",
      "Training:: Epoch 44, Iteration 100, Current loss 0.42184847593307495 Accuracy 84.15559196553188\n",
      "Training:: Epoch 44, Iteration 110, Current loss 0.5519721508026123 Accuracy 81.6102971381624\n",
      "Training:: Epoch 44, Iteration 120, Current loss 0.5111749172210693 Accuracy 81.05506560550656\n",
      "Training:: Epoch 44, Iteration 130, Current loss 0.45897483825683594 Accuracy 80.54195233431277\n",
      "Training:: Epoch 44, Iteration 140, Current loss 0.41971713304519653 Accuracy 79.30073826438223\n",
      "Training:: Epoch 44, Iteration 150, Current loss 0.320690393447876 Accuracy 65.64247326420933\n",
      "Starting Training\n",
      "Training:: Epoch 45, Iteration 0, Current loss 0.5835365056991577 Accuracy 85.69957884885353\n",
      "Training:: Epoch 45, Iteration 10, Current loss 0.37472155690193176 Accuracy 79.05876049294517\n",
      "Training:: Epoch 45, Iteration 20, Current loss 0.3189758360385895 Accuracy 67.91162186196047\n",
      "Training:: Epoch 45, Iteration 30, Current loss 0.8665546178817749 Accuracy 89.04789382573571\n",
      "Training:: Epoch 45, Iteration 40, Current loss 0.6429955363273621 Accuracy 84.55273698264352\n",
      "Training:: Epoch 45, Iteration 50, Current loss 0.32771435379981995 Accuracy 70.60117738003211\n",
      "Training:: Epoch 45, Iteration 60, Current loss 0.6401157975196838 Accuracy 76.93434796993903\n",
      "Training:: Epoch 45, Iteration 70, Current loss 0.5116822123527527 Accuracy 76.37440758293839\n",
      "Training:: Epoch 45, Iteration 80, Current loss 0.4213334918022156 Accuracy 74.0818141396176\n",
      "Training:: Epoch 45, Iteration 90, Current loss 0.6714656352996826 Accuracy 78.41452939010921\n",
      "Training:: Epoch 45, Iteration 100, Current loss 0.5242187976837158 Accuracy 85.54406570492361\n",
      "Training:: Epoch 45, Iteration 110, Current loss 0.7819831967353821 Accuracy 87.31012658227849\n",
      "Training:: Epoch 45, Iteration 120, Current loss 0.6215239763259888 Accuracy 83.06413175872478\n",
      "Training:: Epoch 45, Iteration 130, Current loss 0.4688158631324768 Accuracy 77.21331356959979\n",
      "Training:: Epoch 45, Iteration 140, Current loss 0.690085232257843 Accuracy 90.60172773309503\n",
      "Training:: Epoch 45, Iteration 150, Current loss 0.47633594274520874 Accuracy 85.53257686676427\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 45, Probability Accuracy 59.52593528607532\n",
      "Starting Training\n",
      "Training:: Epoch 46, Iteration 0, Current loss 0.46167024970054626 Accuracy 80.41259982253771\n",
      "Training:: Epoch 46, Iteration 10, Current loss 0.2892775237560272 Accuracy 80.66410562105357\n",
      "Training:: Epoch 46, Iteration 20, Current loss 0.4834046959877014 Accuracy 84.46631187870268\n",
      "Training:: Epoch 46, Iteration 30, Current loss 0.49739065766334534 Accuracy 75.77337749277712\n",
      "Training:: Epoch 46, Iteration 40, Current loss 0.4262770712375641 Accuracy 83.9238634171436\n",
      "Training:: Epoch 46, Iteration 50, Current loss 0.4151318371295929 Accuracy 80.24780032321782\n",
      "Training:: Epoch 46, Iteration 60, Current loss 0.4793182611465454 Accuracy 76.87547014976407\n",
      "Training:: Epoch 46, Iteration 70, Current loss 0.38895177841186523 Accuracy 72.1232397105734\n",
      "Training:: Epoch 46, Iteration 80, Current loss 0.39088571071624756 Accuracy 69.79577708549671\n",
      "Training:: Epoch 46, Iteration 90, Current loss 0.6481828689575195 Accuracy 83.87096774193549\n",
      "Training:: Epoch 46, Iteration 100, Current loss 0.7522793412208557 Accuracy 86.52930966911147\n",
      "Training:: Epoch 46, Iteration 110, Current loss 0.49152106046676636 Accuracy 80.90369393139842\n",
      "Training:: Epoch 46, Iteration 120, Current loss 0.3458388149738312 Accuracy 86.20496546574576\n",
      "Training:: Epoch 46, Iteration 130, Current loss 0.6187118291854858 Accuracy 81.71014320518154\n",
      "Training:: Epoch 46, Iteration 140, Current loss 0.4870714843273163 Accuracy 86.04255319148936\n",
      "Training:: Epoch 46, Iteration 150, Current loss 0.396729439496994 Accuracy 82.56714616235791\n",
      "Starting Training\n",
      "Training:: Epoch 47, Iteration 0, Current loss 0.501829981803894 Accuracy 81.75797712221554\n",
      "Training:: Epoch 47, Iteration 10, Current loss 0.31491100788116455 Accuracy 74.79900660857852\n",
      "Training:: Epoch 47, Iteration 20, Current loss 0.33239686489105225 Accuracy 79.04186627785877\n",
      "Training:: Epoch 47, Iteration 30, Current loss 0.4452857971191406 Accuracy 88.88181818181818\n",
      "Training:: Epoch 47, Iteration 40, Current loss 0.4280807375907898 Accuracy 76.1644522436701\n",
      "Training:: Epoch 47, Iteration 50, Current loss 0.5384282469749451 Accuracy 86.44759598754756\n",
      "Training:: Epoch 47, Iteration 60, Current loss 0.4293349087238312 Accuracy 81.60260586319218\n",
      "Training:: Epoch 47, Iteration 70, Current loss 0.3310020864009857 Accuracy 77.80125536013921\n",
      "Training:: Epoch 47, Iteration 80, Current loss 0.4403739869594574 Accuracy 59.976689976689975\n",
      "Training:: Epoch 47, Iteration 90, Current loss 0.7470268607139587 Accuracy 80.1323706377858\n",
      "Training:: Epoch 47, Iteration 100, Current loss 0.5887876749038696 Accuracy 89.75876487616597\n",
      "Training:: Epoch 47, Iteration 110, Current loss 0.888461172580719 Accuracy 77.58648958845713\n",
      "Training:: Epoch 47, Iteration 120, Current loss 0.4940909445285797 Accuracy 82.11511789181692\n",
      "Training:: Epoch 47, Iteration 130, Current loss 1.3773092031478882 Accuracy 74.82877568364745\n",
      "Training:: Epoch 47, Iteration 140, Current loss 0.43697986006736755 Accuracy 81.20142487046633\n",
      "Training:: Epoch 47, Iteration 150, Current loss 0.49425122141838074 Accuracy 86.53571994297481\n",
      "Starting Training\n",
      "Training:: Epoch 48, Iteration 0, Current loss 0.6895449161529541 Accuracy 75.51326595072646\n",
      "Training:: Epoch 48, Iteration 10, Current loss 0.7963004112243652 Accuracy 83.84666101535274\n",
      "Training:: Epoch 48, Iteration 20, Current loss 0.5841600298881531 Accuracy 77.20420283234354\n",
      "Training:: Epoch 48, Iteration 30, Current loss 0.5837540030479431 Accuracy 83.0717672061468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 48, Iteration 40, Current loss 1.6196078062057495 Accuracy 74.29989915444884\n",
      "Training:: Epoch 48, Iteration 50, Current loss 0.6484573483467102 Accuracy 85.91268375563271\n",
      "Training:: Epoch 48, Iteration 60, Current loss 1.7081228494644165 Accuracy 59.91799077396207\n",
      "Training:: Epoch 48, Iteration 70, Current loss 1.8939533233642578 Accuracy 57.39092276830492\n",
      "Training:: Epoch 48, Iteration 80, Current loss 1.26755952835083 Accuracy 60.95810705973623\n",
      "Training:: Epoch 48, Iteration 90, Current loss 1.3438920974731445 Accuracy 79.02864643656861\n",
      "Training:: Epoch 48, Iteration 100, Current loss 2.7114956378936768 Accuracy 73.11583729093537\n",
      "Training:: Epoch 48, Iteration 110, Current loss 1.318811297416687 Accuracy 71.5345223379834\n",
      "Training:: Epoch 48, Iteration 120, Current loss 3.8544368743896484 Accuracy 49.372362587468615\n",
      "Training:: Epoch 48, Iteration 130, Current loss 1.0425692796707153 Accuracy 66.95154822168168\n",
      "Training:: Epoch 48, Iteration 140, Current loss 1.9927173852920532 Accuracy 68.00483226465941\n",
      "Training:: Epoch 48, Iteration 150, Current loss 1.45036780834198 Accuracy 74.78539056251667\n",
      "Starting Training\n",
      "Training:: Epoch 49, Iteration 0, Current loss 1.392429232597351 Accuracy 57.57478700916021\n",
      "Training:: Epoch 49, Iteration 10, Current loss 1.6787807941436768 Accuracy 58.936834798704304\n",
      "Training:: Epoch 49, Iteration 20, Current loss 1.6015933752059937 Accuracy 77.23814699089797\n",
      "Training:: Epoch 49, Iteration 30, Current loss 3.419053077697754 Accuracy 54.61470662301687\n",
      "Training:: Epoch 49, Iteration 40, Current loss 3.613335609436035 Accuracy 65.60384852044527\n",
      "Training:: Epoch 49, Iteration 50, Current loss 2.173501968383789 Accuracy 69.65586898492172\n",
      "Training:: Epoch 49, Iteration 60, Current loss 0.7957168817520142 Accuracy 67.69396869018992\n",
      "Training:: Epoch 49, Iteration 70, Current loss 1.0458093881607056 Accuracy 73.05583369093597\n",
      "Training:: Epoch 49, Iteration 80, Current loss 1.8610966205596924 Accuracy 68.26127527216174\n",
      "Training:: Epoch 49, Iteration 90, Current loss 1.487424373626709 Accuracy 64.77471184072651\n",
      "Training:: Epoch 49, Iteration 100, Current loss 2.707705497741699 Accuracy 70.18424284962273\n",
      "Training:: Epoch 49, Iteration 110, Current loss 1.8242368698120117 Accuracy 63.61358914823414\n",
      "Training:: Epoch 49, Iteration 120, Current loss 0.9235967397689819 Accuracy 74.86190229587433\n",
      "Training:: Epoch 49, Iteration 130, Current loss 1.0963404178619385 Accuracy 76.27984113821847\n",
      "Training:: Epoch 49, Iteration 140, Current loss 1.3671557903289795 Accuracy 82.27539647665083\n",
      "Training:: Epoch 49, Iteration 150, Current loss 1.1364383697509766 Accuracy 77.68033326024994\n",
      "Starting Training\n",
      "Training:: Epoch 50, Iteration 0, Current loss 0.9953885674476624 Accuracy 70.14771556166266\n",
      "Training:: Epoch 50, Iteration 10, Current loss 1.1937555074691772 Accuracy 75.94245370987079\n",
      "Training:: Epoch 50, Iteration 20, Current loss 0.5735982656478882 Accuracy 60.01634715410908\n",
      "Training:: Epoch 50, Iteration 30, Current loss 0.6241617202758789 Accuracy 82.14592274678111\n",
      "Training:: Epoch 50, Iteration 40, Current loss 1.0253868103027344 Accuracy 73.73056994818653\n",
      "Training:: Epoch 50, Iteration 50, Current loss 0.6843195557594299 Accuracy 76.29318059781636\n",
      "Training:: Epoch 50, Iteration 60, Current loss 1.1585545539855957 Accuracy 77.77236564377334\n",
      "Training:: Epoch 50, Iteration 70, Current loss 0.8507739901542664 Accuracy 83.22153817881099\n",
      "Training:: Epoch 50, Iteration 80, Current loss 0.8008251786231995 Accuracy 78.79803761242846\n",
      "Training:: Epoch 50, Iteration 90, Current loss 0.9014933705329895 Accuracy 79.36035465484484\n",
      "Training:: Epoch 50, Iteration 100, Current loss 0.673145592212677 Accuracy 80.55177533094148\n",
      "Training:: Epoch 50, Iteration 110, Current loss 0.7423316240310669 Accuracy 76.28026412325752\n",
      "Training:: Epoch 50, Iteration 120, Current loss 0.8921581506729126 Accuracy 63.9180082366155\n",
      "Training:: Epoch 50, Iteration 130, Current loss 1.1707260608673096 Accuracy 80.91277890466532\n",
      "Training:: Epoch 50, Iteration 140, Current loss 0.7497600317001343 Accuracy 69.24279572184913\n",
      "Training:: Epoch 50, Iteration 150, Current loss 0.9066439270973206 Accuracy 73.93960430406109\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 50, Probability Accuracy 56.07656295314248\n",
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 0.7255829572677612 Accuracy 80.8848945731075\n",
      "Training:: Epoch 51, Iteration 10, Current loss 0.49206581711769104 Accuracy 73.22080291970804\n",
      "Training:: Epoch 51, Iteration 20, Current loss 0.8103839755058289 Accuracy 81.99096225412015\n",
      "Training:: Epoch 51, Iteration 30, Current loss 0.9664462804794312 Accuracy 78.01202117161569\n",
      "Training:: Epoch 51, Iteration 40, Current loss 0.6260826587677002 Accuracy 79.23246419390378\n",
      "Training:: Epoch 51, Iteration 50, Current loss 0.5192742943763733 Accuracy 76.81767337807607\n",
      "Training:: Epoch 51, Iteration 60, Current loss 0.7279191613197327 Accuracy 77.36943907156673\n",
      "Training:: Epoch 51, Iteration 70, Current loss 0.5263364911079407 Accuracy 68.7755658576493\n",
      "Training:: Epoch 51, Iteration 80, Current loss 0.6085926294326782 Accuracy 71.61340913546364\n",
      "Training:: Epoch 51, Iteration 90, Current loss 0.5794283747673035 Accuracy 65.53574926542605\n",
      "Training:: Epoch 51, Iteration 100, Current loss 0.6763962507247925 Accuracy 76.78643520387566\n",
      "Training:: Epoch 51, Iteration 110, Current loss 0.6595668792724609 Accuracy 82.55896435538216\n",
      "Training:: Epoch 51, Iteration 120, Current loss 0.5354045033454895 Accuracy 83.36871385696509\n",
      "Training:: Epoch 51, Iteration 130, Current loss 1.059035301208496 Accuracy 74.67854725919243\n",
      "Training:: Epoch 51, Iteration 140, Current loss 0.7702869772911072 Accuracy 78.39615931721195\n",
      "Training:: Epoch 51, Iteration 150, Current loss 0.73917156457901 Accuracy 76.39397639397639\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 0.4893668293952942 Accuracy 77.89097130414495\n",
      "Training:: Epoch 52, Iteration 10, Current loss 0.6372855305671692 Accuracy 84.2676859604417\n",
      "Training:: Epoch 52, Iteration 20, Current loss 0.47996625304222107 Accuracy 79.66739639743442\n",
      "Training:: Epoch 52, Iteration 30, Current loss 0.5617275834083557 Accuracy 83.35516739446871\n",
      "Training:: Epoch 52, Iteration 40, Current loss 0.6749662160873413 Accuracy 79.26004636785163\n",
      "Training:: Epoch 52, Iteration 50, Current loss 0.7762157320976257 Accuracy 72.09107866469415\n",
      "Training:: Epoch 52, Iteration 60, Current loss 0.7146955132484436 Accuracy 72.75614306696836\n",
      "Training:: Epoch 52, Iteration 70, Current loss 0.33611130714416504 Accuracy 74.56966736450337\n",
      "Training:: Epoch 52, Iteration 80, Current loss 0.7021580338478088 Accuracy 78.39476236663434\n",
      "Training:: Epoch 52, Iteration 90, Current loss 0.42075246572494507 Accuracy 78.64149821640903\n",
      "Training:: Epoch 52, Iteration 100, Current loss 0.45767277479171753 Accuracy 58.37565481665134\n",
      "Training:: Epoch 52, Iteration 110, Current loss 0.6588044762611389 Accuracy 72.3740414801091\n",
      "Training:: Epoch 52, Iteration 120, Current loss 0.5024261474609375 Accuracy 75.16040912267327\n",
      "Training:: Epoch 52, Iteration 130, Current loss 0.43488314747810364 Accuracy 66.2335260115607\n",
      "Training:: Epoch 52, Iteration 140, Current loss 0.6091083288192749 Accuracy 77.46889065395817\n",
      "Training:: Epoch 52, Iteration 150, Current loss 0.509194552898407 Accuracy 86.4491563386591\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 0.4553735852241516 Accuracy 72.18469757679831\n",
      "Training:: Epoch 53, Iteration 10, Current loss 0.5962146520614624 Accuracy 74.08798011060937\n",
      "Training:: Epoch 53, Iteration 20, Current loss 0.5426624417304993 Accuracy 62.24153989444272\n",
      "Training:: Epoch 53, Iteration 30, Current loss 0.6056907773017883 Accuracy 78.17557182894856\n",
      "Training:: Epoch 53, Iteration 40, Current loss 0.6609038710594177 Accuracy 77.08256992793453\n",
      "Training:: Epoch 53, Iteration 50, Current loss 0.3627384305000305 Accuracy 84.92470813828888\n",
      "Training:: Epoch 53, Iteration 60, Current loss 0.520678699016571 Accuracy 87.1771476698484\n",
      "Training:: Epoch 53, Iteration 70, Current loss 0.5429017543792725 Accuracy 85.43602373222882\n",
      "Training:: Epoch 53, Iteration 80, Current loss 0.492486834526062 Accuracy 74.75334399251258\n",
      "Training:: Epoch 53, Iteration 90, Current loss 0.5108239650726318 Accuracy 69.18338720497653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 53, Iteration 100, Current loss 0.5673775672912598 Accuracy 80.37845963391989\n",
      "Training:: Epoch 53, Iteration 110, Current loss 0.33707237243652344 Accuracy 72.08861509299847\n",
      "Training:: Epoch 53, Iteration 120, Current loss 0.6426824331283569 Accuracy 76.73886590411293\n",
      "Training:: Epoch 53, Iteration 130, Current loss 0.6644101738929749 Accuracy 85.28066932580991\n",
      "Training:: Epoch 53, Iteration 140, Current loss 0.5826817750930786 Accuracy 75.69679355853066\n",
      "Training:: Epoch 53, Iteration 150, Current loss 0.503989577293396 Accuracy 82.84086640791286\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 0.4977918863296509 Accuracy 80.45588692140416\n",
      "Training:: Epoch 54, Iteration 10, Current loss 0.4892454743385315 Accuracy 77.6937391199952\n",
      "Training:: Epoch 54, Iteration 20, Current loss 0.5140408277511597 Accuracy 86.02595089683247\n",
      "Training:: Epoch 54, Iteration 30, Current loss 0.48299291729927063 Accuracy 81.7298652460044\n",
      "Training:: Epoch 54, Iteration 40, Current loss 0.36366209387779236 Accuracy 83.64688271230327\n",
      "Training:: Epoch 54, Iteration 50, Current loss 0.576624870300293 Accuracy 79.94174367215749\n",
      "Training:: Epoch 54, Iteration 60, Current loss 0.5498493313789368 Accuracy 83.45185995623632\n",
      "Training:: Epoch 54, Iteration 70, Current loss 0.6300239562988281 Accuracy 83.83329977853835\n",
      "Training:: Epoch 54, Iteration 80, Current loss 0.3628046214580536 Accuracy 69.71177332681974\n",
      "Training:: Epoch 54, Iteration 90, Current loss 0.6595958471298218 Accuracy 74.56528925619834\n",
      "Training:: Epoch 54, Iteration 100, Current loss 0.4118722081184387 Accuracy 76.03533325685443\n",
      "Training:: Epoch 54, Iteration 110, Current loss 0.47682905197143555 Accuracy 76.7140814329796\n",
      "Training:: Epoch 54, Iteration 120, Current loss 0.42672333121299744 Accuracy 76.03645231677577\n",
      "Training:: Epoch 54, Iteration 130, Current loss 0.5079123377799988 Accuracy 73.21635917946071\n",
      "Training:: Epoch 54, Iteration 140, Current loss 0.49219638109207153 Accuracy 82.7880319509003\n",
      "Training:: Epoch 54, Iteration 150, Current loss 0.3854394555091858 Accuracy 81.902096236303\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 0.4316229224205017 Accuracy 82.68808347062054\n",
      "Training:: Epoch 55, Iteration 10, Current loss 0.5131577849388123 Accuracy 81.43910638759354\n",
      "Training:: Epoch 55, Iteration 20, Current loss 0.90333491563797 Accuracy 83.35114594466215\n",
      "Training:: Epoch 55, Iteration 30, Current loss 0.4231840968132019 Accuracy 84.53100082153385\n",
      "Training:: Epoch 55, Iteration 40, Current loss 0.4545667767524719 Accuracy 78.17523687852656\n",
      "Training:: Epoch 55, Iteration 50, Current loss 0.5588630437850952 Accuracy 81.9517709118312\n",
      "Training:: Epoch 55, Iteration 60, Current loss 0.5960466265678406 Accuracy 86.84453393741086\n",
      "Training:: Epoch 55, Iteration 70, Current loss 0.5613127946853638 Accuracy 72.99444003177125\n",
      "Training:: Epoch 55, Iteration 80, Current loss 1.0695350170135498 Accuracy 75.88662790697674\n",
      "Training:: Epoch 55, Iteration 90, Current loss 0.3233768939971924 Accuracy 80.28392254152023\n",
      "Training:: Epoch 55, Iteration 100, Current loss 0.46021175384521484 Accuracy 80.73790720631787\n",
      "Training:: Epoch 55, Iteration 110, Current loss 0.5385803580284119 Accuracy 65.45304906725192\n",
      "Training:: Epoch 55, Iteration 120, Current loss 0.4381837844848633 Accuracy 80.3116064655424\n",
      "Training:: Epoch 55, Iteration 130, Current loss 0.4543193280696869 Accuracy 78.18616548042705\n",
      "Training:: Epoch 55, Iteration 140, Current loss 0.5357911586761475 Accuracy 75.25403466826062\n",
      "Training:: Epoch 55, Iteration 150, Current loss 0.37909021973609924 Accuracy 87.60431819921905\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 58.38443054232092\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 0.30102208256721497 Accuracy 80.0458353089932\n",
      "Training:: Epoch 56, Iteration 10, Current loss 0.5308201313018799 Accuracy 67.81970267012456\n",
      "Training:: Epoch 56, Iteration 20, Current loss 0.2513652443885803 Accuracy 44.09223255377101\n",
      "Training:: Epoch 56, Iteration 30, Current loss 0.34141117334365845 Accuracy 81.97310002611648\n",
      "Training:: Epoch 56, Iteration 40, Current loss 0.4548041820526123 Accuracy 74.67789039683903\n",
      "Training:: Epoch 56, Iteration 50, Current loss 0.5733311772346497 Accuracy 83.42010706167045\n",
      "Training:: Epoch 56, Iteration 60, Current loss 0.5737685561180115 Accuracy 82.01083631914707\n",
      "Training:: Epoch 56, Iteration 70, Current loss 0.5752525329589844 Accuracy 75.1346624693949\n",
      "Training:: Epoch 56, Iteration 80, Current loss 0.28319603204727173 Accuracy 84.93747943402435\n",
      "Training:: Epoch 56, Iteration 90, Current loss 0.5519656538963318 Accuracy 80.28420275590551\n",
      "Training:: Epoch 56, Iteration 100, Current loss 0.4451341927051544 Accuracy 77.70691806460557\n",
      "Training:: Epoch 56, Iteration 110, Current loss 0.4553062617778778 Accuracy 78.44760672703751\n",
      "Training:: Epoch 56, Iteration 120, Current loss 0.4330146014690399 Accuracy 78.64768683274022\n",
      "Training:: Epoch 56, Iteration 130, Current loss 0.506417453289032 Accuracy 77.40228013029316\n",
      "Training:: Epoch 56, Iteration 140, Current loss 0.4958699345588684 Accuracy 76.07075424823611\n",
      "Training:: Epoch 56, Iteration 150, Current loss 0.395160049200058 Accuracy 78.25356320408291\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 0.29251182079315186 Accuracy 78.37222538417758\n",
      "Training:: Epoch 57, Iteration 10, Current loss 0.39844635128974915 Accuracy 84.5729909856026\n",
      "Training:: Epoch 57, Iteration 20, Current loss 0.5200355052947998 Accuracy 80.82969875240897\n",
      "Training:: Epoch 57, Iteration 30, Current loss 0.2520012855529785 Accuracy 90.02460836684473\n",
      "Training:: Epoch 57, Iteration 40, Current loss 0.37774455547332764 Accuracy 84.98809838667019\n",
      "Training:: Epoch 57, Iteration 50, Current loss 0.4968833327293396 Accuracy 85.3061224489796\n",
      "Training:: Epoch 57, Iteration 60, Current loss 0.3468371629714966 Accuracy 83.27442335021681\n",
      "Training:: Epoch 57, Iteration 70, Current loss 0.49106860160827637 Accuracy 85.72876203344029\n",
      "Training:: Epoch 57, Iteration 80, Current loss 0.5190942287445068 Accuracy 81.93799888620754\n",
      "Training:: Epoch 57, Iteration 90, Current loss 0.38385623693466187 Accuracy 82.19437652811736\n",
      "Training:: Epoch 57, Iteration 100, Current loss 0.4762926399707794 Accuracy 73.26591878653815\n",
      "Training:: Epoch 57, Iteration 110, Current loss 0.3369046449661255 Accuracy 84.5016964747829\n",
      "Training:: Epoch 57, Iteration 120, Current loss 0.6061189770698547 Accuracy 81.33679850564444\n",
      "Training:: Epoch 57, Iteration 130, Current loss 0.2863188087940216 Accuracy 84.9663984070356\n",
      "Training:: Epoch 57, Iteration 140, Current loss 0.4000321626663208 Accuracy 75.61923803176718\n",
      "Training:: Epoch 57, Iteration 150, Current loss 0.6190887689590454 Accuracy 80.91245213257626\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 0.6349055171012878 Accuracy 79.04733492442323\n",
      "Training:: Epoch 58, Iteration 10, Current loss 0.39983245730400085 Accuracy 78.9040056371619\n",
      "Training:: Epoch 58, Iteration 20, Current loss 0.4113365709781647 Accuracy 76.65863374809423\n",
      "Training:: Epoch 58, Iteration 30, Current loss 0.47020283341407776 Accuracy 81.83497202455321\n",
      "Training:: Epoch 58, Iteration 40, Current loss 0.48795536160469055 Accuracy 80.19306399713979\n",
      "Training:: Epoch 58, Iteration 50, Current loss 0.38279348611831665 Accuracy 81.4162144737549\n",
      "Training:: Epoch 58, Iteration 60, Current loss 0.3699408769607544 Accuracy 76.89293760407483\n",
      "Training:: Epoch 58, Iteration 70, Current loss 0.414632648229599 Accuracy 77.50836876866009\n",
      "Training:: Epoch 58, Iteration 80, Current loss 0.2881146967411041 Accuracy 78.70337852075578\n",
      "Training:: Epoch 58, Iteration 90, Current loss 0.4833136796951294 Accuracy 81.23648648648648\n",
      "Training:: Epoch 58, Iteration 100, Current loss 0.3652467429637909 Accuracy 81.91738816738817\n",
      "Training:: Epoch 58, Iteration 110, Current loss 0.39075037837028503 Accuracy 81.66122597025753\n",
      "Training:: Epoch 58, Iteration 120, Current loss 0.39172786474227905 Accuracy 80.9599027946537\n",
      "Training:: Epoch 58, Iteration 130, Current loss 0.5027429461479187 Accuracy 77.13673082676773\n",
      "Training:: Epoch 58, Iteration 140, Current loss 0.6396894454956055 Accuracy 84.23754188351037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 58, Iteration 150, Current loss 0.4685680568218231 Accuracy 80.95013278253172\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 0.7853716611862183 Accuracy 85.24777636594663\n",
      "Training:: Epoch 59, Iteration 10, Current loss 0.3483595550060272 Accuracy 81.49237398262773\n",
      "Training:: Epoch 59, Iteration 20, Current loss 0.42729106545448303 Accuracy 82.44808306709265\n",
      "Training:: Epoch 59, Iteration 30, Current loss 0.42377156019210815 Accuracy 78.66486486486487\n",
      "Training:: Epoch 59, Iteration 40, Current loss 0.48140111565589905 Accuracy 82.7889136352388\n",
      "Training:: Epoch 59, Iteration 50, Current loss 0.46395620703697205 Accuracy 78.14928184600895\n",
      "Training:: Epoch 59, Iteration 60, Current loss 0.5130746960639954 Accuracy 81.5059040024536\n",
      "Training:: Epoch 59, Iteration 70, Current loss 0.6304601430892944 Accuracy 87.08680877355576\n",
      "Training:: Epoch 59, Iteration 80, Current loss 0.43593835830688477 Accuracy 82.9955050462217\n",
      "Training:: Epoch 59, Iteration 90, Current loss 0.2703300714492798 Accuracy 87.2914208663408\n",
      "Training:: Epoch 59, Iteration 100, Current loss 0.5250424742698669 Accuracy 79.01414034624025\n",
      "Training:: Epoch 59, Iteration 110, Current loss 0.2809601128101349 Accuracy 84.27283588573911\n",
      "Training:: Epoch 59, Iteration 120, Current loss 0.35047683119773865 Accuracy 83.13542879317035\n",
      "Training:: Epoch 59, Iteration 130, Current loss 0.34846022725105286 Accuracy 74.89283012524166\n",
      "Training:: Epoch 59, Iteration 140, Current loss 0.41861265897750854 Accuracy 73.23553090616335\n",
      "Training:: Epoch 59, Iteration 150, Current loss 0.4404379427433014 Accuracy 84.83874018726947\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 0.770514190196991 Accuracy 79.64835565272843\n",
      "Training:: Epoch 60, Iteration 10, Current loss 0.4387666583061218 Accuracy 77.38183356244052\n",
      "Training:: Epoch 60, Iteration 20, Current loss 0.5605876445770264 Accuracy 80.68585311156794\n",
      "Training:: Epoch 60, Iteration 30, Current loss 0.37075406312942505 Accuracy 79.53144266337854\n",
      "Training:: Epoch 60, Iteration 40, Current loss 0.36814236640930176 Accuracy 86.99753117680572\n",
      "Training:: Epoch 60, Iteration 50, Current loss 0.4875918924808502 Accuracy 85.37544766042389\n",
      "Training:: Epoch 60, Iteration 60, Current loss 0.46530359983444214 Accuracy 86.19682307832755\n",
      "Training:: Epoch 60, Iteration 70, Current loss 0.253914475440979 Accuracy 82.75440313111547\n",
      "Training:: Epoch 60, Iteration 80, Current loss 0.44052350521087646 Accuracy 83.41594921297504\n",
      "Training:: Epoch 60, Iteration 90, Current loss 0.3415888845920563 Accuracy 77.23445595854922\n",
      "Training:: Epoch 60, Iteration 100, Current loss 0.5352612733840942 Accuracy 76.28395694515633\n",
      "Training:: Epoch 60, Iteration 110, Current loss 0.5651022791862488 Accuracy 83.42159797375086\n",
      "Training:: Epoch 60, Iteration 120, Current loss 0.31476718187332153 Accuracy 82.34734768729199\n",
      "Training:: Epoch 60, Iteration 130, Current loss 0.5207963585853577 Accuracy 70.45438179592425\n",
      "Training:: Epoch 60, Iteration 140, Current loss 0.40391266345977783 Accuracy 83.02880938217048\n",
      "Training:: Epoch 60, Iteration 150, Current loss 0.3411344587802887 Accuracy 76.73068334077713\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 56.24300865890542\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 0.5185528993606567 Accuracy 79.69179041748389\n",
      "Training:: Epoch 61, Iteration 10, Current loss 0.3917926549911499 Accuracy 72.53878355084954\n",
      "Training:: Epoch 61, Iteration 20, Current loss 0.40397387742996216 Accuracy 78.9269760233385\n",
      "Training:: Epoch 61, Iteration 30, Current loss 0.5479296445846558 Accuracy 79.6868357659062\n",
      "Training:: Epoch 61, Iteration 40, Current loss 0.5487831234931946 Accuracy 84.0216684723727\n",
      "Training:: Epoch 61, Iteration 50, Current loss 0.5119888186454773 Accuracy 78.33160426294445\n",
      "Training:: Epoch 61, Iteration 60, Current loss 1.4135929346084595 Accuracy 74.8537459898094\n",
      "Training:: Epoch 61, Iteration 70, Current loss 1.6840336322784424 Accuracy 67.82373215164944\n",
      "Training:: Epoch 61, Iteration 80, Current loss 1.2503546476364136 Accuracy 79.43018637335777\n",
      "Training:: Epoch 61, Iteration 90, Current loss 2.7704966068267822 Accuracy 71.17049951253082\n",
      "Training:: Epoch 61, Iteration 100, Current loss 3.2833335399627686 Accuracy 64.60442665410878\n",
      "Training:: Epoch 61, Iteration 110, Current loss 2.805938720703125 Accuracy 43.40366334338048\n",
      "Training:: Epoch 61, Iteration 120, Current loss 2.591369152069092 Accuracy 68.88304693511157\n",
      "Training:: Epoch 61, Iteration 130, Current loss 2.3819408416748047 Accuracy 59.97903216906693\n",
      "Training:: Epoch 61, Iteration 140, Current loss 1.4721662998199463 Accuracy 63.920186628282124\n",
      "Training:: Epoch 61, Iteration 150, Current loss 0.98619145154953 Accuracy 84.74270237736985\n",
      "Starting Training\n",
      "Training:: Epoch 62, Iteration 0, Current loss 1.208770751953125 Accuracy 76.42870019830539\n",
      "Training:: Epoch 62, Iteration 10, Current loss 1.4507503509521484 Accuracy 65.79356634198248\n",
      "Training:: Epoch 62, Iteration 20, Current loss 1.1787279844284058 Accuracy 67.7613652660261\n",
      "Training:: Epoch 62, Iteration 30, Current loss 1.0803534984588623 Accuracy 76.24474497290178\n",
      "Training:: Epoch 62, Iteration 40, Current loss 1.33378005027771 Accuracy 75.52803525300106\n",
      "Training:: Epoch 62, Iteration 50, Current loss 1.5716304779052734 Accuracy 71.98963160788627\n",
      "Training:: Epoch 62, Iteration 60, Current loss 1.3361124992370605 Accuracy 58.416185774873114\n",
      "Training:: Epoch 62, Iteration 70, Current loss 1.0164358615875244 Accuracy 65.51036930933549\n",
      "Training:: Epoch 62, Iteration 80, Current loss 0.8834786415100098 Accuracy 84.92016116997463\n",
      "Training:: Epoch 62, Iteration 90, Current loss 1.435877799987793 Accuracy 71.69457048975121\n",
      "Training:: Epoch 62, Iteration 100, Current loss 0.9977744817733765 Accuracy 76.86011511855098\n",
      "Training:: Epoch 62, Iteration 110, Current loss 0.6655104160308838 Accuracy 75.03118214995149\n",
      "Training:: Epoch 62, Iteration 120, Current loss 1.4349948167800903 Accuracy 82.18421658102615\n",
      "Training:: Epoch 62, Iteration 130, Current loss 1.7875120639801025 Accuracy 70.03177966101696\n",
      "Training:: Epoch 62, Iteration 140, Current loss 1.3206819295883179 Accuracy 71.48831711973816\n",
      "Training:: Epoch 62, Iteration 150, Current loss 2.0466904640197754 Accuracy 64.58350133053786\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 1.0376595258712769 Accuracy 68.69685767097967\n",
      "Training:: Epoch 63, Iteration 10, Current loss 0.9131827354431152 Accuracy 77.2096671405461\n",
      "Training:: Epoch 63, Iteration 20, Current loss 0.9199084043502808 Accuracy 66.31266415439077\n",
      "Training:: Epoch 63, Iteration 30, Current loss 1.1291011571884155 Accuracy 70.8259325044405\n",
      "Training:: Epoch 63, Iteration 40, Current loss 1.0394928455352783 Accuracy 81.37738483015356\n",
      "Training:: Epoch 63, Iteration 50, Current loss 0.956888735294342 Accuracy 78.78436568400133\n",
      "Training:: Epoch 63, Iteration 60, Current loss 1.0062264204025269 Accuracy 74.01618445848575\n",
      "Training:: Epoch 63, Iteration 70, Current loss 0.8564500212669373 Accuracy 74.31247945655747\n",
      "Training:: Epoch 63, Iteration 80, Current loss 0.8879990577697754 Accuracy 75.94642054967942\n",
      "Training:: Epoch 63, Iteration 90, Current loss 1.1238418817520142 Accuracy 65.39348771182681\n",
      "Training:: Epoch 63, Iteration 100, Current loss 0.9403849244117737 Accuracy 75.69290826284971\n",
      "Training:: Epoch 63, Iteration 110, Current loss 0.9714078307151794 Accuracy 78.99755839130235\n",
      "Training:: Epoch 63, Iteration 120, Current loss 0.566864013671875 Accuracy 82.59435028248588\n",
      "Training:: Epoch 63, Iteration 130, Current loss 1.0297104120254517 Accuracy 66.06640405177265\n",
      "Training:: Epoch 63, Iteration 140, Current loss 0.5870928168296814 Accuracy 74.30064149196481\n",
      "Training:: Epoch 63, Iteration 150, Current loss 0.7054204344749451 Accuracy 54.82243229432805\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 0.8254209160804749 Accuracy 76.57839354612416\n",
      "Training:: Epoch 64, Iteration 10, Current loss 0.4806518852710724 Accuracy 76.76952507693588\n",
      "Training:: Epoch 64, Iteration 20, Current loss 0.5354031324386597 Accuracy 78.41889117043121\n",
      "Training:: Epoch 64, Iteration 30, Current loss 1.1490693092346191 Accuracy 85.93974175035868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 64, Iteration 40, Current loss 1.0374678373336792 Accuracy 85.23775727466288\n",
      "Training:: Epoch 64, Iteration 50, Current loss 0.585638165473938 Accuracy 71.83483290488432\n",
      "Training:: Epoch 64, Iteration 60, Current loss 0.4615536332130432 Accuracy 71.10806230328464\n",
      "Training:: Epoch 64, Iteration 70, Current loss 0.5565945506095886 Accuracy 80.71595994033667\n",
      "Training:: Epoch 64, Iteration 80, Current loss 0.6938977241516113 Accuracy 78.33256563795486\n",
      "Training:: Epoch 64, Iteration 90, Current loss 0.45407742261886597 Accuracy 73.8249170015726\n",
      "Training:: Epoch 64, Iteration 100, Current loss 0.7585833072662354 Accuracy 75.00523194977328\n",
      "Training:: Epoch 64, Iteration 110, Current loss 1.324840784072876 Accuracy 79.72201271414718\n",
      "Training:: Epoch 64, Iteration 120, Current loss 0.8253192901611328 Accuracy 65.88791676519271\n",
      "Training:: Epoch 64, Iteration 130, Current loss 1.2109684944152832 Accuracy 77.85038525309666\n",
      "Training:: Epoch 64, Iteration 140, Current loss 1.095654845237732 Accuracy 80.3228442991418\n",
      "Training:: Epoch 64, Iteration 150, Current loss 1.1424401998519897 Accuracy 71.76039853550208\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 1.118380069732666 Accuracy 67.69979508196721\n",
      "Training:: Epoch 65, Iteration 10, Current loss 0.5150654911994934 Accuracy 80.34668540641836\n",
      "Training:: Epoch 65, Iteration 20, Current loss 1.2334141731262207 Accuracy 76.382998340402\n",
      "Training:: Epoch 65, Iteration 30, Current loss 0.5852351188659668 Accuracy 64.7910134040867\n",
      "Training:: Epoch 65, Iteration 40, Current loss 0.40827661752700806 Accuracy 56.76410967673104\n",
      "Training:: Epoch 65, Iteration 50, Current loss 0.6016026735305786 Accuracy 75.70445297942732\n",
      "Training:: Epoch 65, Iteration 60, Current loss 0.45776116847991943 Accuracy 71.1942837699898\n",
      "Training:: Epoch 65, Iteration 70, Current loss 0.5230143666267395 Accuracy 74.0699238009861\n",
      "Training:: Epoch 65, Iteration 80, Current loss 0.5549679398536682 Accuracy 75.90830053994692\n",
      "Training:: Epoch 65, Iteration 90, Current loss 0.7022014856338501 Accuracy 75.68206229860365\n",
      "Training:: Epoch 65, Iteration 100, Current loss 0.527960479259491 Accuracy 77.77862653731572\n",
      "Training:: Epoch 65, Iteration 110, Current loss 0.5908844470977783 Accuracy 80.23559564295924\n",
      "Training:: Epoch 65, Iteration 120, Current loss 0.4570409953594208 Accuracy 78.14917530287549\n",
      "Training:: Epoch 65, Iteration 130, Current loss 0.6607208847999573 Accuracy 57.608225108225106\n",
      "Training:: Epoch 65, Iteration 140, Current loss 0.5749099850654602 Accuracy 65.22682579507696\n",
      "Training:: Epoch 65, Iteration 150, Current loss 0.7075929045677185 Accuracy 75.74079528718704\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 56.33270497576335\n",
      "Starting Training\n",
      "Training:: Epoch 66, Iteration 0, Current loss 0.47607746720314026 Accuracy 77.28880355699272\n",
      "Training:: Epoch 66, Iteration 10, Current loss 0.7181161046028137 Accuracy 74.97005988023952\n",
      "Training:: Epoch 66, Iteration 20, Current loss 0.629932701587677 Accuracy 75.64356435643565\n",
      "Training:: Epoch 66, Iteration 30, Current loss 0.3678922653198242 Accuracy 66.56013945380593\n",
      "Training:: Epoch 66, Iteration 40, Current loss 0.48316895961761475 Accuracy 66.93565217391304\n",
      "Training:: Epoch 66, Iteration 50, Current loss 0.564974308013916 Accuracy 76.00402484906816\n",
      "Training:: Epoch 66, Iteration 60, Current loss 0.44008636474609375 Accuracy 76.15421227986673\n",
      "Training:: Epoch 66, Iteration 70, Current loss 0.460925817489624 Accuracy 79.28642220019822\n",
      "Training:: Epoch 66, Iteration 80, Current loss 0.32558390498161316 Accuracy 72.43114073701422\n",
      "Training:: Epoch 66, Iteration 90, Current loss 0.4129660129547119 Accuracy 77.63065659319261\n",
      "Training:: Epoch 66, Iteration 100, Current loss 0.5522177815437317 Accuracy 70.8092485549133\n",
      "Training:: Epoch 66, Iteration 110, Current loss 0.6583216190338135 Accuracy 80.42325798379915\n",
      "Training:: Epoch 66, Iteration 120, Current loss 0.8397674560546875 Accuracy 79.81094441071893\n",
      "Training:: Epoch 66, Iteration 130, Current loss 0.5387330651283264 Accuracy 69.21933611720574\n",
      "Training:: Epoch 66, Iteration 140, Current loss 0.5133842825889587 Accuracy 80.36828036828037\n",
      "Training:: Epoch 66, Iteration 150, Current loss 0.7694207429885864 Accuracy 81.12116641528407\n",
      "Starting Training\n",
      "Training:: Epoch 67, Iteration 0, Current loss 0.4806809425354004 Accuracy 81.15932294465341\n",
      "Training:: Epoch 67, Iteration 10, Current loss 0.6397438049316406 Accuracy 76.16127548673923\n",
      "Training:: Epoch 67, Iteration 20, Current loss 0.4011113941669464 Accuracy 81.53165920299303\n",
      "Training:: Epoch 67, Iteration 30, Current loss 0.44403791427612305 Accuracy 78.95447639412527\n",
      "Training:: Epoch 67, Iteration 40, Current loss 0.5225147604942322 Accuracy 83.82962195753495\n",
      "Training:: Epoch 67, Iteration 50, Current loss 0.3300033509731293 Accuracy 81.22935022026432\n",
      "Training:: Epoch 67, Iteration 60, Current loss 0.562409520149231 Accuracy 64.9092808967417\n",
      "Training:: Epoch 67, Iteration 70, Current loss 0.7089394330978394 Accuracy 80.63323594299683\n",
      "Training:: Epoch 67, Iteration 80, Current loss 0.7095388174057007 Accuracy 70.73464472099559\n",
      "Training:: Epoch 67, Iteration 90, Current loss 0.9279162287712097 Accuracy 84.89410117276387\n",
      "Training:: Epoch 67, Iteration 100, Current loss 0.4183897376060486 Accuracy 73.59289216713465\n",
      "Training:: Epoch 67, Iteration 110, Current loss 0.4347177743911743 Accuracy 69.57381269826675\n",
      "Training:: Epoch 67, Iteration 120, Current loss 0.5962257385253906 Accuracy 81.79014108642258\n",
      "Training:: Epoch 67, Iteration 130, Current loss 0.3657974600791931 Accuracy 87.92361075933337\n",
      "Training:: Epoch 67, Iteration 140, Current loss 0.4519537687301636 Accuracy 67.92077693660626\n",
      "Training:: Epoch 67, Iteration 150, Current loss 0.4896969795227051 Accuracy 74.6838496261328\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 0.43489471077919006 Accuracy 64.58599177654898\n",
      "Training:: Epoch 68, Iteration 10, Current loss 0.42013755440711975 Accuracy 74.38282370267024\n",
      "Training:: Epoch 68, Iteration 20, Current loss 0.53971928358078 Accuracy 81.62192603716913\n",
      "Training:: Epoch 68, Iteration 30, Current loss 0.6073697805404663 Accuracy 82.85320468642315\n",
      "Training:: Epoch 68, Iteration 40, Current loss 0.4320950210094452 Accuracy 77.543935777826\n",
      "Training:: Epoch 68, Iteration 50, Current loss 0.6801110506057739 Accuracy 79.42205196535643\n",
      "Training:: Epoch 68, Iteration 60, Current loss 0.43392449617385864 Accuracy 81.76239703687324\n",
      "Training:: Epoch 68, Iteration 70, Current loss 0.4980872571468353 Accuracy 81.17523440025865\n",
      "Training:: Epoch 68, Iteration 80, Current loss 0.399711936712265 Accuracy 75.71179299485041\n",
      "Training:: Epoch 68, Iteration 90, Current loss 0.3332873582839966 Accuracy 87.48365571035664\n",
      "Training:: Epoch 68, Iteration 100, Current loss 0.4738748073577881 Accuracy 74.06165818617181\n",
      "Training:: Epoch 68, Iteration 110, Current loss 0.5858433246612549 Accuracy 76.6512767360784\n",
      "Training:: Epoch 68, Iteration 120, Current loss 0.4952410161495209 Accuracy 83.60243764528492\n",
      "Training:: Epoch 68, Iteration 130, Current loss 0.5780923366546631 Accuracy 73.49300656021785\n",
      "Training:: Epoch 68, Iteration 140, Current loss 0.4726058542728424 Accuracy 82.32750427675347\n",
      "Training:: Epoch 68, Iteration 150, Current loss 0.6604860424995422 Accuracy 78.97054506311773\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 0.4092314839363098 Accuracy 73.17510969285999\n",
      "Training:: Epoch 69, Iteration 10, Current loss 0.6728370189666748 Accuracy 85.24564836749408\n",
      "Training:: Epoch 69, Iteration 20, Current loss 0.4629373550415039 Accuracy 86.76363636363637\n",
      "Training:: Epoch 69, Iteration 30, Current loss 0.36687925457954407 Accuracy 61.88899875327802\n",
      "Training:: Epoch 69, Iteration 40, Current loss 0.5099454522132874 Accuracy 76.45253818958041\n",
      "Training:: Epoch 69, Iteration 50, Current loss 0.3783394694328308 Accuracy 72.24489795918367\n",
      "Training:: Epoch 69, Iteration 60, Current loss 0.4725669026374817 Accuracy 75.4108986527154\n",
      "Training:: Epoch 69, Iteration 70, Current loss 0.5275483727455139 Accuracy 81.3528990694345\n",
      "Training:: Epoch 69, Iteration 80, Current loss 0.49207964539527893 Accuracy 70.0279622509612\n",
      "Training:: Epoch 69, Iteration 90, Current loss 0.43561798334121704 Accuracy 84.33980668617541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 69, Iteration 100, Current loss 0.39123427867889404 Accuracy 86.49929201612025\n",
      "Training:: Epoch 69, Iteration 110, Current loss 0.329881489276886 Accuracy 84.01034467383064\n",
      "Training:: Epoch 69, Iteration 120, Current loss 0.3483869433403015 Accuracy 76.9843165353077\n",
      "Training:: Epoch 69, Iteration 130, Current loss 0.4756404161453247 Accuracy 82.78735909047114\n",
      "Training:: Epoch 69, Iteration 140, Current loss 0.4998505115509033 Accuracy 79.39817466681941\n",
      "Training:: Epoch 69, Iteration 150, Current loss 0.5493791103363037 Accuracy 81.74872665534805\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 0.43037518858909607 Accuracy 75.52325854851607\n",
      "Training:: Epoch 70, Iteration 10, Current loss 0.4025217890739441 Accuracy 81.25316181253162\n",
      "Training:: Epoch 70, Iteration 20, Current loss 0.44827964901924133 Accuracy 82.13794998625995\n",
      "Training:: Epoch 70, Iteration 30, Current loss 0.4131915867328644 Accuracy 85.74506878833084\n",
      "Training:: Epoch 70, Iteration 40, Current loss 0.30248063802719116 Accuracy 82.18296357162421\n",
      "Training:: Epoch 70, Iteration 50, Current loss 0.5734047889709473 Accuracy 78.8546653418842\n",
      "Training:: Epoch 70, Iteration 60, Current loss 0.4186716079711914 Accuracy 84.58392502130077\n",
      "Training:: Epoch 70, Iteration 70, Current loss 0.20107483863830566 Accuracy 78.07577692635164\n",
      "Training:: Epoch 70, Iteration 80, Current loss 0.3733287751674652 Accuracy 74.65487069803616\n",
      "Training:: Epoch 70, Iteration 90, Current loss 0.4788740575313568 Accuracy 69.8163665537529\n",
      "Training:: Epoch 70, Iteration 100, Current loss 0.4431780278682709 Accuracy 80.56251740462267\n",
      "Training:: Epoch 70, Iteration 110, Current loss 0.6319986581802368 Accuracy 74.681858538029\n",
      "Training:: Epoch 70, Iteration 120, Current loss 0.4157882034778595 Accuracy 75.11360918440565\n",
      "Training:: Epoch 70, Iteration 130, Current loss 0.48072320222854614 Accuracy 69.61734510733002\n",
      "Training:: Epoch 70, Iteration 140, Current loss 0.6549139618873596 Accuracy 84.61538461538461\n",
      "Training:: Epoch 70, Iteration 150, Current loss 0.5599110722541809 Accuracy 73.42604489456579\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 55.396486721630694\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 0.40044474601745605 Accuracy 79.28891957197101\n",
      "Training:: Epoch 71, Iteration 10, Current loss 0.4405388832092285 Accuracy 81.64109506618531\n",
      "Training:: Epoch 71, Iteration 20, Current loss 0.5363249182701111 Accuracy 81.12272644872802\n",
      "Training:: Epoch 71, Iteration 30, Current loss 0.4660763144493103 Accuracy 67.75492898028408\n",
      "Training:: Epoch 71, Iteration 40, Current loss 0.3634515106678009 Accuracy 84.83466362599772\n",
      "Training:: Epoch 71, Iteration 50, Current loss 0.3156537413597107 Accuracy 76.38956251818799\n",
      "Training:: Epoch 71, Iteration 60, Current loss 0.4328155219554901 Accuracy 80.01721512282327\n",
      "Training:: Epoch 71, Iteration 70, Current loss 0.7906546592712402 Accuracy 52.40318906605923\n",
      "Training:: Epoch 71, Iteration 80, Current loss 0.4113443195819855 Accuracy 71.5979718678443\n",
      "Training:: Epoch 71, Iteration 90, Current loss 0.5560413002967834 Accuracy 81.28872366790583\n",
      "Training:: Epoch 71, Iteration 100, Current loss 0.4226454794406891 Accuracy 73.65007112375534\n",
      "Training:: Epoch 71, Iteration 110, Current loss 0.4119929373264313 Accuracy 66.89397272327321\n",
      "Training:: Epoch 71, Iteration 120, Current loss 0.6093021035194397 Accuracy 84.67546668749512\n",
      "Training:: Epoch 71, Iteration 130, Current loss 0.3717418611049652 Accuracy 82.10798391728892\n",
      "Training:: Epoch 71, Iteration 140, Current loss 0.30328047275543213 Accuracy 74.37329580438033\n",
      "Training:: Epoch 71, Iteration 150, Current loss 0.47232672572135925 Accuracy 78.22694446750228\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 0.5849086046218872 Accuracy 83.9601202257899\n",
      "Training:: Epoch 72, Iteration 10, Current loss 0.9044545888900757 Accuracy 75.2299484915379\n",
      "Training:: Epoch 72, Iteration 20, Current loss 0.36229872703552246 Accuracy 81.57314304249108\n",
      "Training:: Epoch 72, Iteration 30, Current loss 0.41845768690109253 Accuracy 80.89876242645568\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.31336337327957153 Accuracy 81.78876819967645\n",
      "Training:: Epoch 72, Iteration 50, Current loss 0.30452847480773926 Accuracy 78.82457176609569\n",
      "Training:: Epoch 72, Iteration 60, Current loss 0.279052734375 Accuracy 74.18604651162791\n",
      "Training:: Epoch 72, Iteration 70, Current loss 0.2942260503768921 Accuracy 78.05550560450759\n",
      "Training:: Epoch 72, Iteration 80, Current loss 0.5009142160415649 Accuracy 84.02887972072358\n",
      "Training:: Epoch 72, Iteration 90, Current loss 0.44258829951286316 Accuracy 83.84347826086956\n",
      "Training:: Epoch 72, Iteration 100, Current loss 0.3242054283618927 Accuracy 80.12621224073129\n",
      "Training:: Epoch 72, Iteration 110, Current loss 0.5014594197273254 Accuracy 84.7072300304005\n",
      "Training:: Epoch 72, Iteration 120, Current loss 0.5990403890609741 Accuracy 80.11275127957866\n",
      "Training:: Epoch 72, Iteration 130, Current loss 0.8493629693984985 Accuracy 83.76478108021732\n",
      "Training:: Epoch 72, Iteration 140, Current loss 0.2846810519695282 Accuracy 77.49751222255874\n",
      "Training:: Epoch 72, Iteration 150, Current loss 0.5899856090545654 Accuracy 71.53624990286735\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 0.4559336006641388 Accuracy 73.88157894736842\n",
      "Training:: Epoch 73, Iteration 10, Current loss 0.3923959732055664 Accuracy 74.39749608763694\n",
      "Training:: Epoch 73, Iteration 20, Current loss 0.4879702925682068 Accuracy 81.02994011976048\n",
      "Training:: Epoch 73, Iteration 30, Current loss 0.4465717673301697 Accuracy 77.48369058713887\n",
      "Training:: Epoch 73, Iteration 40, Current loss 0.37030744552612305 Accuracy 72.63163487593646\n",
      "Training:: Epoch 73, Iteration 50, Current loss 0.45267319679260254 Accuracy 70.42986278207712\n",
      "Training:: Epoch 73, Iteration 60, Current loss 0.41971302032470703 Accuracy 83.25870287447638\n",
      "Training:: Epoch 73, Iteration 70, Current loss 0.4351133108139038 Accuracy 66.5617128463476\n",
      "Training:: Epoch 73, Iteration 80, Current loss 0.4573318660259247 Accuracy 76.7529665587918\n",
      "Training:: Epoch 73, Iteration 90, Current loss 0.9769913554191589 Accuracy 72.0118275901285\n",
      "Training:: Epoch 73, Iteration 100, Current loss 0.3723028600215912 Accuracy 73.85560268646165\n",
      "Training:: Epoch 73, Iteration 110, Current loss 0.6328516006469727 Accuracy 79.600245463312\n",
      "Training:: Epoch 73, Iteration 120, Current loss 0.8327221274375916 Accuracy 85.94594594594595\n",
      "Training:: Epoch 73, Iteration 130, Current loss 0.5072610378265381 Accuracy 83.24006870186714\n",
      "Training:: Epoch 73, Iteration 140, Current loss 0.4655695855617523 Accuracy 80.35027416318847\n",
      "Training:: Epoch 73, Iteration 150, Current loss 0.44787898659706116 Accuracy 63.03124393557151\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 0.36092814803123474 Accuracy 77.28790459965929\n",
      "Training:: Epoch 74, Iteration 10, Current loss 0.47836196422576904 Accuracy 70.82223558724976\n",
      "Training:: Epoch 74, Iteration 20, Current loss 0.33435770869255066 Accuracy 80.01304206064559\n",
      "Training:: Epoch 74, Iteration 30, Current loss 0.5136012434959412 Accuracy 79.65328467153284\n",
      "Training:: Epoch 74, Iteration 40, Current loss 0.4191749691963196 Accuracy 84.26753601850878\n",
      "Training:: Epoch 74, Iteration 50, Current loss 0.24121664464473724 Accuracy 85.42960019865905\n",
      "Training:: Epoch 74, Iteration 60, Current loss 0.28491902351379395 Accuracy 78.34707061706393\n",
      "Training:: Epoch 74, Iteration 70, Current loss 0.5905919075012207 Accuracy 72.64057643402091\n",
      "Training:: Epoch 74, Iteration 80, Current loss 0.7578080892562866 Accuracy 87.2993450622833\n",
      "Training:: Epoch 74, Iteration 90, Current loss 0.5472093820571899 Accuracy 83.27841845140033\n",
      "Training:: Epoch 74, Iteration 100, Current loss 0.4233086407184601 Accuracy 64.22199523323118\n",
      "Training:: Epoch 74, Iteration 110, Current loss 0.5174989700317383 Accuracy 80.78310645052672\n",
      "Training:: Epoch 74, Iteration 120, Current loss 0.4463709592819214 Accuracy 78.15108067622512\n",
      "Training:: Epoch 74, Iteration 130, Current loss 0.7752086520195007 Accuracy 74.62040816326531\n",
      "Training:: Epoch 74, Iteration 140, Current loss 0.43371206521987915 Accuracy 72.65120132560067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 74, Iteration 150, Current loss 0.6025605797767639 Accuracy 64.60756524288925\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 0.7355557084083557 Accuracy 78.38296760710554\n",
      "Training:: Epoch 75, Iteration 10, Current loss 0.5584880709648132 Accuracy 80.86721274517171\n",
      "Training:: Epoch 75, Iteration 20, Current loss 0.8440248966217041 Accuracy 87.3217550274223\n",
      "Training:: Epoch 75, Iteration 30, Current loss 0.6039531230926514 Accuracy 76.72737998672665\n",
      "Training:: Epoch 75, Iteration 40, Current loss 0.6086082458496094 Accuracy 71.14427860696517\n",
      "Training:: Epoch 75, Iteration 50, Current loss 1.0347744226455688 Accuracy 56.42661223448364\n",
      "Training:: Epoch 75, Iteration 60, Current loss 0.7085978984832764 Accuracy 84.89900609169605\n",
      "Training:: Epoch 75, Iteration 70, Current loss 1.5621459484100342 Accuracy 78.4837954754982\n",
      "Training:: Epoch 75, Iteration 80, Current loss 0.8096140027046204 Accuracy 71.77660919922785\n",
      "Training:: Epoch 75, Iteration 90, Current loss 1.1317858695983887 Accuracy 72.77797387927991\n",
      "Training:: Epoch 75, Iteration 100, Current loss 0.6709967851638794 Accuracy 76.55970889844525\n",
      "Training:: Epoch 75, Iteration 110, Current loss 0.958580493927002 Accuracy 71.31576858979446\n",
      "Training:: Epoch 75, Iteration 120, Current loss 1.345633625984192 Accuracy 64.86080465705835\n",
      "Training:: Epoch 75, Iteration 130, Current loss 0.9573822617530823 Accuracy 71.12266577211227\n",
      "Training:: Epoch 75, Iteration 140, Current loss 1.7813374996185303 Accuracy 50.213797035347774\n",
      "Training:: Epoch 75, Iteration 150, Current loss 1.0306271314620972 Accuracy 46.75581132927105\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 45.123565480382815\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 1.0775259733200073 Accuracy 56.45879732739421\n",
      "Training:: Epoch 76, Iteration 10, Current loss 1.6400671005249023 Accuracy 77.13260845196727\n",
      "Training:: Epoch 76, Iteration 20, Current loss 1.2888298034667969 Accuracy 72.36459978600907\n",
      "Training:: Epoch 76, Iteration 30, Current loss 0.998487114906311 Accuracy 75.9864406779661\n",
      "Training:: Epoch 76, Iteration 40, Current loss 0.9513052105903625 Accuracy 65.31086577571179\n",
      "Training:: Epoch 76, Iteration 50, Current loss 1.0044175386428833 Accuracy 64.4179350061703\n",
      "Training:: Epoch 76, Iteration 60, Current loss 0.9131326675415039 Accuracy 75.09759271307742\n",
      "Training:: Epoch 76, Iteration 70, Current loss 0.9374815225601196 Accuracy 68.37881219903691\n",
      "Training:: Epoch 76, Iteration 80, Current loss 0.6149082779884338 Accuracy 74.92316289466332\n",
      "Training:: Epoch 76, Iteration 90, Current loss 0.5286832451820374 Accuracy 74.97095208803226\n",
      "Training:: Epoch 76, Iteration 100, Current loss 1.1360774040222168 Accuracy 76.6929593923459\n",
      "Training:: Epoch 76, Iteration 110, Current loss 0.9460456371307373 Accuracy 81.00626925937732\n",
      "Training:: Epoch 76, Iteration 120, Current loss 0.9235727787017822 Accuracy 72.23178427997705\n",
      "Training:: Epoch 76, Iteration 130, Current loss 0.5862938761711121 Accuracy 75.78012135221034\n",
      "Training:: Epoch 76, Iteration 140, Current loss 0.7669976949691772 Accuracy 86.56735260699406\n",
      "Training:: Epoch 76, Iteration 150, Current loss 0.8986808657646179 Accuracy 71.6271669039444\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 0.9636955857276917 Accuracy 79.4243577545195\n",
      "Training:: Epoch 77, Iteration 10, Current loss 0.7003900408744812 Accuracy 73.42001238272134\n",
      "Training:: Epoch 77, Iteration 20, Current loss 0.5752754211425781 Accuracy 66.41130772550711\n",
      "Training:: Epoch 77, Iteration 30, Current loss 0.9724737405776978 Accuracy 74.26734574601319\n",
      "Training:: Epoch 77, Iteration 40, Current loss 0.7825762033462524 Accuracy 79.92047713717695\n",
      "Training:: Epoch 77, Iteration 50, Current loss 0.6856154203414917 Accuracy 78.67603983596953\n",
      "Training:: Epoch 77, Iteration 60, Current loss 0.6931975483894348 Accuracy 65.93475288142216\n",
      "Training:: Epoch 77, Iteration 70, Current loss 0.782375156879425 Accuracy 55.01225217197594\n",
      "Training:: Epoch 77, Iteration 80, Current loss 1.143829107284546 Accuracy 78.41075794621027\n",
      "Training:: Epoch 77, Iteration 90, Current loss 0.9351279139518738 Accuracy 77.48705369310439\n",
      "Training:: Epoch 77, Iteration 100, Current loss 0.39789795875549316 Accuracy 75.84145775301764\n",
      "Training:: Epoch 77, Iteration 110, Current loss 1.0754780769348145 Accuracy 61.68744136174775\n",
      "Training:: Epoch 77, Iteration 120, Current loss 0.4885249435901642 Accuracy 75.02208759308343\n",
      "Training:: Epoch 77, Iteration 130, Current loss 0.5229597687721252 Accuracy 74.33837163566893\n",
      "Training:: Epoch 77, Iteration 140, Current loss 1.0119216442108154 Accuracy 87.9782335705316\n",
      "Training:: Epoch 77, Iteration 150, Current loss 0.4563366770744324 Accuracy 82.40369181380417\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 0.96290522813797 Accuracy 76.25013983667077\n",
      "Training:: Epoch 78, Iteration 10, Current loss 0.6564294099807739 Accuracy 80.63965884861408\n",
      "Training:: Epoch 78, Iteration 20, Current loss 0.4661612808704376 Accuracy 87.91672336372295\n",
      "Training:: Epoch 78, Iteration 30, Current loss 0.6832268834114075 Accuracy 72.32662342004949\n",
      "Training:: Epoch 78, Iteration 40, Current loss 0.5072701573371887 Accuracy 66.1413043478261\n",
      "Training:: Epoch 78, Iteration 50, Current loss 0.6721938848495483 Accuracy 79.92182514358647\n",
      "Training:: Epoch 78, Iteration 60, Current loss 0.5439052581787109 Accuracy 83.68203497615262\n",
      "Training:: Epoch 78, Iteration 70, Current loss 0.4123040437698364 Accuracy 81.90341411639837\n",
      "Training:: Epoch 78, Iteration 80, Current loss 0.36732372641563416 Accuracy 60.35544606721802\n",
      "Training:: Epoch 78, Iteration 90, Current loss 0.5142588019371033 Accuracy 82.25404732254047\n",
      "Training:: Epoch 78, Iteration 100, Current loss 0.5296899080276489 Accuracy 77.46120803015693\n",
      "Training:: Epoch 78, Iteration 110, Current loss 0.4314365088939667 Accuracy 78.32721653467259\n",
      "Training:: Epoch 78, Iteration 120, Current loss 0.34440702199935913 Accuracy 70.41554959785523\n",
      "Training:: Epoch 78, Iteration 130, Current loss 0.46834203600883484 Accuracy 70.73035780154925\n",
      "Training:: Epoch 78, Iteration 140, Current loss 0.3970467448234558 Accuracy 77.91432942376339\n",
      "Training:: Epoch 78, Iteration 150, Current loss 0.4542200267314911 Accuracy 78.92765957446808\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 0.4439525306224823 Accuracy 74.98946077492047\n",
      "Training:: Epoch 79, Iteration 10, Current loss 0.3598613142967224 Accuracy 77.22455159562078\n",
      "Training:: Epoch 79, Iteration 20, Current loss 0.42344263195991516 Accuracy 78.04812257229176\n",
      "Training:: Epoch 79, Iteration 30, Current loss 0.3621048629283905 Accuracy 80.40280906320392\n",
      "Training:: Epoch 79, Iteration 40, Current loss 0.621504008769989 Accuracy 84.79940194368304\n",
      "Training:: Epoch 79, Iteration 50, Current loss 0.6451454162597656 Accuracy 79.1834758754236\n",
      "Training:: Epoch 79, Iteration 60, Current loss 0.4438841938972473 Accuracy 75.39113757948925\n",
      "Training:: Epoch 79, Iteration 70, Current loss 0.37567228078842163 Accuracy 80.41501373207201\n",
      "Training:: Epoch 79, Iteration 80, Current loss 0.4431404173374176 Accuracy 79.54513743546812\n",
      "Training:: Epoch 79, Iteration 90, Current loss 0.4158107042312622 Accuracy 80.82444228903977\n",
      "Training:: Epoch 79, Iteration 100, Current loss 0.7255076766014099 Accuracy 85.04222972972973\n",
      "Training:: Epoch 79, Iteration 110, Current loss 0.4474897086620331 Accuracy 77.04176642724128\n",
      "Training:: Epoch 79, Iteration 120, Current loss 0.5914326906204224 Accuracy 79.10891807561134\n",
      "Training:: Epoch 79, Iteration 130, Current loss 0.451983243227005 Accuracy 81.55770424909075\n",
      "Training:: Epoch 79, Iteration 140, Current loss 0.4194682538509369 Accuracy 81.9595385387059\n",
      "Training:: Epoch 79, Iteration 150, Current loss 0.4214887022972107 Accuracy 85.30459565372284\n",
      "Starting Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-102:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 282, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/connection.py\", line 499, in Client\n",
      "    deliver_challenge(c, authkey)\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/connection.py\", line 729, in deliver_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/dipika16/anaconda3/envs/video_r/lib/python3.7/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4276ddb7c601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmiddle_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mboundary_target_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_single_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_stages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmiddle_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mfinal_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ar/c2f_active_learn/mstcn_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_dilated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 259\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    print(\"Starting Training\")\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        boundary_target_tensor = get_single_random(item_2, item[4])\n",
    "        \n",
    "        loss = 0\n",
    "        for p in predictions:\n",
    "            loss += ce_criterion(p, boundary_target_tensor)\n",
    "            loss += 0.15 * torch.mean(torch.clamp(\n",
    "                mse_criterion(F.log_softmax(p[:, :, 1:], dim=1), F.log_softmax(p.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                max=16) * src_mask_mse[:, :, 1:])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(predictions[-1], dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Calculating Validation Data Accuracy\")\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for i, item in enumerate(testloader):\n",
    "            with torch.no_grad():\n",
    "                item_0 = item[0].to(device)\n",
    "                item_1 = item[1].to(device)\n",
    "                item_2 = item[2].to(device)\n",
    "                src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "                src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "                middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "                \n",
    "                pred = torch.argmax(predictions[-1], dim=1)\n",
    "                correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total += float(torch.sum(src_mask).item())\n",
    "\n",
    "        print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_labels(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            select_item = np.random.randint(start, i, 1)[0]\n",
    "            unique_ids.append(select_item)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    select_item = np.random.randint(start, len(labels_arr), 1)[0]\n",
    "    unique_ids.append(select_item)\n",
    "    return unique_ids\n",
    "# get_selected_labels(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), config.output_dir+'mstcn_single_frame.wt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            unique_ids.append(i - 1)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    unique_ids.append(len(labels_arr) - 1)\n",
    "    return unique_ids\n",
    "# get_boundary(np.array([2, 2, 2, 2, 3, 3, 4, 4, 4, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[5]\n",
    "    for i, count in enumerate(count_all):\n",
    "        video_id = video_ids[i]\n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_selected_labels(labels)\n",
    "\n",
    "        loaded_vidid_selected_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_boundary_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[4]\n",
    "    for i, count in enumerate(count_all):\n",
    "        \n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_boundary(labels)\n",
    "        video_id = video_ids[i]\n",
    "        video_id_boundary_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in video_id_boundary_frames.keys():\n",
    "    if len(video_id_boundary_frames[ele]) != len(loaded_vidid_selected_frames[ele + \".txt\"]):\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(video_id_boundary_frames, open(\"dump_dir/video_id_boundary_frames_dict.pkl\", \"wb\"))\n",
    "# pickle.dump(loaded_vidid_selected_frames, open(\"dump_dir/loaded_vidid_selected_frames_dict.pkl\", \"wb\"))\n",
    "pickle.dump(video_id_boundary_frames, open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "    \n",
    "    weights = [1, 1, 1, 1, 0, 0]\n",
    "    ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "    for i, outp_ele in enumerate(outp[1]):\n",
    "        upped_logit = F.upsample(outp_ele, size=outp[0].shape[-1], mode='linear', align_corners=True)\n",
    "        ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i + 1] / sum(weights)\n",
    "    \n",
    "    return ensemble_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/results/c2f-tcn-model/split2_c2ftcn_model.wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item[0] = item[0].to(device)\n",
    "        item[1] = item[1].to(device)\n",
    "        y, y_list, _ = model(item[0].permute(0,2,1))\n",
    "        \n",
    "        probs = get_ensemble_out([y, y_list])\n",
    "        features = torch.log(probs + 1e-4).permute(0,2,1)\n",
    "        get_estimated_boundary(features, item[1], item[4])\n",
    "get_boundary_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    idx = 4\n",
    "\n",
    "    cur_vid_feat = features[idx]\n",
    "    cur_vidid = item[4][idx]\n",
    "\n",
    "    selected_frames = torch.tensor(loaded_vidid_selected_frames[cur_vidid], dtype=torch.long, \n",
    "                                   device=cur_vid_feat.device)\n",
    "    bound_list = torch.tensor(boundaries_dict[cur_vidid], dtype=selected_frames.dtype, device=selected_frames.device)\n",
    "\n",
    "    cumsum_feat = torch.cumsum(cur_vid_feat, dim=0)\n",
    "\n",
    "    prob_each_segment = prob_vals_per_segment(selected_frames, cur_vid_feat, cumsum_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBEAAAEzCAYAAABnp5vxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3hU5bn//8+TmcmBcJZT5CCIKCoKKmKhVbCFFjxUUXdBK1upim6r4mkrtv4E/WrV7m3VeqiHWuqhCG4t3bYKFq2gVlCDgoLgFgEhAgKSQEggySTP74+JSCGQFTMzdybr/bquXJNJlmveiSST3FnrWc57LwAAAAAAgPpkWQcAAAAAAIDMwBABAAAAAAAEwhABAAAAAAAEwhABAAAAAAAEwhABAAAAAAAEwhABAAAAAAAEUu8QwTn3B+fcRufckn283znnfuucW+Gc+9A5d2zyMwEAAAAAgLUgRyL8UdLI/bx/lKQ+tS8TJP2u8VkAAAAAAKCpqXeI4L1/Q9KW/WxyhqSnfMICSW2dcwXJCgQAAAAAAE1DMtZE6Cpp7W73i2rfBgAAAAAAmpFoEvbh6nibr3ND5yYoccqD8vPzj+vbt28SHr4J2vxp4rZDH9uOZmDz5s2SpA4dOhiXIF1Wb1stSerZuud+tysvXyVJatGiV4qLACAcKlclvq9m92oe31c/K6+QJPVukWNc0nSVfFkuSWrbuYVxCeqyclOZJOngjvnGJclRvO4LSVK7Aw3+3pzk389WbV4pSerV4eCk7K8pWrhw4Wbvfce63peMIUKRpO673e8maV1dG3rvH5P0mCQNHDjQFxYWJuHhm6BXpyRuh08xjGgepk6dKkkaP368cQnS5b6F90mSrj7u6v1ut/D98yRJxx07LeVNABAGG+/5jSSp03XXGpckx+gPEr80zDyGP+rsy/yZn0mSBo/ubVyCuox5dL4kacalg41LkuPNaX+UJJ143oXpf/CppyZux7+UlN2Nm3auJOnp855Nyv6aIufc5/t6XzKGCC9KusI5N13SCZK2eu/XJ2G/mWv4FOsCIGPVNzwAAKRGcxkeIDiGB0gnk+FBikTb51onmKp3iOCce1bSMEkdnHNFkiZLikmS9/4RSS9LOkXSCknlkviTMQAAAAAAzVC9QwTv/bn1vN9L+nnSipqDGecnbsc8Y9sBZKBrXr9GknTvyfcalwBAuBRdeZUkqdsDvzUuQbrMevQjSdKoS48yLkEYvHjPryRJP77uF8YljVdVu55IWCXjdAbsqbzYugDIWCUVJdYJABBK1SV8/w2bndurrBMQIju2b7NOSJ6aOq8jEBrJuMQjAAAAAAAIAYYIAAAAAAAgEIYIAAAAAAAgENZESIWDh1oXABnrhIITrBMAIJRaDP6OdQLSrFvfdtYJCJEe/fpbJyRNVm64f40O90efKkNvsC4AMtZl/S+zTgCAUOp4+eXWCUiz40/tZZ2AEBl89n4v+pdRIu1yrBNMcToDAAAAAAAIhCMRUuGZsxO3579g2wFkoMteTRyJ8MjwR4xLACBc1lwyQZLU4/HHjEuQLn99YJEk6fQrBxiXIAxeuHOyJOnsm241Lmm8qg1l1gmmGCKkQtVO6wIgY1XEK6wTACCU/E5+fgmbeGWNdQJCJF7ZjH7G89YBtjidAQAAAAAABMIQAQAAAAAABMIQAQAAAAAABMKaCKlw6I+sC4CMNbTbUOsEAAillsOGWScgzXoe1cE6ASFy8LGDrBOSJqtFuH+NDvdHnyrfvcq6AMhYF/a70DoBAELpgIt+Zp2ANDvmhz2sExAix59+lnVC0kTa5FgnmOJ0BgAAAAAAEAhHIqTC1FMTt+Nfsu0AMtD42eMlSVNHTjUuAYBw+Xzcv0uSDnr6KeMSpMvMe96XJI2+7ljjEoTBjFsnSZLGTL7LuKTxqtaXWSeY4kgEAAAAAAAQCEMEAAAAAAAQCEMEAAAAAAAQCEMEAAAAAAAQCAsrpsKRZ1oXABnrRz1/ZJ0AAKHUatRI6wSk2SHHdbJOQIgc9p0TrROSJis/Zp1giiFCKgy6xLoAyFhj+461TgCAUGp/3nnWCUizo4Z1s05AiAz40anWCUkTaZ1tnWCKIUIqVJYnbrNb2HYAGWhHfIckKS+aZ1wCAOFSsyPx/Tcrj++/YVFVWS1JimVHjEsQBlUVOyVJsZxc45Ik8N66wBRDhFT4078lbse/ZNsBZKDLX71ckjR15FTjEgAIl7UTLpUkHfT0U8YlSJe/PbBYkjT6umONSxAGf75riiRpzOS7bEOSoGpDuXWCKRZWBAAAAAAAgTBEAAAAAAAAgTBEAAAAAAAAgTBEAAAAAAAAgbCwYioM4BJJwLd1xiFnWCcAQCi1GT3aOgFp1ndwgXUCQuTIocOtE5Imq2XMOsEUQ4RUOOan1gVAxjrzkDOtEwAglNqexRAhbA4fwhAB6dNvWPMZIkRaZVsnmGKIkAplXyVu8w+w7QAyUPHOYklSu9x2xiUAEC7x4sT332g7vv+GxY7tlZKkvJbh/oUI6VG+baskqUXrNsYljeervXWCKYYIqfDcvydux79k2wFkoGvnXitJmjpyqnEJAITLF1dNlCQd9PRTxiVIl9mPLpEkjb7uWOMShMFf771TkjRm8l3GJY0X31hunWCKhRUBAAAAAEAgDBEAAAAAAEAgDBEAAAAAAEAgDBEAAAAAAEAgLKyYCsf/zLoAyFhjDhtjnQAAodTu3LHWCUizfkO7WicgRPqPOMU6IWm4xCOSr9/Z1gVAxhrZa6R1AgCEUutTms8P+Aimz8DO1gkIkb5DTrJOSJqsljHrBFMMEVJha1Hitk032w4gA20o2yBJ6pLfxbgEAMKlav16SVKsoMC4BOlSumWnJKlV+1zjEoTBts2bJEmtO3Q0Lmk8H6+xTjDFECEV/nxp4nb8S7YdQAa66c2bJElTR041LgGAcFl3w42SpIOefsq4BOny6tSPJUmjrzvWuARhMOuheyRJYybfZVzSePFNO6wTTLGwIgAAAAAACIQhAgAAAAAACIQhAgAAAAAACIQhAgAAAAAACISFFVNhyBXWBUDGuuDIC6wTACCU2o8fb52ANBswood1AkJk4GmjrROSJtIm2zrBFEOEVDhslHUBkLGGdR9mnQAAodTq+ydbJyDNeh3dwToBIdL7uBOsE5Imq0XMOsFUoNMZnHMjnXOfOOdWOOcm1fH+Hs65151zHzjnPnTOnZL81Ayy+dPEC4AGW7V1lVZtXWWdAQChU7FylSpW8v03TIo3lKl4Q5l1BkJiy7oibVlXZJ2RFL6qRr6qxjrDTL1HIjjnIpIekjRCUpGk95xzL3rvP95ts5slPee9/51z7ghJL0vqmYLezPDXqxO341+y7QAy0G3zb5MkTR051bgEAMJlw+TJkqSDnn7KuATpMvdPn0iSRl93rHEJwmDO4w9KksZMvsu4pPHim3dYJ5gKciTCIEkrvPcrvfeVkqZLOmOPbbyk1rWvt5G0LnmJAAAAAACgKQgyROgqae1u94tq37a7KZLOd84VKXEUwpV17cg5N8E5V+icK9y0adO3yAUAAAAAAFaCDBFcHW/ze9w/V9IfvffdJJ0i6Wnn3F779t4/5r0f6L0f2LFjx4bXAgAAAAAAM0GGCEWSuu92v5v2Pl3hIknPSZL3fr6kXEks9woAAAAAQDMS5BKP70nq45zrJekLSWMlnbfHNmsk/UDSH51zhysxRAjv+QonXW9dAGSsCUdPsE4AgFDq8B+XWScgzQae0tM6ASFy9MgzNffd91RaWqpWrVpZ5zRKpG2OdYKpeocI3vu4c+4KSa9Iikj6g/d+qXPuNkmF3vsXJV0n6XHn3DVKnOpwofd+z1MewqM311kGvq3BBw62TgCAUMofMsQ6AWnW/fD21gkIkc82bdbGLVs0b948nXbaadY5jZKVF+Rv8c1XoI/ee/+yEgsm7v62W3Z7/WNJ301uWgZb/2HituBo2w4gAy3fslyS1Ld9X+MSAAiXncuWSZJyDz/cuATpsmltqSSpY/fM/qswmrbbb79d8Xh81/3CwkIVFhYqGo3q5ptvNiz79nxltXWCqSBrIqChZt+UeAHQYHe/e7fufvdu6wwACJ0vf3WnvvzVndYZSKO3nvtUbz33qXUGmrmJEyeqX79+crVr80ejUR111FGaOHGicdm3F/9qp+Jf7bTOMMMQAQAAAEBaVW+r1MZHF6u6tNI6BSnWqlUr5eTkyEty8qqurlZOTk7Gr4sQZgwRAAAAAKTVttfWqHL1Nm17dY11CtKgrKxMbVWtnqrUcccdp+3bt1snoRHCvSIEAAAAgLQpuvktKf7N+utl76xX2TvrpahTt9u/Z1iGVBo7dqxm3DpJkjJ+UUVwJAIAAACANCm4YZDyBnSUYrW/hsSylDegowpuHGQbBiAwjkRIhR/cUv82AOo08djMXWQHADJZx2uusU5Amn3nzN5pf8xI62xl5USkeI0UdVK8Rlk5UUVaZae9Ben1vbEXWCckTaRdrnWCKYYIqdDjBOsCIGMN6DTAOgEAQqnFscdYJyDNCnq3MXnc6u1Vyj+hQPmDuqjs3Q0srhgSXQ9rPpePzcqNWCeYYoiQCmveSdwyTAAabNHGRZIYJgBAupW//4Ekhglhsv6zrZLSP0zoMO6IXa9nn3lIWh8bdr74ZJmk5jFMqNlZbZ1gijURUuG12xIvABrs/vfv1/3v32+dAQChs+nee7Xp3nutM5BGC/7ymRb85TPrDITEW9Of1FvTn7TOSIrq4p2qLt5pnWGGIQIAAAAAAAiEIQIAAAAAAAiEIQIAAAAAAAiEIQIAAAAAAAiEqzOkwsg7rQuAjHXjoButEwAglDr/4ibrBKTZ937SxzoBIXLyBROsE5ImekCudYIphgipUHC0dQGQsfq272udAAChlHt45l92DQ3TsXsr6wSESKeeB1snJI3LjlgnmGKIkAqfvZ647X2ybQeQgeavmy9JGnzgYOMSAAiXsrffliTlDxliXIJ0WbtsiySp++HtjUsQBp9/uEiSdNDRA4xLGq9mR9w6wRRDhFR4478TtwwRgAZ77MPHJDFEAIB02/y7RyQxRAiTwpdXS2KIgPRYMHO6pOYxRKguqbBOMMXCigAAAAAAIBCGCAAAAAAAIBCGCAAAAAAAIBCGCAAAAAAAIBAWVkyF0++zLgAy1i2Db7FOAIBQ6nLrrdYJSLNhPz3MOgEhMuKSK6wTkibaIc86wRRDhFTo0Me6AMhYvdr0sk4AgFDKOZjvv2HTrku+dQJCpP2B3awTksbFwn1AP0OEVPhkVuL2sFG2HUAGmrt2riRpWPdhph0AEDal/3hdktTq+1yiOixWfbhZktTr6A7GJQiDzxa+I0nqfdwJxiWNV1NeZZ1giiFCKrz9YOKWIQLQYE8ufVISQwQASLctU6dKYogQJovmrJHEEAHpUfi3mZKaxxChemuldYKpcB+HAQAAAAAAAmOIAAAAAAAAAmGIAAAAAAAAAmGIAAAAAAAAAmFhxVQ461HrAiBj3XnindYJABBKB/76busEpNnw8UdYJyBERv38OuuEpIl2zLNOMMUQIRXaNJ9roALp1iW/i3UCAIRSrKDAOgFp1qp9rnUCQqR1h47WCUnjouE+oJ8hQioseSFx2+9s2w4gA81eNVuSNLLXSOMSAAiXbS+/LElqfcopxiVIl08Lv5Qk9RnY2bgEYbD87TckSX2HnGRc0ng126usE0wxREiF9/6QuGWIADTYjE9mSGKIAADpVvzsdEkMEcJkybwvJDFEQHosnpMYVDaHIUJ1aaV1gqlwH4cBAAAAAAACY4gAAAAAAAACYYgAAAAAAAACYYgAAAAAAAACYWHFVPjJU9YFQMb6zbDfWCcAQCh1/e391glIs5GX9rNOQIicfs1N1glJE+3UwjrBFEOEVMg/wLoAyFjtcttZJwBAKEXb8f03bPJaZlsnIERatG5jnZA0LuKsE0wxREiFD/6UuD3mp7YdQAb6y4q/SJLOPORM4xIACJeSP8+UJLU9a7RxCdJl2dvrJUmHDykwLkEYLJn7qiSp37DhxiWNF/ZLPDJESIVF0xK3DBGABvvfFf8riSECAKTb1pkMEcJm+XyGCEifpfOazxChZnuVdYIpFlYEAAAAAACBMEQAAAAAAACBMEQAAAAAAACBMEQAAAAAAACBBFpY0Tk3UtL9kiKSfu+9v6uObX4iaYokL2mx9/68JHZmlp/+j3UBkLEeHv6wdQIAhFL3xx61TkCanXZlf+sEhMhZk6ZYJyRNrEsL6wRT9Q4RnHMRSQ9JGiGpSNJ7zrkXvfcf77ZNH0k3Sfqu977YOdcpVcEZITvc/6iAxsiL5lknAEAoZeXx/TdsYtkR6wSESCwn1zoheZyzLjAV5HSGQZJWeO9Xeu8rJU2XdMYe21wi6SHvfbEkee83Jjczw7z7eOIFQINNXz5d05dPt84AgNDZMm2atkybZp2BNPpobpE+mltknYGQWPTKS1r0ykvWGUlRva1S1dsqrTPMBBkidJW0drf7RbVv292hkg51zv3TObeg9vSH8Fr6l8QLgAZ7ZfUremX1K9YZABA6pbNmq3TWbOsMpNGKhRu1YmG4//aH9PlkwZv6ZMGb1hlJUVNWpZqyKusMM0HWRKjrWA1fx376SBomqZukN51z/bz3Jf+yI+cmSJogST169GhwLAAAAAAAsBPkSIQiSd13u99N0ro6tvlf732V936VpE+UGCr8C+/9Y977gd77gR07dvy2zQAAAAAAwECQIcJ7kvo453o557IljZX04h7b/EXSyZLknOugxOkNK5MZCgAAAAAAbNU7RPDexyVdIekVScskPee9X+qcu8059+PazV6R9JVz7mNJr0v6T+/9V6mKBgAAAAAA6RdkTQR571+W9PIeb7tlt9e9pGtrXzC+eaw6CliYOnKqdQIAhNJBTz9lnYA0G33dsdYJCJExk++yTkiaWEG+dYKpIKczAAAAAAAABDsSAQ30z98mbr97lW0HkIH+uOSPkqQL+11o2gEAYfPVE3+QJB1w0c+MS5AuH/x9jSTpmB9y1TSk3nt//bMk6fjTzzIuabzqrRXWCaY4EiEV/u+VxAuABptXNE/ziuZZZwBA6GyfO1fb5861zkAarf5os1Z/tNk6AyGx8v13tfL9d60zkqKmPK6a8rh1hhmGCAAAAAAAIBCGCAAAAAAAIBCGCAAAAAAAIBAWVkyFWK51AZCxcqI51gkAEEoul59fwiaazd8TkT7R7Gb0M56zDrDFECEVzn/BugDIWI8Mf8Q6AQBCqcfjj1knIM1Ov3KAdQJC5OybbrVOSJpYl3zrBFOMHwEAAAAAQCAciZAK836duB16g20HkIEeWZw4EuGy/pcZlwBAuGx6+GFJUsfLLzcuQbq899IqSdLxp/YyLkEYzH/hWUnS4LPPNS5pvOriCusEUxyJkAor5yVeADTYO+vf0Tvr37HOAIDQKZ+/QOXzF1hnII2KlheraHmxdQZCYs2SxVqzZLF1RlLU7IyrZmfcOsMMQwQAAAAAABAIQwQAAAAAABAIQwQAAAAAABAICyumQot21gVAxmqb09Y6AQBCKdKW779hk9syZp2AEMlr2do6IXmynHWBKYYIqTDmGesCIGPde/K91gkAEErdHvitdQLSbNSlR1knIER+fN0vrBOSJta5hXWCKU5nAAAAAAAAgXAkQiq8OiVxO3yKYQSQme5beJ8k6erjrjYuAYBw2XjPbyRJna671rgE6TJ/5meSpMGjexuXIAzenPZHSdKJ511o2pEM8S07rRNMMURIhbXvWRcAGWvxpuZx/WAAyDQ7Fi2yTkCabVi51ToBIbLu0+XWCUnjK6qtE0xxOgMAAAAAAAiEIQIAAAAAAAiEIQIAAAAAAAiENRFSofWB1gVAxuqc39k6AQBCKdqli3UC0qxluxzrBIRIq/YdrBOSxkWcdYIphgipcPbj1gVAxrrrxLusEwAglLr+16+tE5BmI352pHUCQuSUK6+3TkiaaKcW1gmmOJ0BAAAAAAAEwpEIqTBrUuJ2FH9RBRrq7nfvliTdOOhG4xIACJcNv/qVJKnLL35hXIJ0efO5/5MknfiTQ41LEAav//ExSdLJF04wLmm8+Fc7rBNMMURIhQ0fWRcAGWv5luZzDWEAyCQVy/j+Gzab1263TkCIbPx8pXVC0vjKGusEU5zOgNCp2rhRq88fp/imTdYpAAAAAJBRGCIgdDY//DvtWLhQmx562DoFAAAAADIKpzMgNJb3HyBfUbHrfsn06SqZPl0uJ0d9Fy8yLAMAAACAzMAQIRUO6G1dgDr0nvN3bfz1r1X66mvyO3fK5eaq1Yjh6nzDDdZp2M1BrQ+yTgCAUMru2dM6AWnWtnO4L1OH9GpX0NU6IWlcLNwH9DNESIUf/9a6AHWIdeqkrJYt5Ssq5HJy5CsqlJXfUtGOHa3TsJspQ6ZYJwBAKBX8v9usE5BmJ5/f1zoBIfLDCVdaJyRNtEOedYIphggIlfjmr9R27Fi1G/MTFc94jsUVAQAAAKABGCKkwotXJW45IqHJ6f7gA7teL5h8i2EJ9mXK21MStxyRAABptf7/SzwvckRCeLz+TOKynhyRgHT4+2OJn8ObwxEJ8c07rBNMMURIha8+sy4AMtbn2z63TgCAUKpcvdo6AWlW8mW5dQJCpHj9F9YJSeOraqwTTIV7RQgAAAAAABAYQwQAAAAAABAIQwQAAAAAABAIayKkQpejrAuAjNW3PYs7AYCFnMP5/hs2Hbq3tE5AiHQ66GDrhKRx2eH+WzxDhFQYdZd1AZCxbhx0o3UCAIRSl1/8wjoBaXbiTw61TkCInHzhBOuEpIkekGedYCrcIxQAAAAAABAYRyKkwguXJG7Pfty2A8hAk96cJEm660SO6AGAdPriP2+QJHX9r18blyBd5vxhqSRpxM+ONC5BGLz8wH9Lkk658nrjksaLbwz35VEZIqTCtnXWBUDG+rLsS+sEAAil+IYN1glIs+3FFdYJCJHSLZutE5LGV3vrBFOczgAAAAAAAAJhiAAAAAAAAAIJNERwzo10zn3inFvhnJu0n+3Occ5559zA5CUCAAAAAICmoN41EZxzEUkPSRohqUjSe865F733H++xXStJV0l6JxWhGaX78dYFQMbq37G/dQIAhFLegAHWCUizLge3sU5AiBzYp691QtK4nIh1gqkgCysOkrTCe79Skpxz0yWdIenjPbb7f5J+LSnzl9tsrOFTrAuAjHX1cVdbJwBAKHW67lrrBKTZ4NG9rRMQIieed6F1QtJE2+daJ5gKcjpDV0lrd7tfVPu2XZxzx0jq7r3/WxLbAAAAAABAExLkSARXx9t2XdPCOZcl6V5JF9a7I+cmSJogST169AhWmIlmnJ+4HfOMbQeQga55/RpJ0r0n32tcAgDhUnTlVZKkbg/81rgE6TLr0Y8kSaMuPcq4BGHw4j2/kiT9+LpfGJc0XtWX5dYJpoIMEYokdd/tfjdJ63a730pSP0lznXOS1EXSi865H3vvC3ffkff+MUmPSdLAgQOb78U1y4utC4CMVVJRYp0AAKFUXcL337DZub3KOgEhsmP7NuuE5Klpvr/KBhHkdIb3JPVxzvVyzmVLGivpxa/f6b3f6r3v4L3v6b3vKWmBpL0GCAAAAAAAILPVO0Tw3sclXSHpFUnLJD3nvV/qnLvNOffjVAcCAAAAAICmIcjpDPLevyzp5T3edss+th3W+CwAAAAAANDUBBoioIEOHmpdAGSsEwpOsE4AgFBqMfg71glIs25921knIER69OtvnZA0Wbnh/jU63B99qgy9wboAyFiX9b/MOgEAQqnj5ZdbJyDNjj+1l3UCQmTw2edaJyRNpF2OdYKpIAsrAgAAAAAAcCRCSjxzduL2/BdsO4AMdNmriSMRHhn+iHEJAITLmksmSJJ6PP6YcQnS5a8PLJIknX7lAOMShMELd06WJJ19063GJY1XtaHMOsEUQ4RUqNppXQBkrIp4hXUCAISS38nPL2ETr6yxTkCIxCub0c943jrAFqczAAAAAACAQBgiAAAAAACAQBgiAAAAAACAQFgTIRUO/ZF1AZCxhnYbap0AAKHUctgw6wSkWc+jOlgnIEQOPnaQdULSZLUI96/R4f7oU+W7V1kXABnrwn4XWicAQCgdcNHPrBOQZsf8sId1AkLk+NPPsk5ImkibHOsEU5zOAAAAAAAAAmGIkApTT028AGiw8bPHa/zs8dYZABA6n4/7d30+7t+tM7AP24u3aMaUSSorKU7aPmfe875m3vN+0vYH7M+MWydpxq2TrDOSomp9marWl1lnmGGIAAAAADRxC154VkXLl2r+89OsUwCEHGsiAAAAAE3UfeePVnVV1a77i+fM0uI5sxSJxXT1MzMNywCEFUciAAAAAE3UxQ88ob7fHapodmIht2h2jvp+b5guefAPxmUAwoohAgAAANBEtWzXXtl5LRSvqlQkFlO8qlI5eXnKb9vOOg1ASHE6QyoceaZ1AZCxftTzR9YJABBKrUaNtE7APpRvLVH/EaN09A9G6sPXZqusODmLKx5yXKek7AcI4rDvnGidkDRZ+THrBFMMEVJh0CXWBUDGGtt3rHUCAIRS+/POs07APpxx/S93vT78osuTtt+jhnVL2r6A+gz4UfO5el2kdbZ1gimGCKlQWZ64zW5h2wFkoB3xHZKkvGiecQkAhEvNjsT336w8vv+GRVVltSQplh0xLkEYVFXslCTFcnKNS5LAe+sCUwwRUuFP/5a4Hf+SbQeQgS5/NfEXlqkjpxqXAEC4rJ1wqSTpoKefMi5BuvztgcWSpNHXHWtcgjD4811TJEljJt9lG5IEVRvKrRNMsbAiAAAAAAAIhCECAAAAAAAIhCECAAAAYGR78RbNmDJJZSXJueICAKQaQwQAAADAyIIXnlXR8qWa//w06xQACISFFVNhAJdIAr6tMw45wzoBAEKpzejR1gmhct/5o1VdVbXr/uI5s7R4zixFYjFd/czMtDT0HVyQlscBJOnIocOtE5Imq2XMOsEUQ4RUOOan1gVAxjrzkDOtEwAglNqexRAhnS5+4AnNe/oJrXhvgeKVFYpm54jHiDcAAB7RSURBVOiQQYM1bNxFaWs4fAhDBKRPv2HNZ4gQaZVtnWCKIUIqlH2VuM0/wLYDyEDFOxPnhLbLbWdcAgDhEi9OfP+NtuP7bzq0bNde2XktFK+qVCQWU7yqUjl5ecpvm77P/47tlZKkvJbh/oUI6VG+baskqUXrNsYljeervXWCKYYIqfDcvydux79k2wFkoGvnXitJmjpyqnEJAITLF1dNlCQd9PRTxiXhUb61RP1HjNLRPxipD1+brbLi9C6uOPvRJZKk0dcdm9bHRTj99d47JUljJt9lXNJ48Y3l1gmmGCIAAAAABs64/pe7Xh9+0eWGJQAQHFdnAAAAAAAAgTBEAAAAAAAAgTBEAAAAAAAAgbAmQioc/zPrAiBjjTlsjHUCAIRSu3PHWicgzfoN7WqdgBDpP+IU64Sk4RKPSL5+Z1sXABlrZK+R1gkAEEqtT2k+P+AjmD4DO1snIET6DjnJOiFpslrGrBNMMURIha1Fids23Ww7gAy0oWyDJKlLfhfjEgAIl6r16yVJsYIC4xKkS+mWnZKkVu1zjUsQBts2b5Ikte7Q0bik8Xy8xjrBFGsipMKfL028IG2qNm7U6vPHKb5pk3UKGummN2/STW/eZJ0BAKGz7oYbte6GG60zkEavTv1Yr0792DoDITHroXs066F7rDOSIr5ph+KbdlhnmGGIgGZh88O/046FC7XpoYetUwAAAACg2eJ0BmS05f0HyFdU7LpfMn26SqZPl8vJUd/FiwzLAAAAAKD54UgEZLTec/6u1qedKpebOJfP5eaq9emn6ZBX5xiXAQAAAEDzwxABGS3WqZOyWraUr6iQy8mRr6hQVn5LRTtm/oItAAAAANDUcDpDKgy5wrogVOKbv1LbsWPVbsxPVDzjuaQtrli1caO+uPY6dbv3Nwwl0uiCIy+wTgCAUGo/frx1AtJswIge1gkIkYGnjbZOSJpIm2zrBFMMEVLhsFHWBaHS/cEHdr1eMPmWpO1398UaC6ZMTtp+sX/Dug+zTgCAUGr1/ZOtE5BmvY7uYJ2AEOl93AnWCUmT1SJmnWCKIUIqbP40cduhj20HvhUWa7S1ausqSVKvNr2MSwAgXCpWJr7/5hzM99+wKN5QJklq1yXfuARhsGVdkSSp/YHd9rtdaWmpnn/+eZ1zzjlq1apVOtIazFfVWCeYYk2EVPjr1YkXZCQWa7R12/zbdNv826wzACB0NkyerA2TOfIuTOb+6RPN/dMn1hkIiTmPP6g5jz9Y73bz5s3TmjVrNG/evDRUfTvxzTsU37zDOsMMRyIAe2CxRgAAACC9br/9dsXj8V33CwsLVVhYqGg0qptvvtmwDHviSASgDl8v1thzxnS1HTtW8c2brZMAAACAZmvixInq16+fotHE37mj0aiOOuooTZw40bgMewp0JIJzbqSk+yVFJP3ee3/XHu+/VtLFkuKSNkn6mff+8yS3AmmTqsUaAQAAAOytVatWysnJUXV1taLRqKqrq5WTk9Nk10UIs3qHCM65iKSHJI2QVCTpPefci977j3fb7ANJA7335c65/5D0a0ljUhEMAAAAAGh+ysrKdNxxx2ngwIEqLCzU9u3brZNQhyBHIgyStMJ7v1KSnHPTJZ0hadcQwXv/+m7bL5B0fjIjM85J11sXABlrwtETrBMAIJQ6/Mdl1glIs4Gn9LROQIh8Z/TYercZO/abbU477bRU5jRKpG2OdYKpIEOErpLW7na/SNL+LvJ5kaRZdb3DOTdB0gRJ6tGjR8DEDNSb6ywD39bgAwdbJwBAKOUPGWKdgDTrfnh76wSEyEFHD7BOSJqsvHBfnyDIwoqujrf5Ojd07nxJAyX9V13v994/5r0f6L0f2LE5r3S//sPEC4AGW75luZZvWW6dAQChs3PZMu1ctsw6A2m0aW2pNq0ttc5ASGxcvVIbV6+0zkgKX1ktX1ltnWEmyAilSFL33e53k7Ruz42cc8Ml/VLSUO99RXLyMtTsmxK341+y7QAy0N3v3i1JmjpyqnEJAITLl7+6U5J00NNPGZcgXd567lNJ0ujrjjUuQRi8/uRjkqQxk++qZ8umL/7VTusEU0GORHhPUh/nXC/nXLaksZJe3H0D59wxkh6V9GPv/cbkZwIAAAAIm+ptldr46GJVl1ZapwCoVe8QwXsfl3SFpFckLZP0nPd+qXPuNufcj2s3+y9JLSX9j3NukXPuxX3sDgAAAAAC2fbaGlWu3qZtr66xTgFQK9CKEN77lyW9vMfbbtnt9eFJ7gKAtCvbWqG//36JfnhxP+W3CfequwAAWCq6+S0p/s0ybGXvrFfZO+ulqFO3279nWAYgyOkMABAKhS+t0roVW1X40qr9ble2tUIz71mosq3hXv4FAIBUKbhhkPIGdJRitb+uxLKUN6CjCm4cZBsGINiRCGigH9xS/zYA6jTx2Ilpf8xHrpir6njNrvtL3linJW+sUySapcseHLbX9rsPG4ae13e/++boBgCZouM111gnIM2+c2Zv64R9irTOVlZORIrXSFEnxWuUlRNVpFW2dRq+pe+NvcA6IWki7XKtE0wxREiFHidYFwAZa0Cn9F9DeNwdg/XP51do1aJNilfVKBrL0sHHdNSQsw/5l+0aOmyQGjZwAABLLY49xjoBaVbQu411wn5Vb69S/gkFyh/URWXvbmBxxQzX9bDDrROSJis3Yp1giiFCKqx5J3HLMAFosEUbF0lK7zAhv02OsnMjisdrFIllKR6vUXZuZK8jB4IOG6RvN3AAAEvl738giWFCmKz/bKukpjtM6DDuiF2vZ5+593MtMssXnyyT1DyGCTU7q60TTLEmQiq8dlviBUCD3f/+/br//fvT/rg7SivV76SuOufG49TvpK4q37b3XzuCDhukxMChz/GdFa09lzMay9Khgzpr3B2DU/6xAMC3senee7Xp3nutM5AE24u3aMaUSSorKd7vdgv+8pkW/OWzNFUh7N6a/qTemv6kdUZSVBfvVHXxTusMMxyJAACSRl129K7Xh5572D63+3rYcOSJB2rpm+tUvo/FFRsycAAAIJkWvPCsipYv1fznp2n4xT+3zgHQzDBEAIAGCDpskIIPHAAASIb7zh+t6qqqXfcXz5mlxXNmKRKL6epnZhqWAWhOGCIAQIo0ZOAAAEBjXfzAE5r39BNa8d4CxSsrFM3O0SGDBmvYuIus0wA0I6yJAAAAADQDLdu1V3ZeC8WrKhWJxRSvqlROXp7y27azTgPQjHAkQiqMvNO6AMhYNw660ToBAEKp8y9usk5AEpRvLVH/EaN09A9G6sPXZquseN+LK37vJ33SWIawO/mCCdYJSRM9INc6wRRDhFQoOLr+bQDUqW/7vtYJJsq2Vujvv1+iH17cj8UXAZjIPTzzL7sG6Yzrf7nr9eEXXb7fbTt2b5XqHGCXTj0Ptk5IGpcdsU4wxekMqfDZ64kXAA02f918zV833zoj7QpfWqV1K7aq8KVV1ikAQqrs7bdV9vbb1hlIo7XLtmjtsi3WGQiJzz9cpM8/XGSdkRQ1O+Kq2RG3zjDDkQip8MZ/J257n2zbAWSgxz58TJI0+MDBxiXp8cgVc1Udr9l1f8kb67TkjXWKRLN02YPD7MIAhM7m3z0iScofMsS4BOlS+PJqSVL3w9vbhiAUFsycLkk66OgBxiWNV10S7itucSQCgGatbGuFZt6zUGVN9PKK4+4YrD7Hd1Y0lvh2HI1l6dBBnTXujnAMUQCgOdpevEUzpkxSWcm+1yMAgEzFEAFAs9bUTxPIb5Oj7NyI4vEaRWJZisdrlJ0bYV0EAMhgC154VkXLl2r+89OsUwAg6TidAUCzlEmnCeworVS/k7rqyBMP1NI316m8iR41AQDYv/vOH63qqqpd9xfPmaXFc2YpEovp6mdmGpYBQPIwRADQLI27Y7D++fwKrVq0SfGqGkVjWTr4mI4acvYh1ml7GXXZN1d0GXruYYYlAIDGuPiBJzTv6Se04r0FildWKJqdo0MGDdawcRdZpwFA0jBESIXT77MuADLWLYNvScp+OE0AABqmy623WidkvJbt2is7r4XiVZWKxGKKV1UqJy9P+W3bWafVadhPGVwjfUZccoV1QtJEO+RZJ5hiiJAKHfpYFwAZq1ebXknbF6cJAEBwOQcn7/tvmJVvLVH/EaN09A9G6sPXZqusuOkurtiuS751AkKk/YHdrBOSxsXCvbQgQ4RU+GRW4vawUbYdQAaau3auJGlY92GN3henCQBAcKX/eF2S1Or7XKK6Mc64/pe7Xh9+0eWGJfVb9eFmSVKvozsYlyAMPlv4jiSp93EnGJc0Xk15Vf0bNWMMEVLh7QcTtwwRgAZ7cumTkpIzRGiOyrZW6O+/X6IfXtyPUzMAJNWWqVMlMUQIk0Vz1khiiID0KPxbYnHR5jBEqN5aaZ1gKtzHYQBAhmnql6wEAABA88aRCACQATLpkpUAAABovjgSAQAywLg7BqvP8Z0VrV3IJxrL0qGDOmvcHYONywAAABAmDBEAIANwyUoAsLe9eItmTJmkspKme8UFAEg1TmdIhbMetS4AMtadJ95pndBkcclKAKl04K/vtk5o8ha88KyKli/V/OenafjFP7fOabTh44+wTkCIjPr5ddYJSRPtmGedYIohQiq0aT7XQAXSrUt+F+uEJotLVgJIpVhBgXVCk3Xf+aNVXfXNJd0Wz5mlxXNmKRKL6epnZhqWNU6r9rnWCQiR1h06WickjYuG+4D+cH/0qbLkhcQLgAabvWq2Zq+abZ0BAKGz7eWXte3ll60zmqSLH3hCfb87VNHsxClk0ewc9f3eMF3y4B+Myxrn08Iv9Wnhl9YZCInlb7+h5W+/YZ2RFDXbq1Szvar+DZspjkRIhfdqn1D6nW3bAWSgGZ/MkCSN7DXSuAQAwqX42emSpNannGJc0vS0bNde2XktFK+qVCQWU7yqUjl5ecpv2846rVGWzPtCktRnYGfjEoTB4jmJIWXfIScZlzRedWmldYIphggAAABAPcq3lqj/iFE6+gcj9eFrs1VWzOKKAMKJIQIAAABQjzOu/+Wu14dfdLlhCQDYYk0EAAAAAAAQCEMEAAAAAElRva1SGx9dHPpzxoHmjNMZUuEnT1kXABnrjmPu1ltPrVLZ4Arlt8mxzgGA0Oj62/utE5BmIy/tl/R9bnttjSpXb9O2V9eo3ehDkr5/ZK7Tr7nJOiFpop1aWCeYYoiQCvkHWBcAGevT17Zo08oyFb60SkPP62udAwChEW2X2VcaQMPltcxO2r6Kbn5Livtd98veWa+yd9ZLUadut38vaY+DzNWidRvrhKRxEWedYIohQip88KfE7TE/te0AMsgjV8xVdbxm1/0lb6zTkjfWKRLN0mUPDttr+8ryPH366vfVtzdHLOxL2dYK/f33S/TDi/vxOQJQr5I/z5QktT1rtHEJ0mXZ2+slSYcPKWj0vgpuGKSSl1dqx9KvpKoaKZalvCMPUNtTD270vtE8LJn7qiSp37DhxiWNF/bTdVgTIRUWTUu8oNFi27friGnTFN+0yToFKTbujsHqc3xnVUfikqRoLEuHDuqscXcMrnP7ooXHqHRDFxW+tCqdmRml8KVVWrdiK58jAIFsnTlTW2fOtM5AGi2fv17L569Pyr4irbOVlROR4jVS1EnxGmXlRBVplbyjHZDZls57VUvnvWqdkRQ126tUs73KOsMMRyKgSev6z7fVam2RNj30sAqmTLbOQQrlt8lRdm5EWdUR1WTFFY9HlZ0b2esv6N8csXCEpPqPWAijhh7VAQBAMlRvr1L+CQXKH9RFZe9uCP1fa4HmiiECmqTl/QfIV1SoS+39kunTVTJ9ulxOjvouXmTahtTZUVqpTT0/0aZeKzSm5lKVb63Ya5txdwzWP59foc/e/0I11VFFY1k6+JiOGnI2izd97evP0apFmxSvquFzBABIiw7jjtj1evaZPOcAzRWnM6BJ6j3n72p92qmqjibmXC43V61PP02HvDrHuAypNOqyo7VmwHva0aZYQ889TKMuO3qvbb4+YqGmOiIXiSser6nziIUw+/pzFI/XKBLL4nMEAACApGGIgCYp1qmTslq2VFY8rppIRL6iQln5LRXt2NE6DU3AjtJKdT5imfqd+aL6ndRV5ds4XHJPO0or1e+krjrnxuP4HAFAPbYXb9GMKZNUVlJsnQIATR6nM6TCT//HuqBZiG/+Sl8eM0AbBwzQSTsrWFwxJB4e/nC924y67GgtfP8uSdJxIw5LdVJG2v0ojqHn8jkCUL/ujz1qnWBmwQvPqmj5Us1/fpqGX/xz65y0Oe3K/tYJCJGzJk2xTkiaWJcW1gmmGCKkQna4/1EFUbVxo7649jp1u/c3+zy6oPuDD+jVqVMlSQXjx6czD4byonnWCaHDpSABSFJWXvi+/953/mhVV32zwvriObO0eM4sRWIxXf1M879SRSw7Yp2AEInl5FonJI9z1gWmOJ0hFd59PPGCfdr88O+0Y+FCbXqo/r86I1ymL5+u6cunW2eECpeCBCBJW6ZN05Zp4bpE9cUPPKG+3x2qaHZigBrNzlHf7w3TJQ/+wbgsPT6aW6SP5hZZZyAkFr3ykha98pJ1RlJUb6tUdYhPFeVIhFRY+pfE7aBLbDuaoK+vuvA1rrqAPb2y+hVJ0ti+Y41Lmj8uBQlgd6WzZkuS2p93nnFJ+rRs117ZeS0Ur6pUJBZTvKpSOXl5ym/bzjotLVYs3ChJOmpYt/1uV72tUl89u0wHnHe4Iq2y05GGZuiTBW9Kkgb86NSk7K+0tFTPP/+8zjnnHLVq1Sop+wyqpqyq/o2asUBHIjjnRjrnPnHOrXDOTarj/TnOuRm173/HOdcz2aFIvqqNG7X6/HH1rjWQzO2+vuqCy00czsRVFwA74+4YrD7Hd1Y0lngqiMaydOigzhp3x+A6ty/bWqGZ9yxUWR2X3gTQdIX1aze2fauO+tOD9S6WWL61RP1HjNJ5t9+j/iNGqaykJE2FmWPba2tUuXqbtr26xjplv6q3VWrjo4tVXRrevxCHybx587RmzRrNmzfPOiV06h0iOOcikh6SNErSEZLOdc4dscdmF0kq9t4fIuleSXcnOzSTVG2Pa/W0dUn75bwh2zZkn0FPKUjmdl9fdcFXVMjl5DSbqy5YDGSa42MjvRp6KciGnPYQ9JeWhvxyk+x9hvWxU7HPsD52KvaZiscO+rW7w9dobtW2QFcoCHo1g4Zc9SDZ++z+zzlqvXal5j+//1M0zrj+lxp+0eXq1PNgDb/ocp1x/S/rbW0uYtU1Oqxkxz5/6S66+S0VTXpTZe+sl7xU9s56FU16U0U3v5Xm0mCCDjsaMmwIum2yt5OkVnGvy9fFTR47FftMlttvv11TpkxRYWGhvPcqLCzUlClTdPvtt+/zvymNxzR1w2EqLS3d775LS0s1derUercLuyBHIgyStMJ7v9J7XylpuqQz9tjmDElP1r7+vKQfOBfe1SY2/7NYO9buTNov5w3ZNsh2y/sP0LK+h6tk+nTJe5VMn65lfQ/X8v4DUrrd1+Kbv1LbsWPVc8Z0tR07VvHNm+v9+Js6i4FMc3xspF+QS0E+csVcPXTZP7TkjXWST5z28NBl/9AjV8zd536D/tLSkMFEsvcZ1sdOxT7D+tip2GcyH7uhX7vLqndos4/X+0u39K9XM0jGdsnc533nj9Y9Y07TgR/8U05ei+fM0j1jTtN954+utyFsCsqr1KqqZp+/dBfcMEh5AzpKtUesKZalvAEdVXDjoDRW1q+hw46GHFkRdNtkbydJI0pq1KtCJo+din0my8SJE9WvXz9Fo4kz86PRqI466ihNnDhxn//NvK0FWlPRqt6jFji6IRjnvd//Bs6dI2mk9/7i2vvjJJ3gvb9it22W1G5TVHv/s9pt9vnb4cCBA31hYWESPoT0ufWvS/Xxum37fP8vf3upYtV7nx9TFYnpjqsebfB2qdpny+0l+uEbM9T3sw+UHa9UZTRbyw85Vn8/aYy257dJ2XbfRo+v3pUkrTmgaT1ZfS3Z/38s/21Y/7v82urs/5Yk9ay8vs73f+20rolp89++uHm/26Hxsqu8en1RpQ4l1Yp4qdpJm9tGtLJrTFWxf50Xf3fRDkXqeFqpdtI/B+Q1eLtU7DOsj93cPh4+l/VvG/Rrd+Dcu5VVE99rfzVZURUOu/Ff3hZ0W8t9xipK1X3Fa2qz+f8Uq65SdVZUxR0P09pDhqsqp+Ve/30Y3bU6rlgd/4aqnDSp578umXbW5moNLvWqdlLES/NbOf25Q9O6qkOruNfpW2p0VLlXtpcqnfRRC6e/ts9SafSbf+sN+biDbpvs7ZrbY++u7/tPS5KWHzuuzvc3ROetS9WuvEheWXKqUXGL7vqyzZ4Hy0uHrZ+jLNXs9fYaZemTghEN3u5rlf6/JEnZ7j8D9R5xYGtNPv3IQNs2Fc65hd77gXW+L8AQ4d8k/WiPIcIg7/2Vu22ztHab3YcIg7z3X+2xrwmSJtTePUzSJ9/uQzLVQVKdw5GYFCuIxbrlZ2W1zZLLqpGvKaupKVlXVbU2LsUbul2q9ilJXWOxHm2zIh29vHdyrqSmetMXVVV7jRCTvV1zk+z/P5b/NprCv0s0XQe06tKjRU7rjpL3knPlFds2fVW6Ya+v8UhWNNauZaduedn5bSWXJfmaHZVlJcXbN66tronHG7pdKvYZ1sdubh8Pn8tg2wb52o1kuVjbFnndcqPRts65LO99zc54vKSkfMfa6hof/zbbWu+zfX5ejxbZ2R29994558orKzdtKdvR7H8uCSqWFY11bd25W8uc/F2fy9KKspJ1275cW7XHv6Fe7br1jtdUV31VXrzpgBbtOkazIrFVxUWfWbXvS/c2BT3a5rb+5mfRnds2rd26/l/+nzfk4w66bbK3a26PnSrt27fvXVNTU1VWVrYpPz+/Y1ZWVmzLli17/buMRCKx1q1bd8vJydnVWVFRUbJ169a1NTXfTCWDbhcyB3nv6zznPMjVGYokdd/tfjdJ6/axTZFzLiqpjaQte+7Ie/+YpMeCFDdVzrnCfU1kAOwbXzvAt8fXD/Dt8LUDfHt8/WBfgqyJ8J6kPs65Xs65bEljJb24xzYvSrqg9vVzJP3D13eIAwAAAAAAyCj1HongvY87566Q9IqkiKQ/eO+XOuduk1TovX9R0hOSnnbOrVDiCAQu8A4AAAAAQDMT5HQGee9flvTyHm+7ZbfXd0r6t+SmNVkZfToGYIivHeDb4+sH+Hb42gG+Pb5+UKd6F1YEAAAAAACQgq2JAAAAAAAAwBAhKOfcSOfcJ865Fc65SdY9QFPmnOvunHvdObfMObfUOTex9u3tnXNznHOf1t62s24FmiLnXMQ594Fz7m+193s5596p/dqZUbvQMYA9OOfaOueed84tr30OGsxzD1A/59w1tT+zLXHOPeucy+W5B/vCECEA51xE0kOSRkk6QtK5zrkjbKuAJi0u6Trv/eGSviPp57VfM5Mkvea97yPptdr7APY2UdKy3e7fLene2q+dYkkXmVQBTd/9kmZ77/tK6q/E1xHPPcB+OOe6SrpK0kDvfT8lFtMfK557sA8MEYIZJGmF936l975S0nRJZxg3AU2W93699/792tdLlfghrqsSXzdP1m72pKQzbQqBpss5103SqZJ+X3vfSfq+pOdrN+FrB6iDc661pJOUuGqYvPeV3vsS8dwDBBGVlOeci0pqIWm9eO7BPjBECKarpLW73S+qfRuAejjneko6RtI7kjp779dLiUGDpE52ZUCTdZ+kGyTV1N4/QFKJ9z5ee5/nIKBuB0vaJGlq7elAv3fO5YvnHmC/vPdfSPpvSWuUGB5slbRQPPdgHxgiBOPqeBuXtQDq4ZxrKekFSVd777dZ9wBNnXPuNEkbvfcLd39zHZvyHATsLSrpWEm/894fI6lMnLoA1Kt2nZAzJPWSdKCkfCVO494Tzz2QxBAhqCJJ3Xe7303SOqMWICM452JKDBD+5L3/c+2bv3TOFdS+v0DSRqs+oIn6rqQfO+dWK3Hq3PeVODKhbe0hphLPQcC+FEkq8t6/U3v/eSWGCjz3APs3XNIq7/0m732VpD9LGiKee7APDBGCeU9Sn9oVSrOVWGjkReMmoMmqPYf7CUnLvPe/2e1dL0q6oPb1CyT9b7rbgKbMe3+T976b976nEs81//De/1TS65LOqd2Mrx2gDt77DZLWOucOq33TDyR9LJ57gPqskfQd51yL2p/hvv7a4bkHdXLec1RKEM65U5T4a1BE0h+893cYJwFNlnPue5LelPSRvjmv+xdKrIvwnKQeSjxh/Zv3fotJJNDEOeeGSbree3+ac+5gJY5MaC/pA0nne+8rLPuApsg5N0CJRUmzJa2UNF6JP5rx3APsh3PuVkljlLjC1geSLlZiDQSee7AXhggAAAAAACAQTmcAAAAAAACBMEQAAAAAAACBMEQAAAAAAACBMEQAAAAAAACBMEQAAAAAAACBMEQAAAAAAACBMEQAAAAAAACBMEQAAAAAAACB/P99ViZmF6HElgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ymax = 1\n",
    "\n",
    "plt.plot([0, item[1][idx].item()], [0, 0])\n",
    "\n",
    "for pivot in loaded_vidid_selected_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax], '--')\n",
    "    \n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax])\n",
    "    \n",
    "all_prob = []\n",
    "for i, segment in enumerate(prob_each_segment):\n",
    "#     all_prob.append(segment/torch.sum(segment))\n",
    "    prob = segment / torch.sum(segment)\n",
    "    prob = prob.cpu().numpy()\n",
    "    xs = np.arange(loaded_vidid_selected_frames[cur_vidid][i], loaded_vidid_selected_frames[cur_vidid][i+1])\n",
    "    plt.plot(xs, prob, '*')\n",
    "\n",
    "plt.ylim([0, ymax])\n",
    "# all_prob = torch.cat(all_prob).cpu().numpy()\n",
    "# xs = np.arange(loaded_vidid_selected_frames[cur_vidid][0] + 1, loaded_vidid_selected_frames[cur_vidid][-1])\n",
    "# plt.plot(xs, all_prob, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.1846523, 2.9907308)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(prob), np.min(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_features_shortened = cur_vid_feat[:item[1][idx]]\n",
    "cur_features_shortened[:, item[2][idx][:item[1][idx]]].shape\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 47, 47, 47, 47, 47, 47,  4,  4,  4,  4,  4,  4,  4,  4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[2][idx][video_id_boundary_frames[cur_vidid][1]-6:video_id_boundary_frames[cur_vidid][1]+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid][0]-6,video_id_boundary_frames[cur_vidid][0]+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0794, 13.3350],\n",
       "        [ 1.0831, 13.5102],\n",
       "        [ 1.1767, 13.7662],\n",
       "        [ 1.3750, 14.0177],\n",
       "        [ 1.7298, 14.1727],\n",
       "        [ 2.1706, 14.0367],\n",
       "        [ 2.7338, 13.4563],\n",
       "        [ 4.5600, 12.1745],\n",
       "        [ 7.8450, 10.3246],\n",
       "        [11.0151,  8.3563],\n",
       "        [13.4775,  6.6546],\n",
       "        [14.1836,  5.2898],\n",
       "        [11.2142,  4.2791],\n",
       "        [ 5.9829,  3.5694],\n",
       "        [ 2.3440,  2.9389],\n",
       "        [ 0.7406,  2.3515]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_vid_feat[video_id_boundary_frames[cur_vidid][1]-7:video_id_boundary_frames[cur_vidid][1]+9][:,[47,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frames_and_get_classification_value(model_output, count_list, video_ids_list, labels_all, p_criterion):\n",
    "    new_selected_frames = {}\n",
    "#     new_selected_frames_probs_values = {}\n",
    "#     new_selected_frames_labels = {}\n",
    "    \n",
    "    labels_arr = []\n",
    "    probs_arr = []\n",
    "    \n",
    "    for i, video_id in enumerate(video_ids_list):\n",
    "        current_boundary_frames = boundaries_dict[video_id]\n",
    "        current_video_logits = model_output[i].T\n",
    "        current_video_count = count_list[i]\n",
    "        current_selected_frames = loaded_vidid_selected_frames[video_id]\n",
    "        labels = labels_all[i]\n",
    "        cur_seg_start = 0\n",
    "        for seg_num, ele in enumerate(current_boundary_frames):\n",
    "            cur_seg_end = ele + 1\n",
    "            total_seg_len = cur_seg_end - cur_seg_start\n",
    "            per_frames_far_from_boundary = int(0.3 * total_seg_len)\n",
    "            new_frames_start = cur_seg_start + per_frames_far_from_boundary\n",
    "            new_frames_end = cur_seg_end - per_frames_far_from_boundary\n",
    "            \n",
    "            frames_added = list(range(new_frames_start, new_frames_end))\n",
    "            frames_added.append(current_selected_frames[seg_num])\n",
    "            frames_added = list(set(frames_added))\n",
    "            frames_added.sort()\n",
    "            new_selected_frames[video_id] = frames_added\n",
    "\n",
    "            \n",
    "            labels_seg = torch.stack([labels[current_selected_frames[seg_num]]] * len(frames_added))\n",
    "            frames_added = torch.tensor(np.array(frames_added), dtype=torch.long, device=current_video_logits.device)\n",
    "            prob_seg = current_video_logits[frames_added, :]\n",
    "            \n",
    "            labels_arr.append(labels_seg)\n",
    "            probs_arr.append(prob_seg)\n",
    "            \n",
    "            cur_seg_start = cur_seg_end\n",
    "            \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    probs_arr = torch.cat(probs_arr)\n",
    "    loss = p_criterion(probs_arr, labels_arr)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
