{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import C2F_TCN\n",
    "from utility.adaptive_data_loader import Breakfast, collate_fn_override\n",
    "from utils import calculate_mof, dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipika_singhania\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"992b3b1371ba79f48484cfca522b3786d7fa52c2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "def set_seed():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# Device configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='6'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 500, 'num_class': 48, 'batch_size': 12, 'learning_rate': 0.0005, 'weight_decay': 0, 'dataset': 'Breakfast', 'architecture': 'unet-ensemble', 'features_file_name': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/', 'chunk_size': 1, 'max_frames_per_video': 1200, 'feature_size': 2048, 'ground_truth_files_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/', 'label_id_csv': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv', 'gamma': 0.1, 'step_size': 500, 'split': 2, 'output_dir': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-c2ftcn-speed/', 'project_name': 'breakfast-split-2', 'train_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split2.bundle', 'test_split_file': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split2.bundle', 'all_files': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt', 'cutoff': 8, 'data_per': 0.2, 'budget': 40, 'semi_supervised_split': '/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split2_amt0.2.bundle'}\n"
     ]
    }
   ],
   "source": [
    "config = dotdict(\n",
    "    epochs=500,\n",
    "    num_class=48,\n",
    "    batch_size=12,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0,\n",
    "    dataset=\"Breakfast\",\n",
    "    architecture=\"unet-ensemble\",\n",
    "    features_file_name=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/features/\",\n",
    "    chunk_size=1,\n",
    "    max_frames_per_video=1200,\n",
    "    feature_size=2048,\n",
    "    ground_truth_files_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/groundTruth/\",\n",
    "    label_id_csv=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/mapping.csv\",\n",
    "    gamma=0.1,\n",
    "    step_size=500,\n",
    "    split=2,\n",
    "#     output_dir=\"/mnt/data/ar-datasets/dipika/breakfast/ms_tcn/data/breakfast/results/unsuper-finetune-split2-0.05-data-llr/\",\n",
    "    output_dir=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-c2ftcn-speed/\",\n",
    "    project_name=\"breakfast-split-2\",\n",
    "    train_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/train.split{}.bundle\",\n",
    "    test_split_file=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/test.split{}.bundle\",\n",
    "    all_files=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/splits/all_files.txt\",\n",
    "    cutoff=8,\n",
    "    data_per = 0.2,\n",
    "    budget=40,\n",
    "    semi_supervised_split=\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/semi_supervised/train.split{}_amt{}.bundle\")\n",
    "\n",
    "config.train_split_file = config.train_split_file.format(config.split)\n",
    "config.semi_supervised_split = config.semi_supervised_split.format(config.split, config.data_per)\n",
    "config.test_split_file = config.test_split_file.format(config.split)\n",
    "\n",
    "if not os.path.exists(config.output_dir):\n",
    "    os.mkdir(config.output_dir)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos logged in train fold is 1261\n",
      "Number of videos not found in train fold is 0\n",
      "Number of videos logged in test fold is 451\n",
      "Number of videos not found in test fold is 0\n"
     ]
    }
   ],
   "source": [
    "traindataset = Breakfast(config, fold='train', fold_file_name=config.train_split_file)\n",
    "testdataset = Breakfast(config, fold='test', fold_file_name=config.test_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=traindataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          pin_memory=True, num_workers=4, \n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=config.batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          pin_memory=True, num_workers=4,\n",
    "                                          collate_fn=lambda x: collate_fn_override(x, config.max_frames_per_video),\n",
    "                                          worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = iter(trainloader).next()\n",
    "# print(item[0]) # -- data feature\n",
    "# print(item[1]) # -- valid count\n",
    "# item[2] # -- Actual labels\n",
    "# print(item[5]) # video names\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = np.load(\"/home/dipika16/ar/TimestampActionSeg/data/breakfast_annotation_all.npy\", allow_pickle=True).item()\n",
    "# loaded_vidid_selected_frames\n",
    "video_id_boundary_frames = pickle.load(open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"rb\"))\n",
    "# video_id_boundary_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mean_var_actions = pickle.load(open(\"dump_dir/mean_var_actions.pkl\", \"rb\"))\n",
    "def get_possion_prob(cur_len, cur_class):\n",
    "    mean_class, std_class = loaded_mean_var_actions[cur_class]\n",
    "    mean_class = mean_class * 10\n",
    "    prob = cur_len * torch.log(torch.tensor(mean_class, device=device) + 1e-5)\n",
    "    prob = prob - mean_class\n",
    "    factorials = torch.cumsum(torch.log(torch.arange(1, torch.max(cur_len)+1, 1).type(torch.float).to(device)), \n",
    "                              dim=0)[min(cur_len)-1:]\n",
    "    prob = prob - factorials\n",
    "    return prob\n",
    "\n",
    "# get_possion_prob(torch.arange(10, 21), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_video_each_segment_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vals_per_segment(selected_frames, cur_vid_feat, labels):\n",
    "    prob_each_segment = []\n",
    "    \n",
    "    log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "    cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "    cur_boundary = 0\n",
    "    for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "\n",
    "        next_ele = selected_frames[i + 1]\n",
    "        label_cur_ele = labels[cur_ele]\n",
    "        label_next_ele = labels[next_ele]\n",
    "\n",
    "        indices = torch.arange(cur_ele, next_ele).to(cumsum_feat.device)\n",
    "        strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "        end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "        left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "        right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "\n",
    "        cur_boundary_len = indices - cur_boundary\n",
    "        len_prob = get_possion_prob(cur_boundary_len, label_cur_ele.item())\n",
    "\n",
    "        prob = torch.softmax((left_sum + right_sum + len_prob), dim=0)\n",
    "#         prob = torch.softmax((left_sum + right_sum), dim=0)\n",
    "\n",
    "        cur_boundary = round(np.sum(np.arange(cur_ele, next_ele, 1) * prob.detach().cpu().numpy()).item())\n",
    "        prob_each_segment.append(prob)\n",
    "    \n",
    "    return prob_each_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frames = torch.tensor([2, 10, 17, 21])\n",
    "cur_vid_feat = torch.randn((27, 48))\n",
    "labels = torch.tensor([47, 47, 47, 47, 47, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10])\n",
    "# print(len(labels))\n",
    "# probs_all_segs = prob_vals_per_segment_new(selected_frames, cur_vid_feat, labels)\n",
    "# print(probs_all_segs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_element_probb(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_video = prob_vals_per_segment(selected_frames, cur_vid_feat, labels)\n",
    "        prob_video_each_segment_dict[cur_vidid] = prob_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_loss(data_feat, data_count, video_ids, labels_all): #, loaded_vidid_selected_frames, boundaries_dict):\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    loss_arr = []\n",
    "    for iter_num in range(len(data_count)):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        cur_vid_feat = data_feat[iter_num]\n",
    "        cur_vid_count = data_count[iter_num]\n",
    "        labels = labels_all[iter_num]\n",
    "        \n",
    "        selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "        prob_each_video = prob_video_each_segment_dict[cur_vidid]\n",
    "        \n",
    "        log_probs = torch.log(cur_vid_feat + 1e-8)\n",
    "        cumsum_feat = torch.cumsum(log_probs, dim=0)\n",
    "        \n",
    "        \n",
    "        current_vid_prob_arr = []\n",
    "        # Start segment log probability\n",
    "        start_ele_idx = selected_frames[0]\n",
    "        if start_ele_idx > 0:\n",
    "            start_ele_label_idx = labels[selected_frames[0]]\n",
    "            first_segment_prob = (cumsum_feat[start_ele_idx - 1, :])[start_ele_label_idx]\n",
    "            current_vid_prob_arr.append(first_segment_prob.unsqueeze(0))\n",
    "        \n",
    "        for i, cur_ele in enumerate(selected_frames[:-1]):\n",
    "            next_ele = selected_frames[i + 1]\n",
    "            label_cur_ele = labels[cur_ele]\n",
    "            label_next_ele = labels[next_ele]\n",
    "\n",
    "            indices = torch.arange(cur_ele, next_ele)\n",
    "            strt_index = cumsum_feat[cur_ele - 1, :][None, :] if cur_ele > 0 else 0\n",
    "            end_index = cumsum_feat[next_ele - 1, :][None, :]\n",
    "            left_sum = (cumsum_feat[indices, :] - strt_index)[:,label_cur_ele]\n",
    "            right_sum = (end_index - cumsum_feat[indices, :])[:,label_next_ele]\n",
    "            current_vid_prob_arr.append((left_sum + right_sum)  * (prob_each_video[i]))\n",
    "        \n",
    "        # End segment log probability\n",
    "        end_index = cur_vid_count.item() - 1\n",
    "        last_ele_label_idx = labels[selected_frames[-1]]\n",
    "        last_segment_sum_prob = (cumsum_feat[end_index, :] - cumsum_feat[selected_frames[-1] - 1, :])[last_ele_label_idx]\n",
    "        current_vid_prob_arr.append(last_segment_sum_prob.unsqueeze(0))\n",
    "            \n",
    "        loss_arr.append(torch.cat(current_vid_prob_arr))\n",
    "    return -torch.mean(torch.cat(loss_arr)), loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_estimated_boundaries():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames, video_id_boundary_frames\n",
    "    estimated_boundary_dict = {}\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if (ele + \".txt\") not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        selected_ele_list = loaded_vidid_selected_frames[ele + \".txt\"]\n",
    "        boundary_list = []\n",
    "        for i, segment in enumerate(estimated_boundary_probs):\n",
    "            estimated_boundary = np.sum(np.arange(selected_ele_list[i], selected_ele_list[i + 1], 1) \\\n",
    "                                        * segment.detach().cpu().numpy())\n",
    "            estimated_boundary = round(estimated_boundary.item())\n",
    "            \n",
    "            if (estimated_boundary < selected_ele_list[i]) or (estimated_boundary > selected_ele_list[i + 1]):\n",
    "                print(\"Estimated value wrong\")\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            boundary_list.append(estimated_boundary)\n",
    "\n",
    "        estimated_boundary_dict[ele] = boundary_list\n",
    "    return estimated_boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_file_list = set(open(config.train_split_file).read().split(\"\\n\")[0:-1])\n",
    "\n",
    "def get_boundary_err():\n",
    "    global prob_video_each_segment_dict, loaded_vidid_selected_frames\n",
    "    err_list = []\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    estimated_boundar_dict = get_estimated_boundaries()\n",
    "    for ele in video_id_boundary_frames.keys():\n",
    "        if ele + \".txt\" not in train_split_file_list:\n",
    "            continue\n",
    "        estimated_boundary_probs = prob_video_each_segment_dict[ele]\n",
    "        estimated_boundary_list = estimated_boundar_dict[ele]\n",
    "        \n",
    "        estimated = np.array(estimated_boundary_list) #.detach().cpu().numpy()\n",
    "        actual = np.array(video_id_boundary_frames[ele][:-1])\n",
    "        if len(actual) != len(estimated):\n",
    "            print(ele)\n",
    "            continue\n",
    "        \n",
    "        mse_err = (actual - estimated)**2\n",
    "        err_list.append(mse_err)\n",
    "        \n",
    "        estimated_labels = []\n",
    "        actual_labels = []\n",
    "        start_v1 = 0\n",
    "        start_v2 = 0\n",
    "        for i, v1 in enumerate(estimated):\n",
    "            estimated_labels.extend([i] * (v1 - start_v1))\n",
    "            start_v1 = v1\n",
    "            v2 = actual[i]\n",
    "            actual_labels.extend([i] * (v2 - start_v2))\n",
    "            start_v2 = v2\n",
    "            \n",
    "        last_ele = video_id_boundary_frames[ele][-1]\n",
    "        estimated_labels.extend([i+1] * (last_ele - start_v1))\n",
    "        actual_labels.extend([i+1] * (last_ele - start_v2))\n",
    "        \n",
    "        correct += np.sum(np.array(actual_labels) == np.array(estimated_labels))\n",
    "        total += len(actual_labels)\n",
    "        \n",
    "    print(f\"Train Boundary avergage error = {np.sqrt(np.mean(np.concatenate(err_list))):.3f}\")\n",
    "    print(f\"Train From boundary avergage accuracy = {correct * 100.0 / total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "        \n",
    "        weights = [1, 1, 1, 1, 1, 1]\n",
    "        vidlen = outp[0].shape[-1]\n",
    "        ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "        for i, outp_ele in enumerate(outp[1]):\n",
    "            upped_logit = F.upsample(outp_ele, size=vidlen, mode='nearest')\n",
    "            ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i+1] / sum(weights)\n",
    "        \n",
    "        return ensemble_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = C2F_TCN(n_channels=2048, n_classes=48).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Requires loaded_vidid_selected_frames, boundaries_dict\n",
    "ce_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "mse_criterion = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_random(labels_all, video_ids):\n",
    "    # Generate target for only timestamps. Do not generate pseudo labels at first 30 epochs.\n",
    "    boundary_target_tensor = torch.ones((labels_all.shape[0], labels_all.shape[1]), dtype=torch.long, device=labels_all.device) * (-100)\n",
    "    for iter_num, labels in enumerate(labels_all):\n",
    "        cur_vidid = video_ids[iter_num]\n",
    "        frame_idx_tensor = torch.from_numpy(np.array(loaded_vidid_selected_frames[cur_vidid + \".txt\"]))\n",
    "        boundary_target_tensor[iter_num, frame_idx_tensor] = labels[frame_idx_tensor]\n",
    "\n",
    "    return boundary_target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Training:: Epoch 51, Iteration 0, Current loss 1.7082586315795611 Accuracy 55.74887261329837\n",
      "Training:: Epoch 51, Iteration 10, Current loss 1.4466733176037352 Accuracy 55.95289823585644\n",
      "Training:: Epoch 51, Iteration 20, Current loss 1.3042550413756027 Accuracy 60.6830122591944\n",
      "Training:: Epoch 51, Iteration 30, Current loss 1.7367972136614942 Accuracy 52.99371115990027\n",
      "Training:: Epoch 51, Iteration 40, Current loss 1.4932500063856358 Accuracy 60.20248770610356\n",
      "Training:: Epoch 51, Iteration 50, Current loss 2.765516308336366 Accuracy 36.45797972940207\n",
      "Training:: Epoch 51, Iteration 60, Current loss 1.2996258656499822 Accuracy 58.630607747189124\n",
      "Training:: Epoch 51, Iteration 70, Current loss 1.6169483494873738 Accuracy 46.61617028581237\n",
      "Training:: Epoch 51, Iteration 80, Current loss 1.1245592758752196 Accuracy 65.85720736541067\n",
      "Training:: Epoch 51, Iteration 90, Current loss 1.1701000103446915 Accuracy 66.42120765832107\n",
      "Training:: Epoch 51, Iteration 100, Current loss 1.8871357675401659 Accuracy 48.03878787878788\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 51, Probability Accuracy 38.13564237477731\n",
      "Starting Training\n",
      "Training:: Epoch 52, Iteration 0, Current loss 1.6172508391967992 Accuracy 57.32663670808001\n",
      "Training:: Epoch 52, Iteration 10, Current loss 1.0147433640076575 Accuracy 64.15190468725505\n",
      "Training:: Epoch 52, Iteration 20, Current loss 0.7979698813967858 Accuracy 73.36844924560307\n",
      "Training:: Epoch 52, Iteration 30, Current loss 1.0375132638420337 Accuracy 67.92764563452499\n",
      "Training:: Epoch 52, Iteration 40, Current loss 0.9998254997038635 Accuracy 65.18998987947373\n",
      "Training:: Epoch 52, Iteration 50, Current loss 1.0442487923730004 Accuracy 69.1913678730372\n",
      "Training:: Epoch 52, Iteration 60, Current loss 1.202158216020979 Accuracy 63.35810939584525\n",
      "Training:: Epoch 52, Iteration 70, Current loss 1.1809924190273922 Accuracy 61.549180885052\n",
      "Training:: Epoch 52, Iteration 80, Current loss 0.9436071521464596 Accuracy 72.82441230555142\n",
      "Training:: Epoch 52, Iteration 90, Current loss 1.072806908582008 Accuracy 63.792307692307695\n",
      "Training:: Epoch 52, Iteration 100, Current loss 1.3691852777741005 Accuracy 56.72597864768683\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 52, Probability Accuracy 40.51228404524174\n",
      "Starting Training\n",
      "Training:: Epoch 53, Iteration 0, Current loss 1.3341927911520086 Accuracy 55.780072211313104\n",
      "Training:: Epoch 53, Iteration 10, Current loss 0.9309494730108309 Accuracy 69.70514742628686\n",
      "Training:: Epoch 53, Iteration 20, Current loss 0.8169381186439062 Accuracy 71.09721136304404\n",
      "Training:: Epoch 53, Iteration 30, Current loss 0.8677071257134473 Accuracy 73.1169044105591\n",
      "Training:: Epoch 53, Iteration 40, Current loss 0.8509060563909366 Accuracy 68.04016168992045\n",
      "Training:: Epoch 53, Iteration 50, Current loss 0.7998736644743271 Accuracy 72.4279362158847\n",
      "Training:: Epoch 53, Iteration 60, Current loss 1.3767923933517607 Accuracy 53.529705895722536\n",
      "Training:: Epoch 53, Iteration 70, Current loss 0.9006960885607116 Accuracy 67.30947368421053\n",
      "Training:: Epoch 53, Iteration 80, Current loss 0.949324667454077 Accuracy 69.46518105849582\n",
      "Training:: Epoch 53, Iteration 90, Current loss 0.9088350313725999 Accuracy 66.568141875699\n",
      "Training:: Epoch 53, Iteration 100, Current loss 0.831914522908933 Accuracy 72.41619722237715\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 53, Probability Accuracy 41.278327878361026\n",
      "Starting Training\n",
      "Training:: Epoch 54, Iteration 0, Current loss 0.6656923173601673 Accuracy 74.49428320140721\n",
      "Training:: Epoch 54, Iteration 10, Current loss 1.0351229770981887 Accuracy 65.42196489427975\n",
      "Training:: Epoch 54, Iteration 20, Current loss 0.81073751335098 Accuracy 73.31594958713603\n",
      "Training:: Epoch 54, Iteration 30, Current loss 0.9237296577519873 Accuracy 67.00982686008423\n",
      "Training:: Epoch 54, Iteration 40, Current loss 0.770401426865976 Accuracy 75.30423024917906\n",
      "Training:: Epoch 54, Iteration 50, Current loss 0.9729019813933765 Accuracy 66.57608695652173\n",
      "Training:: Epoch 54, Iteration 60, Current loss 0.7477392669962147 Accuracy 74.7414997671169\n",
      "Training:: Epoch 54, Iteration 70, Current loss 0.9726524130611768 Accuracy 68.01214209533185\n",
      "Training:: Epoch 54, Iteration 80, Current loss 0.7802277044619379 Accuracy 71.48232498267156\n",
      "Training:: Epoch 54, Iteration 90, Current loss 0.6939603384940078 Accuracy 72.60470453241538\n",
      "Training:: Epoch 54, Iteration 100, Current loss 0.9329756793745323 Accuracy 72.46589716684156\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 54, Probability Accuracy 42.53863363301156\n",
      "Starting Training\n",
      "Training:: Epoch 55, Iteration 0, Current loss 0.7429723806952349 Accuracy 71.98326093865876\n",
      "Training:: Epoch 55, Iteration 10, Current loss 0.9237524446422779 Accuracy 66.06744321134856\n",
      "Training:: Epoch 55, Iteration 20, Current loss 0.7731242903153266 Accuracy 72.43893377913996\n",
      "Training:: Epoch 55, Iteration 30, Current loss 0.7388854490331265 Accuracy 74.66487389479688\n",
      "Training:: Epoch 55, Iteration 40, Current loss 0.5843609576835623 Accuracy 77.80574304999428\n",
      "Training:: Epoch 55, Iteration 50, Current loss 0.5956020548901005 Accuracy 76.9616677219814\n",
      "Training:: Epoch 55, Iteration 60, Current loss 0.83703494793229 Accuracy 65.26807544533706\n",
      "Training:: Epoch 55, Iteration 70, Current loss 0.854071100252092 Accuracy 61.75648506840582\n",
      "Training:: Epoch 55, Iteration 80, Current loss 0.8490698472710884 Accuracy 70.66353187042843\n",
      "Training:: Epoch 55, Iteration 90, Current loss 0.9906587073608679 Accuracy 72.06407949101417\n",
      "Training:: Epoch 55, Iteration 100, Current loss 0.8558094405625367 Accuracy 70.33490731024834\n",
      "Calculating Expectation\n",
      "Epoch 55 iter 0\n",
      "Epoch 55 iter 10\n",
      "Epoch 55 iter 20\n",
      "Epoch 55 iter 30\n",
      "Epoch 55 iter 40\n",
      "Epoch 55 iter 50\n",
      "Epoch 55 iter 60\n",
      "Epoch 55 iter 70\n",
      "Epoch 55 iter 80\n",
      "Epoch 55 iter 90\n",
      "Epoch 55 iter 100\n",
      "Train Boundary avergage error = 88.850\n",
      "Train From boundary avergage accuracy = 88.061\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 55, Probability Accuracy 41.99154824543233\n",
      "Starting Training\n",
      "Training:: Epoch 56, Iteration 0, Current loss 0.6332753807121763 Accuracy 74.66489240855539\n",
      "Training:: Epoch 56, Iteration 10, Current loss 0.5358106030584292 Accuracy 78.67332626321371\n",
      "Training:: Epoch 56, Iteration 20, Current loss 0.7098397621450128 Accuracy 75.74314094752779\n",
      "Training:: Epoch 56, Iteration 30, Current loss 0.594982293316678 Accuracy 73.83248047401024\n",
      "Training:: Epoch 56, Iteration 40, Current loss 0.6696110497414939 Accuracy 75.37435061627788\n",
      "Training:: Epoch 56, Iteration 50, Current loss 0.5952242017441874 Accuracy 75.98737118007982\n",
      "Training:: Epoch 56, Iteration 60, Current loss 0.7711290930244797 Accuracy 67.68864717878994\n",
      "Training:: Epoch 56, Iteration 70, Current loss 1.1278616932044723 Accuracy 57.923996185606754\n",
      "Training:: Epoch 56, Iteration 80, Current loss 0.8539348911066321 Accuracy 66.91906173475964\n",
      "Training:: Epoch 56, Iteration 90, Current loss 1.0121139070982006 Accuracy 63.48017124831005\n",
      "Training:: Epoch 56, Iteration 100, Current loss 0.906662083282782 Accuracy 71.44869687720121\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 56, Probability Accuracy 43.300534449185896\n",
      "Starting Training\n",
      "Training:: Epoch 57, Iteration 0, Current loss 0.36384444527413057 Accuracy 84.05232954773466\n",
      "Training:: Epoch 57, Iteration 10, Current loss 0.45768337809930765 Accuracy 80.53156891331165\n",
      "Training:: Epoch 57, Iteration 20, Current loss 0.6554496941371084 Accuracy 73.95917172751884\n",
      "Training:: Epoch 57, Iteration 30, Current loss 0.7228307832758589 Accuracy 77.50407166123779\n",
      "Training:: Epoch 57, Iteration 40, Current loss 1.058035601465934 Accuracy 64.06929347826087\n",
      "Training:: Epoch 57, Iteration 50, Current loss 0.7915618148664397 Accuracy 69.10446716146198\n",
      "Training:: Epoch 57, Iteration 60, Current loss 1.183261479331683 Accuracy 58.25965915503555\n",
      "Training:: Epoch 57, Iteration 70, Current loss 0.5657727351180803 Accuracy 80.11915155872836\n",
      "Training:: Epoch 57, Iteration 80, Current loss 0.6171023037920338 Accuracy 74.63870784924907\n",
      "Training:: Epoch 57, Iteration 90, Current loss 0.6434260017670059 Accuracy 76.60402202537706\n",
      "Training:: Epoch 57, Iteration 100, Current loss 0.4921195574285613 Accuracy 77.75574619618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 57, Probability Accuracy 42.526929610142105\n",
      "Starting Training\n",
      "Training:: Epoch 58, Iteration 0, Current loss 0.5035427637736076 Accuracy 76.49915464585057\n",
      "Training:: Epoch 58, Iteration 10, Current loss 0.5303166561646392 Accuracy 78.00705161551356\n",
      "Training:: Epoch 58, Iteration 20, Current loss 0.4646450940555518 Accuracy 79.0\n",
      "Training:: Epoch 58, Iteration 30, Current loss 0.5453710693090695 Accuracy 78.29837569602724\n",
      "Training:: Epoch 58, Iteration 40, Current loss 0.6154524851507321 Accuracy 74.49436017113963\n",
      "Training:: Epoch 58, Iteration 50, Current loss 0.5878837850514795 Accuracy 79.97468487948947\n",
      "Training:: Epoch 58, Iteration 60, Current loss 0.4902387269140978 Accuracy 84.39799823373565\n",
      "Training:: Epoch 58, Iteration 70, Current loss 0.44969043868282976 Accuracy 78.2397592833208\n",
      "Training:: Epoch 58, Iteration 80, Current loss 0.5575066151812721 Accuracy 74.24500160244558\n",
      "Training:: Epoch 58, Iteration 90, Current loss 0.6287712613469967 Accuracy 74.36553936188172\n",
      "Training:: Epoch 58, Iteration 100, Current loss 0.5748930711800393 Accuracy 78.04063800849029\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 58, Probability Accuracy 42.040125119111735\n",
      "Starting Training\n",
      "Training:: Epoch 59, Iteration 0, Current loss 0.4471189613185967 Accuracy 82.46915914354587\n",
      "Training:: Epoch 59, Iteration 10, Current loss 0.4775669583875698 Accuracy 77.07586618876941\n",
      "Training:: Epoch 59, Iteration 20, Current loss 0.8033725633964836 Accuracy 69.0642378844626\n",
      "Training:: Epoch 59, Iteration 30, Current loss 0.5586983169029898 Accuracy 78.77471374397113\n",
      "Training:: Epoch 59, Iteration 40, Current loss 0.5104061490701358 Accuracy 77.93422992448176\n",
      "Training:: Epoch 59, Iteration 50, Current loss 0.5577075445460404 Accuracy 74.89252721119546\n",
      "Training:: Epoch 59, Iteration 60, Current loss 0.5687206408944177 Accuracy 72.74465562736624\n",
      "Training:: Epoch 59, Iteration 70, Current loss 0.4608301030370258 Accuracy 79.19029258335223\n",
      "Training:: Epoch 59, Iteration 80, Current loss 0.5184507216256726 Accuracy 75.53982779606005\n",
      "Training:: Epoch 59, Iteration 90, Current loss 0.40348282620902076 Accuracy 79.08168713294181\n",
      "Training:: Epoch 59, Iteration 100, Current loss 0.5726827535887401 Accuracy 79.68524698224364\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 59, Probability Accuracy 42.83185565728964\n",
      "Starting Training\n",
      "Training:: Epoch 60, Iteration 0, Current loss 0.6035146137426075 Accuracy 72.42567858681603\n",
      "Training:: Epoch 60, Iteration 10, Current loss 0.497412427307319 Accuracy 77.33671528218136\n",
      "Training:: Epoch 60, Iteration 20, Current loss 0.39594058227271683 Accuracy 79.91414773743516\n",
      "Training:: Epoch 60, Iteration 30, Current loss 0.4209008815527868 Accuracy 79.35236187614755\n",
      "Training:: Epoch 60, Iteration 40, Current loss 0.6050502241260998 Accuracy 75.50864361702128\n",
      "Training:: Epoch 60, Iteration 50, Current loss 0.6746172540274318 Accuracy 74.99424361040755\n",
      "Training:: Epoch 60, Iteration 60, Current loss 0.4306561921049106 Accuracy 84.57048401260396\n",
      "Training:: Epoch 60, Iteration 70, Current loss 0.682010497168842 Accuracy 74.52520559058893\n",
      "Training:: Epoch 60, Iteration 80, Current loss 0.6444438823799852 Accuracy 73.30831833278401\n",
      "Training:: Epoch 60, Iteration 90, Current loss 0.6573327610496451 Accuracy 74.06437578052612\n",
      "Training:: Epoch 60, Iteration 100, Current loss 0.6399402642160287 Accuracy 71.86516853932584\n",
      "Calculating Expectation\n",
      "Epoch 60 iter 0\n",
      "Epoch 60 iter 10\n",
      "Epoch 60 iter 20\n",
      "Epoch 60 iter 30\n",
      "Epoch 60 iter 40\n",
      "Epoch 60 iter 50\n",
      "Epoch 60 iter 60\n",
      "Epoch 60 iter 70\n",
      "Epoch 60 iter 80\n",
      "Epoch 60 iter 90\n",
      "Epoch 60 iter 100\n",
      "Train Boundary avergage error = 90.653\n",
      "Train From boundary avergage accuracy = 87.951\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 60, Probability Accuracy 45.550917678253306\n",
      "Starting Training\n",
      "Training:: Epoch 61, Iteration 0, Current loss 0.37968539492094094 Accuracy 83.19570014949811\n",
      "Training:: Epoch 61, Iteration 10, Current loss 0.6013320075762509 Accuracy 77.31052667162464\n",
      "Training:: Epoch 61, Iteration 20, Current loss 0.3468195681548376 Accuracy 84.67541809027153\n",
      "Training:: Epoch 61, Iteration 30, Current loss 0.40611119039538696 Accuracy 79.52880285003697\n",
      "Training:: Epoch 61, Iteration 40, Current loss 0.5135821665597737 Accuracy 72.37304001872221\n",
      "Training:: Epoch 61, Iteration 50, Current loss 0.3991384917640294 Accuracy 81.80602947901643\n",
      "Training:: Epoch 61, Iteration 60, Current loss 0.8518329095313356 Accuracy 69.97513100630607\n",
      "Training:: Epoch 61, Iteration 70, Current loss 0.4833413191881314 Accuracy 82.14106215446473\n",
      "Training:: Epoch 61, Iteration 80, Current loss 0.497303380011071 Accuracy 78.77213695395514\n",
      "Training:: Epoch 61, Iteration 90, Current loss 0.41241727137834056 Accuracy 83.16376954828208\n",
      "Training:: Epoch 61, Iteration 100, Current loss 0.6349389283602572 Accuracy 72.69596921207211\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 61, Probability Accuracy 43.424203504992335\n",
      "Starting Training\n",
      "Training:: Epoch 62, Iteration 0, Current loss 0.3508262777657869 Accuracy 81.40424481737413\n",
      "Training:: Epoch 62, Iteration 10, Current loss 0.6661564721429493 Accuracy 76.34881165789626\n",
      "Training:: Epoch 62, Iteration 20, Current loss 0.42761437225956656 Accuracy 77.56472157079138\n",
      "Training:: Epoch 62, Iteration 30, Current loss 0.33505735673334924 Accuracy 85.46543629593654\n",
      "Training:: Epoch 62, Iteration 40, Current loss 0.4151141949107568 Accuracy 79.11683532658694\n",
      "Training:: Epoch 62, Iteration 50, Current loss 0.5957159431167661 Accuracy 77.9336262107659\n",
      "Training:: Epoch 62, Iteration 60, Current loss 0.4703208055298413 Accuracy 77.35402602825528\n",
      "Training:: Epoch 62, Iteration 70, Current loss 0.5145707636672648 Accuracy 77.42648865590958\n",
      "Training:: Epoch 62, Iteration 80, Current loss 0.5033951778336516 Accuracy 78.56369736984522\n",
      "Training:: Epoch 62, Iteration 90, Current loss 0.6512610291243685 Accuracy 73.11249187197457\n",
      "Training:: Epoch 62, Iteration 100, Current loss 0.6311641017033316 Accuracy 71.97382138306499\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 62, Probability Accuracy 45.01377553134192\n",
      "Starting Training\n",
      "Training:: Epoch 63, Iteration 0, Current loss 0.3978763429055391 Accuracy 79.92094529246037\n",
      "Training:: Epoch 63, Iteration 10, Current loss 0.637369603332397 Accuracy 74.34307724778388\n",
      "Training:: Epoch 63, Iteration 20, Current loss 0.4555080160976351 Accuracy 81.98020582784591\n",
      "Training:: Epoch 63, Iteration 30, Current loss 0.8442701181402021 Accuracy 68.47558243727599\n",
      "Training:: Epoch 63, Iteration 40, Current loss 1.1345459388765053 Accuracy 61.22478595269192\n",
      "Training:: Epoch 63, Iteration 50, Current loss 0.893645694302074 Accuracy 64.25442721629538\n",
      "Training:: Epoch 63, Iteration 60, Current loss 0.5607922707846051 Accuracy 74.16249245088636\n",
      "Training:: Epoch 63, Iteration 70, Current loss 0.5208779188374016 Accuracy 75.84907041428781\n",
      "Training:: Epoch 63, Iteration 80, Current loss 0.37734857688125006 Accuracy 81.76285233305995\n",
      "Training:: Epoch 63, Iteration 90, Current loss 0.6768376864492802 Accuracy 76.0807111231284\n",
      "Training:: Epoch 63, Iteration 100, Current loss 0.5257371760476822 Accuracy 71.88684747739866\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 63, Probability Accuracy 43.45641546173924\n",
      "Starting Training\n",
      "Training:: Epoch 64, Iteration 0, Current loss 0.5156454210903779 Accuracy 80.01410492496963\n",
      "Training:: Epoch 64, Iteration 10, Current loss 0.4879697537561833 Accuracy 80.38129189754717\n",
      "Training:: Epoch 64, Iteration 20, Current loss 0.26866782267355643 Accuracy 83.685535968466\n",
      "Training:: Epoch 64, Iteration 30, Current loss 0.44480965435460795 Accuracy 77.29835820430543\n",
      "Training:: Epoch 64, Iteration 40, Current loss 0.619321809263692 Accuracy 76.09263064363859\n",
      "Training:: Epoch 64, Iteration 50, Current loss 0.32470514399853306 Accuracy 86.51543793320731\n",
      "Training:: Epoch 64, Iteration 60, Current loss 0.5351431937861968 Accuracy 77.72995696126515\n",
      "Training:: Epoch 64, Iteration 70, Current loss 0.3784776113066722 Accuracy 83.03958710810959\n",
      "Training:: Epoch 64, Iteration 80, Current loss 0.3980150619700273 Accuracy 82.08648150118832\n",
      "Training:: Epoch 64, Iteration 90, Current loss 0.43695502825222987 Accuracy 80.95380564863571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 64, Iteration 100, Current loss 0.3515173999004345 Accuracy 77.4405064722175\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 64, Probability Accuracy 44.16352487881675\n",
      "Starting Training\n",
      "Training:: Epoch 65, Iteration 0, Current loss 0.31822084247959526 Accuracy 83.6670651524208\n",
      "Training:: Epoch 65, Iteration 10, Current loss 0.38412456069454926 Accuracy 83.42776203966005\n",
      "Training:: Epoch 65, Iteration 20, Current loss 0.3955176677198494 Accuracy 82.53643388969265\n",
      "Training:: Epoch 65, Iteration 30, Current loss 0.2467095790645749 Accuracy 87.55678068097993\n",
      "Training:: Epoch 65, Iteration 40, Current loss 0.2789398281399923 Accuracy 81.74326255275314\n",
      "Training:: Epoch 65, Iteration 50, Current loss 0.359071452080386 Accuracy 82.50088141967329\n",
      "Training:: Epoch 65, Iteration 60, Current loss 0.3702529328900645 Accuracy 79.24905838041431\n",
      "Training:: Epoch 65, Iteration 70, Current loss 0.3370985396771773 Accuracy 84.99045541314426\n",
      "Training:: Epoch 65, Iteration 80, Current loss 0.34098562226505724 Accuracy 84.23285327249394\n",
      "Training:: Epoch 65, Iteration 90, Current loss 0.6972888266456573 Accuracy 67.85877804344508\n",
      "Training:: Epoch 65, Iteration 100, Current loss 0.40103075056382587 Accuracy 83.04297328687572\n",
      "Calculating Expectation\n",
      "Epoch 65 iter 0\n",
      "Epoch 65 iter 10\n",
      "Epoch 65 iter 20\n",
      "Epoch 65 iter 30\n",
      "Epoch 65 iter 40\n",
      "Epoch 65 iter 50\n",
      "Epoch 65 iter 60\n",
      "Epoch 65 iter 70\n",
      "Epoch 65 iter 80\n",
      "Epoch 65 iter 90\n",
      "Epoch 65 iter 100\n",
      "Train Boundary avergage error = 90.775\n",
      "Train From boundary avergage accuracy = 87.911\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 65, Probability Accuracy 44.21997348469155\n",
      "Starting Training\n",
      "Training:: Epoch 66, Iteration 0, Current loss 0.6926866361797158 Accuracy 72.24056034135738\n",
      "Training:: Epoch 66, Iteration 10, Current loss 0.26918298463602525 Accuracy 83.34300825593395\n",
      "Training:: Epoch 66, Iteration 20, Current loss 0.2663756791740088 Accuracy 82.33807181104075\n",
      "Training:: Epoch 66, Iteration 30, Current loss 0.32359659783928785 Accuracy 82.46871074303262\n",
      "Training:: Epoch 66, Iteration 40, Current loss 0.2456188990556117 Accuracy 87.83088611044867\n",
      "Training:: Epoch 66, Iteration 50, Current loss 0.3509442587480843 Accuracy 79.5159652487871\n",
      "Training:: Epoch 66, Iteration 60, Current loss 0.20701407390544338 Accuracy 88.18441273326016\n",
      "Training:: Epoch 66, Iteration 70, Current loss 0.6851150124206122 Accuracy 71.56481667000601\n",
      "Training:: Epoch 66, Iteration 80, Current loss 0.2820556429343888 Accuracy 84.16583830351226\n",
      "Training:: Epoch 66, Iteration 90, Current loss 0.21183132466835616 Accuracy 87.64137931034483\n",
      "Training:: Epoch 66, Iteration 100, Current loss 0.3941394253768952 Accuracy 85.35795658951913\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 66, Probability Accuracy 43.3495256245598\n",
      "Starting Training\n",
      "Training:: Epoch 67, Iteration 0, Current loss 0.20064962718490623 Accuracy 86.18187747116129\n",
      "Training:: Epoch 67, Iteration 10, Current loss 0.5562610294776887 Accuracy 75.30710835058662\n",
      "Training:: Epoch 67, Iteration 20, Current loss 0.3434185496528687 Accuracy 83.85147180312187\n",
      "Training:: Epoch 67, Iteration 30, Current loss 0.3780242467421351 Accuracy 76.87212091217117\n",
      "Training:: Epoch 67, Iteration 40, Current loss 0.4396969585596025 Accuracy 84.38382312005409\n",
      "Training:: Epoch 67, Iteration 50, Current loss 0.3758614287751121 Accuracy 80.62670173981263\n",
      "Training:: Epoch 67, Iteration 60, Current loss 0.31787303133450595 Accuracy 84.29925106702815\n",
      "Training:: Epoch 67, Iteration 70, Current loss 0.2677076176115873 Accuracy 85.041216339989\n",
      "Training:: Epoch 67, Iteration 80, Current loss 0.32872321237810215 Accuracy 84.0284135753749\n",
      "Training:: Epoch 67, Iteration 90, Current loss 1.0546601903778197 Accuracy 61.20148401826484\n",
      "Training:: Epoch 67, Iteration 100, Current loss 0.6096699926152579 Accuracy 74.03753430754395\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 67, Probability Accuracy 43.3036417118946\n",
      "Starting Training\n",
      "Training:: Epoch 68, Iteration 0, Current loss 0.34806611520556774 Accuracy 80.46941436589375\n",
      "Training:: Epoch 68, Iteration 10, Current loss 0.5934464593025355 Accuracy 74.95961227786754\n",
      "Training:: Epoch 68, Iteration 20, Current loss 0.4678437447099859 Accuracy 78.25349724459517\n",
      "Training:: Epoch 68, Iteration 30, Current loss 0.45094909844155157 Accuracy 78.98323161699354\n",
      "Training:: Epoch 68, Iteration 40, Current loss 0.37660096844761887 Accuracy 83.04033092037228\n",
      "Training:: Epoch 68, Iteration 50, Current loss 0.494841174967248 Accuracy 76.31981460023175\n",
      "Training:: Epoch 68, Iteration 60, Current loss 0.478181131539856 Accuracy 79.9091877347385\n",
      "Training:: Epoch 68, Iteration 70, Current loss 0.3105886851091569 Accuracy 83.95680503030027\n",
      "Training:: Epoch 68, Iteration 80, Current loss 0.2878297782488864 Accuracy 80.32889412110956\n",
      "Training:: Epoch 68, Iteration 90, Current loss 0.5506769929242147 Accuracy 68.80050073972915\n",
      "Training:: Epoch 68, Iteration 100, Current loss 0.349074702982391 Accuracy 81.16866822011113\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 68, Probability Accuracy 42.19807764013755\n",
      "Starting Training\n",
      "Training:: Epoch 69, Iteration 0, Current loss 0.44860790595991484 Accuracy 78.45800588596747\n",
      "Training:: Epoch 69, Iteration 10, Current loss 0.31605828976237765 Accuracy 83.8993758854386\n",
      "Training:: Epoch 69, Iteration 20, Current loss 0.33575726360293123 Accuracy 82.94784420353827\n",
      "Training:: Epoch 69, Iteration 30, Current loss 0.38318032432712174 Accuracy 79.47361139932212\n",
      "Training:: Epoch 69, Iteration 40, Current loss 0.33699313711120527 Accuracy 83.94850832774084\n",
      "Training:: Epoch 69, Iteration 50, Current loss 0.3240483708458464 Accuracy 78.42602153709987\n",
      "Training:: Epoch 69, Iteration 60, Current loss 0.28819193633738843 Accuracy 81.85785246738777\n",
      "Training:: Epoch 69, Iteration 70, Current loss 0.27531386905405775 Accuracy 78.69327958310801\n",
      "Training:: Epoch 69, Iteration 80, Current loss 0.3163970685982416 Accuracy 80.77182040469188\n",
      "Training:: Epoch 69, Iteration 90, Current loss 0.18410185614760521 Accuracy 86.52288323615913\n",
      "Training:: Epoch 69, Iteration 100, Current loss 0.3624583919248498 Accuracy 82.21308375874456\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 69, Probability Accuracy 44.00795459253428\n",
      "Starting Training\n",
      "Training:: Epoch 70, Iteration 0, Current loss 0.27695983112960776 Accuracy 84.88032112592299\n",
      "Training:: Epoch 70, Iteration 10, Current loss 0.24800393438677207 Accuracy 82.66531027466938\n",
      "Training:: Epoch 70, Iteration 20, Current loss 0.34150607272087397 Accuracy 80.94383882986064\n",
      "Training:: Epoch 70, Iteration 30, Current loss 0.23506019193694705 Accuracy 83.90381035596246\n",
      "Training:: Epoch 70, Iteration 40, Current loss 0.47854386610894745 Accuracy 78.26112724471048\n",
      "Training:: Epoch 70, Iteration 50, Current loss 0.24064065940282323 Accuracy 86.20668278470026\n",
      "Training:: Epoch 70, Iteration 60, Current loss 0.25882648722370816 Accuracy 86.53821723480839\n",
      "Training:: Epoch 70, Iteration 70, Current loss 0.1647911676994224 Accuracy 86.72143390589993\n",
      "Training:: Epoch 70, Iteration 80, Current loss 0.355069385470666 Accuracy 80.79327862446269\n",
      "Training:: Epoch 70, Iteration 90, Current loss 0.2141862745678611 Accuracy 86.9171631111259\n",
      "Training:: Epoch 70, Iteration 100, Current loss 0.24225984733331596 Accuracy 84.40003141196796\n",
      "Calculating Expectation\n",
      "Epoch 70 iter 0\n",
      "Epoch 70 iter 10\n",
      "Epoch 70 iter 20\n",
      "Epoch 70 iter 30\n",
      "Epoch 70 iter 40\n",
      "Epoch 70 iter 50\n",
      "Epoch 70 iter 60\n",
      "Epoch 70 iter 70\n",
      "Epoch 70 iter 80\n",
      "Epoch 70 iter 90\n",
      "Epoch 70 iter 100\n",
      "Train Boundary avergage error = 92.183\n",
      "Train From boundary avergage accuracy = 87.881\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 70, Probability Accuracy 44.61190288768281\n",
      "Starting Training\n",
      "Training:: Epoch 71, Iteration 0, Current loss 0.24921725257467783 Accuracy 86.24472573839662\n",
      "Training:: Epoch 71, Iteration 10, Current loss 0.366098507126509 Accuracy 80.63340943313742\n",
      "Training:: Epoch 71, Iteration 20, Current loss 0.36983846649475766 Accuracy 81.04245344445891\n",
      "Training:: Epoch 71, Iteration 30, Current loss 0.3138177366843545 Accuracy 80.61601774748416\n",
      "Training:: Epoch 71, Iteration 40, Current loss 0.2490277447386527 Accuracy 85.31678887008836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 71, Iteration 50, Current loss 0.2352578394013487 Accuracy 86.4583989418656\n",
      "Training:: Epoch 71, Iteration 60, Current loss 0.2862039717660691 Accuracy 84.54848651158864\n",
      "Training:: Epoch 71, Iteration 70, Current loss 0.3528328407051055 Accuracy 77.63077132919594\n",
      "Training:: Epoch 71, Iteration 80, Current loss 0.27971384359843543 Accuracy 84.32117771577023\n",
      "Training:: Epoch 71, Iteration 90, Current loss 0.3158706241667717 Accuracy 82.55491329479769\n",
      "Training:: Epoch 71, Iteration 100, Current loss 0.37072460967170967 Accuracy 82.49471458773785\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 71, Probability Accuracy 42.26001574346439\n",
      "Starting Training\n",
      "Training:: Epoch 72, Iteration 0, Current loss 0.42907035624781054 Accuracy 79.01319563970166\n",
      "Training:: Epoch 72, Iteration 10, Current loss 0.31085213179199606 Accuracy 78.34084688455965\n",
      "Training:: Epoch 72, Iteration 20, Current loss 0.1483118019473762 Accuracy 89.67274772399733\n",
      "Training:: Epoch 72, Iteration 30, Current loss 0.207044190524328 Accuracy 81.40121469582822\n",
      "Training:: Epoch 72, Iteration 40, Current loss 0.442377575135346 Accuracy 79.26652142338416\n",
      "Training:: Epoch 72, Iteration 50, Current loss 0.938731883779983 Accuracy 72.44121715076072\n",
      "Training:: Epoch 72, Iteration 60, Current loss 0.2847183335912134 Accuracy 85.1909715084488\n",
      "Training:: Epoch 72, Iteration 70, Current loss 0.2797125959201775 Accuracy 85.42298768516866\n",
      "Training:: Epoch 72, Iteration 80, Current loss 0.1899909397687918 Accuracy 87.15698787492023\n",
      "Training:: Epoch 72, Iteration 90, Current loss 0.24517667345924252 Accuracy 83.63561878514767\n",
      "Training:: Epoch 72, Iteration 100, Current loss 0.32144782885908696 Accuracy 82.99678080187299\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 72, Probability Accuracy 43.37407299995857\n",
      "Starting Training\n",
      "Training:: Epoch 73, Iteration 0, Current loss 0.23951635542268848 Accuracy 83.73418849100302\n",
      "Training:: Epoch 73, Iteration 10, Current loss 0.1885323471930962 Accuracy 87.78706058037696\n",
      "Training:: Epoch 73, Iteration 20, Current loss 0.5658326722510709 Accuracy 75.73535273017139\n",
      "Training:: Epoch 73, Iteration 30, Current loss 0.32465936825863556 Accuracy 82.3955084217093\n",
      "Training:: Epoch 73, Iteration 40, Current loss 0.24454124195348403 Accuracy 86.83505549177191\n",
      "Training:: Epoch 73, Iteration 50, Current loss 0.32613193578759353 Accuracy 84.66577264287189\n",
      "Training:: Epoch 73, Iteration 60, Current loss 0.27499642951705455 Accuracy 82.86623108665749\n",
      "Training:: Epoch 73, Iteration 70, Current loss 0.1993002808468297 Accuracy 86.58084101641556\n",
      "Training:: Epoch 73, Iteration 80, Current loss 0.16226034030346437 Accuracy 83.45068919899288\n",
      "Training:: Epoch 73, Iteration 90, Current loss 0.19003348540897275 Accuracy 90.32670818049988\n",
      "Training:: Epoch 73, Iteration 100, Current loss 0.44469704009758215 Accuracy 83.042582072294\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 73, Probability Accuracy 44.856548038281474\n",
      "Starting Training\n",
      "Training:: Epoch 74, Iteration 0, Current loss 0.1924239585976534 Accuracy 86.67823070251518\n",
      "Training:: Epoch 74, Iteration 10, Current loss 0.36943868757938664 Accuracy 78.82958604748897\n",
      "Training:: Epoch 74, Iteration 20, Current loss 0.22040392801474662 Accuracy 83.22159188585181\n",
      "Training:: Epoch 74, Iteration 30, Current loss 0.43044970773339125 Accuracy 83.32435966464118\n",
      "Training:: Epoch 74, Iteration 40, Current loss 0.1934001328366265 Accuracy 88.37565884044082\n",
      "Training:: Epoch 74, Iteration 50, Current loss 0.16745670441499153 Accuracy 88.25700772868844\n",
      "Training:: Epoch 74, Iteration 60, Current loss 0.22172609664444848 Accuracy 88.59015812552597\n",
      "Training:: Epoch 74, Iteration 70, Current loss 0.14038208862087587 Accuracy 89.76740729731738\n",
      "Training:: Epoch 74, Iteration 80, Current loss 0.20839176481851865 Accuracy 83.7261022239563\n",
      "Training:: Epoch 74, Iteration 90, Current loss 0.33670995798140707 Accuracy 82.12051180880447\n",
      "Training:: Epoch 74, Iteration 100, Current loss 0.5910335446312897 Accuracy 76.7850767583006\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 74, Probability Accuracy 42.25017607822016\n",
      "Starting Training\n",
      "Training:: Epoch 75, Iteration 0, Current loss 0.24117278806082368 Accuracy 83.43676252075868\n",
      "Training:: Epoch 75, Iteration 10, Current loss 0.6583648706238301 Accuracy 71.5835279813138\n",
      "Training:: Epoch 75, Iteration 20, Current loss 0.46661462226965117 Accuracy 76.6562569707785\n",
      "Training:: Epoch 75, Iteration 30, Current loss 0.5175069768353384 Accuracy 75.36425449247207\n",
      "Training:: Epoch 75, Iteration 40, Current loss 0.28071153379737707 Accuracy 80.27315725142158\n",
      "Training:: Epoch 75, Iteration 50, Current loss 0.28682645396863626 Accuracy 82.40432237730752\n",
      "Training:: Epoch 75, Iteration 60, Current loss 0.2528201638965314 Accuracy 87.49405752317566\n",
      "Training:: Epoch 75, Iteration 70, Current loss 0.2858594579083922 Accuracy 86.79583333333333\n",
      "Training:: Epoch 75, Iteration 80, Current loss 0.2520280643185939 Accuracy 84.5642922483395\n",
      "Training:: Epoch 75, Iteration 90, Current loss 0.26194494851906874 Accuracy 81.10131507658967\n",
      "Training:: Epoch 75, Iteration 100, Current loss 0.2608522703046246 Accuracy 84.3787864665859\n",
      "Calculating Expectation\n",
      "Epoch 75 iter 0\n",
      "Epoch 75 iter 10\n",
      "Epoch 75 iter 20\n",
      "Epoch 75 iter 30\n",
      "Epoch 75 iter 40\n",
      "Epoch 75 iter 50\n",
      "Epoch 75 iter 60\n",
      "Epoch 75 iter 70\n",
      "Epoch 75 iter 80\n",
      "Epoch 75 iter 90\n",
      "Epoch 75 iter 100\n",
      "Train Boundary avergage error = 93.263\n",
      "Train From boundary avergage accuracy = 87.676\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 75, Probability Accuracy 45.050337655881016\n",
      "Starting Training\n",
      "Training:: Epoch 76, Iteration 0, Current loss 0.22525851055326354 Accuracy 85.41123778501628\n",
      "Training:: Epoch 76, Iteration 10, Current loss 0.2156865631709102 Accuracy 87.11536245246053\n",
      "Training:: Epoch 76, Iteration 20, Current loss 0.3029970390544746 Accuracy 80.40837096876788\n",
      "Training:: Epoch 76, Iteration 30, Current loss 0.3084676789503813 Accuracy 84.02939464493598\n",
      "Training:: Epoch 76, Iteration 40, Current loss 0.5973142464926386 Accuracy 74.21317666806546\n",
      "Training:: Epoch 76, Iteration 50, Current loss 0.17855674062637128 Accuracy 86.79583882455206\n",
      "Training:: Epoch 76, Iteration 60, Current loss 0.27766870667012833 Accuracy 86.11305419539656\n",
      "Training:: Epoch 76, Iteration 70, Current loss 0.21714988708610497 Accuracy 80.90102936589278\n",
      "Training:: Epoch 76, Iteration 80, Current loss 0.2251514374956247 Accuracy 83.25033821709219\n",
      "Training:: Epoch 76, Iteration 90, Current loss 0.1931938263876385 Accuracy 78.08773234200744\n",
      "Training:: Epoch 76, Iteration 100, Current loss 0.14681885055625998 Accuracy 87.77866104448208\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 76, Probability Accuracy 43.05195343248954\n",
      "Starting Training\n",
      "Training:: Epoch 77, Iteration 0, Current loss 0.4086968935220459 Accuracy 75.87476051225168\n",
      "Training:: Epoch 77, Iteration 10, Current loss 0.267365468540197 Accuracy 77.57092744168872\n",
      "Training:: Epoch 77, Iteration 20, Current loss 0.20995732941994297 Accuracy 85.73076331573282\n",
      "Training:: Epoch 77, Iteration 30, Current loss 0.160990092739541 Accuracy 87.5855589853711\n",
      "Training:: Epoch 77, Iteration 40, Current loss 0.2673507240781788 Accuracy 85.5437531613556\n",
      "Training:: Epoch 77, Iteration 50, Current loss 0.2215323602167251 Accuracy 87.46665635752116\n",
      "Training:: Epoch 77, Iteration 60, Current loss 0.19515282954449803 Accuracy 86.7889657060716\n",
      "Training:: Epoch 77, Iteration 70, Current loss 0.18732448509442562 Accuracy 88.97882352941177\n",
      "Training:: Epoch 77, Iteration 80, Current loss 0.36934356844169525 Accuracy 80.70028597335565\n",
      "Training:: Epoch 77, Iteration 90, Current loss 0.25387623799315673 Accuracy 84.90976531849633\n",
      "Training:: Epoch 77, Iteration 100, Current loss 0.3507328639875821 Accuracy 82.59665508253693\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 77, Probability Accuracy 45.31528358950988\n",
      "Starting Training\n",
      "Training:: Epoch 78, Iteration 0, Current loss 0.21627287265272443 Accuracy 85.77657367819498\n",
      "Training:: Epoch 78, Iteration 10, Current loss 0.9951861377320189 Accuracy 67.72421370690839\n",
      "Training:: Epoch 78, Iteration 20, Current loss 0.2014813292237947 Accuracy 87.30485295897374\n",
      "Training:: Epoch 78, Iteration 30, Current loss 0.18218062187024386 Accuracy 89.5065507530305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 78, Iteration 40, Current loss 0.35476069946016836 Accuracy 84.10124685610317\n",
      "Training:: Epoch 78, Iteration 50, Current loss 0.1662276416475642 Accuracy 88.06446023219546\n",
      "Training:: Epoch 78, Iteration 60, Current loss 0.1950916595684444 Accuracy 85.14796863301414\n",
      "Training:: Epoch 78, Iteration 70, Current loss 0.18394260043443428 Accuracy 89.66733714993038\n",
      "Training:: Epoch 78, Iteration 80, Current loss 0.30497398058523745 Accuracy 82.02429149797571\n",
      "Training:: Epoch 78, Iteration 90, Current loss 0.26335113450750824 Accuracy 86.38045540796963\n",
      "Training:: Epoch 78, Iteration 100, Current loss 0.35672054399512304 Accuracy 81.57624067883775\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 78, Probability Accuracy 41.53146621369682\n",
      "Starting Training\n",
      "Training:: Epoch 79, Iteration 0, Current loss 0.2916728833883058 Accuracy 82.54025412829878\n",
      "Training:: Epoch 79, Iteration 10, Current loss 0.46389578495891326 Accuracy 78.12749268811487\n",
      "Training:: Epoch 79, Iteration 20, Current loss 0.20243597400959207 Accuracy 88.87972130546387\n",
      "Training:: Epoch 79, Iteration 30, Current loss 0.16141324603751292 Accuracy 83.81517485166015\n",
      "Training:: Epoch 79, Iteration 40, Current loss 0.1526619415662237 Accuracy 83.08951899343808\n",
      "Training:: Epoch 79, Iteration 50, Current loss 0.08837323704892484 Accuracy 84.62873434982025\n",
      "Training:: Epoch 79, Iteration 60, Current loss 0.13552988976067135 Accuracy 87.16728893171143\n",
      "Training:: Epoch 79, Iteration 70, Current loss 0.15041306985270086 Accuracy 84.14534364772389\n",
      "Training:: Epoch 79, Iteration 80, Current loss 0.2195454025798452 Accuracy 86.69398521030568\n",
      "Training:: Epoch 79, Iteration 90, Current loss 0.19701621538127448 Accuracy 83.83388950685948\n",
      "Training:: Epoch 79, Iteration 100, Current loss 0.14183651896500613 Accuracy 86.09040552554195\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 79, Probability Accuracy 44.89093507892447\n",
      "Starting Training\n",
      "Training:: Epoch 80, Iteration 0, Current loss 0.20329709852355243 Accuracy 86.51611266176097\n",
      "Training:: Epoch 80, Iteration 10, Current loss 0.18572799461505904 Accuracy 86.4427166115383\n",
      "Training:: Epoch 80, Iteration 20, Current loss 0.1552022739244854 Accuracy 88.82340410423916\n",
      "Training:: Epoch 80, Iteration 30, Current loss 0.12992919973463724 Accuracy 86.50519031141869\n",
      "Training:: Epoch 80, Iteration 40, Current loss 0.1514487085716327 Accuracy 84.54549756305305\n",
      "Training:: Epoch 80, Iteration 50, Current loss 0.16809875042706804 Accuracy 86.25625713532976\n",
      "Training:: Epoch 80, Iteration 60, Current loss 0.1609388047487654 Accuracy 90.824016563147\n",
      "Training:: Epoch 80, Iteration 70, Current loss 0.3999857477171191 Accuracy 78.55801895226176\n",
      "Training:: Epoch 80, Iteration 80, Current loss 0.2534057174726667 Accuracy 82.50747892303508\n",
      "Training:: Epoch 80, Iteration 90, Current loss 0.20918242169302528 Accuracy 87.16903590918278\n",
      "Training:: Epoch 80, Iteration 100, Current loss 0.14549399283659395 Accuracy 85.36694320357371\n",
      "Calculating Expectation\n",
      "Epoch 80 iter 0\n",
      "Epoch 80 iter 10\n",
      "Epoch 80 iter 20\n",
      "Epoch 80 iter 30\n",
      "Epoch 80 iter 40\n",
      "Epoch 80 iter 50\n",
      "Epoch 80 iter 60\n",
      "Epoch 80 iter 70\n",
      "Epoch 80 iter 80\n",
      "Epoch 80 iter 90\n",
      "Epoch 80 iter 100\n",
      "Train Boundary avergage error = 93.535\n",
      "Train From boundary avergage accuracy = 87.518\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 80, Probability Accuracy 41.00644239134938\n",
      "Starting Training\n",
      "Training:: Epoch 81, Iteration 0, Current loss 0.222024741632791 Accuracy 85.99348534201954\n",
      "Training:: Epoch 81, Iteration 10, Current loss 0.21518473268304059 Accuracy 83.89647472270015\n",
      "Training:: Epoch 81, Iteration 20, Current loss 0.2333785801212834 Accuracy 87.05568032215596\n",
      "Training:: Epoch 81, Iteration 30, Current loss 0.20450334794294497 Accuracy 84.49992783951508\n",
      "Training:: Epoch 81, Iteration 40, Current loss 0.24702640523739164 Accuracy 83.22685435841906\n",
      "Training:: Epoch 81, Iteration 50, Current loss 0.18929010414370417 Accuracy 85.19555077143882\n",
      "Training:: Epoch 81, Iteration 60, Current loss 0.13891691845695386 Accuracy 90.28957201086956\n",
      "Training:: Epoch 81, Iteration 70, Current loss 0.2240783361450229 Accuracy 86.79452054794521\n",
      "Training:: Epoch 81, Iteration 80, Current loss 0.5751533396682614 Accuracy 73.1474996498109\n",
      "Training:: Epoch 81, Iteration 90, Current loss 0.1841499847388716 Accuracy 82.00070724136576\n",
      "Training:: Epoch 81, Iteration 100, Current loss 0.26763007132883165 Accuracy 82.96467619848612\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 81, Probability Accuracy 37.50921821270249\n",
      "Starting Training\n",
      "Training:: Epoch 82, Iteration 0, Current loss 0.7202593601762293 Accuracy 72.18588640275387\n",
      "Training:: Epoch 82, Iteration 10, Current loss 0.28442982960673496 Accuracy 81.14502010882423\n",
      "Training:: Epoch 82, Iteration 20, Current loss 0.24924516158441762 Accuracy 84.85992798805655\n",
      "Training:: Epoch 82, Iteration 30, Current loss 0.5777955663252319 Accuracy 66.95556122015465\n",
      "Training:: Epoch 82, Iteration 40, Current loss 0.21418754830676467 Accuracy 84.87458675806012\n",
      "Training:: Epoch 82, Iteration 50, Current loss 0.19697409076357586 Accuracy 87.76103818615752\n",
      "Training:: Epoch 82, Iteration 60, Current loss 0.2807572162972484 Accuracy 84.25320287583112\n",
      "Training:: Epoch 82, Iteration 70, Current loss 0.21645978835987134 Accuracy 77.8558809156862\n",
      "Training:: Epoch 82, Iteration 80, Current loss 0.18654252808103028 Accuracy 85.06986372261514\n",
      "Training:: Epoch 82, Iteration 90, Current loss 0.21243439602631226 Accuracy 86.45594965675058\n",
      "Training:: Epoch 82, Iteration 100, Current loss 0.2631939493442171 Accuracy 80.62237495227185\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 82, Probability Accuracy 44.39429092264987\n",
      "Starting Training\n",
      "Training:: Epoch 83, Iteration 0, Current loss 0.17121374747071982 Accuracy 86.24139756484912\n",
      "Training:: Epoch 83, Iteration 10, Current loss 0.3151517309328723 Accuracy 79.35028248587571\n",
      "Training:: Epoch 83, Iteration 20, Current loss 0.18227704108333534 Accuracy 83.15024681916151\n",
      "Training:: Epoch 83, Iteration 30, Current loss 0.21156309635917742 Accuracy 84.84593099815643\n",
      "Training:: Epoch 83, Iteration 40, Current loss 0.217418730751702 Accuracy 82.90895305490618\n",
      "Training:: Epoch 83, Iteration 50, Current loss 0.24068967577282313 Accuracy 83.74414700945313\n",
      "Training:: Epoch 83, Iteration 60, Current loss 0.30559050041312374 Accuracy 82.13852924293641\n",
      "Training:: Epoch 83, Iteration 70, Current loss 0.19238473135864848 Accuracy 87.93851376838929\n",
      "Training:: Epoch 83, Iteration 80, Current loss 0.166369678488528 Accuracy 86.58820261778287\n",
      "Training:: Epoch 83, Iteration 90, Current loss 0.1828267512407461 Accuracy 87.86158324050452\n",
      "Training:: Epoch 83, Iteration 100, Current loss 0.218770933007225 Accuracy 85.52643996350939\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 83, Probability Accuracy 43.79262128682106\n",
      "Starting Training\n",
      "Training:: Epoch 84, Iteration 0, Current loss 0.3557541088086789 Accuracy 79.85297039539043\n",
      "Training:: Epoch 84, Iteration 10, Current loss 0.20675578977451706 Accuracy 79.553076698746\n",
      "Training:: Epoch 84, Iteration 20, Current loss 0.2417345896565058 Accuracy 86.21403352136683\n",
      "Training:: Epoch 84, Iteration 30, Current loss 0.29513755921304363 Accuracy 84.82831114225648\n",
      "Training:: Epoch 84, Iteration 40, Current loss 0.13996281423421977 Accuracy 85.97711557458018\n",
      "Training:: Epoch 84, Iteration 50, Current loss 0.16823537181030723 Accuracy 85.82969143159846\n",
      "Training:: Epoch 84, Iteration 60, Current loss 0.1640135762589615 Accuracy 80.6667705250039\n",
      "Training:: Epoch 84, Iteration 70, Current loss 0.2920166585600392 Accuracy 85.39503017780436\n",
      "Training:: Epoch 84, Iteration 80, Current loss 0.16962600846406575 Accuracy 86.71635831225875\n",
      "Training:: Epoch 84, Iteration 90, Current loss 0.2769258106320869 Accuracy 84.17686379888517\n",
      "Training:: Epoch 84, Iteration 100, Current loss 0.29157427114861156 Accuracy 82.47675455356006\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 84, Probability Accuracy 44.83749016033475\n",
      "Starting Training\n",
      "Training:: Epoch 85, Iteration 0, Current loss 0.21930187973967855 Accuracy 81.96441628077017\n",
      "Training:: Epoch 85, Iteration 10, Current loss 0.2229551735473541 Accuracy 87.29293352904173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 85, Iteration 20, Current loss 0.17188909917632778 Accuracy 82.7431597528685\n",
      "Training:: Epoch 85, Iteration 30, Current loss 0.14223232653059725 Accuracy 87.12485383915812\n",
      "Training:: Epoch 85, Iteration 40, Current loss 0.21762826205064076 Accuracy 86.00719175713989\n",
      "Training:: Epoch 85, Iteration 50, Current loss 0.13971072751730837 Accuracy 87.91205393036057\n",
      "Training:: Epoch 85, Iteration 60, Current loss 0.23579425330821055 Accuracy 85.26766221290717\n",
      "Training:: Epoch 85, Iteration 70, Current loss 0.2251862062359879 Accuracy 85.32970602566185\n",
      "Training:: Epoch 85, Iteration 80, Current loss 0.25330538276412035 Accuracy 86.66745815030274\n",
      "Training:: Epoch 85, Iteration 90, Current loss 0.2775747725442822 Accuracy 84.4775260321281\n",
      "Training:: Epoch 85, Iteration 100, Current loss 0.3415957887856533 Accuracy 72.2694426756001\n",
      "Calculating Expectation\n",
      "Epoch 85 iter 0\n",
      "Epoch 85 iter 10\n",
      "Epoch 85 iter 20\n",
      "Epoch 85 iter 30\n",
      "Epoch 85 iter 40\n",
      "Epoch 85 iter 50\n",
      "Epoch 85 iter 60\n",
      "Epoch 85 iter 70\n",
      "Epoch 85 iter 80\n",
      "Epoch 85 iter 90\n",
      "Epoch 85 iter 100\n",
      "Train Boundary avergage error = 94.847\n",
      "Train From boundary avergage accuracy = 87.344\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 85, Probability Accuracy 45.54511745453039\n",
      "Starting Training\n",
      "Training:: Epoch 86, Iteration 0, Current loss 0.14405169702493964 Accuracy 88.26552183952906\n",
      "Training:: Epoch 86, Iteration 10, Current loss 0.25470365355700353 Accuracy 86.07646817729375\n",
      "Training:: Epoch 86, Iteration 20, Current loss 0.1908668666577611 Accuracy 88.61955200555651\n",
      "Training:: Epoch 86, Iteration 30, Current loss 0.6076979221916476 Accuracy 74.16267942583733\n",
      "Training:: Epoch 86, Iteration 40, Current loss 0.21716737034712477 Accuracy 83.59744231303864\n",
      "Training:: Epoch 86, Iteration 50, Current loss 0.20921561532543492 Accuracy 81.3537150545672\n",
      "Training:: Epoch 86, Iteration 60, Current loss 0.38694781092428854 Accuracy 86.12282878411911\n",
      "Training:: Epoch 86, Iteration 70, Current loss 0.11407792641621545 Accuracy 89.8395121768733\n",
      "Training:: Epoch 86, Iteration 80, Current loss 0.2514617404926757 Accuracy 83.90635533492676\n",
      "Training:: Epoch 86, Iteration 90, Current loss 0.1940285409585669 Accuracy 83.89090909090909\n",
      "Training:: Epoch 86, Iteration 100, Current loss 0.3077721728492297 Accuracy 83.40494092373791\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 86, Probability Accuracy 46.32120810374114\n",
      "Starting Training\n",
      "Training:: Epoch 87, Iteration 0, Current loss 0.16734414856706692 Accuracy 84.98155751966038\n",
      "Training:: Epoch 87, Iteration 10, Current loss 0.27571003664060856 Accuracy 80.64217480957791\n",
      "Training:: Epoch 87, Iteration 20, Current loss 0.16506925754253415 Accuracy 83.16215510488293\n",
      "Training:: Epoch 87, Iteration 30, Current loss 0.28314053282309226 Accuracy 89.80093286689988\n",
      "Training:: Epoch 87, Iteration 40, Current loss 0.1942372766562705 Accuracy 82.24042844846824\n",
      "Training:: Epoch 87, Iteration 50, Current loss 0.13507523442252267 Accuracy 86.04436837601212\n",
      "Training:: Epoch 87, Iteration 60, Current loss 0.21594966635818386 Accuracy 87.01647722266279\n",
      "Training:: Epoch 87, Iteration 70, Current loss 0.16595363593086807 Accuracy 88.5885441703953\n",
      "Training:: Epoch 87, Iteration 80, Current loss 0.17756993420876047 Accuracy 85.34263959390863\n",
      "Training:: Epoch 87, Iteration 90, Current loss 0.23170496101126828 Accuracy 85.48876457760501\n",
      "Training:: Epoch 87, Iteration 100, Current loss 0.2558876479356726 Accuracy 78.75375110326567\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 87, Probability Accuracy 42.86613912250901\n",
      "Starting Training\n",
      "Training:: Epoch 88, Iteration 0, Current loss 0.15515621438588642 Accuracy 84.85179322015394\n",
      "Training:: Epoch 88, Iteration 10, Current loss 0.32216415489599226 Accuracy 82.55664026524222\n",
      "Training:: Epoch 88, Iteration 20, Current loss 0.5041167323200738 Accuracy 76.29839471199244\n",
      "Training:: Epoch 88, Iteration 30, Current loss 0.15223294487315284 Accuracy 90.74445995166286\n",
      "Training:: Epoch 88, Iteration 40, Current loss 0.20607099595664596 Accuracy 83.89061991350044\n",
      "Training:: Epoch 88, Iteration 50, Current loss 0.215786494679418 Accuracy 85.67122763371925\n",
      "Training:: Epoch 88, Iteration 60, Current loss 0.09885307781484902 Accuracy 88.34371233427079\n",
      "Training:: Epoch 88, Iteration 70, Current loss 0.18993581425812586 Accuracy 85.43111183620198\n",
      "Training:: Epoch 88, Iteration 80, Current loss 0.1752282693254797 Accuracy 85.47067475638904\n",
      "Training:: Epoch 88, Iteration 90, Current loss 0.22473233466964135 Accuracy 83.07201201765092\n",
      "Training:: Epoch 88, Iteration 100, Current loss 0.237149542674468 Accuracy 84.6735201614796\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 88, Probability Accuracy 45.787587521232965\n",
      "Starting Training\n",
      "Training:: Epoch 89, Iteration 0, Current loss 0.0856421181822872 Accuracy 88.60028860028861\n",
      "Training:: Epoch 89, Iteration 10, Current loss 0.13138628870046218 Accuracy 88.83553421368548\n",
      "Training:: Epoch 89, Iteration 20, Current loss 0.4714202803773912 Accuracy 78.59089279311581\n",
      "Training:: Epoch 89, Iteration 30, Current loss 0.127577917346381 Accuracy 87.65885459646806\n",
      "Training:: Epoch 89, Iteration 40, Current loss 0.11167782722503677 Accuracy 89.8685836234977\n",
      "Training:: Epoch 89, Iteration 50, Current loss 0.17998003797533063 Accuracy 82.91792006875806\n",
      "Training:: Epoch 89, Iteration 60, Current loss 0.20422632651391198 Accuracy 87.63062083313632\n",
      "Training:: Epoch 89, Iteration 70, Current loss 0.16726022519979505 Accuracy 88.22476000352289\n",
      "Training:: Epoch 89, Iteration 80, Current loss 0.30689304822879776 Accuracy 80.19334532374101\n",
      "Training:: Epoch 89, Iteration 90, Current loss 0.18283182420073785 Accuracy 83.7988541027215\n",
      "Training:: Epoch 89, Iteration 100, Current loss 0.15591104659898322 Accuracy 88.57251950035455\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 89, Probability Accuracy 45.27675353192195\n",
      "Starting Training\n",
      "Training:: Epoch 90, Iteration 0, Current loss 0.17927511611569102 Accuracy 85.52185142046893\n",
      "Training:: Epoch 90, Iteration 10, Current loss 0.14375797130088044 Accuracy 82.05651617034903\n",
      "Training:: Epoch 90, Iteration 20, Current loss 0.4088429474699466 Accuracy 81.35524033064598\n",
      "Training:: Epoch 90, Iteration 30, Current loss 0.17197313826484195 Accuracy 85.17316780701135\n",
      "Training:: Epoch 90, Iteration 40, Current loss 0.20246339885058987 Accuracy 83.94166919838548\n",
      "Training:: Epoch 90, Iteration 50, Current loss 0.2343864821512974 Accuracy 83.5276034328167\n",
      "Training:: Epoch 90, Iteration 60, Current loss 0.1608056130495862 Accuracy 88.2103825136612\n",
      "Training:: Epoch 90, Iteration 70, Current loss 0.17581228946865568 Accuracy 86.10326516777765\n",
      "Training:: Epoch 90, Iteration 80, Current loss 0.15721912533118254 Accuracy 88.85375494071147\n",
      "Training:: Epoch 90, Iteration 90, Current loss 0.18043790315921415 Accuracy 85.05493736440619\n",
      "Training:: Epoch 90, Iteration 100, Current loss 0.18549903296636014 Accuracy 85.19589476256415\n",
      "Calculating Expectation\n",
      "Epoch 90 iter 0\n",
      "Epoch 90 iter 10\n",
      "Epoch 90 iter 20\n",
      "Epoch 90 iter 30\n",
      "Epoch 90 iter 40\n",
      "Epoch 90 iter 50\n",
      "Epoch 90 iter 60\n",
      "Epoch 90 iter 70\n",
      "Epoch 90 iter 80\n",
      "Epoch 90 iter 90\n",
      "Epoch 90 iter 100\n",
      "Train Boundary avergage error = 96.068\n",
      "Train From boundary avergage accuracy = 87.339\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 90, Probability Accuracy 43.77718854870116\n",
      "Starting Training\n",
      "Training:: Epoch 91, Iteration 0, Current loss 0.14462572425885467 Accuracy 78.7686627066945\n",
      "Training:: Epoch 91, Iteration 10, Current loss 0.2760733842015579 Accuracy 90.27584732171496\n",
      "Training:: Epoch 91, Iteration 20, Current loss 0.2330585316791345 Accuracy 85.62734510844696\n",
      "Training:: Epoch 91, Iteration 30, Current loss 0.14872834329273257 Accuracy 84.70859497645212\n",
      "Training:: Epoch 91, Iteration 40, Current loss 0.1492608342046106 Accuracy 88.29689379466497\n",
      "Training:: Epoch 91, Iteration 50, Current loss 0.09629635314034975 Accuracy 84.52818387929013\n",
      "Validation:: Epoch 92, Probability Accuracy 44.613870820731655\n",
      "Starting Training\n",
      "Training:: Epoch 93, Iteration 0, Current loss 0.11375478589076313 Accuracy 85.4555266974736\n",
      "Training:: Epoch 93, Iteration 10, Current loss 0.16163521737502154 Accuracy 84.73555515305199\n",
      "Training:: Epoch 93, Iteration 20, Current loss 0.27887085247433263 Accuracy 81.9283170769469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 93, Iteration 30, Current loss 0.18629063154046413 Accuracy 87.3236834000616\n",
      "Training:: Epoch 93, Iteration 40, Current loss 0.15898990681927735 Accuracy 85.32543398102499\n",
      "Training:: Epoch 93, Iteration 50, Current loss 0.1779974341797212 Accuracy 84.81060199592231\n",
      "Training:: Epoch 93, Iteration 60, Current loss 0.15081384672610731 Accuracy 88.8247477195611\n",
      "Training:: Epoch 93, Iteration 70, Current loss 0.16015822368926333 Accuracy 85.301736208345\n",
      "Training:: Epoch 93, Iteration 80, Current loss 0.10074161499852319 Accuracy 84.8199384341505\n",
      "Training:: Epoch 93, Iteration 90, Current loss 0.2524214939324082 Accuracy 83.32345469940728\n",
      "Training:: Epoch 93, Iteration 100, Current loss 0.12169355649141353 Accuracy 86.60691744508962\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 93, Probability Accuracy 44.097236607697724\n",
      "Starting Training\n",
      "Training:: Epoch 94, Iteration 0, Current loss 0.16053017363008049 Accuracy 89.22097487486027\n",
      "Training:: Epoch 94, Iteration 10, Current loss 0.15633034725611095 Accuracy 81.3575688724794\n",
      "Training:: Epoch 94, Iteration 20, Current loss 0.19331841596663563 Accuracy 86.39192035228795\n",
      "Training:: Epoch 94, Iteration 30, Current loss 0.21933947102291437 Accuracy 82.25207873349932\n",
      "Training:: Epoch 94, Iteration 40, Current loss 0.1386983589618522 Accuracy 84.91033678378773\n",
      "Training:: Epoch 94, Iteration 50, Current loss 0.14086830442113968 Accuracy 81.63244032106745\n",
      "Training:: Epoch 94, Iteration 60, Current loss 0.10094005035670822 Accuracy 80.56533911315813\n",
      "Training:: Epoch 94, Iteration 70, Current loss 0.21034733499998626 Accuracy 80.04783544606553\n",
      "Training:: Epoch 94, Iteration 80, Current loss 0.16482640746252003 Accuracy 85.34934791377223\n",
      "Training:: Epoch 94, Iteration 90, Current loss 0.16643172135875953 Accuracy 87.4009269670625\n",
      "Training:: Epoch 94, Iteration 100, Current loss 0.26791657075113423 Accuracy 86.90032463628712\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 94, Probability Accuracy 45.58934416041762\n",
      "Starting Training\n",
      "Training:: Epoch 95, Iteration 0, Current loss 0.09264042252790487 Accuracy 85.9642094017094\n",
      "Training:: Epoch 95, Iteration 10, Current loss 0.13439660110203216 Accuracy 76.71178659067522\n",
      "Training:: Epoch 95, Iteration 20, Current loss 0.18049001196388936 Accuracy 82.81602150962793\n",
      "Training:: Epoch 95, Iteration 30, Current loss 0.41916387198527844 Accuracy 76.78873036978474\n",
      "Training:: Epoch 95, Iteration 40, Current loss 0.21070970855035925 Accuracy 83.38691159586682\n",
      "Training:: Epoch 95, Iteration 50, Current loss 0.2070231824223578 Accuracy 88.90900012482837\n",
      "Training:: Epoch 95, Iteration 60, Current loss 0.11343821603887977 Accuracy 88.03191489361703\n",
      "Training:: Epoch 95, Iteration 70, Current loss 0.20907850025631725 Accuracy 78.53077196727143\n",
      "Training:: Epoch 95, Iteration 80, Current loss 0.23596046591847628 Accuracy 83.30148669431887\n",
      "Training:: Epoch 95, Iteration 90, Current loss 0.13768489682032412 Accuracy 83.88757251409429\n",
      "Training:: Epoch 95, Iteration 100, Current loss 0.17164558586250567 Accuracy 80.34534370094549\n",
      "Calculating Expectation\n",
      "Epoch 95 iter 0\n",
      "Epoch 95 iter 10\n",
      "Epoch 95 iter 20\n",
      "Epoch 95 iter 30\n",
      "Epoch 95 iter 40\n",
      "Epoch 95 iter 50\n",
      "Epoch 95 iter 60\n",
      "Epoch 95 iter 70\n",
      "Epoch 95 iter 80\n",
      "Epoch 95 iter 90\n",
      "Epoch 95 iter 100\n",
      "Train Boundary avergage error = 97.508\n",
      "Train From boundary avergage accuracy = 87.244\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 95, Probability Accuracy 44.52666031404068\n",
      "Starting Training\n",
      "Training:: Epoch 96, Iteration 0, Current loss 0.15509238698850586 Accuracy 81.46826792623486\n",
      "Training:: Epoch 96, Iteration 10, Current loss 0.3399751302924843 Accuracy 84.64276435437776\n",
      "Training:: Epoch 96, Iteration 20, Current loss 0.20928569205324787 Accuracy 82.21962030609684\n",
      "Training:: Epoch 96, Iteration 30, Current loss 0.09452092175524733 Accuracy 92.00113286939634\n",
      "Training:: Epoch 96, Iteration 40, Current loss 0.3952342399976177 Accuracy 81.74831892411143\n",
      "Training:: Epoch 96, Iteration 50, Current loss 0.45592896990261794 Accuracy 74.69248291571753\n",
      "Training:: Epoch 96, Iteration 60, Current loss 0.178422845040893 Accuracy 83.34267040149393\n",
      "Training:: Epoch 96, Iteration 70, Current loss 0.14660245596090132 Accuracy 88.0483636133443\n",
      "Training:: Epoch 96, Iteration 80, Current loss 0.33741842399060645 Accuracy 80.4974907265983\n",
      "Training:: Epoch 96, Iteration 90, Current loss 0.18354731599182583 Accuracy 89.56630836526398\n",
      "Training:: Epoch 96, Iteration 100, Current loss 0.16683313938926783 Accuracy 87.75559081519624\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 96, Probability Accuracy 42.10113104362597\n",
      "Starting Training\n",
      "Training:: Epoch 97, Iteration 0, Current loss 0.20004487130377063 Accuracy 86.02154674874951\n",
      "Training:: Epoch 97, Iteration 10, Current loss 0.22806342192904505 Accuracy 81.38725875733559\n",
      "Training:: Epoch 97, Iteration 20, Current loss 0.16998187019850328 Accuracy 83.72594504525108\n",
      "Training:: Epoch 97, Iteration 30, Current loss 0.15220348873258693 Accuracy 87.06807430844898\n",
      "Training:: Epoch 97, Iteration 40, Current loss 0.24224231123544024 Accuracy 87.39485554065585\n",
      "Training:: Epoch 97, Iteration 50, Current loss 0.2713839504276067 Accuracy 87.4360224226176\n",
      "Training:: Epoch 97, Iteration 60, Current loss 0.32011808290427557 Accuracy 84.95457197632741\n",
      "Training:: Epoch 97, Iteration 70, Current loss 0.22647678433664178 Accuracy 83.12234899549534\n",
      "Training:: Epoch 97, Iteration 80, Current loss 0.18901460830328623 Accuracy 84.91811497326204\n",
      "Training:: Epoch 97, Iteration 90, Current loss 0.1157442660116108 Accuracy 89.34461165207698\n",
      "Training:: Epoch 97, Iteration 100, Current loss 0.21216079301869148 Accuracy 85.74577139928242\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 97, Probability Accuracy 44.93578323735344\n",
      "Starting Training\n",
      "Training:: Epoch 98, Iteration 0, Current loss 0.28617991976950646 Accuracy 84.83708079268293\n",
      "Training:: Epoch 98, Iteration 10, Current loss 0.16660160963517506 Accuracy 84.35201514043615\n",
      "Training:: Epoch 98, Iteration 20, Current loss 0.1354833206897701 Accuracy 85.86628491836261\n",
      "Training:: Epoch 98, Iteration 30, Current loss 0.2255951348538441 Accuracy 79.32005289425415\n",
      "Training:: Epoch 98, Iteration 40, Current loss 0.17180437612598268 Accuracy 81.90639575403421\n",
      "Training:: Epoch 98, Iteration 50, Current loss 0.11597324660156501 Accuracy 82.54335260115607\n",
      "Training:: Epoch 98, Iteration 60, Current loss 0.09818363577006142 Accuracy 90.29452544010756\n",
      "Training:: Epoch 98, Iteration 70, Current loss 0.2608144805107396 Accuracy 79.97905949833842\n",
      "Training:: Epoch 98, Iteration 80, Current loss 0.21258213877520568 Accuracy 80.2491103202847\n",
      "Training:: Epoch 98, Iteration 90, Current loss 0.16320126672115254 Accuracy 86.84467477842873\n",
      "Training:: Epoch 98, Iteration 100, Current loss 0.1567789443554518 Accuracy 81.02282065870156\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 98, Probability Accuracy 44.00443302813108\n",
      "Starting Training\n",
      "Training:: Epoch 99, Iteration 0, Current loss 0.22090729779627952 Accuracy 82.32223838452983\n",
      "Training:: Epoch 99, Iteration 10, Current loss 0.15392644872518252 Accuracy 87.63523956723338\n",
      "Training:: Epoch 99, Iteration 20, Current loss 0.3641980547930748 Accuracy 83.7173947831653\n",
      "Training:: Epoch 99, Iteration 30, Current loss 0.17935241330019586 Accuracy 80.25142622894818\n",
      "Training:: Epoch 99, Iteration 40, Current loss 0.12276483246340897 Accuracy 88.24977106227107\n",
      "Training:: Epoch 99, Iteration 50, Current loss 0.12181262940141903 Accuracy 85.14110474243364\n",
      "Training:: Epoch 99, Iteration 60, Current loss 0.13343580134934102 Accuracy 87.10172744721689\n",
      "Training:: Epoch 99, Iteration 70, Current loss 0.12303435571365276 Accuracy 89.0708400269472\n",
      "Training:: Epoch 99, Iteration 80, Current loss 0.12637140336870017 Accuracy 88.27031854718666\n",
      "Training:: Epoch 99, Iteration 90, Current loss 0.1692911623649345 Accuracy 87.12761805645896\n",
      "Training:: Epoch 99, Iteration 100, Current loss 0.24360575822367364 Accuracy 86.14665952994989\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 99, Probability Accuracy 45.440920578365166\n",
      "Starting Training\n",
      "Training:: Epoch 100, Iteration 0, Current loss 0.1918030133867266 Accuracy 80.39287756268888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 100, Iteration 10, Current loss 0.13868104125652578 Accuracy 83.74646340948439\n",
      "Training:: Epoch 100, Iteration 20, Current loss 0.20098186645396687 Accuracy 80.74216250799743\n",
      "Training:: Epoch 100, Iteration 30, Current loss 0.14379040170470636 Accuracy 86.18456268065518\n",
      "Training:: Epoch 100, Iteration 40, Current loss 0.14178703673165516 Accuracy 86.8533762643905\n",
      "Training:: Epoch 100, Iteration 50, Current loss 0.1589761880114555 Accuracy 78.31014690144956\n",
      "Training:: Epoch 100, Iteration 60, Current loss 0.12464772006390748 Accuracy 84.88366040289418\n",
      "Training:: Epoch 100, Iteration 70, Current loss 0.1657141242832013 Accuracy 82.61122289291303\n",
      "Training:: Epoch 100, Iteration 80, Current loss 0.14061426387101894 Accuracy 83.46138495310318\n",
      "Training:: Epoch 100, Iteration 90, Current loss 0.1459294625350513 Accuracy 83.44549125168237\n",
      "Training:: Epoch 100, Iteration 100, Current loss 0.10442325737075812 Accuracy 85.17662966927715\n",
      "Calculating Expectation\n",
      "Epoch 100 iter 0\n",
      "Epoch 100 iter 10\n",
      "Epoch 100 iter 20\n",
      "Epoch 100 iter 30\n",
      "Epoch 100 iter 40\n",
      "Epoch 100 iter 50\n",
      "Epoch 100 iter 60\n",
      "Epoch 100 iter 70\n",
      "Epoch 100 iter 80\n",
      "Epoch 100 iter 90\n",
      "Epoch 100 iter 100\n",
      "Train Boundary avergage error = 98.151\n",
      "Train From boundary avergage accuracy = 87.181\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 100, Probability Accuracy 45.07581721009239\n",
      "Starting Training\n",
      "Training:: Epoch 101, Iteration 0, Current loss 0.06289812610781481 Accuracy 91.1687421455501\n",
      "Training:: Epoch 101, Iteration 10, Current loss 0.08615696992121968 Accuracy 91.38864016817845\n",
      "Training:: Epoch 101, Iteration 20, Current loss 0.12721657146755486 Accuracy 86.00097677929227\n",
      "Training:: Epoch 101, Iteration 30, Current loss 0.15991791712535966 Accuracy 80.54480427220616\n",
      "Training:: Epoch 101, Iteration 40, Current loss 0.1482460854361594 Accuracy 87.10234278668311\n",
      "Training:: Epoch 101, Iteration 50, Current loss 0.10936039683025343 Accuracy 84.68751966769463\n",
      "Training:: Epoch 101, Iteration 60, Current loss 0.296516900225777 Accuracy 81.98970840480274\n",
      "Training:: Epoch 101, Iteration 70, Current loss 0.11555040674774321 Accuracy 87.40107628996518\n",
      "Training:: Epoch 101, Iteration 80, Current loss 0.14991739822683547 Accuracy 78.14444736611448\n",
      "Training:: Epoch 101, Iteration 90, Current loss 0.11546757071375936 Accuracy 88.07513178770664\n",
      "Training:: Epoch 101, Iteration 100, Current loss 0.16692463352085607 Accuracy 83.5064414032898\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 101, Probability Accuracy 44.026183867092016\n",
      "Starting Training\n",
      "Training:: Epoch 102, Iteration 0, Current loss 0.1377902888551824 Accuracy 84.26751094434022\n",
      "Training:: Epoch 102, Iteration 10, Current loss 0.17503557157672273 Accuracy 88.82350633556942\n",
      "Training:: Epoch 102, Iteration 20, Current loss 0.2697615287386714 Accuracy 84.6319807484856\n",
      "Training:: Epoch 102, Iteration 30, Current loss 0.2547238961627126 Accuracy 81.84114383654324\n",
      "Training:: Epoch 102, Iteration 40, Current loss 0.1665073627991605 Accuracy 82.85300396462335\n",
      "Training:: Epoch 102, Iteration 50, Current loss 0.18434412589165097 Accuracy 87.21350136340187\n",
      "Training:: Epoch 102, Iteration 60, Current loss 0.17838877228543856 Accuracy 86.66941297631308\n",
      "Training:: Epoch 102, Iteration 70, Current loss 0.16853784109996772 Accuracy 80.9016973125884\n",
      "Training:: Epoch 102, Iteration 80, Current loss 0.11227278814259956 Accuracy 86.12143742255266\n",
      "Training:: Epoch 102, Iteration 90, Current loss 0.10597036241190354 Accuracy 87.2434576943618\n",
      "Training:: Epoch 102, Iteration 100, Current loss 0.09380459844188951 Accuracy 86.95552046321885\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 102, Probability Accuracy 45.64382483324357\n",
      "Starting Training\n",
      "Training:: Epoch 103, Iteration 0, Current loss 0.21405822652888432 Accuracy 83.07592649415294\n",
      "Training:: Epoch 103, Iteration 10, Current loss 0.1313953757466313 Accuracy 81.03434310707921\n",
      "Training:: Epoch 103, Iteration 20, Current loss 0.12839495148630165 Accuracy 84.93707893733317\n",
      "Training:: Epoch 103, Iteration 30, Current loss 0.11489988283385578 Accuracy 79.51426468678298\n",
      "Training:: Epoch 103, Iteration 40, Current loss 0.10787404354821413 Accuracy 86.13096106168878\n",
      "Training:: Epoch 103, Iteration 50, Current loss 0.2334239754581815 Accuracy 82.03566121842496\n",
      "Training:: Epoch 103, Iteration 60, Current loss 0.15565776968062398 Accuracy 86.47689702443351\n",
      "Training:: Epoch 103, Iteration 70, Current loss 0.1841183841625134 Accuracy 84.03341193397546\n",
      "Training:: Epoch 103, Iteration 80, Current loss 0.1656631058155294 Accuracy 83.65604381387728\n",
      "Training:: Epoch 103, Iteration 90, Current loss 0.1476119274189674 Accuracy 85.57238037552999\n",
      "Training:: Epoch 103, Iteration 100, Current loss 0.14293870120769664 Accuracy 88.22404978884197\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 103, Probability Accuracy 43.03693499606413\n",
      "Starting Training\n",
      "Training:: Epoch 104, Iteration 0, Current loss 0.3451283408745406 Accuracy 80.52695496066318\n",
      "Training:: Epoch 104, Iteration 10, Current loss 0.23348578179609378 Accuracy 83.03578663065497\n",
      "Training:: Epoch 104, Iteration 20, Current loss 0.22028423821270254 Accuracy 80.59135708870356\n",
      "Training:: Epoch 104, Iteration 30, Current loss 0.27279440985925524 Accuracy 76.2306826322792\n",
      "Training:: Epoch 104, Iteration 40, Current loss 0.20857831647362396 Accuracy 88.14696849050165\n",
      "Training:: Epoch 104, Iteration 50, Current loss 0.14224170748198817 Accuracy 87.78673279603224\n",
      "Training:: Epoch 104, Iteration 60, Current loss 0.11719973992379579 Accuracy 84.74780359452348\n",
      "Training:: Epoch 104, Iteration 70, Current loss 0.11713496025614992 Accuracy 89.47467307981361\n",
      "Training:: Epoch 104, Iteration 80, Current loss 0.14228386568003112 Accuracy 86.52403001756346\n",
      "Training:: Epoch 104, Iteration 90, Current loss 0.12348635330461193 Accuracy 83.58881245853556\n",
      "Training:: Epoch 104, Iteration 100, Current loss 0.13199355094718082 Accuracy 80.60720573710654\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 104, Probability Accuracy 44.554314952148154\n",
      "Starting Training\n",
      "Training:: Epoch 105, Iteration 0, Current loss 0.20227273532310522 Accuracy 85.71820928316397\n",
      "Training:: Epoch 105, Iteration 10, Current loss 0.1316884933399096 Accuracy 86.4431276195982\n",
      "Training:: Epoch 105, Iteration 20, Current loss 0.08937157824971577 Accuracy 88.2299624498252\n",
      "Training:: Epoch 105, Iteration 30, Current loss 0.19471863712932647 Accuracy 86.20064098687791\n",
      "Training:: Epoch 105, Iteration 40, Current loss 0.18424361416456952 Accuracy 81.02427632650844\n",
      "Training:: Epoch 105, Iteration 50, Current loss 0.17134264237428593 Accuracy 87.8832028167304\n",
      "Training:: Epoch 105, Iteration 60, Current loss 0.09731895299092477 Accuracy 87.05804902606204\n",
      "Training:: Epoch 105, Iteration 70, Current loss 0.11524312641309321 Accuracy 90.15824739818467\n",
      "Training:: Epoch 105, Iteration 80, Current loss 0.12500013389170256 Accuracy 86.05205393540295\n",
      "Training:: Epoch 105, Iteration 90, Current loss 0.0990120017002256 Accuracy 89.31621145948765\n",
      "Training:: Epoch 105, Iteration 100, Current loss 0.14749583957125925 Accuracy 88.59053390450319\n",
      "Calculating Expectation\n",
      "Epoch 105 iter 0\n",
      "Epoch 105 iter 10\n",
      "Epoch 105 iter 20\n",
      "Epoch 105 iter 30\n",
      "Epoch 105 iter 40\n",
      "Epoch 105 iter 50\n",
      "Epoch 105 iter 60\n",
      "Epoch 105 iter 70\n",
      "Epoch 105 iter 80\n",
      "Epoch 105 iter 90\n",
      "Epoch 105 iter 100\n",
      "Train Boundary avergage error = 98.854\n",
      "Train From boundary avergage accuracy = 87.141\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 105, Probability Accuracy 46.04776898537515\n",
      "Starting Training\n",
      "Training:: Epoch 106, Iteration 0, Current loss 0.07080451746155957 Accuracy 89.24719546946578\n",
      "Training:: Epoch 106, Iteration 10, Current loss 0.08757808221424462 Accuracy 89.38102556937264\n",
      "Training:: Epoch 106, Iteration 20, Current loss 0.10324508591381351 Accuracy 90.17969554767083\n",
      "Training:: Epoch 106, Iteration 30, Current loss 0.10582085336929226 Accuracy 86.11995666590066\n",
      "Training:: Epoch 106, Iteration 40, Current loss 0.1218649388176971 Accuracy 85.57712486883526\n",
      "Training:: Epoch 106, Iteration 50, Current loss 0.13030525601081935 Accuracy 84.49970288430772\n",
      "Training:: Epoch 106, Iteration 60, Current loss 0.08907873779145664 Accuracy 85.13162206444566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 106, Iteration 70, Current loss 0.11137535040407552 Accuracy 82.91149443817508\n",
      "Training:: Epoch 106, Iteration 80, Current loss 0.3057542204316487 Accuracy 82.78710848842488\n",
      "Training:: Epoch 106, Iteration 90, Current loss 0.14738033998693587 Accuracy 84.47976985210047\n",
      "Training:: Epoch 106, Iteration 100, Current loss 0.14383702363087933 Accuracy 90.45207417889327\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 106, Probability Accuracy 44.58228031652649\n",
      "Starting Training\n",
      "Training:: Epoch 107, Iteration 0, Current loss 0.18288348014573771 Accuracy 84.01218729955099\n",
      "Training:: Epoch 107, Iteration 10, Current loss 0.1325534844595868 Accuracy 87.22081218274111\n",
      "Training:: Epoch 107, Iteration 20, Current loss 0.1335472340006699 Accuracy 86.19276261857361\n",
      "Training:: Epoch 107, Iteration 30, Current loss 0.17734821346316954 Accuracy 86.17750848924696\n",
      "Training:: Epoch 107, Iteration 40, Current loss 0.14350427030070875 Accuracy 86.81797259497928\n",
      "Training:: Epoch 107, Iteration 50, Current loss 0.14223241032492176 Accuracy 90.07220216606498\n",
      "Training:: Epoch 107, Iteration 60, Current loss 0.1395339405935379 Accuracy 85.80552031844626\n",
      "Training:: Epoch 107, Iteration 70, Current loss 0.15495756425632948 Accuracy 80.77669902912622\n",
      "Training:: Epoch 107, Iteration 80, Current loss 0.11841857914655665 Accuracy 84.61352306277527\n",
      "Training:: Epoch 107, Iteration 90, Current loss 0.1198806495230808 Accuracy 83.89419614996879\n",
      "Training:: Epoch 107, Iteration 100, Current loss 0.10430143378339361 Accuracy 87.64488554042306\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 107, Probability Accuracy 44.13908107884161\n",
      "Starting Training\n",
      "Training:: Epoch 108, Iteration 0, Current loss 0.25038960603749677 Accuracy 84.13067038241213\n",
      "Training:: Epoch 108, Iteration 10, Current loss 0.08247214342466577 Accuracy 89.0020125488339\n",
      "Training:: Epoch 108, Iteration 20, Current loss 0.1498707859787861 Accuracy 85.32067608242649\n",
      "Training:: Epoch 108, Iteration 30, Current loss 0.1556471956474043 Accuracy 82.92962442048237\n",
      "Training:: Epoch 108, Iteration 40, Current loss 0.09866345536505032 Accuracy 89.9071418153532\n",
      "Training:: Epoch 108, Iteration 50, Current loss 0.1743140200054395 Accuracy 82.67866786678668\n",
      "Training:: Epoch 108, Iteration 60, Current loss 0.12496809613444945 Accuracy 81.28894579598806\n",
      "Training:: Epoch 108, Iteration 70, Current loss 0.16139754478168217 Accuracy 85.59534610663393\n",
      "Training:: Epoch 108, Iteration 80, Current loss 0.16195131973506985 Accuracy 87.17540842648323\n",
      "Training:: Epoch 108, Iteration 90, Current loss 0.1134000374509324 Accuracy 89.40480591497227\n",
      "Training:: Epoch 108, Iteration 100, Current loss 0.13676041985999 Accuracy 85.76800694243563\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 108, Probability Accuracy 43.675166756432034\n",
      "Starting Training\n",
      "Training:: Epoch 109, Iteration 0, Current loss 0.10492636075792158 Accuracy 85.58961808462678\n",
      "Training:: Epoch 109, Iteration 10, Current loss 0.13755293677767674 Accuracy 87.1777610580921\n",
      "Training:: Epoch 109, Iteration 20, Current loss 0.11429904008244149 Accuracy 86.16560818737106\n",
      "Training:: Epoch 109, Iteration 30, Current loss 0.2079964688952602 Accuracy 80.20059039015543\n",
      "Training:: Epoch 109, Iteration 40, Current loss 0.33630146533102195 Accuracy 75.42001120029867\n",
      "Training:: Epoch 109, Iteration 50, Current loss 0.11786876827899401 Accuracy 89.18636761635348\n",
      "Training:: Epoch 109, Iteration 60, Current loss 0.11390396522750669 Accuracy 87.32656939016958\n",
      "Training:: Epoch 109, Iteration 70, Current loss 0.0958522798932778 Accuracy 87.30668035679908\n",
      "Training:: Epoch 109, Iteration 80, Current loss 0.20877151894158955 Accuracy 82.34494341584356\n",
      "Training:: Epoch 109, Iteration 90, Current loss 0.11346578400829076 Accuracy 89.00181488203266\n",
      "Training:: Epoch 109, Iteration 100, Current loss 0.18100212863688547 Accuracy 80.5891432981734\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 109, Probability Accuracy 41.57693582466752\n",
      "Starting Training\n",
      "Training:: Epoch 110, Iteration 0, Current loss 0.1592070656908494 Accuracy 86.29947643979058\n",
      "Training:: Epoch 110, Iteration 10, Current loss 0.14744025920792847 Accuracy 86.98737867802203\n",
      "Training:: Epoch 110, Iteration 20, Current loss 0.09546022970015003 Accuracy 88.57520096306939\n",
      "Training:: Epoch 110, Iteration 30, Current loss 0.12936930057519766 Accuracy 88.25337292285253\n",
      "Training:: Epoch 110, Iteration 40, Current loss 0.10698013067050804 Accuracy 86.49513800809325\n",
      "Training:: Epoch 110, Iteration 50, Current loss 0.17785689112287875 Accuracy 84.35973257906917\n",
      "Training:: Epoch 110, Iteration 60, Current loss 0.12220018038382287 Accuracy 86.75924944344324\n",
      "Training:: Epoch 110, Iteration 70, Current loss 0.19316203632067097 Accuracy 83.65875480237882\n",
      "Training:: Epoch 110, Iteration 80, Current loss 0.24576965242801616 Accuracy 78.10796877301517\n",
      "Training:: Epoch 110, Iteration 90, Current loss 0.15052241153317258 Accuracy 80.55608095328164\n",
      "Training:: Epoch 110, Iteration 100, Current loss 0.17605349755183713 Accuracy 85.5438780872307\n",
      "Calculating Expectation\n",
      "Epoch 110 iter 0\n",
      "Epoch 110 iter 10\n",
      "Epoch 110 iter 20\n",
      "Epoch 110 iter 30\n",
      "Epoch 110 iter 40\n",
      "Epoch 110 iter 50\n",
      "Epoch 110 iter 60\n",
      "Epoch 110 iter 70\n",
      "Epoch 110 iter 80\n",
      "Epoch 110 iter 90\n",
      "Epoch 110 iter 100\n",
      "Train Boundary avergage error = 99.285\n",
      "Train From boundary avergage accuracy = 86.964\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 110, Probability Accuracy 44.073621411111574\n",
      "Starting Training\n",
      "Training:: Epoch 111, Iteration 0, Current loss 0.06430424269625763 Accuracy 89.1281411854978\n",
      "Training:: Epoch 111, Iteration 10, Current loss 0.12167302744230887 Accuracy 88.40473412235373\n",
      "Training:: Epoch 111, Iteration 20, Current loss 0.1514754242641715 Accuracy 88.0952380952381\n",
      "Training:: Epoch 111, Iteration 30, Current loss 0.34653171127413246 Accuracy 81.49264442052386\n",
      "Training:: Epoch 111, Iteration 40, Current loss 0.08874006490915605 Accuracy 80.87034617896799\n",
      "Training:: Epoch 111, Iteration 50, Current loss 0.5360264550499836 Accuracy 80.39552880481513\n",
      "Training:: Epoch 111, Iteration 60, Current loss 0.2547100043218693 Accuracy 86.02013452323384\n",
      "Training:: Epoch 111, Iteration 70, Current loss 0.1697776318681141 Accuracy 83.1053042442171\n",
      "Training:: Epoch 111, Iteration 80, Current loss 0.09705432779034039 Accuracy 86.34722180074652\n",
      "Training:: Epoch 111, Iteration 90, Current loss 0.15962906584281325 Accuracy 82.93133202231408\n",
      "Training:: Epoch 111, Iteration 100, Current loss 0.13901019628673025 Accuracy 90.59429719226235\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 111, Probability Accuracy 44.82961842813937\n",
      "Starting Training\n",
      "Training:: Epoch 112, Iteration 0, Current loss 0.4787734051303064 Accuracy 70.05556698909241\n",
      "Training:: Epoch 112, Iteration 10, Current loss 0.22043897114952435 Accuracy 89.4162877527633\n",
      "Training:: Epoch 112, Iteration 20, Current loss 0.44045477765873625 Accuracy 78.91282464264144\n",
      "Training:: Epoch 112, Iteration 30, Current loss 0.32374440931580833 Accuracy 82.9295101553166\n",
      "Training:: Epoch 112, Iteration 40, Current loss 0.1692613189260912 Accuracy 88.01518026565465\n",
      "Training:: Epoch 112, Iteration 50, Current loss 0.4683917018306091 Accuracy 78.27345703382024\n",
      "Training:: Epoch 112, Iteration 60, Current loss 0.1484709972641469 Accuracy 90.06422236569371\n",
      "Training:: Epoch 112, Iteration 70, Current loss 0.20114839279109653 Accuracy 86.5572876651218\n",
      "Training:: Epoch 112, Iteration 80, Current loss 0.12350881090586346 Accuracy 89.63860053825452\n",
      "Training:: Epoch 112, Iteration 90, Current loss 0.15251880017687638 Accuracy 86.7748063714745\n",
      "Training:: Epoch 112, Iteration 100, Current loss 0.12137607523167204 Accuracy 86.04179803621295\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 112, Probability Accuracy 44.302108795624974\n",
      "Starting Training\n",
      "Training:: Epoch 113, Iteration 0, Current loss 0.14022276607359604 Accuracy 90.18911329882607\n",
      "Training:: Epoch 113, Iteration 10, Current loss 0.09207145226116645 Accuracy 87.76264973489559\n",
      "Training:: Epoch 113, Iteration 20, Current loss 0.17226707576212455 Accuracy 83.43677628531728\n",
      "Training:: Epoch 113, Iteration 30, Current loss 0.13802148034932263 Accuracy 86.15060300029414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 113, Iteration 40, Current loss 0.1355728918739168 Accuracy 86.48108769894968\n",
      "Training:: Epoch 113, Iteration 50, Current loss 0.08193145563216257 Accuracy 83.77181404898079\n",
      "Training:: Epoch 113, Iteration 60, Current loss 0.0842819362953601 Accuracy 84.71407624633432\n",
      "Training:: Epoch 113, Iteration 70, Current loss 1.1021882424802296 Accuracy 68.00802046601673\n",
      "Training:: Epoch 113, Iteration 80, Current loss 0.10044993180461444 Accuracy 86.34755574614066\n",
      "Training:: Epoch 113, Iteration 90, Current loss 0.15150521593325383 Accuracy 81.83147927171946\n",
      "Training:: Epoch 113, Iteration 100, Current loss 0.14837051369954485 Accuracy 85.07163648735494\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 113, Probability Accuracy 41.97652980900692\n",
      "Starting Training\n",
      "Training:: Epoch 114, Iteration 0, Current loss 0.10729399784596377 Accuracy 85.17342187894756\n",
      "Training:: Epoch 114, Iteration 10, Current loss 0.12101861628709243 Accuracy 79.4068252048905\n",
      "Training:: Epoch 114, Iteration 20, Current loss 0.13249889393177583 Accuracy 87.2217254200593\n",
      "Training:: Epoch 114, Iteration 30, Current loss 0.12253144732371256 Accuracy 86.26423346655606\n",
      "Training:: Epoch 114, Iteration 40, Current loss 0.10115323621406633 Accuracy 89.17721518987342\n",
      "Training:: Epoch 114, Iteration 50, Current loss 0.10559634025374026 Accuracy 80.0445697969985\n",
      "Training:: Epoch 114, Iteration 60, Current loss 0.11594766486754506 Accuracy 85.95874713521772\n",
      "Training:: Epoch 114, Iteration 70, Current loss 0.13322140100901236 Accuracy 89.77559708492676\n",
      "Training:: Epoch 114, Iteration 80, Current loss 0.0934472648317124 Accuracy 89.1584986851542\n",
      "Training:: Epoch 114, Iteration 90, Current loss 0.07863174744495635 Accuracy 85.15467929178287\n",
      "Training:: Epoch 114, Iteration 100, Current loss 0.08863374355129197 Accuracy 87.95661578770984\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 114, Probability Accuracy 45.75931143058375\n",
      "Starting Training\n",
      "Training:: Epoch 115, Iteration 0, Current loss 0.1055286474015447 Accuracy 81.5394698514419\n",
      "Training:: Epoch 115, Iteration 10, Current loss 0.16116774567592965 Accuracy 83.17612014397419\n",
      "Training:: Epoch 115, Iteration 20, Current loss 0.10958075838406701 Accuracy 79.14134229923704\n",
      "Training:: Epoch 115, Iteration 30, Current loss 0.1387084410757962 Accuracy 87.78405061136486\n",
      "Training:: Epoch 115, Iteration 40, Current loss 0.08574793072590241 Accuracy 82.37878228024798\n",
      "Training:: Epoch 115, Iteration 50, Current loss 0.08653984352146887 Accuracy 86.27183406113537\n",
      "Training:: Epoch 115, Iteration 60, Current loss 0.0752042679806812 Accuracy 89.48661115437609\n",
      "Training:: Epoch 115, Iteration 70, Current loss 0.06926918837630537 Accuracy 91.96428571428571\n",
      "Training:: Epoch 115, Iteration 80, Current loss 0.09488759216323253 Accuracy 85.97776811774203\n",
      "Training:: Epoch 115, Iteration 90, Current loss 0.0842289520156395 Accuracy 88.31400053951982\n",
      "Training:: Epoch 115, Iteration 100, Current loss 0.0863299891420307 Accuracy 90.86835291774547\n",
      "Calculating Expectation\n",
      "Epoch 115 iter 0\n",
      "Epoch 115 iter 10\n",
      "Epoch 115 iter 20\n",
      "Epoch 115 iter 30\n",
      "Epoch 115 iter 40\n",
      "Epoch 115 iter 50\n",
      "Epoch 115 iter 60\n",
      "Epoch 115 iter 70\n",
      "Epoch 115 iter 80\n",
      "Epoch 115 iter 90\n",
      "Epoch 115 iter 100\n",
      "Train Boundary avergage error = 100.023\n",
      "Train From boundary avergage accuracy = 86.926\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 115, Probability Accuracy 44.850229937440446\n",
      "Starting Training\n",
      "Training:: Epoch 116, Iteration 0, Current loss 0.25699380202292765 Accuracy 83.82825040128411\n",
      "Training:: Epoch 116, Iteration 10, Current loss 0.1935139489884144 Accuracy 81.76753121998078\n",
      "Training:: Epoch 116, Iteration 20, Current loss 0.12947081213428271 Accuracy 88.27368312011188\n",
      "Training:: Epoch 116, Iteration 30, Current loss 0.08532746087295894 Accuracy 89.28471360525157\n",
      "Training:: Epoch 116, Iteration 40, Current loss 0.0876839468838293 Accuracy 87.51580407031283\n",
      "Training:: Epoch 116, Iteration 50, Current loss 0.11491694656251007 Accuracy 85.29767301299486\n",
      "Training:: Epoch 116, Iteration 60, Current loss 0.14237868065218853 Accuracy 78.6653677545056\n",
      "Training:: Epoch 116, Iteration 70, Current loss 0.14401472814123958 Accuracy 81.20756706874958\n",
      "Training:: Epoch 116, Iteration 80, Current loss 0.09922940314778442 Accuracy 88.95109327136369\n",
      "Training:: Epoch 116, Iteration 90, Current loss 0.09777148969072685 Accuracy 87.92489478795727\n",
      "Training:: Epoch 116, Iteration 100, Current loss 0.0768527895751952 Accuracy 85.79087128613658\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 116, Probability Accuracy 44.497866346273355\n",
      "Starting Training\n",
      "Training:: Epoch 117, Iteration 0, Current loss 0.09338670516233408 Accuracy 84.09738977846018\n",
      "Training:: Epoch 117, Iteration 10, Current loss 0.1757215249908077 Accuracy 87.56300679377603\n",
      "Training:: Epoch 117, Iteration 20, Current loss 0.17763184511632743 Accuracy 81.6830688893352\n",
      "Training:: Epoch 117, Iteration 30, Current loss 0.24658702492296683 Accuracy 84.06958739411604\n",
      "Training:: Epoch 117, Iteration 40, Current loss 0.13449648503933098 Accuracy 85.40491539081385\n",
      "Training:: Epoch 117, Iteration 50, Current loss 0.28502289633318406 Accuracy 81.97870511209577\n",
      "Training:: Epoch 117, Iteration 60, Current loss 0.12794867466641735 Accuracy 92.08018867924528\n",
      "Training:: Epoch 117, Iteration 70, Current loss 0.23659271359552025 Accuracy 86.72631080906706\n",
      "Training:: Epoch 117, Iteration 80, Current loss 0.13738346813969307 Accuracy 80.75828346226828\n",
      "Training:: Epoch 117, Iteration 90, Current loss 0.10387156816474781 Accuracy 87.64044943820225\n",
      "Training:: Epoch 117, Iteration 100, Current loss 0.08595390069659502 Accuracy 92.78344505974934\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 117, Probability Accuracy 42.9401955503998\n",
      "Starting Training\n",
      "Training:: Epoch 118, Iteration 0, Current loss 0.13905139689772963 Accuracy 86.31981981981981\n",
      "Training:: Epoch 118, Iteration 10, Current loss 0.08209167055057352 Accuracy 87.59869372907279\n",
      "Training:: Epoch 118, Iteration 20, Current loss 0.07848075354007185 Accuracy 89.59242178736133\n",
      "Training:: Epoch 118, Iteration 30, Current loss 0.1309980499552159 Accuracy 88.39766933721778\n",
      "Training:: Epoch 118, Iteration 40, Current loss 0.10384876218472634 Accuracy 86.9473361910594\n",
      "Training:: Epoch 118, Iteration 50, Current loss 0.07506489603190554 Accuracy 89.66292134831461\n",
      "Training:: Epoch 118, Iteration 60, Current loss 0.08303327880501576 Accuracy 84.837113001844\n",
      "Training:: Epoch 118, Iteration 70, Current loss 0.07559778786239144 Accuracy 88.29209414604708\n",
      "Training:: Epoch 118, Iteration 80, Current loss 0.09685184613630585 Accuracy 86.10645771413131\n",
      "Training:: Epoch 118, Iteration 90, Current loss 0.11368030024802867 Accuracy 87.49609643370184\n",
      "Training:: Epoch 118, Iteration 100, Current loss 0.19161601220694685 Accuracy 80.68008464150387\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 118, Probability Accuracy 45.16820648796453\n",
      "Starting Training\n",
      "Training:: Epoch 119, Iteration 0, Current loss 0.13938072969302676 Accuracy 86.45071209159453\n",
      "Training:: Epoch 119, Iteration 10, Current loss 0.11805022909235208 Accuracy 83.83876259667214\n",
      "Training:: Epoch 119, Iteration 20, Current loss 0.06526621109113516 Accuracy 86.84351469707948\n",
      "Training:: Epoch 119, Iteration 30, Current loss 0.12143389748829286 Accuracy 86.3926866758353\n",
      "Training:: Epoch 119, Iteration 40, Current loss 0.17668723919612053 Accuracy 81.89972062931922\n",
      "Training:: Epoch 119, Iteration 50, Current loss 0.13182797752451933 Accuracy 84.01226456416995\n",
      "Training:: Epoch 119, Iteration 60, Current loss 0.13182073633501887 Accuracy 84.22666207371684\n",
      "Training:: Epoch 119, Iteration 70, Current loss 0.13369734562214175 Accuracy 89.8036498036498\n",
      "Training:: Epoch 119, Iteration 80, Current loss 0.17560104668373533 Accuracy 86.3851551272297\n",
      "Training:: Epoch 119, Iteration 90, Current loss 0.08461561997501656 Accuracy 86.01539103074415\n",
      "Training:: Epoch 119, Iteration 100, Current loss 0.08621776630796751 Accuracy 87.41245359562173\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 119, Probability Accuracy 44.16166052119153\n",
      "Starting Training\n",
      "Training:: Epoch 120, Iteration 0, Current loss 0.07296427884821553 Accuracy 84.0204211869815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 120, Iteration 10, Current loss 0.08665040843572333 Accuracy 82.30504637087996\n",
      "Training:: Epoch 120, Iteration 20, Current loss 0.23414697036681714 Accuracy 86.56901688182721\n",
      "Training:: Epoch 120, Iteration 30, Current loss 0.14385286235063882 Accuracy 87.46962332928311\n",
      "Training:: Epoch 120, Iteration 40, Current loss 0.10340694325496842 Accuracy 90.08539118393723\n",
      "Training:: Epoch 120, Iteration 50, Current loss 0.11994033824300447 Accuracy 86.90819012230996\n",
      "Training:: Epoch 120, Iteration 60, Current loss 0.09756912468243109 Accuracy 87.39343293660842\n",
      "Training:: Epoch 120, Iteration 70, Current loss 0.08912445630961058 Accuracy 90.72948328267478\n",
      "Training:: Epoch 120, Iteration 80, Current loss 0.07023815943687813 Accuracy 90.15340706386014\n",
      "Training:: Epoch 120, Iteration 90, Current loss 0.08812277974763506 Accuracy 85.6421449052082\n",
      "Training:: Epoch 120, Iteration 100, Current loss 0.09219167412772306 Accuracy 88.26363792806745\n",
      "Calculating Expectation\n",
      "Epoch 120 iter 0\n",
      "Epoch 120 iter 10\n",
      "Epoch 120 iter 20\n",
      "Epoch 120 iter 30\n",
      "Epoch 120 iter 40\n",
      "Epoch 120 iter 50\n",
      "Epoch 120 iter 60\n",
      "Epoch 120 iter 70\n",
      "Epoch 120 iter 80\n",
      "Epoch 120 iter 90\n",
      "Epoch 120 iter 100\n",
      "Train Boundary avergage error = 101.368\n",
      "Train From boundary avergage accuracy = 86.572\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 120, Probability Accuracy 45.47199320545221\n",
      "Starting Training\n",
      "Training:: Epoch 121, Iteration 0, Current loss 0.1736557710188035 Accuracy 81.95611285266457\n",
      "Training:: Epoch 121, Iteration 10, Current loss 0.15728495086151176 Accuracy 83.4683525136281\n",
      "Training:: Epoch 121, Iteration 20, Current loss 0.09011621540685053 Accuracy 82.85961123110151\n",
      "Training:: Epoch 121, Iteration 30, Current loss 0.13736172146152853 Accuracy 84.46177847113884\n",
      "Training:: Epoch 121, Iteration 40, Current loss 0.16109022492209826 Accuracy 82.52645217015764\n",
      "Training:: Epoch 121, Iteration 50, Current loss 0.1333486058136235 Accuracy 85.41778549992102\n",
      "Training:: Epoch 121, Iteration 60, Current loss 0.16472623324308083 Accuracy 87.4194487174157\n",
      "Training:: Epoch 121, Iteration 70, Current loss 0.1476553152043593 Accuracy 82.25817233772311\n",
      "Training:: Epoch 121, Iteration 80, Current loss 0.16540937528371216 Accuracy 81.84989671792518\n",
      "Training:: Epoch 121, Iteration 90, Current loss 0.31594543736198616 Accuracy 81.48624738619912\n",
      "Training:: Epoch 121, Iteration 100, Current loss 0.13181023789848353 Accuracy 89.15881795983455\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 121, Probability Accuracy 46.29262128682106\n",
      "Starting Training\n",
      "Training:: Epoch 122, Iteration 0, Current loss 0.25319176735353305 Accuracy 81.83860820440393\n",
      "Training:: Epoch 122, Iteration 10, Current loss 0.31146126682045566 Accuracy 83.40595160841754\n",
      "Training:: Epoch 122, Iteration 20, Current loss 0.1716394326254568 Accuracy 85.43678204785712\n",
      "Training:: Epoch 122, Iteration 30, Current loss 0.1519357973550154 Accuracy 88.09721071527203\n",
      "Training:: Epoch 122, Iteration 40, Current loss 0.17682089658009023 Accuracy 84.2303997880163\n",
      "Training:: Epoch 122, Iteration 50, Current loss 0.14921414247328413 Accuracy 83.85506003430531\n",
      "Training:: Epoch 122, Iteration 60, Current loss 0.2845684294395541 Accuracy 79.44519360431516\n",
      "Training:: Epoch 122, Iteration 70, Current loss 0.28399919261626083 Accuracy 82.38474672737621\n",
      "Training:: Epoch 122, Iteration 80, Current loss 0.19550976741267237 Accuracy 83.91534947224702\n",
      "Training:: Epoch 122, Iteration 90, Current loss 0.16014547965795106 Accuracy 89.8084768028763\n",
      "Training:: Epoch 122, Iteration 100, Current loss 0.09798188222586715 Accuracy 91.4416403785489\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 122, Probability Accuracy 44.76540166549281\n",
      "Starting Training\n",
      "Training:: Epoch 123, Iteration 0, Current loss 0.1468094316209115 Accuracy 78.56563122555892\n",
      "Training:: Epoch 123, Iteration 10, Current loss 0.09031039119414917 Accuracy 84.90824330324108\n",
      "Training:: Epoch 123, Iteration 20, Current loss 0.09135641258720203 Accuracy 85.9257461781121\n",
      "Training:: Epoch 123, Iteration 30, Current loss 0.11817543948545176 Accuracy 85.4260272853525\n",
      "Training:: Epoch 123, Iteration 40, Current loss 0.09324136631881129 Accuracy 86.88500585708708\n",
      "Training:: Epoch 123, Iteration 50, Current loss 0.10106747102038449 Accuracy 87.77568650161672\n",
      "Training:: Epoch 123, Iteration 60, Current loss 0.10805023588342451 Accuracy 87.08255082877439\n",
      "Training:: Epoch 123, Iteration 70, Current loss 0.27413395629959353 Accuracy 81.96106447579925\n",
      "Training:: Epoch 123, Iteration 80, Current loss 0.18534208226268323 Accuracy 84.51549627304826\n",
      "Training:: Epoch 123, Iteration 90, Current loss 0.16118192050289326 Accuracy 86.61471861471861\n",
      "Training:: Epoch 123, Iteration 100, Current loss 0.07301024428457252 Accuracy 84.0137760390544\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 123, Probability Accuracy 44.124476944110704\n",
      "Starting Training\n",
      "Training:: Epoch 124, Iteration 0, Current loss 0.10832550524052303 Accuracy 83.7774272986359\n",
      "Training:: Epoch 124, Iteration 10, Current loss 0.09062954167173626 Accuracy 83.15630345705533\n",
      "Training:: Epoch 124, Iteration 20, Current loss 0.09846099263791859 Accuracy 82.08116119799652\n",
      "Training:: Epoch 124, Iteration 30, Current loss 0.08136332779603786 Accuracy 84.7901933050448\n",
      "Training:: Epoch 124, Iteration 40, Current loss 0.08114512321601194 Accuracy 87.51238441215324\n",
      "Training:: Epoch 124, Iteration 50, Current loss 0.09178521552826988 Accuracy 86.98717684410175\n",
      "Training:: Epoch 124, Iteration 60, Current loss 0.11580365621242339 Accuracy 85.96627124645893\n",
      "Training:: Epoch 124, Iteration 70, Current loss 0.09425665081894023 Accuracy 85.75534972852124\n",
      "Training:: Epoch 124, Iteration 80, Current loss 0.09239207922135402 Accuracy 76.88713156002876\n",
      "Training:: Epoch 124, Iteration 90, Current loss 0.10566667606133168 Accuracy 86.80017633352226\n",
      "Training:: Epoch 124, Iteration 100, Current loss 0.0843845400149015 Accuracy 85.00759817643765\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 124, Probability Accuracy 45.62828851970004\n",
      "Starting Training\n",
      "Training:: Epoch 125, Iteration 0, Current loss 0.07757934282826322 Accuracy 89.04976565065529\n",
      "Training:: Epoch 125, Iteration 10, Current loss 0.10178892987865841 Accuracy 82.88275211380049\n",
      "Training:: Epoch 125, Iteration 20, Current loss 0.2046873118585864 Accuracy 86.38343284012575\n",
      "Training:: Epoch 125, Iteration 30, Current loss 0.09757438211148453 Accuracy 90.7520981278244\n",
      "Training:: Epoch 125, Iteration 40, Current loss 0.10748418537125079 Accuracy 90.26149027605798\n",
      "Training:: Epoch 125, Iteration 50, Current loss 0.09888653992861013 Accuracy 85.67388015523252\n",
      "Training:: Epoch 125, Iteration 60, Current loss 0.09405000911633761 Accuracy 85.37761663702285\n",
      "Training:: Epoch 125, Iteration 70, Current loss 0.16555941795442825 Accuracy 85.50850524791893\n",
      "Training:: Epoch 125, Iteration 80, Current loss 0.07745452766039657 Accuracy 85.18700183936235\n",
      "Training:: Epoch 125, Iteration 90, Current loss 0.10394997440167127 Accuracy 84.60295180979116\n",
      "Training:: Epoch 125, Iteration 100, Current loss 0.10767316865982592 Accuracy 77.8890934690783\n",
      "Calculating Expectation\n",
      "Epoch 125 iter 0\n",
      "Epoch 125 iter 10\n",
      "Epoch 125 iter 20\n",
      "Epoch 125 iter 30\n",
      "Epoch 125 iter 40\n",
      "Epoch 125 iter 50\n",
      "Epoch 125 iter 60\n",
      "Epoch 125 iter 70\n",
      "Epoch 125 iter 80\n",
      "Epoch 125 iter 90\n",
      "Epoch 125 iter 100\n",
      "Train Boundary avergage error = 102.189\n",
      "Train From boundary avergage accuracy = 86.351\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 125, Probability Accuracy 44.57616936653271\n",
      "Starting Training\n",
      "Training:: Epoch 126, Iteration 0, Current loss 0.048547057870031315 Accuracy 90.86601307189542\n",
      "Training:: Epoch 126, Iteration 10, Current loss 0.17712484392862415 Accuracy 80.74416272740295\n",
      "Training:: Epoch 126, Iteration 20, Current loss 0.08880754933406798 Accuracy 89.5524790727624\n",
      "Training:: Epoch 126, Iteration 30, Current loss 0.07951471302351015 Accuracy 88.90966564266951\n",
      "Training:: Epoch 126, Iteration 40, Current loss 0.1966304866861479 Accuracy 85.17040221062328\n",
      "Training:: Epoch 126, Iteration 50, Current loss 0.07675114131663556 Accuracy 90.68108108108108\n",
      "Training:: Epoch 126, Iteration 60, Current loss 0.14012157704478706 Accuracy 85.78122288424828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 126, Iteration 70, Current loss 0.14330571830890773 Accuracy 85.75942423311537\n",
      "Training:: Epoch 126, Iteration 80, Current loss 0.1564208902048089 Accuracy 84.47814451382693\n",
      "Training:: Epoch 126, Iteration 90, Current loss 0.1567167477986581 Accuracy 84.25531914893617\n",
      "Training:: Epoch 126, Iteration 100, Current loss 0.12847045701502835 Accuracy 83.80647894919302\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 126, Probability Accuracy 45.048576873679416\n",
      "Starting Training\n",
      "Training:: Epoch 127, Iteration 0, Current loss 0.08596400241523608 Accuracy 87.80463393884803\n",
      "Training:: Epoch 127, Iteration 10, Current loss 0.11593475306666935 Accuracy 78.37917526641675\n",
      "Training:: Epoch 127, Iteration 20, Current loss 0.09160203552145574 Accuracy 80.6988806988807\n",
      "Training:: Epoch 127, Iteration 30, Current loss 0.07422411360200393 Accuracy 86.30741458848699\n",
      "Training:: Epoch 127, Iteration 40, Current loss 0.08656826480164377 Accuracy 87.26201550387597\n",
      "Training:: Epoch 127, Iteration 50, Current loss 0.11425692063439005 Accuracy 88.4588491753383\n",
      "Training:: Epoch 127, Iteration 60, Current loss 0.10541435987093195 Accuracy 87.89093415711405\n",
      "Training:: Epoch 127, Iteration 70, Current loss 0.10669297729412688 Accuracy 87.58059043094673\n",
      "Training:: Epoch 127, Iteration 80, Current loss 0.06256925638329679 Accuracy 89.63634067807311\n",
      "Training:: Epoch 127, Iteration 90, Current loss 0.09069735016023403 Accuracy 90.98566384481218\n",
      "Training:: Epoch 127, Iteration 100, Current loss 0.10779581844238982 Accuracy 84.67125860989356\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 127, Probability Accuracy 43.64347267680325\n",
      "Starting Training\n",
      "Training:: Epoch 128, Iteration 0, Current loss 0.11039827539676673 Accuracy 86.5494205862304\n",
      "Training:: Epoch 128, Iteration 10, Current loss 0.0948238228246362 Accuracy 84.98255137650251\n",
      "Training:: Epoch 128, Iteration 20, Current loss 0.05977224626881864 Accuracy 87.66004102709203\n",
      "Training:: Epoch 128, Iteration 30, Current loss 0.08508824236112571 Accuracy 88.59081097748998\n",
      "Training:: Epoch 128, Iteration 40, Current loss 0.11886213749543743 Accuracy 84.94279466294806\n",
      "Training:: Epoch 128, Iteration 50, Current loss 0.10598660361001333 Accuracy 80.28649386084584\n",
      "Training:: Epoch 128, Iteration 60, Current loss 0.23547318366777215 Accuracy 88.42786236127377\n",
      "Training:: Epoch 128, Iteration 70, Current loss 0.05802857148411726 Accuracy 86.67213816728518\n",
      "Training:: Epoch 128, Iteration 80, Current loss 0.09135463497897556 Accuracy 87.16272097591234\n",
      "Training:: Epoch 128, Iteration 90, Current loss 0.15775861599868596 Accuracy 84.06610299769409\n",
      "Training:: Epoch 128, Iteration 100, Current loss 0.14836650921090838 Accuracy 80.79416815617004\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 128, Probability Accuracy 42.390727928077226\n",
      "Starting Training\n",
      "Training:: Epoch 129, Iteration 0, Current loss 0.08871081358237648 Accuracy 84.25743102620204\n",
      "Training:: Epoch 129, Iteration 10, Current loss 0.15898091838831807 Accuracy 86.96928194091176\n",
      "Training:: Epoch 129, Iteration 20, Current loss 0.14642783423815775 Accuracy 78.9422168219252\n",
      "Training:: Epoch 129, Iteration 30, Current loss 0.16967166478154402 Accuracy 86.43877700297459\n",
      "Training:: Epoch 129, Iteration 40, Current loss 0.1364781406596914 Accuracy 86.36262016543706\n",
      "Training:: Epoch 129, Iteration 50, Current loss 0.09088965361257083 Accuracy 84.05501942599324\n",
      "Training:: Epoch 129, Iteration 60, Current loss 0.19486420921776954 Accuracy 84.95900777801135\n",
      "Training:: Epoch 129, Iteration 70, Current loss 0.10026531517094456 Accuracy 84.41584158415841\n",
      "Training:: Epoch 129, Iteration 80, Current loss 0.10334456006219749 Accuracy 87.21065083267617\n",
      "Training:: Epoch 129, Iteration 90, Current loss 0.15424512868864873 Accuracy 80.08500614502253\n",
      "Training:: Epoch 129, Iteration 100, Current loss 0.13455456550971356 Accuracy 89.1891891891892\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 129, Probability Accuracy 43.40338484484401\n",
      "Starting Training\n",
      "Training:: Epoch 130, Iteration 0, Current loss 0.1872404153877126 Accuracy 88.26107011070111\n",
      "Training:: Epoch 130, Iteration 10, Current loss 0.15040614193144225 Accuracy 82.67051561588306\n",
      "Training:: Epoch 130, Iteration 20, Current loss 0.1420800706320719 Accuracy 87.02216608018286\n",
      "Training:: Epoch 130, Iteration 30, Current loss 0.09528372656860236 Accuracy 86.5279949953081\n",
      "Training:: Epoch 130, Iteration 40, Current loss 0.11523602200628881 Accuracy 87.65432098765432\n",
      "Training:: Epoch 130, Iteration 50, Current loss 0.08432670289900221 Accuracy 86.91850795913518\n",
      "Training:: Epoch 130, Iteration 60, Current loss 0.12270365048772232 Accuracy 88.63949909795183\n",
      "Training:: Epoch 130, Iteration 70, Current loss 0.11364946819758434 Accuracy 82.19610864958582\n",
      "Training:: Epoch 130, Iteration 80, Current loss 0.08993819871544212 Accuracy 84.48118701574923\n",
      "Training:: Epoch 130, Iteration 90, Current loss 0.0841392236196879 Accuracy 87.48884070909322\n",
      "Training:: Epoch 130, Iteration 100, Current loss 0.20169772258252022 Accuracy 85.31714171015656\n",
      "Calculating Expectation\n",
      "Epoch 130 iter 0\n",
      "Epoch 130 iter 10\n",
      "Epoch 130 iter 20\n",
      "Epoch 130 iter 30\n",
      "Epoch 130 iter 40\n",
      "Epoch 130 iter 50\n",
      "Epoch 130 iter 60\n",
      "Epoch 130 iter 70\n",
      "Epoch 130 iter 80\n",
      "Epoch 130 iter 90\n",
      "Epoch 130 iter 100\n",
      "Train Boundary avergage error = 102.049\n",
      "Train From boundary avergage accuracy = 86.449\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 130, Probability Accuracy 45.283485934457474\n",
      "Starting Training\n",
      "Training:: Epoch 131, Iteration 0, Current loss 0.1353761734734006 Accuracy 86.9554636164517\n",
      "Training:: Epoch 131, Iteration 10, Current loss 0.10215691567884509 Accuracy 83.06526225891396\n",
      "Training:: Epoch 131, Iteration 20, Current loss 0.22971318106860247 Accuracy 82.14269758298708\n",
      "Training:: Epoch 131, Iteration 30, Current loss 0.2736554798902869 Accuracy 82.97742186631075\n",
      "Training:: Epoch 131, Iteration 40, Current loss 0.09189258600517636 Accuracy 87.05074562815149\n",
      "Training:: Epoch 131, Iteration 50, Current loss 0.11367108481239686 Accuracy 84.82951452466966\n",
      "Training:: Epoch 131, Iteration 60, Current loss 0.08710372945999184 Accuracy 85.80309139784946\n",
      "Training:: Epoch 131, Iteration 70, Current loss 0.15330417690828066 Accuracy 86.79854290588833\n",
      "Training:: Epoch 131, Iteration 80, Current loss 0.08321545718736702 Accuracy 86.57832350616984\n",
      "Training:: Epoch 131, Iteration 90, Current loss 0.10248038094168296 Accuracy 85.84592056919578\n",
      "Training:: Epoch 131, Iteration 100, Current loss 0.1287483598769523 Accuracy 86.73693456302152\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 131, Probability Accuracy 45.31942660645482\n",
      "Starting Training\n",
      "Training:: Epoch 132, Iteration 0, Current loss 0.09535171830060588 Accuracy 89.21931605557546\n",
      "Training:: Epoch 132, Iteration 10, Current loss 0.08601477576420075 Accuracy 84.39511567120509\n",
      "Training:: Epoch 132, Iteration 20, Current loss 0.21764496015888427 Accuracy 84.7313490529571\n",
      "Training:: Epoch 132, Iteration 30, Current loss 0.21855125116873741 Accuracy 82.0853306338762\n",
      "Training:: Epoch 132, Iteration 40, Current loss 0.11655237158295213 Accuracy 88.81389352343395\n",
      "Training:: Epoch 132, Iteration 50, Current loss 0.14240985196809075 Accuracy 87.40771123872027\n",
      "Training:: Epoch 132, Iteration 60, Current loss 0.14184558611635537 Accuracy 84.29340856629905\n",
      "Training:: Epoch 132, Iteration 70, Current loss 0.10078493292236225 Accuracy 86.355548965824\n",
      "Training:: Epoch 132, Iteration 80, Current loss 0.12402626702295633 Accuracy 85.71036470305692\n",
      "Training:: Epoch 132, Iteration 90, Current loss 0.06445445770083784 Accuracy 85.27552802922027\n",
      "Training:: Epoch 132, Iteration 100, Current loss 0.1017523211147815 Accuracy 88.71526092723802\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 132, Probability Accuracy 45.38685420723371\n",
      "Starting Training\n",
      "Training:: Epoch 133, Iteration 0, Current loss 0.12032357605435395 Accuracy 81.4693657635468\n",
      "Training:: Epoch 133, Iteration 10, Current loss 0.13297788428752372 Accuracy 84.99016909162407\n",
      "Training:: Epoch 133, Iteration 20, Current loss 0.10271049341578632 Accuracy 80.8637134110861\n",
      "Training:: Epoch 133, Iteration 30, Current loss 0.1539067060251757 Accuracy 85.87541211174735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 133, Iteration 40, Current loss 0.10947229692991368 Accuracy 85.79464324347713\n",
      "Training:: Epoch 133, Iteration 50, Current loss 0.1663135059900342 Accuracy 82.39332691690728\n",
      "Training:: Epoch 133, Iteration 60, Current loss 0.10086088349952364 Accuracy 76.55153641422696\n",
      "Training:: Epoch 133, Iteration 70, Current loss 0.18303148093841604 Accuracy 82.179437812861\n",
      "Training:: Epoch 133, Iteration 80, Current loss 0.1826289676665437 Accuracy 85.83901773533424\n",
      "Training:: Epoch 133, Iteration 90, Current loss 0.08956327931334551 Accuracy 86.40412329863891\n",
      "Training:: Epoch 133, Iteration 100, Current loss 0.10610024789111559 Accuracy 83.29879499577865\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 133, Probability Accuracy 45.8993454033227\n",
      "Starting Training\n",
      "Training:: Epoch 134, Iteration 0, Current loss 0.0741303702111602 Accuracy 82.56106968372332\n",
      "Training:: Epoch 134, Iteration 10, Current loss 0.11320660775363023 Accuracy 85.02561156589593\n",
      "Training:: Epoch 134, Iteration 20, Current loss 0.14061942332214622 Accuracy 87.0194777897146\n",
      "Training:: Epoch 134, Iteration 30, Current loss 0.12794077401784198 Accuracy 84.45447510962084\n",
      "Training:: Epoch 134, Iteration 40, Current loss 0.07575305077484594 Accuracy 83.02940322112126\n",
      "Training:: Epoch 134, Iteration 50, Current loss 0.16057525403449102 Accuracy 78.48917869034406\n",
      "Training:: Epoch 134, Iteration 60, Current loss 0.087705713859929 Accuracy 85.07368087466328\n",
      "Training:: Epoch 134, Iteration 70, Current loss 0.08285342088638807 Accuracy 83.90944710491947\n",
      "Training:: Epoch 134, Iteration 80, Current loss 0.07283628767273162 Accuracy 91.1660568865809\n",
      "Training:: Epoch 134, Iteration 90, Current loss 0.1069416014385321 Accuracy 89.12582781456953\n",
      "Training:: Epoch 134, Iteration 100, Current loss 0.052088498845280044 Accuracy 87.62138335598051\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 134, Probability Accuracy 44.563326014003394\n",
      "Starting Training\n",
      "Training:: Epoch 135, Iteration 0, Current loss 0.08871784766103895 Accuracy 89.4702842377261\n",
      "Training:: Epoch 135, Iteration 10, Current loss 0.303977476730398 Accuracy 82.68319000752052\n",
      "Training:: Epoch 135, Iteration 20, Current loss 0.34394543440623104 Accuracy 83.70586329770003\n",
      "Training:: Epoch 135, Iteration 30, Current loss 0.2455251405771357 Accuracy 84.05915230554261\n",
      "Training:: Epoch 135, Iteration 40, Current loss 0.2589016782616384 Accuracy 85.25917568195393\n",
      "Training:: Epoch 135, Iteration 50, Current loss 0.18567978790374426 Accuracy 84.33999025815879\n",
      "Training:: Epoch 135, Iteration 60, Current loss 0.1560036425850831 Accuracy 86.96745617656995\n",
      "Training:: Epoch 135, Iteration 70, Current loss 0.21062179317200397 Accuracy 87.20363602286571\n",
      "Training:: Epoch 135, Iteration 80, Current loss 0.1688772191603807 Accuracy 83.44126241963764\n",
      "Training:: Epoch 135, Iteration 90, Current loss 0.17875374689260434 Accuracy 81.01156332513885\n",
      "Training:: Epoch 135, Iteration 100, Current loss 0.23357108007604568 Accuracy 86.21587830798083\n",
      "Calculating Expectation\n",
      "Epoch 135 iter 0\n",
      "Epoch 135 iter 10\n",
      "Epoch 135 iter 20\n",
      "Epoch 135 iter 30\n",
      "Epoch 135 iter 40\n",
      "Epoch 135 iter 50\n",
      "Epoch 135 iter 60\n",
      "Epoch 135 iter 70\n",
      "Epoch 135 iter 80\n",
      "Epoch 135 iter 90\n",
      "Epoch 135 iter 100\n",
      "Train Boundary avergage error = 103.299\n",
      "Train From boundary avergage accuracy = 86.324\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 135, Probability Accuracy 44.25912499482123\n",
      "Starting Training\n",
      "Training:: Epoch 136, Iteration 0, Current loss 0.14173186020938808 Accuracy 83.2586240498928\n",
      "Training:: Epoch 136, Iteration 10, Current loss 0.16347181029120392 Accuracy 87.86735596851567\n",
      "Training:: Epoch 136, Iteration 20, Current loss 0.12879884397917563 Accuracy 86.7235332580396\n",
      "Training:: Epoch 136, Iteration 30, Current loss 0.24779488092004853 Accuracy 80.9020874751491\n",
      "Training:: Epoch 136, Iteration 40, Current loss 0.21712382486312898 Accuracy 82.21020568807967\n",
      "Training:: Epoch 136, Iteration 50, Current loss 0.12719620556154504 Accuracy 88.03230254087059\n",
      "Training:: Epoch 136, Iteration 60, Current loss 0.25278030549205627 Accuracy 85.00343957807843\n",
      "Training:: Epoch 136, Iteration 70, Current loss 0.2317059223012673 Accuracy 81.43043323723391\n",
      "Training:: Epoch 136, Iteration 80, Current loss 0.18577946858350675 Accuracy 83.51196856020007\n",
      "Training:: Epoch 136, Iteration 90, Current loss 0.1982463034673592 Accuracy 85.62736723345958\n",
      "Training:: Epoch 136, Iteration 100, Current loss 0.1236033702915509 Accuracy 85.10147601476015\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 136, Probability Accuracy 45.60291254091229\n",
      "Starting Training\n",
      "Training:: Epoch 137, Iteration 0, Current loss 0.1368173821306655 Accuracy 83.37789661319073\n",
      "Training:: Epoch 137, Iteration 10, Current loss 0.10111328664962156 Accuracy 82.51810443399822\n",
      "Training:: Epoch 137, Iteration 20, Current loss 0.12384379084151566 Accuracy 85.61623396001194\n",
      "Training:: Epoch 137, Iteration 30, Current loss 0.2062367539892253 Accuracy 88.12109744560075\n",
      "Training:: Epoch 137, Iteration 40, Current loss 0.23087662203180856 Accuracy 87.16974303293522\n",
      "Training:: Epoch 137, Iteration 50, Current loss 0.2333449405028042 Accuracy 83.35031068554548\n",
      "Training:: Epoch 137, Iteration 60, Current loss 0.10812373864195071 Accuracy 86.6621367760502\n",
      "Training:: Epoch 137, Iteration 70, Current loss 0.17266859849406727 Accuracy 85.83283775747418\n",
      "Training:: Epoch 137, Iteration 80, Current loss 0.08772800073560462 Accuracy 87.49609531884511\n",
      "Training:: Epoch 137, Iteration 90, Current loss 0.1582076214112693 Accuracy 87.86198729680267\n",
      "Training:: Epoch 137, Iteration 100, Current loss 0.10866010306351824 Accuracy 80.28707196394454\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 137, Probability Accuracy 45.73559265857397\n",
      "Starting Training\n",
      "Training:: Epoch 138, Iteration 0, Current loss 0.10967479557110339 Accuracy 85.7203896769783\n",
      "Training:: Epoch 138, Iteration 10, Current loss 0.08869773362888513 Accuracy 87.73690754161991\n",
      "Training:: Epoch 138, Iteration 20, Current loss 0.10725766900665712 Accuracy 87.43770468460772\n",
      "Training:: Epoch 138, Iteration 30, Current loss 0.11570476537990347 Accuracy 84.26729966046693\n",
      "Training:: Epoch 138, Iteration 40, Current loss 0.10288179494074066 Accuracy 82.6086956521739\n",
      "Training:: Epoch 138, Iteration 50, Current loss 0.09198774232447307 Accuracy 84.0460236289896\n",
      "Training:: Epoch 138, Iteration 60, Current loss 0.11957956003774262 Accuracy 86.2370118237191\n",
      "Training:: Epoch 138, Iteration 70, Current loss 0.16539235514894166 Accuracy 81.27256758075535\n",
      "Training:: Epoch 138, Iteration 80, Current loss 0.16093717636294533 Accuracy 84.1809715120526\n",
      "Training:: Epoch 138, Iteration 90, Current loss 0.2977860674051403 Accuracy 81.95107248706485\n",
      "Training:: Epoch 138, Iteration 100, Current loss 0.11218476003941845 Accuracy 88.52542951416544\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 138, Probability Accuracy 45.517255665575675\n",
      "Starting Training\n",
      "Training:: Epoch 139, Iteration 0, Current loss 0.09925115880936909 Accuracy 87.6159857811234\n",
      "Training:: Epoch 139, Iteration 10, Current loss 0.2643297732425922 Accuracy 82.29690502473527\n",
      "Training:: Epoch 139, Iteration 20, Current loss 0.2021063974125294 Accuracy 75.24987075650526\n",
      "Training:: Epoch 139, Iteration 30, Current loss 0.2512118410082645 Accuracy 75.54903349863375\n",
      "Training:: Epoch 139, Iteration 40, Current loss 0.23505558536010168 Accuracy 83.48977661354871\n",
      "Training:: Epoch 139, Iteration 50, Current loss 0.2006986972577395 Accuracy 85.16428642764406\n",
      "Training:: Epoch 139, Iteration 60, Current loss 0.2452195424895192 Accuracy 86.82489954368998\n",
      "Training:: Epoch 139, Iteration 70, Current loss 0.29343654275745296 Accuracy 84.9108566458907\n",
      "Training:: Epoch 139, Iteration 80, Current loss 0.4107217670225029 Accuracy 79.33292155651637\n",
      "Training:: Epoch 139, Iteration 90, Current loss 0.20468718980851114 Accuracy 76.61332446934803\n",
      "Training:: Epoch 139, Iteration 100, Current loss 0.14842082257007425 Accuracy 85.85520597948896\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 139, Probability Accuracy 45.256763475162614\n",
      "Starting Training\n",
      "Training:: Epoch 140, Iteration 0, Current loss 0.16381363615593086 Accuracy 82.49137667554469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 140, Iteration 10, Current loss 0.2532619082948945 Accuracy 87.12330220713073\n",
      "Training:: Epoch 140, Iteration 20, Current loss 0.11763288997518043 Accuracy 82.9358647835147\n",
      "Training:: Epoch 140, Iteration 30, Current loss 0.19468247189519522 Accuracy 84.45281573234581\n",
      "Training:: Epoch 140, Iteration 40, Current loss 0.12239220014082605 Accuracy 79.05794001616046\n",
      "Training:: Epoch 140, Iteration 50, Current loss 0.1994399647276901 Accuracy 85.22640234287002\n",
      "Training:: Epoch 140, Iteration 60, Current loss 0.13820768225704583 Accuracy 87.85581807918716\n",
      "Training:: Epoch 140, Iteration 70, Current loss 0.15550252424281386 Accuracy 77.83516247041102\n",
      "Training:: Epoch 140, Iteration 80, Current loss 0.0956779574668318 Accuracy 83.31520425429055\n",
      "Training:: Epoch 140, Iteration 90, Current loss 0.11450576084195671 Accuracy 87.3997559274756\n",
      "Training:: Epoch 140, Iteration 100, Current loss 0.1343067923476882 Accuracy 88.16203335980937\n",
      "Calculating Expectation\n",
      "Epoch 140 iter 0\n",
      "Epoch 140 iter 10\n",
      "Epoch 140 iter 20\n",
      "Epoch 140 iter 30\n",
      "Epoch 140 iter 40\n",
      "Epoch 140 iter 50\n",
      "Epoch 140 iter 60\n",
      "Epoch 140 iter 70\n",
      "Epoch 140 iter 80\n",
      "Epoch 140 iter 90\n",
      "Epoch 140 iter 100\n",
      "Train Boundary avergage error = 103.678\n",
      "Train From boundary avergage accuracy = 86.488\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 140, Probability Accuracy 45.71467042300203\n",
      "Starting Training\n",
      "Training:: Epoch 141, Iteration 0, Current loss 0.11129713040182573 Accuracy 81.11123106265069\n",
      "Training:: Epoch 141, Iteration 10, Current loss 0.10932898905219993 Accuracy 87.90917510299236\n",
      "Training:: Epoch 141, Iteration 20, Current loss 0.10448435354143003 Accuracy 85.48346648515029\n",
      "Training:: Epoch 141, Iteration 30, Current loss 0.0737421004027827 Accuracy 89.6067318056956\n",
      "Training:: Epoch 141, Iteration 40, Current loss 0.1286119701139369 Accuracy 85.47470381043868\n",
      "Training:: Epoch 141, Iteration 50, Current loss 0.11027404215316125 Accuracy 86.94189051138754\n",
      "Training:: Epoch 141, Iteration 60, Current loss 0.2562800065773541 Accuracy 86.13557512676807\n",
      "Training:: Epoch 141, Iteration 70, Current loss 0.10380517390940401 Accuracy 84.53941990124588\n",
      "Training:: Epoch 141, Iteration 80, Current loss 0.19872965941642665 Accuracy 83.9390983502269\n",
      "Training:: Epoch 141, Iteration 90, Current loss 0.14153788967162273 Accuracy 85.9306994163096\n",
      "Training:: Epoch 141, Iteration 100, Current loss 0.07091503051240161 Accuracy 84.48045620240373\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 141, Probability Accuracy 45.61523801632349\n",
      "Starting Training\n",
      "Training:: Epoch 142, Iteration 0, Current loss 0.07288728239085306 Accuracy 91.12026227410843\n",
      "Training:: Epoch 142, Iteration 10, Current loss 0.19065753722894233 Accuracy 83.24101288718064\n",
      "Training:: Epoch 142, Iteration 20, Current loss 0.1383298097898207 Accuracy 88.95390167878821\n",
      "Training:: Epoch 142, Iteration 30, Current loss 0.28246711184754963 Accuracy 78.2611557193629\n",
      "Training:: Epoch 142, Iteration 40, Current loss 0.11258647947827355 Accuracy 84.89515384860152\n",
      "Training:: Epoch 142, Iteration 50, Current loss 0.09425925107409566 Accuracy 89.0491260878947\n",
      "Training:: Epoch 142, Iteration 60, Current loss 0.14573064265595892 Accuracy 85.1032536919198\n",
      "Training:: Epoch 142, Iteration 70, Current loss 0.08072794770863892 Accuracy 84.8012470771629\n",
      "Training:: Epoch 142, Iteration 80, Current loss 0.11659907083839707 Accuracy 85.41633306645316\n",
      "Training:: Epoch 142, Iteration 90, Current loss 0.18869730089229553 Accuracy 82.55871243225488\n",
      "Training:: Epoch 142, Iteration 100, Current loss 0.09757031359823536 Accuracy 86.03978001075136\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 142, Probability Accuracy 43.98071425612131\n",
      "Starting Training\n",
      "Training:: Epoch 143, Iteration 0, Current loss 0.059158865996236196 Accuracy 83.6977465024713\n",
      "Training:: Epoch 143, Iteration 10, Current loss 0.10951009717991088 Accuracy 83.65071015255128\n",
      "Training:: Epoch 143, Iteration 20, Current loss 0.12195105124318954 Accuracy 84.57892082702975\n",
      "Training:: Epoch 143, Iteration 30, Current loss 0.1131503514352112 Accuracy 83.92779783393502\n",
      "Training:: Epoch 143, Iteration 40, Current loss 0.0817089775248535 Accuracy 87.89572615749901\n",
      "Training:: Epoch 143, Iteration 50, Current loss 0.1164139665631007 Accuracy 80.2329066232821\n",
      "Training:: Epoch 143, Iteration 60, Current loss 0.08821811617768333 Accuracy 89.08614321458359\n",
      "Training:: Epoch 143, Iteration 70, Current loss 0.09322483871533023 Accuracy 83.77600882723833\n",
      "Training:: Epoch 143, Iteration 80, Current loss 0.05521865660497196 Accuracy 87.604809879753\n",
      "Training:: Epoch 143, Iteration 90, Current loss 0.07607590310809721 Accuracy 89.44412824890912\n",
      "Training:: Epoch 143, Iteration 100, Current loss 0.09572800865332164 Accuracy 86.83526917679376\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 143, Probability Accuracy 46.13062932427393\n",
      "Starting Training\n",
      "Training:: Epoch 144, Iteration 0, Current loss 0.10420212400720906 Accuracy 87.63433898744012\n",
      "Training:: Epoch 144, Iteration 10, Current loss 0.06720179429190555 Accuracy 87.45847575947234\n",
      "Training:: Epoch 144, Iteration 20, Current loss 0.07555190733002343 Accuracy 86.1803722173332\n",
      "Training:: Epoch 144, Iteration 30, Current loss 0.0832159102294117 Accuracy 91.39784946236558\n",
      "Training:: Epoch 144, Iteration 40, Current loss 0.05701701814219467 Accuracy 87.5477331111574\n",
      "Training:: Epoch 144, Iteration 50, Current loss 0.06019455633035732 Accuracy 86.16957705331652\n",
      "Training:: Epoch 144, Iteration 60, Current loss 0.10840344242159332 Accuracy 87.58765391198719\n",
      "Training:: Epoch 144, Iteration 70, Current loss 0.07658043066000897 Accuracy 89.17469660928114\n",
      "Training:: Epoch 144, Iteration 80, Current loss 0.11649760101976765 Accuracy 84.1020551144325\n",
      "Training:: Epoch 144, Iteration 90, Current loss 0.06535834068154364 Accuracy 88.12883836404025\n",
      "Training:: Epoch 144, Iteration 100, Current loss 0.07764111405327437 Accuracy 85.19060932854036\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 144, Probability Accuracy 46.399821850271366\n",
      "Starting Training\n",
      "Training:: Epoch 145, Iteration 0, Current loss 0.05287000805795205 Accuracy 85.31975099037918\n",
      "Training:: Epoch 145, Iteration 10, Current loss 0.12117002561076824 Accuracy 82.9746835443038\n",
      "Training:: Epoch 145, Iteration 20, Current loss 0.05708588430309377 Accuracy 89.20570264765784\n",
      "Training:: Epoch 145, Iteration 30, Current loss 0.12684133442675952 Accuracy 83.2\n",
      "Training:: Epoch 145, Iteration 40, Current loss 0.07582022845535197 Accuracy 91.33596662852081\n",
      "Training:: Epoch 145, Iteration 50, Current loss 0.10285898429935764 Accuracy 85.7596502804355\n",
      "Training:: Epoch 145, Iteration 60, Current loss 0.05638615670522994 Accuracy 87.4340200919462\n",
      "Training:: Epoch 145, Iteration 70, Current loss 0.05784825151211213 Accuracy 88.11363961643052\n",
      "Training:: Epoch 145, Iteration 80, Current loss 0.08502837040387495 Accuracy 88.622812079246\n",
      "Training:: Epoch 145, Iteration 90, Current loss 0.07880525232267835 Accuracy 88.63086410116307\n",
      "Training:: Epoch 145, Iteration 100, Current loss 0.07589355277093512 Accuracy 88.88924252068746\n",
      "Calculating Expectation\n",
      "Epoch 145 iter 0\n",
      "Epoch 145 iter 10\n",
      "Epoch 145 iter 20\n",
      "Epoch 145 iter 30\n",
      "Epoch 145 iter 40\n",
      "Epoch 145 iter 50\n",
      "Epoch 145 iter 60\n",
      "Epoch 145 iter 70\n",
      "Epoch 145 iter 80\n",
      "Epoch 145 iter 90\n",
      "Epoch 145 iter 100\n",
      "Train Boundary avergage error = 103.734\n",
      "Train From boundary avergage accuracy = 86.541\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 145, Probability Accuracy 46.12451837428015\n",
      "Starting Training\n",
      "Training:: Epoch 146, Iteration 0, Current loss 0.051070099052901136 Accuracy 90.52170100832969\n",
      "Training:: Epoch 146, Iteration 10, Current loss 0.10653946762967735 Accuracy 87.18268571072164\n",
      "Training:: Epoch 146, Iteration 20, Current loss 0.10788566680422078 Accuracy 83.40047909407666\n",
      "Training:: Epoch 146, Iteration 30, Current loss 0.08150421681740946 Accuracy 88.62071895980284\n",
      "Training:: Epoch 146, Iteration 40, Current loss 0.07624330067402685 Accuracy 90.8007040616386\n",
      "Training:: Epoch 146, Iteration 50, Current loss 0.16662290030496402 Accuracy 82.59030398540206\n",
      "Training:: Epoch 146, Iteration 60, Current loss 0.08760793792385908 Accuracy 84.89797115046323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 146, Iteration 70, Current loss 0.0838180250515103 Accuracy 88.02617480483698\n",
      "Training:: Epoch 146, Iteration 80, Current loss 0.064648118653783 Accuracy 87.55331723110129\n",
      "Training:: Epoch 146, Iteration 90, Current loss 0.07967288841648004 Accuracy 86.95974530071089\n",
      "Training:: Epoch 146, Iteration 100, Current loss 0.09493119628213909 Accuracy 82.40711437432749\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 146, Probability Accuracy 45.49643700542735\n",
      "Starting Training\n",
      "Training:: Epoch 147, Iteration 0, Current loss 0.08530038688594412 Accuracy 82.54917199736953\n",
      "Training:: Epoch 147, Iteration 10, Current loss 0.11650131515700926 Accuracy 87.80918139751657\n",
      "Training:: Epoch 147, Iteration 20, Current loss 0.30742419331927906 Accuracy 74.54555204032388\n",
      "Training:: Epoch 147, Iteration 30, Current loss 0.09017850534455131 Accuracy 82.71389584447007\n",
      "Training:: Epoch 147, Iteration 40, Current loss 0.1286279611295533 Accuracy 81.90194142810135\n",
      "Training:: Epoch 147, Iteration 50, Current loss 0.14141359302791126 Accuracy 88.63409770687936\n",
      "Training:: Epoch 147, Iteration 60, Current loss 0.07359041208855349 Accuracy 86.04297394942004\n",
      "Training:: Epoch 147, Iteration 70, Current loss 0.08017714516420074 Accuracy 89.3646677471637\n",
      "Training:: Epoch 147, Iteration 80, Current loss 0.096973780073011 Accuracy 85.43772977941177\n",
      "Training:: Epoch 147, Iteration 90, Current loss 0.11115844592913941 Accuracy 89.2665811514374\n",
      "Training:: Epoch 147, Iteration 100, Current loss 0.12325482575666524 Accuracy 89.58093381766635\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 147, Probability Accuracy 46.41856900194722\n",
      "Starting Training\n",
      "Training:: Epoch 148, Iteration 0, Current loss 0.07636828263827782 Accuracy 89.85308848080133\n",
      "Training:: Epoch 148, Iteration 10, Current loss 0.1183078097585115 Accuracy 87.17562427027231\n",
      "Training:: Epoch 148, Iteration 20, Current loss 0.1769635306971755 Accuracy 87.47569669475048\n",
      "Training:: Epoch 148, Iteration 30, Current loss 0.08409490519309397 Accuracy 86.15324438263288\n",
      "Training:: Epoch 148, Iteration 40, Current loss 0.08316371329063432 Accuracy 86.15867958759237\n",
      "Training:: Epoch 148, Iteration 50, Current loss 0.09454905258329244 Accuracy 87.70532060027286\n",
      "Training:: Epoch 148, Iteration 60, Current loss 0.0912149450879109 Accuracy 88.9496267560101\n",
      "Training:: Epoch 148, Iteration 70, Current loss 0.08295978216860506 Accuracy 88.84600026620524\n",
      "Training:: Epoch 148, Iteration 80, Current loss 0.1753397162429793 Accuracy 82.4296435272045\n",
      "Training:: Epoch 148, Iteration 90, Current loss 0.08889673223540134 Accuracy 88.28174104336895\n",
      "Training:: Epoch 148, Iteration 100, Current loss 0.09220842394995171 Accuracy 87.9715662522297\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 148, Probability Accuracy 45.38167543605253\n",
      "Starting Training\n",
      "Training:: Epoch 149, Iteration 0, Current loss 0.08376936173756037 Accuracy 89.40946068092799\n",
      "Training:: Epoch 149, Iteration 10, Current loss 0.06646204840805497 Accuracy 89.55372916506644\n",
      "Training:: Epoch 149, Iteration 20, Current loss 0.09874767302575121 Accuracy 83.09696396680323\n",
      "Training:: Epoch 149, Iteration 30, Current loss 0.06145689046237918 Accuracy 81.83027021892957\n",
      "Training:: Epoch 149, Iteration 40, Current loss 0.11241018178862403 Accuracy 83.23561222747196\n",
      "Training:: Epoch 149, Iteration 50, Current loss 0.06807130770904961 Accuracy 88.87048946740093\n",
      "Training:: Epoch 149, Iteration 60, Current loss 0.05460230391210376 Accuracy 85.88011933821535\n",
      "Training:: Epoch 149, Iteration 70, Current loss 0.1503378285520161 Accuracy 84.24579110300665\n",
      "Training:: Epoch 149, Iteration 80, Current loss 0.2152182682515708 Accuracy 81.16285305684661\n",
      "Training:: Epoch 149, Iteration 90, Current loss 0.05562470313824375 Accuracy 87.1248945486683\n",
      "Training:: Epoch 149, Iteration 100, Current loss 0.3303245681267615 Accuracy 84.87503220819376\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 149, Probability Accuracy 41.7678253304056\n",
      "Starting Training\n",
      "Training:: Epoch 150, Iteration 0, Current loss 0.15062799775509436 Accuracy 86.52626143405135\n",
      "Training:: Epoch 150, Iteration 10, Current loss 0.09852421779427997 Accuracy 85.89381779698387\n",
      "Training:: Epoch 150, Iteration 20, Current loss 0.12532040396547456 Accuracy 80.65367649612726\n",
      "Training:: Epoch 150, Iteration 30, Current loss 0.0755081305330498 Accuracy 90.7627472569194\n",
      "Training:: Epoch 150, Iteration 40, Current loss 0.09279876071190288 Accuracy 88.58707399668565\n",
      "Training:: Epoch 150, Iteration 50, Current loss 0.06557183363218402 Accuracy 84.6636354584979\n",
      "Training:: Epoch 150, Iteration 60, Current loss 0.05245511457132388 Accuracy 83.82381079943362\n",
      "Training:: Epoch 150, Iteration 70, Current loss 0.08807783751211026 Accuracy 85.42985287337049\n",
      "Training:: Epoch 150, Iteration 80, Current loss 0.10224277612988492 Accuracy 87.86619259852772\n",
      "Training:: Epoch 150, Iteration 90, Current loss 0.09557211549540666 Accuracy 88.34060133119118\n",
      "Training:: Epoch 150, Iteration 100, Current loss 0.05726440235479899 Accuracy 83.98527261332777\n",
      "Calculating Expectation\n",
      "Epoch 150 iter 0\n",
      "Epoch 150 iter 10\n",
      "Epoch 150 iter 20\n",
      "Epoch 150 iter 30\n",
      "Epoch 150 iter 40\n",
      "Epoch 150 iter 50\n",
      "Epoch 150 iter 60\n",
      "Epoch 150 iter 70\n",
      "Epoch 150 iter 80\n",
      "Epoch 150 iter 90\n",
      "Epoch 150 iter 100\n",
      "Train Boundary avergage error = 103.974\n",
      "Train From boundary avergage accuracy = 86.426\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 150, Probability Accuracy 45.4763433732444\n",
      "Starting Training\n",
      "Training:: Epoch 151, Iteration 0, Current loss 0.07670016748291615 Accuracy 82.25171055359735\n",
      "Training:: Epoch 151, Iteration 10, Current loss 0.11759429197174297 Accuracy 83.21163845558137\n",
      "Training:: Epoch 151, Iteration 20, Current loss 0.09458219216685337 Accuracy 88.99981705564122\n",
      "Training:: Epoch 151, Iteration 30, Current loss 0.13164408494576668 Accuracy 84.22726983521028\n",
      "Training:: Epoch 151, Iteration 40, Current loss 0.08531177038259952 Accuracy 87.07910342748015\n",
      "Training:: Epoch 151, Iteration 50, Current loss 0.09740066844940234 Accuracy 85.9735859493846\n",
      "Training:: Epoch 151, Iteration 60, Current loss 0.09308905172210212 Accuracy 80.77116659736963\n",
      "Training:: Epoch 151, Iteration 70, Current loss 0.09444974684594717 Accuracy 82.74074708704592\n",
      "Training:: Epoch 151, Iteration 80, Current loss 0.06704006212965494 Accuracy 85.7281799729364\n",
      "Training:: Epoch 151, Iteration 90, Current loss 0.1057906428522273 Accuracy 83.16801516639019\n",
      "Training:: Epoch 151, Iteration 100, Current loss 0.11958186943006463 Accuracy 80.16111707841031\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 151, Probability Accuracy 44.269378961759955\n",
      "Starting Training\n",
      "Training:: Epoch 152, Iteration 0, Current loss 0.05578030120963753 Accuracy 90.888\n",
      "Training:: Epoch 152, Iteration 10, Current loss 0.20566290248119898 Accuracy 81.63103536707621\n",
      "Training:: Epoch 152, Iteration 20, Current loss 0.17306922514575288 Accuracy 83.28863501793738\n",
      "Training:: Epoch 152, Iteration 30, Current loss 0.11924878185665413 Accuracy 87.52545133704358\n",
      "Training:: Epoch 152, Iteration 40, Current loss 0.0797245556313635 Accuracy 80.2743332767114\n",
      "Training:: Epoch 152, Iteration 50, Current loss 0.10364484813196341 Accuracy 83.91638225255973\n",
      "Training:: Epoch 152, Iteration 60, Current loss 0.10978253025657825 Accuracy 81.38285168378314\n",
      "Training:: Epoch 152, Iteration 70, Current loss 0.08119248558189192 Accuracy 89.99943958753643\n",
      "Training:: Epoch 152, Iteration 80, Current loss 0.29306421092503704 Accuracy 77.23390275952694\n",
      "Training:: Epoch 152, Iteration 90, Current loss 0.12655679520858457 Accuracy 85.88615461098682\n",
      "Training:: Epoch 152, Iteration 100, Current loss 0.323635557312217 Accuracy 84.21824581988854\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 152, Probability Accuracy 45.273128392095124\n",
      "Starting Training\n",
      "Training:: Epoch 153, Iteration 0, Current loss 0.1384041362303803 Accuracy 89.18080526715625\n",
      "Training:: Epoch 153, Iteration 10, Current loss 0.08712805576441232 Accuracy 84.51351756533906\n",
      "Training:: Epoch 153, Iteration 20, Current loss 0.08288233993914837 Accuracy 84.7264070230257\n",
      "Training:: Epoch 153, Iteration 30, Current loss 0.15594111104002545 Accuracy 85.45904034161141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 153, Iteration 40, Current loss 0.09857155878413808 Accuracy 90.02090144166354\n",
      "Training:: Epoch 153, Iteration 50, Current loss 0.0959544862908192 Accuracy 86.44164555607094\n",
      "Training:: Epoch 153, Iteration 60, Current loss 0.23528277580474513 Accuracy 76.10538791138663\n",
      "Training:: Epoch 153, Iteration 70, Current loss 0.06675234289069602 Accuracy 86.14659975987439\n",
      "Training:: Epoch 153, Iteration 80, Current loss 0.06131741949940776 Accuracy 89.99740417063252\n",
      "Training:: Epoch 153, Iteration 90, Current loss 0.11869174842898693 Accuracy 79.14674512688488\n",
      "Training:: Epoch 153, Iteration 100, Current loss 0.10748289495188797 Accuracy 85.1664637088436\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 153, Probability Accuracy 46.638770352570745\n",
      "Starting Training\n",
      "Training:: Epoch 154, Iteration 0, Current loss 0.07878919862424878 Accuracy 89.81340270924913\n",
      "Training:: Epoch 154, Iteration 10, Current loss 0.08990949960981688 Accuracy 81.65174328447802\n",
      "Training:: Epoch 154, Iteration 20, Current loss 0.05158941349281812 Accuracy 88.01942525282868\n",
      "Training:: Epoch 154, Iteration 30, Current loss 0.06749396202708238 Accuracy 87.879020201633\n",
      "Training:: Epoch 154, Iteration 40, Current loss 0.0958305855566406 Accuracy 88.8835839898146\n",
      "Training:: Epoch 154, Iteration 50, Current loss 0.09907987393465394 Accuracy 84.55243820975284\n",
      "Training:: Epoch 154, Iteration 60, Current loss 0.10655626425796628 Accuracy 82.56074106056442\n",
      "Training:: Epoch 154, Iteration 70, Current loss 0.07940156045972402 Accuracy 85.40931545518701\n",
      "Training:: Epoch 154, Iteration 80, Current loss 0.09007276518742627 Accuracy 88.58546332528516\n",
      "Training:: Epoch 154, Iteration 90, Current loss 0.14077637611871807 Accuracy 80.04788147077423\n",
      "Training:: Epoch 154, Iteration 100, Current loss 0.15175708230365006 Accuracy 76.5190820256549\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 154, Probability Accuracy 46.00944607863446\n",
      "Starting Training\n",
      "Training:: Epoch 155, Iteration 0, Current loss 0.07464335041457716 Accuracy 88.01244757137059\n",
      "Training:: Epoch 155, Iteration 10, Current loss 0.05042434300054099 Accuracy 88.8427681958062\n",
      "Training:: Epoch 155, Iteration 20, Current loss 0.15983731752918273 Accuracy 81.07805023923444\n",
      "Training:: Epoch 155, Iteration 30, Current loss 0.09449843000010495 Accuracy 80.38721503035858\n",
      "Training:: Epoch 155, Iteration 40, Current loss 0.08063047000360478 Accuracy 87.91890953290502\n",
      "Training:: Epoch 155, Iteration 50, Current loss 0.06929656314242123 Accuracy 90.59192789022575\n",
      "Training:: Epoch 155, Iteration 60, Current loss 0.07815134623711542 Accuracy 83.29400121802679\n",
      "Training:: Epoch 155, Iteration 70, Current loss 0.1045259184323777 Accuracy 86.47179129046741\n",
      "Training:: Epoch 155, Iteration 80, Current loss 0.0881714730895802 Accuracy 82.96184087138425\n",
      "Training:: Epoch 155, Iteration 90, Current loss 0.09723869772448004 Accuracy 86.01953920902774\n",
      "Training:: Epoch 155, Iteration 100, Current loss 0.06008733157247592 Accuracy 84.35284304184624\n",
      "Calculating Expectation\n",
      "Epoch 155 iter 0\n",
      "Epoch 155 iter 10\n",
      "Epoch 155 iter 20\n",
      "Epoch 155 iter 30\n",
      "Epoch 155 iter 40\n",
      "Epoch 155 iter 50\n",
      "Epoch 155 iter 60\n",
      "Epoch 155 iter 70\n",
      "Epoch 155 iter 80\n",
      "Epoch 155 iter 90\n",
      "Epoch 155 iter 100\n",
      "Train Boundary avergage error = 104.254\n",
      "Train From boundary avergage accuracy = 86.462\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 155, Probability Accuracy 46.254091229233126\n",
      "Starting Training\n",
      "Training:: Epoch 156, Iteration 0, Current loss 0.04052799613340651 Accuracy 84.37053176278341\n",
      "Training:: Epoch 156, Iteration 10, Current loss 0.08224617994017633 Accuracy 90.80397845006216\n",
      "Training:: Epoch 156, Iteration 20, Current loss 0.046268084447260445 Accuracy 90.48141428631703\n",
      "Training:: Epoch 156, Iteration 30, Current loss 0.042040524411430344 Accuracy 87.71667325391\n",
      "Training:: Epoch 156, Iteration 40, Current loss 0.09068655566664705 Accuracy 85.66063044936284\n",
      "Training:: Epoch 156, Iteration 50, Current loss 0.05211714158705165 Accuracy 89.55573376102646\n",
      "Training:: Epoch 156, Iteration 60, Current loss 0.06437294585221297 Accuracy 87.7329510546795\n",
      "Training:: Epoch 156, Iteration 70, Current loss 0.08112114871297393 Accuracy 89.18971657154007\n",
      "Training:: Epoch 156, Iteration 80, Current loss 0.06272135507837741 Accuracy 88.03107443820225\n",
      "Training:: Epoch 156, Iteration 90, Current loss 0.04353876646618578 Accuracy 91.24793038388972\n",
      "Training:: Epoch 156, Iteration 100, Current loss 0.0811060339417733 Accuracy 85.93851835759287\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 156, Probability Accuracy 46.47926420019058\n",
      "Starting Training\n",
      "Training:: Epoch 157, Iteration 0, Current loss 0.06576572008565702 Accuracy 86.52326945639422\n",
      "Training:: Epoch 157, Iteration 10, Current loss 0.05332121826606978 Accuracy 86.36458044862918\n",
      "Training:: Epoch 157, Iteration 20, Current loss 0.04722418023112407 Accuracy 87.78988099970846\n",
      "Training:: Epoch 157, Iteration 30, Current loss 0.05630456040140563 Accuracy 85.94529563709706\n",
      "Training:: Epoch 157, Iteration 40, Current loss 0.06238543568729633 Accuracy 86.03657583493982\n",
      "Training:: Epoch 157, Iteration 50, Current loss 0.1389572949610258 Accuracy 79.10625517812758\n",
      "Training:: Epoch 157, Iteration 60, Current loss 0.11806235604950505 Accuracy 86.62782696453966\n",
      "Training:: Epoch 157, Iteration 70, Current loss 0.06483132609119964 Accuracy 85.69734907763076\n",
      "Training:: Epoch 157, Iteration 80, Current loss 0.07345173389595068 Accuracy 85.91590708615114\n",
      "Training:: Epoch 157, Iteration 90, Current loss 0.09767311061078722 Accuracy 89.91892844554107\n",
      "Training:: Epoch 157, Iteration 100, Current loss 0.0689505543057243 Accuracy 90.06859897410543\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 157, Probability Accuracy 44.1672535940672\n",
      "Starting Training\n",
      "Training:: Epoch 158, Iteration 0, Current loss 0.2991359174836893 Accuracy 82.9223343571742\n",
      "Training:: Epoch 158, Iteration 10, Current loss 0.12043077708362948 Accuracy 88.54696157897148\n",
      "Training:: Epoch 158, Iteration 20, Current loss 0.07044915326711093 Accuracy 88.45613164640702\n",
      "Training:: Epoch 158, Iteration 30, Current loss 0.12349500771373108 Accuracy 85.30555123619966\n",
      "Training:: Epoch 158, Iteration 40, Current loss 0.09822993121108299 Accuracy 85.61849229487713\n",
      "Training:: Epoch 158, Iteration 50, Current loss 0.0827096063100688 Accuracy 84.09025499338053\n",
      "Training:: Epoch 158, Iteration 60, Current loss 0.14956918618042234 Accuracy 86.48595560793557\n",
      "Training:: Epoch 158, Iteration 70, Current loss 0.13670576830699027 Accuracy 84.01578687715836\n",
      "Training:: Epoch 158, Iteration 80, Current loss 0.0682566272306819 Accuracy 86.2142444380237\n",
      "Training:: Epoch 158, Iteration 90, Current loss 0.09727406427424591 Accuracy 84.32635840127715\n",
      "Training:: Epoch 158, Iteration 100, Current loss 0.12530964777630124 Accuracy 84.09631163254352\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 158, Probability Accuracy 45.159299001532915\n",
      "Starting Training\n",
      "Training:: Epoch 159, Iteration 0, Current loss 0.0656178943212315 Accuracy 89.15232515562066\n",
      "Training:: Epoch 159, Iteration 10, Current loss 0.049911374541128824 Accuracy 85.97410604192355\n",
      "Training:: Epoch 159, Iteration 20, Current loss 0.07859741470317881 Accuracy 86.96799357577149\n",
      "Training:: Epoch 159, Iteration 30, Current loss 0.06426761886248919 Accuracy 83.03436123348018\n",
      "Training:: Epoch 159, Iteration 40, Current loss 0.06898747757325226 Accuracy 86.97774128808612\n",
      "Training:: Epoch 159, Iteration 50, Current loss 0.06690059304033516 Accuracy 86.31201736233785\n",
      "Training:: Epoch 159, Iteration 60, Current loss 0.08772458391043102 Accuracy 88.95744052017142\n",
      "Training:: Epoch 159, Iteration 70, Current loss 0.06408874910732786 Accuracy 87.96578590785907\n",
      "Training:: Epoch 159, Iteration 80, Current loss 0.07183760978091908 Accuracy 83.76114563631451\n",
      "Training:: Epoch 159, Iteration 90, Current loss 0.10314484661553595 Accuracy 80.56618640252138\n",
      "Training:: Epoch 159, Iteration 100, Current loss 0.06437958880069816 Accuracy 89.37885253674727\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 159, Probability Accuracy 45.856465177942574\n",
      "Starting Training\n",
      "Training:: Epoch 160, Iteration 0, Current loss 0.06684005681321036 Accuracy 80.25095471903983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 160, Iteration 10, Current loss 0.06867750671242351 Accuracy 78.96304318397185\n",
      "Training:: Epoch 160, Iteration 20, Current loss 0.12414963456059763 Accuracy 88.89130041598843\n",
      "Training:: Epoch 160, Iteration 30, Current loss 0.12515749958720376 Accuracy 89.14796730125637\n",
      "Training:: Epoch 160, Iteration 40, Current loss 0.04969360377256735 Accuracy 89.09064241713112\n",
      "Training:: Epoch 160, Iteration 50, Current loss 0.09550744719632885 Accuracy 87.31313400961025\n",
      "Training:: Epoch 160, Iteration 60, Current loss 0.13591766037312342 Accuracy 84.91668906208008\n",
      "Training:: Epoch 160, Iteration 70, Current loss 0.04872499125170011 Accuracy 85.27551942186089\n",
      "Training:: Epoch 160, Iteration 80, Current loss 0.22123802733591105 Accuracy 80.02633528104683\n",
      "Training:: Epoch 160, Iteration 90, Current loss 0.09200443192552046 Accuracy 87.71632337539275\n",
      "Training:: Epoch 160, Iteration 100, Current loss 0.09494354804238914 Accuracy 83.91805034815212\n",
      "Calculating Expectation\n",
      "Epoch 160 iter 0\n",
      "Epoch 160 iter 10\n",
      "Epoch 160 iter 20\n",
      "Epoch 160 iter 30\n",
      "Epoch 160 iter 40\n",
      "Epoch 160 iter 50\n",
      "Epoch 160 iter 60\n",
      "Epoch 160 iter 70\n",
      "Epoch 160 iter 80\n",
      "Epoch 160 iter 90\n",
      "Epoch 160 iter 100\n",
      "Train Boundary avergage error = 104.956\n",
      "Train From boundary avergage accuracy = 86.297\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 160, Probability Accuracy 45.56966482992915\n",
      "Starting Training\n",
      "Training:: Epoch 161, Iteration 0, Current loss 0.06606296146708932 Accuracy 86.68580045726516\n",
      "Training:: Epoch 161, Iteration 10, Current loss 0.2863662253351333 Accuracy 84.75182087941732\n",
      "Training:: Epoch 161, Iteration 20, Current loss 0.05956695325963394 Accuracy 86.20467365028203\n",
      "Training:: Epoch 161, Iteration 30, Current loss 0.06664429143618565 Accuracy 87.84230904611053\n",
      "Training:: Epoch 161, Iteration 40, Current loss 0.08949296517681855 Accuracy 79.47894361170593\n",
      "Training:: Epoch 161, Iteration 50, Current loss 0.09741587479484606 Accuracy 87.48355503581348\n",
      "Training:: Epoch 161, Iteration 60, Current loss 0.13178496129035996 Accuracy 86.03221583470886\n",
      "Training:: Epoch 161, Iteration 70, Current loss 0.08698632765737836 Accuracy 77.34483076639306\n",
      "Training:: Epoch 161, Iteration 80, Current loss 0.15399242578333427 Accuracy 84.13546032175206\n",
      "Training:: Epoch 161, Iteration 90, Current loss 0.10827260013397007 Accuracy 83.3607331491133\n",
      "Training:: Epoch 161, Iteration 100, Current loss 0.07624780775845436 Accuracy 87.03986872039683\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 161, Probability Accuracy 44.707813729958154\n",
      "Starting Training\n",
      "Training:: Epoch 162, Iteration 0, Current loss 0.06515485638027395 Accuracy 88.15033390277993\n",
      "Training:: Epoch 162, Iteration 10, Current loss 0.07610019144255947 Accuracy 85.11931109412096\n",
      "Training:: Epoch 162, Iteration 20, Current loss 0.07184764140010981 Accuracy 85.32097587569851\n",
      "Training:: Epoch 162, Iteration 30, Current loss 0.12573528458658587 Accuracy 84.05883704895426\n",
      "Training:: Epoch 162, Iteration 40, Current loss 0.17424624179833129 Accuracy 81.62395963038207\n",
      "Training:: Epoch 162, Iteration 50, Current loss 0.06301188233163443 Accuracy 86.78857035963148\n",
      "Training:: Epoch 162, Iteration 60, Current loss 0.07096722373145276 Accuracy 90.45077592577387\n",
      "Training:: Epoch 162, Iteration 70, Current loss 0.09282542262784285 Accuracy 88.98382667073543\n",
      "Training:: Epoch 162, Iteration 80, Current loss 0.06981783970477025 Accuracy 84.7424268355297\n",
      "Training:: Epoch 162, Iteration 90, Current loss 0.08833765537780346 Accuracy 89.62734094205183\n",
      "Training:: Epoch 162, Iteration 100, Current loss 0.08799367752358189 Accuracy 84.05103668261563\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 162, Probability Accuracy 44.43737829887724\n",
      "Starting Training\n",
      "Training:: Epoch 163, Iteration 0, Current loss 0.10570063978849403 Accuracy 87.1918027537624\n",
      "Training:: Epoch 163, Iteration 10, Current loss 0.05996438544077038 Accuracy 85.06825341267063\n",
      "Training:: Epoch 163, Iteration 20, Current loss 0.09079623933234486 Accuracy 85.69826584965357\n",
      "Training:: Epoch 163, Iteration 30, Current loss 0.1044417578316032 Accuracy 88.59517132891047\n",
      "Training:: Epoch 163, Iteration 40, Current loss 0.14042662241361903 Accuracy 84.20922161626177\n",
      "Training:: Epoch 163, Iteration 50, Current loss 0.14009769813266737 Accuracy 86.6433635289484\n",
      "Training:: Epoch 163, Iteration 60, Current loss 0.1746603882614972 Accuracy 84.9609457462529\n",
      "Training:: Epoch 163, Iteration 70, Current loss 0.14912184683028604 Accuracy 82.32787925486107\n",
      "Training:: Epoch 163, Iteration 80, Current loss 0.08970847625418897 Accuracy 84.05301287948483\n",
      "Training:: Epoch 163, Iteration 90, Current loss 0.08559047037989533 Accuracy 84.98124065558972\n",
      "Training:: Epoch 163, Iteration 100, Current loss 0.0858935611309221 Accuracy 86.55633119134575\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 163, Probability Accuracy 44.55617930977338\n",
      "Starting Training\n",
      "Training:: Epoch 164, Iteration 0, Current loss 0.08307277719325143 Accuracy 83.4242358786046\n",
      "Training:: Epoch 164, Iteration 10, Current loss 0.08262194699743826 Accuracy 91.21404362561918\n",
      "Training:: Epoch 164, Iteration 20, Current loss 0.09988413809656915 Accuracy 86.93510446935105\n",
      "Training:: Epoch 164, Iteration 30, Current loss 0.06445640916458853 Accuracy 90.18019060848091\n",
      "Training:: Epoch 164, Iteration 40, Current loss 0.1384424482088587 Accuracy 86.6464051666825\n",
      "Training:: Epoch 164, Iteration 50, Current loss 0.12230002325817436 Accuracy 82.76553106212425\n",
      "Training:: Epoch 164, Iteration 60, Current loss 0.09699819612139035 Accuracy 88.56349681563333\n",
      "Training:: Epoch 164, Iteration 70, Current loss 0.08447692613212666 Accuracy 89.32378243136718\n",
      "Training:: Epoch 164, Iteration 80, Current loss 0.09544944528191342 Accuracy 88.68559661003525\n",
      "Training:: Epoch 164, Iteration 90, Current loss 0.0806278841902788 Accuracy 87.74632331570075\n",
      "Training:: Epoch 164, Iteration 100, Current loss 0.07788341539726622 Accuracy 86.16071428571429\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 164, Probability Accuracy 46.93178522600157\n",
      "Starting Training\n",
      "Training:: Epoch 165, Iteration 0, Current loss 0.08103271163238789 Accuracy 84.12459583186606\n",
      "Training:: Epoch 165, Iteration 10, Current loss 0.14122528265803122 Accuracy 90.00291885580852\n",
      "Training:: Epoch 165, Iteration 20, Current loss 0.09332089314901666 Accuracy 84.046875\n",
      "Training:: Epoch 165, Iteration 30, Current loss 0.06457485968383854 Accuracy 87.66503075965991\n",
      "Training:: Epoch 165, Iteration 40, Current loss 0.07340632080992562 Accuracy 85.74507057429156\n",
      "Training:: Epoch 165, Iteration 50, Current loss 0.06115244742898241 Accuracy 80.77464520792908\n",
      "Training:: Epoch 165, Iteration 60, Current loss 0.10238449618327208 Accuracy 84.92485549132948\n",
      "Training:: Epoch 165, Iteration 70, Current loss 0.07653712013751642 Accuracy 84.26447747947637\n",
      "Training:: Epoch 165, Iteration 80, Current loss 0.07534101864476678 Accuracy 85.62454479242534\n",
      "Training:: Epoch 165, Iteration 90, Current loss 0.05276191644102224 Accuracy 89.6103896103896\n",
      "Training:: Epoch 165, Iteration 100, Current loss 0.0557202631584206 Accuracy 90.6410392552476\n",
      "Calculating Expectation\n",
      "Epoch 165 iter 0\n",
      "Epoch 165 iter 10\n",
      "Epoch 165 iter 20\n",
      "Epoch 165 iter 30\n",
      "Epoch 165 iter 40\n",
      "Epoch 165 iter 50\n",
      "Epoch 165 iter 60\n",
      "Epoch 165 iter 70\n",
      "Epoch 165 iter 80\n",
      "Epoch 165 iter 90\n",
      "Epoch 165 iter 100\n",
      "Train Boundary avergage error = 105.687\n",
      "Train From boundary avergage accuracy = 86.186\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 165, Probability Accuracy 45.00859676016075\n",
      "Starting Training\n",
      "Training:: Epoch 166, Iteration 0, Current loss 0.06277887183317361 Accuracy 87.41071428571429\n",
      "Training:: Epoch 166, Iteration 10, Current loss 0.08014732380350319 Accuracy 86.32177514061438\n",
      "Training:: Epoch 166, Iteration 20, Current loss 0.057078794217654556 Accuracy 86.15962723583345\n",
      "Training:: Epoch 166, Iteration 30, Current loss 0.1091350242766167 Accuracy 87.29141835518475\n",
      "Training:: Epoch 166, Iteration 40, Current loss 0.06641226692406896 Accuracy 91.45261646458201\n",
      "Training:: Epoch 166, Iteration 50, Current loss 0.1314653678163596 Accuracy 80.75889688735678\n",
      "Training:: Epoch 166, Iteration 60, Current loss 0.1259902722153052 Accuracy 87.75178394002349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 166, Iteration 70, Current loss 0.07610146660465567 Accuracy 84.45686634406808\n",
      "Training:: Epoch 166, Iteration 80, Current loss 0.09026828061734973 Accuracy 87.71572455157585\n",
      "Training:: Epoch 166, Iteration 90, Current loss 0.0691895140739194 Accuracy 88.37701963512188\n",
      "Training:: Epoch 166, Iteration 100, Current loss 0.07088144456380809 Accuracy 90.36125997609592\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 166, Probability Accuracy 45.280067945477896\n",
      "Starting Training\n",
      "Training:: Epoch 167, Iteration 0, Current loss 0.0730358373584035 Accuracy 81.41597952800683\n",
      "Training:: Epoch 167, Iteration 10, Current loss 0.10939642821095706 Accuracy 89.64149280047017\n",
      "Training:: Epoch 167, Iteration 20, Current loss 0.07596162772042359 Accuracy 88.59040345342852\n",
      "Training:: Epoch 167, Iteration 30, Current loss 0.07157456876087225 Accuracy 81.85028029857219\n",
      "Training:: Epoch 167, Iteration 40, Current loss 0.05889778632040354 Accuracy 85.73121137404335\n",
      "Training:: Epoch 167, Iteration 50, Current loss 0.06815928832055337 Accuracy 85.32443746729462\n",
      "Training:: Epoch 167, Iteration 60, Current loss 0.0952279499287469 Accuracy 88.34349030470914\n",
      "Training:: Epoch 167, Iteration 70, Current loss 0.061916042021214604 Accuracy 86.46062936734981\n",
      "Training:: Epoch 167, Iteration 80, Current loss 0.06324207901809792 Accuracy 89.67659935037425\n",
      "Training:: Epoch 167, Iteration 90, Current loss 0.06074994071528992 Accuracy 76.23318385650224\n",
      "Training:: Epoch 167, Iteration 100, Current loss 0.1064124224101623 Accuracy 86.18292786301602\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 167, Probability Accuracy 44.45788623275469\n",
      "Starting Training\n",
      "Training:: Epoch 168, Iteration 0, Current loss 0.0658584775591645 Accuracy 82.73611572996342\n",
      "Training:: Epoch 168, Iteration 10, Current loss 0.10458866099629949 Accuracy 82.82059800664452\n",
      "Training:: Epoch 168, Iteration 20, Current loss 0.07395173029956403 Accuracy 82.5290913193437\n",
      "Training:: Epoch 168, Iteration 30, Current loss 0.08112253454265342 Accuracy 86.32132296572117\n",
      "Training:: Epoch 168, Iteration 40, Current loss 0.11113042200869323 Accuracy 84.592511935673\n",
      "Training:: Epoch 168, Iteration 50, Current loss 0.06499750525143136 Accuracy 87.36696613683483\n",
      "Training:: Epoch 168, Iteration 60, Current loss 0.2638580911991908 Accuracy 76.8756621400366\n",
      "Training:: Epoch 168, Iteration 70, Current loss 0.06984343781109048 Accuracy 88.42327404280977\n",
      "Training:: Epoch 168, Iteration 80, Current loss 0.05980153758067101 Accuracy 90.91571037455583\n",
      "Training:: Epoch 168, Iteration 90, Current loss 0.06815866914004542 Accuracy 86.60611018646506\n",
      "Training:: Epoch 168, Iteration 100, Current loss 0.1399112483149634 Accuracy 81.45311124873692\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 168, Probability Accuracy 45.10481832870696\n",
      "Starting Training\n",
      "Training:: Epoch 169, Iteration 0, Current loss 0.06871725662513319 Accuracy 87.87518337893026\n",
      "Training:: Epoch 169, Iteration 10, Current loss 0.11228208566376222 Accuracy 85.96707818930041\n",
      "Training:: Epoch 169, Iteration 20, Current loss 0.09801931613948196 Accuracy 87.05847557884063\n",
      "Training:: Epoch 169, Iteration 30, Current loss 0.13411253844846782 Accuracy 85.21643835616439\n",
      "Training:: Epoch 169, Iteration 40, Current loss 0.05891031433939957 Accuracy 81.92291676305928\n",
      "Training:: Epoch 169, Iteration 50, Current loss 0.06582682281120103 Accuracy 87.40235178028122\n",
      "Training:: Epoch 169, Iteration 60, Current loss 0.06721290073933939 Accuracy 86.7583638264506\n",
      "Training:: Epoch 169, Iteration 70, Current loss 0.08203964510579943 Accuracy 83.96612618397816\n",
      "Training:: Epoch 169, Iteration 80, Current loss 0.05920587566297798 Accuracy 88.72818885882243\n",
      "Training:: Epoch 169, Iteration 90, Current loss 0.06705699977958021 Accuracy 80.09708737864078\n",
      "Training:: Epoch 169, Iteration 100, Current loss 0.07421901821374828 Accuracy 81.74533567214161\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 169, Probability Accuracy 45.34293822761735\n",
      "Starting Training\n",
      "Training:: Epoch 170, Iteration 0, Current loss 0.037589799041017724 Accuracy 90.6521826271696\n",
      "Training:: Epoch 170, Iteration 10, Current loss 0.06370030399396431 Accuracy 89.1388174807198\n",
      "Training:: Epoch 170, Iteration 20, Current loss 0.07235379912729298 Accuracy 79.03465872847119\n",
      "Training:: Epoch 170, Iteration 30, Current loss 0.20567498036530737 Accuracy 82.83488706750923\n",
      "Training:: Epoch 170, Iteration 40, Current loss 0.06286718473752824 Accuracy 86.25485122897801\n",
      "Training:: Epoch 170, Iteration 50, Current loss 0.05392938903425315 Accuracy 86.98817489374237\n",
      "Training:: Epoch 170, Iteration 60, Current loss 0.03429960673902451 Accuracy 90.16600505232768\n",
      "Training:: Epoch 170, Iteration 70, Current loss 0.04895384790769862 Accuracy 85.64516129032258\n",
      "Training:: Epoch 170, Iteration 80, Current loss 0.05084621300666434 Accuracy 83.16349315902724\n",
      "Training:: Epoch 170, Iteration 90, Current loss 0.07523387808773024 Accuracy 87.68780119058869\n",
      "Training:: Epoch 170, Iteration 100, Current loss 0.04564987382082413 Accuracy 80.0\n",
      "Calculating Expectation\n",
      "Epoch 170 iter 0\n",
      "Epoch 170 iter 10\n",
      "Epoch 170 iter 20\n",
      "Epoch 170 iter 30\n",
      "Epoch 170 iter 40\n",
      "Epoch 170 iter 50\n",
      "Epoch 170 iter 60\n",
      "Epoch 170 iter 70\n",
      "Epoch 170 iter 80\n",
      "Epoch 170 iter 90\n",
      "Epoch 170 iter 100\n",
      "Train Boundary avergage error = 106.304\n",
      "Train From boundary avergage accuracy = 86.081\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 170, Probability Accuracy 46.36926710030244\n",
      "Starting Training\n",
      "Training:: Epoch 171, Iteration 0, Current loss 0.04885983090089929 Accuracy 80.8372122279532\n",
      "Training:: Epoch 171, Iteration 10, Current loss 0.07778184773032898 Accuracy 86.52777777777777\n",
      "Training:: Epoch 171, Iteration 20, Current loss 0.05706457090904899 Accuracy 81.83115274861358\n",
      "Training:: Epoch 171, Iteration 30, Current loss 0.0518526616160575 Accuracy 82.92638317329676\n",
      "Training:: Epoch 171, Iteration 40, Current loss 0.048868640122003516 Accuracy 86.95818272649798\n",
      "Training:: Epoch 171, Iteration 50, Current loss 0.04839761410033705 Accuracy 87.96194291437156\n",
      "Training:: Epoch 171, Iteration 60, Current loss 0.049307384725296756 Accuracy 87.69850965062301\n",
      "Training:: Epoch 171, Iteration 70, Current loss 0.08020600163789832 Accuracy 86.94228249980735\n",
      "Training:: Epoch 171, Iteration 80, Current loss 0.06869963649040532 Accuracy 86.67894413750767\n",
      "Training:: Epoch 171, Iteration 90, Current loss 0.07075642861653496 Accuracy 91.19070369571249\n",
      "Training:: Epoch 171, Iteration 100, Current loss 0.059691620613008096 Accuracy 86.08026525696769\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 171, Probability Accuracy 45.93166093549323\n",
      "Starting Training\n",
      "Training:: Epoch 172, Iteration 0, Current loss 0.0694324699338462 Accuracy 85.99474691715265\n",
      "Training:: Epoch 172, Iteration 10, Current loss 0.13172730934739513 Accuracy 86.77621283255085\n",
      "Training:: Epoch 172, Iteration 20, Current loss 0.0597801928439857 Accuracy 91.31271759164449\n",
      "Training:: Epoch 172, Iteration 30, Current loss 0.1436843875843741 Accuracy 86.63762731407341\n",
      "Training:: Epoch 172, Iteration 40, Current loss 0.1445483973157107 Accuracy 78.57969208863575\n",
      "Training:: Epoch 172, Iteration 50, Current loss 0.1910611650614825 Accuracy 81.01618774688251\n",
      "Training:: Epoch 172, Iteration 60, Current loss 0.10155801340305015 Accuracy 85.42635658914729\n",
      "Training:: Epoch 172, Iteration 70, Current loss 0.1276141427651517 Accuracy 84.3686192658469\n",
      "Training:: Epoch 172, Iteration 80, Current loss 0.10715652178692413 Accuracy 86.0171261487051\n",
      "Training:: Epoch 172, Iteration 90, Current loss 0.07594837535825115 Accuracy 87.22814445200474\n",
      "Training:: Epoch 172, Iteration 100, Current loss 0.08388504791174176 Accuracy 88.2690150791343\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 172, Probability Accuracy 42.201806355387994\n",
      "Starting Training\n",
      "Training:: Epoch 173, Iteration 0, Current loss 0.1182524071899439 Accuracy 78.03373493975904\n",
      "Training:: Epoch 173, Iteration 10, Current loss 0.27418171989272705 Accuracy 80.89571816733815\n",
      "Training:: Epoch 173, Iteration 20, Current loss 0.18729850000373902 Accuracy 84.40760374109465\n",
      "Training:: Epoch 173, Iteration 30, Current loss 0.17205807775555343 Accuracy 86.54356479433879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 173, Iteration 40, Current loss 0.09549806142203462 Accuracy 90.330225944067\n",
      "Training:: Epoch 173, Iteration 50, Current loss 0.1300852289407681 Accuracy 88.4769316286826\n",
      "Training:: Epoch 173, Iteration 60, Current loss 0.15477081926820122 Accuracy 81.0859831094485\n",
      "Training:: Epoch 173, Iteration 70, Current loss 0.1085618959954366 Accuracy 84.21377437669712\n",
      "Training:: Epoch 173, Iteration 80, Current loss 0.21921078596220825 Accuracy 82.20088290174304\n",
      "Training:: Epoch 173, Iteration 90, Current loss 0.16903031627686593 Accuracy 81.54943008946006\n",
      "Training:: Epoch 173, Iteration 100, Current loss 0.13311123994641047 Accuracy 83.1827027027027\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 173, Probability Accuracy 45.638749637486015\n",
      "Starting Training\n",
      "Training:: Epoch 174, Iteration 0, Current loss 0.06058527055933349 Accuracy 84.65075715531594\n",
      "Training:: Epoch 174, Iteration 10, Current loss 0.10821484332346645 Accuracy 84.07884761182714\n",
      "Training:: Epoch 174, Iteration 20, Current loss 0.08106695444090838 Accuracy 85.507851653859\n",
      "Training:: Epoch 174, Iteration 30, Current loss 0.05026162262593372 Accuracy 85.82858356639643\n",
      "Training:: Epoch 174, Iteration 40, Current loss 0.10664035515938079 Accuracy 85.14506959419721\n",
      "Training:: Epoch 174, Iteration 50, Current loss 0.09392634136283347 Accuracy 87.06430868167203\n",
      "Training:: Epoch 174, Iteration 60, Current loss 0.10647840415416437 Accuracy 79.62275701113364\n",
      "Training:: Epoch 174, Iteration 70, Current loss 0.0614443565166182 Accuracy 88.12225752761385\n",
      "Training:: Epoch 174, Iteration 80, Current loss 0.08709415626757072 Accuracy 86.62917479628142\n",
      "Training:: Epoch 174, Iteration 90, Current loss 0.07415250432885323 Accuracy 84.49442675159236\n",
      "Training:: Epoch 174, Iteration 100, Current loss 0.07142479918002297 Accuracy 83.9374316710117\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 174, Probability Accuracy 46.176098935244646\n",
      "Starting Training\n",
      "Training:: Epoch 175, Iteration 0, Current loss 0.0301755635206596 Accuracy 88.36633663366337\n",
      "Training:: Epoch 175, Iteration 10, Current loss 0.06991908730011262 Accuracy 87.22870272180982\n",
      "Training:: Epoch 175, Iteration 20, Current loss 0.08216549828295813 Accuracy 86.42192745885849\n",
      "Training:: Epoch 175, Iteration 30, Current loss 0.09919050215088264 Accuracy 83.04758852747958\n",
      "Training:: Epoch 175, Iteration 40, Current loss 0.0684168973037247 Accuracy 85.18827499905198\n",
      "Training:: Epoch 175, Iteration 50, Current loss 0.04617395141733329 Accuracy 87.77025347912524\n",
      "Training:: Epoch 175, Iteration 60, Current loss 0.053931018507226224 Accuracy 86.46535011336378\n",
      "Training:: Epoch 175, Iteration 70, Current loss 0.06848809458885144 Accuracy 83.760532673384\n",
      "Training:: Epoch 175, Iteration 80, Current loss 0.050987913598418616 Accuracy 86.74933904066474\n",
      "Training:: Epoch 175, Iteration 90, Current loss 0.07645066160213301 Accuracy 86.65413533834587\n",
      "Training:: Epoch 175, Iteration 100, Current loss 0.055756824701410815 Accuracy 86.3548308983776\n",
      "Calculating Expectation\n",
      "Epoch 175 iter 0\n",
      "Epoch 175 iter 10\n",
      "Epoch 175 iter 20\n",
      "Epoch 175 iter 30\n",
      "Epoch 175 iter 40\n",
      "Epoch 175 iter 50\n",
      "Epoch 175 iter 60\n",
      "Epoch 175 iter 70\n",
      "Epoch 175 iter 80\n",
      "Epoch 175 iter 90\n",
      "Epoch 175 iter 100\n",
      "Train Boundary avergage error = 106.681\n",
      "Train From boundary avergage accuracy = 85.921\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 175, Probability Accuracy 45.373700128433526\n",
      "Starting Training\n",
      "Training:: Epoch 176, Iteration 0, Current loss 0.06367894140687563 Accuracy 81.88053903120068\n",
      "Training:: Epoch 176, Iteration 10, Current loss 0.15770202736849515 Accuracy 83.6407903361738\n",
      "Training:: Epoch 176, Iteration 20, Current loss 0.08689723917817291 Accuracy 86.65273026732548\n",
      "Training:: Epoch 176, Iteration 30, Current loss 0.05212649571744641 Accuracy 86.58655120615003\n",
      "Training:: Epoch 176, Iteration 40, Current loss 0.055536791575569906 Accuracy 88.56816233209489\n",
      "Training:: Epoch 176, Iteration 50, Current loss 0.06514621832286574 Accuracy 88.81460744578615\n",
      "Training:: Epoch 176, Iteration 60, Current loss 0.10099686490435779 Accuracy 82.35772070512594\n",
      "Training:: Epoch 176, Iteration 70, Current loss 0.040947681892389294 Accuracy 86.18417911872822\n",
      "Training:: Epoch 176, Iteration 80, Current loss 0.09945733463720333 Accuracy 85.75338412604482\n",
      "Training:: Epoch 176, Iteration 90, Current loss 0.05434187332498589 Accuracy 83.95525674525116\n",
      "Training:: Epoch 176, Iteration 100, Current loss 0.09167199731514812 Accuracy 87.83092727519573\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 176, Probability Accuracy 45.526266727430915\n",
      "Starting Training\n",
      "Training:: Epoch 177, Iteration 0, Current loss 0.0663115701620457 Accuracy 86.26974143955276\n",
      "Training:: Epoch 177, Iteration 10, Current loss 0.09255099519696425 Accuracy 76.86148222289917\n",
      "Training:: Epoch 177, Iteration 20, Current loss 0.04670196495405276 Accuracy 85.1624202769241\n",
      "Training:: Epoch 177, Iteration 30, Current loss 0.10175981972411158 Accuracy 88.2314425528982\n",
      "Training:: Epoch 177, Iteration 40, Current loss 0.040626676623359205 Accuracy 90.96536533495367\n",
      "Training:: Epoch 177, Iteration 50, Current loss 0.05425577697523728 Accuracy 83.67530597552197\n",
      "Training:: Epoch 177, Iteration 60, Current loss 0.06206717265189683 Accuracy 86.44574312657724\n",
      "Training:: Epoch 177, Iteration 70, Current loss 0.07292742164878666 Accuracy 89.22978853136\n",
      "Training:: Epoch 177, Iteration 80, Current loss 0.058645845675596504 Accuracy 83.3011420893517\n",
      "Training:: Epoch 177, Iteration 90, Current loss 0.08564301840213766 Accuracy 83.40728744939271\n",
      "Training:: Epoch 177, Iteration 100, Current loss 0.06570773473330097 Accuracy 89.79254635579218\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 177, Probability Accuracy 45.44185275717778\n",
      "Starting Training\n",
      "Training:: Epoch 178, Iteration 0, Current loss 0.04460702161314673 Accuracy 88.9811605641536\n",
      "Training:: Epoch 178, Iteration 10, Current loss 0.15966224873104098 Accuracy 80.25694034258713\n",
      "Training:: Epoch 178, Iteration 20, Current loss 0.09572289067536466 Accuracy 87.22175152122006\n",
      "Training:: Epoch 178, Iteration 30, Current loss 0.10068696184534144 Accuracy 76.37329202854455\n",
      "Training:: Epoch 178, Iteration 40, Current loss 0.10378714261526602 Accuracy 85.2793924935318\n",
      "Training:: Epoch 178, Iteration 50, Current loss 0.08805571194711835 Accuracy 88.56480645355488\n",
      "Training:: Epoch 178, Iteration 60, Current loss 0.07960427457143883 Accuracy 88.44874405974203\n",
      "Training:: Epoch 178, Iteration 70, Current loss 0.09318647713602397 Accuracy 88.48215096739031\n",
      "Training:: Epoch 178, Iteration 80, Current loss 0.09773153074998514 Accuracy 85.14071498424427\n",
      "Training:: Epoch 178, Iteration 90, Current loss 0.04501348166777303 Accuracy 84.41582097319802\n",
      "Training:: Epoch 178, Iteration 100, Current loss 0.10195088406434923 Accuracy 88.63559033684926\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 178, Probability Accuracy 44.026701744210136\n",
      "Starting Training\n",
      "Training:: Epoch 179, Iteration 0, Current loss 0.06546813033318163 Accuracy 85.84095707248417\n",
      "Training:: Epoch 179, Iteration 10, Current loss 0.09223557042021333 Accuracy 81.55287091081851\n",
      "Training:: Epoch 179, Iteration 20, Current loss 0.09431290062945584 Accuracy 88.6199421965318\n",
      "Training:: Epoch 179, Iteration 30, Current loss 0.12398294439896454 Accuracy 86.9294299830396\n",
      "Training:: Epoch 179, Iteration 40, Current loss 0.08939975631530656 Accuracy 86.2508955682167\n",
      "Training:: Epoch 179, Iteration 50, Current loss 0.2177840672102571 Accuracy 82.09050664043286\n",
      "Training:: Epoch 179, Iteration 60, Current loss 0.10712660031960496 Accuracy 86.02372156328991\n",
      "Training:: Epoch 179, Iteration 70, Current loss 0.07889834607439902 Accuracy 91.54373641077103\n",
      "Training:: Epoch 179, Iteration 80, Current loss 0.06697691721369714 Accuracy 89.40932910655367\n",
      "Training:: Epoch 179, Iteration 90, Current loss 0.3002348829272709 Accuracy 86.75927029927269\n",
      "Training:: Epoch 179, Iteration 100, Current loss 0.15635907200423324 Accuracy 82.92690621155242\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 179, Probability Accuracy 43.26801176616812\n",
      "Starting Training\n",
      "Training:: Epoch 180, Iteration 0, Current loss 0.12374562720067425 Accuracy 79.89861058956065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 180, Iteration 10, Current loss 0.30654642947534827 Accuracy 81.93979933110369\n",
      "Training:: Epoch 180, Iteration 20, Current loss 0.196518209193121 Accuracy 83.98761649431891\n",
      "Training:: Epoch 180, Iteration 30, Current loss 0.13770133651487118 Accuracy 85.99686643164904\n",
      "Training:: Epoch 180, Iteration 40, Current loss 0.0833273539415442 Accuracy 87.63903627716857\n",
      "Training:: Epoch 180, Iteration 50, Current loss 0.39875006499302973 Accuracy 77.39754524651549\n",
      "Training:: Epoch 180, Iteration 60, Current loss 0.08048904056349059 Accuracy 86.29622882127892\n",
      "Training:: Epoch 180, Iteration 70, Current loss 0.09912621530880451 Accuracy 89.27552447552448\n",
      "Training:: Epoch 180, Iteration 80, Current loss 0.12010648510013228 Accuracy 86.75416904444867\n",
      "Training:: Epoch 180, Iteration 90, Current loss 0.0812689139129516 Accuracy 85.74324324324324\n",
      "Training:: Epoch 180, Iteration 100, Current loss 0.1394855186867165 Accuracy 80.44428548720394\n",
      "Calculating Expectation\n",
      "Epoch 180 iter 0\n",
      "Epoch 180 iter 10\n",
      "Epoch 180 iter 20\n",
      "Epoch 180 iter 30\n",
      "Epoch 180 iter 40\n",
      "Epoch 180 iter 50\n",
      "Epoch 180 iter 60\n",
      "Epoch 180 iter 70\n",
      "Epoch 180 iter 80\n",
      "Epoch 180 iter 90\n",
      "Epoch 180 iter 100\n",
      "Train Boundary avergage error = 107.029\n",
      "Train From boundary avergage accuracy = 85.752\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 180, Probability Accuracy 45.11849028462527\n",
      "Starting Training\n",
      "Training:: Epoch 181, Iteration 0, Current loss 0.10594610770736435 Accuracy 82.10634951838018\n",
      "Training:: Epoch 181, Iteration 10, Current loss 0.18460248940311774 Accuracy 78.52413352207863\n",
      "Training:: Epoch 181, Iteration 20, Current loss 0.18156871215547232 Accuracy 82.37833295133318\n",
      "Training:: Epoch 181, Iteration 30, Current loss 0.17598141382166502 Accuracy 80.53544815839898\n",
      "Training:: Epoch 181, Iteration 40, Current loss 0.10896741781480582 Accuracy 88.6732869154967\n",
      "Training:: Epoch 181, Iteration 50, Current loss 0.09956071104792177 Accuracy 86.0410164957646\n",
      "Training:: Epoch 181, Iteration 60, Current loss 0.2004989455573707 Accuracy 87.20275972938576\n",
      "Training:: Epoch 181, Iteration 70, Current loss 0.15136443737397567 Accuracy 86.14260346947138\n",
      "Training:: Epoch 181, Iteration 80, Current loss 0.076792472614758 Accuracy 82.23465220822895\n",
      "Training:: Epoch 181, Iteration 90, Current loss 0.10712161099747894 Accuracy 87.48516088722275\n",
      "Training:: Epoch 181, Iteration 100, Current loss 0.13485836061025996 Accuracy 86.00215299737603\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 181, Probability Accuracy 45.23449475908357\n",
      "Starting Training\n",
      "Training:: Epoch 182, Iteration 0, Current loss 0.08225371628242342 Accuracy 85.1842361502588\n",
      "Training:: Epoch 182, Iteration 10, Current loss 0.07473269461777764 Accuracy 85.18837218002918\n",
      "Training:: Epoch 182, Iteration 20, Current loss 0.15554424183001253 Accuracy 79.16193111627217\n",
      "Training:: Epoch 182, Iteration 30, Current loss 0.12464570451567183 Accuracy 83.36899907134493\n",
      "Training:: Epoch 182, Iteration 40, Current loss 0.11145157254857048 Accuracy 84.20658194851744\n",
      "Training:: Epoch 182, Iteration 50, Current loss 0.0993105289284177 Accuracy 88.1732606695449\n",
      "Training:: Epoch 182, Iteration 60, Current loss 0.07324764903743715 Accuracy 88.6658453511124\n",
      "Training:: Epoch 182, Iteration 70, Current loss 0.10686359422339285 Accuracy 88.2062352110922\n",
      "Training:: Epoch 182, Iteration 80, Current loss 0.09206646769339003 Accuracy 89.60617133576939\n",
      "Training:: Epoch 182, Iteration 90, Current loss 0.13027948934731967 Accuracy 86.48532158421531\n",
      "Training:: Epoch 182, Iteration 100, Current loss 0.073521062809015 Accuracy 86.8648852088822\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 182, Probability Accuracy 43.57759870737871\n",
      "Starting Training\n",
      "Training:: Epoch 183, Iteration 0, Current loss 0.11912856169199851 Accuracy 86.68656221503772\n",
      "Training:: Epoch 183, Iteration 10, Current loss 0.12952174185129542 Accuracy 83.00624877953524\n",
      "Training:: Epoch 183, Iteration 20, Current loss 0.08514609164576747 Accuracy 89.67455773892414\n",
      "Training:: Epoch 183, Iteration 30, Current loss 0.08542494522788042 Accuracy 87.81376906596745\n",
      "Training:: Epoch 183, Iteration 40, Current loss 0.13419088690261824 Accuracy 82.1524589485875\n",
      "Training:: Epoch 183, Iteration 50, Current loss 0.06888973142945204 Accuracy 84.74075047644082\n",
      "Training:: Epoch 183, Iteration 60, Current loss 0.06032792245771315 Accuracy 90.44059405940594\n",
      "Training:: Epoch 183, Iteration 70, Current loss 0.07910226671491095 Accuracy 87.60914592048086\n",
      "Training:: Epoch 183, Iteration 80, Current loss 0.1084943434119202 Accuracy 85.4100325286766\n",
      "Training:: Epoch 183, Iteration 90, Current loss 0.12231750193264046 Accuracy 82.7009836550551\n",
      "Training:: Epoch 183, Iteration 100, Current loss 0.09076472920475628 Accuracy 88.69175781586426\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 183, Probability Accuracy 45.0052823466048\n",
      "Starting Training\n",
      "Training:: Epoch 184, Iteration 0, Current loss 0.06295987994310069 Accuracy 80.10075566750629\n",
      "Training:: Epoch 184, Iteration 10, Current loss 0.13379020370458114 Accuracy 73.86519944979368\n",
      "Training:: Epoch 184, Iteration 20, Current loss 0.06435347951219902 Accuracy 84.53746462088485\n",
      "Training:: Epoch 184, Iteration 30, Current loss 0.06322711969420379 Accuracy 89.42293261293712\n",
      "Training:: Epoch 184, Iteration 40, Current loss 0.11009726513022014 Accuracy 87.27836116371148\n",
      "Training:: Epoch 184, Iteration 50, Current loss 0.0676666055586779 Accuracy 87.96587874197792\n",
      "Training:: Epoch 184, Iteration 60, Current loss 0.08168280983647219 Accuracy 84.88394036610681\n",
      "Training:: Epoch 184, Iteration 70, Current loss 0.05214086667572701 Accuracy 83.34627786015204\n",
      "Training:: Epoch 184, Iteration 80, Current loss 0.0879666666787234 Accuracy 86.02748395262634\n",
      "Training:: Epoch 184, Iteration 90, Current loss 0.0829239387567018 Accuracy 89.59352335975713\n",
      "Training:: Epoch 184, Iteration 100, Current loss 0.057765651456573835 Accuracy 89.31999839338073\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 184, Probability Accuracy 46.29966441562746\n",
      "Starting Training\n",
      "Training:: Epoch 185, Iteration 0, Current loss 0.09575691146373717 Accuracy 82.510403080554\n",
      "Training:: Epoch 185, Iteration 10, Current loss 0.05160258648788943 Accuracy 87.52347172960567\n",
      "Training:: Epoch 185, Iteration 20, Current loss 0.08912967633167113 Accuracy 84.42932261057841\n",
      "Training:: Epoch 185, Iteration 30, Current loss 0.0674339316880086 Accuracy 88.24619771863118\n",
      "Training:: Epoch 185, Iteration 40, Current loss 0.06805656089705754 Accuracy 81.74942096120441\n",
      "Training:: Epoch 185, Iteration 50, Current loss 0.07611421832896108 Accuracy 88.87764445086984\n",
      "Training:: Epoch 185, Iteration 60, Current loss 0.09914946420030352 Accuracy 85.09281294621609\n",
      "Training:: Epoch 185, Iteration 70, Current loss 0.060372255986669086 Accuracy 86.83536897535288\n",
      "Training:: Epoch 185, Iteration 80, Current loss 0.05840813470813569 Accuracy 88.2491570283891\n",
      "Training:: Epoch 185, Iteration 90, Current loss 0.10524612988281974 Accuracy 86.18666424088056\n",
      "Training:: Epoch 185, Iteration 100, Current loss 0.06652060138938115 Accuracy 81.48092670261634\n",
      "Calculating Expectation\n",
      "Epoch 185 iter 0\n",
      "Epoch 185 iter 10\n",
      "Epoch 185 iter 20\n",
      "Epoch 185 iter 30\n",
      "Epoch 185 iter 40\n",
      "Epoch 185 iter 50\n",
      "Epoch 185 iter 60\n",
      "Epoch 185 iter 70\n",
      "Epoch 185 iter 80\n",
      "Epoch 185 iter 90\n",
      "Epoch 185 iter 100\n",
      "Train Boundary avergage error = 107.609\n",
      "Train From boundary avergage accuracy = 85.745\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 185, Probability Accuracy 45.251170402286945\n",
      "Starting Training\n",
      "Training:: Epoch 186, Iteration 0, Current loss 0.0766889529817354 Accuracy 79.61193305845258\n",
      "Training:: Epoch 186, Iteration 10, Current loss 0.10459335911004865 Accuracy 82.63473053892216\n",
      "Training:: Epoch 186, Iteration 20, Current loss 0.10165797044053403 Accuracy 85.70464031426467\n",
      "Training:: Epoch 186, Iteration 30, Current loss 0.08157293876856199 Accuracy 86.27326169169132\n",
      "Training:: Epoch 186, Iteration 40, Current loss 0.10495507012648683 Accuracy 84.54461821527138\n",
      "Training:: Epoch 186, Iteration 50, Current loss 0.12012023674682568 Accuracy 82.3207443897099\n",
      "Training:: Epoch 186, Iteration 60, Current loss 0.09547892943185218 Accuracy 82.48037516566419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 186, Iteration 70, Current loss 0.07350484381434395 Accuracy 86.93467336683418\n",
      "Training:: Epoch 186, Iteration 80, Current loss 0.06751493053190813 Accuracy 89.28311763850516\n",
      "Training:: Epoch 186, Iteration 90, Current loss 0.2836777554179629 Accuracy 80.95892626131953\n",
      "Training:: Epoch 186, Iteration 100, Current loss 0.0555690295401295 Accuracy 88.30267956239099\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 186, Probability Accuracy 44.93992625429838\n",
      "Starting Training\n",
      "Training:: Epoch 187, Iteration 0, Current loss 0.22664328710727263 Accuracy 84.35644947272854\n",
      "Training:: Epoch 187, Iteration 10, Current loss 0.12633095800980418 Accuracy 84.6617915904936\n",
      "Training:: Epoch 187, Iteration 20, Current loss 0.07406122843052296 Accuracy 87.50049133288786\n",
      "Training:: Epoch 187, Iteration 30, Current loss 0.13865418115849076 Accuracy 83.89731026386976\n",
      "Training:: Epoch 187, Iteration 40, Current loss 0.07477801589098422 Accuracy 86.55918889294848\n",
      "Training:: Epoch 187, Iteration 50, Current loss 0.06497979891864766 Accuracy 91.84125227290694\n",
      "Training:: Epoch 187, Iteration 60, Current loss 0.09671470738493076 Accuracy 85.17410228509249\n",
      "Training:: Epoch 187, Iteration 70, Current loss 0.0768083322696532 Accuracy 89.49310172552939\n",
      "Training:: Epoch 187, Iteration 80, Current loss 0.23779315396059117 Accuracy 85.35328992127515\n",
      "Training:: Epoch 187, Iteration 90, Current loss 0.17545559375936365 Accuracy 82.66591922786321\n",
      "Training:: Epoch 187, Iteration 100, Current loss 0.13579932380229034 Accuracy 85.00487142393197\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 187, Probability Accuracy 41.97280109375647\n",
      "Starting Training\n",
      "Training:: Epoch 188, Iteration 0, Current loss 0.09110562376111411 Accuracy 85.65273231967518\n",
      "Training:: Epoch 188, Iteration 10, Current loss 0.09989534374567821 Accuracy 82.33542390470605\n",
      "Training:: Epoch 188, Iteration 20, Current loss 0.0785624574612034 Accuracy 85.55421201128577\n",
      "Training:: Epoch 188, Iteration 30, Current loss 0.09827142950024444 Accuracy 86.10540223135644\n",
      "Training:: Epoch 188, Iteration 40, Current loss 0.08862574844670028 Accuracy 87.99351626722242\n",
      "Training:: Epoch 188, Iteration 50, Current loss 0.10129834333431972 Accuracy 87.69420678499979\n",
      "Training:: Epoch 188, Iteration 60, Current loss 0.09284331088311988 Accuracy 83.94253500636479\n",
      "Training:: Epoch 188, Iteration 70, Current loss 0.1223759315161443 Accuracy 87.68105340160936\n",
      "Training:: Epoch 188, Iteration 80, Current loss 0.06857958167056476 Accuracy 88.76287370220572\n",
      "Training:: Epoch 188, Iteration 90, Current loss 0.2325193079251192 Accuracy 80.76216533124878\n",
      "Training:: Epoch 188, Iteration 100, Current loss 0.0774392778562596 Accuracy 83.88252391091284\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 188, Probability Accuracy 44.768198201930645\n",
      "Starting Training\n",
      "Training:: Epoch 189, Iteration 0, Current loss 0.06899460126309337 Accuracy 88.99841017488076\n",
      "Training:: Epoch 189, Iteration 10, Current loss 0.17896404350789266 Accuracy 81.10915492957747\n",
      "Training:: Epoch 189, Iteration 20, Current loss 0.17804202147993073 Accuracy 86.4938684503902\n",
      "Training:: Epoch 189, Iteration 30, Current loss 0.0894960935994807 Accuracy 84.91514500537058\n",
      "Training:: Epoch 189, Iteration 40, Current loss 0.20684032803912952 Accuracy 83.74639461595983\n",
      "Training:: Epoch 189, Iteration 50, Current loss 0.1189793827079539 Accuracy 88.8441596083862\n",
      "Training:: Epoch 189, Iteration 60, Current loss 0.0763755341871642 Accuracy 86.0643097835399\n",
      "Training:: Epoch 189, Iteration 70, Current loss 0.08869026268730695 Accuracy 89.05526020582354\n",
      "Training:: Epoch 189, Iteration 80, Current loss 0.11990621343213698 Accuracy 88.35376532399299\n",
      "Training:: Epoch 189, Iteration 90, Current loss 0.11227820293744606 Accuracy 87.29223414308744\n",
      "Training:: Epoch 189, Iteration 100, Current loss 0.1303216946815751 Accuracy 87.15760495526497\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 189, Probability Accuracy 44.905332062808135\n",
      "Starting Training\n",
      "Training:: Epoch 190, Iteration 0, Current loss 0.06085061775906217 Accuracy 87.91492693110648\n",
      "Training:: Epoch 190, Iteration 10, Current loss 0.05622151025678472 Accuracy 88.688744685612\n",
      "Training:: Epoch 190, Iteration 20, Current loss 0.09052019826008974 Accuracy 86.80812250972109\n",
      "Training:: Epoch 190, Iteration 30, Current loss 0.06011748333567712 Accuracy 88.52714328092554\n",
      "Training:: Epoch 190, Iteration 40, Current loss 0.07938727619551951 Accuracy 81.23045547246771\n",
      "Training:: Epoch 190, Iteration 50, Current loss 0.055052816894813235 Accuracy 87.62138271445623\n",
      "Training:: Epoch 190, Iteration 60, Current loss 0.046710755773047063 Accuracy 89.71370012169346\n",
      "Training:: Epoch 190, Iteration 70, Current loss 0.04087872202490174 Accuracy 88.02019776982958\n",
      "Training:: Epoch 190, Iteration 80, Current loss 0.03868554627036283 Accuracy 88.86692552064949\n",
      "Training:: Epoch 190, Iteration 90, Current loss 0.08616875392426229 Accuracy 83.06785270997104\n",
      "Training:: Epoch 190, Iteration 100, Current loss 0.14713571546233759 Accuracy 86.99560999973713\n",
      "Calculating Expectation\n",
      "Epoch 190 iter 0\n",
      "Epoch 190 iter 10\n",
      "Epoch 190 iter 20\n",
      "Epoch 190 iter 30\n",
      "Epoch 190 iter 40\n",
      "Epoch 190 iter 50\n",
      "Epoch 190 iter 60\n",
      "Epoch 190 iter 70\n",
      "Epoch 190 iter 80\n",
      "Epoch 190 iter 90\n",
      "Epoch 190 iter 100\n",
      "Train Boundary avergage error = 107.913\n",
      "Train From boundary avergage accuracy = 85.796\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 190, Probability Accuracy 45.10336827277624\n",
      "Starting Training\n",
      "Training:: Epoch 191, Iteration 0, Current loss 0.10995273669355723 Accuracy 81.42668209621425\n",
      "Training:: Epoch 191, Iteration 10, Current loss 0.1764241271044034 Accuracy 81.14363794459857\n",
      "Training:: Epoch 191, Iteration 20, Current loss 0.04093991987281452 Accuracy 88.06877752149298\n",
      "Training:: Epoch 191, Iteration 30, Current loss 0.07536909807128198 Accuracy 85.77471297053283\n",
      "Training:: Epoch 191, Iteration 40, Current loss 0.07102817802526261 Accuracy 86.45011442429211\n",
      "Training:: Epoch 191, Iteration 50, Current loss 0.06325929085571178 Accuracy 79.86993013138814\n",
      "Training:: Epoch 191, Iteration 60, Current loss 0.09900050745393428 Accuracy 84.93068113321277\n",
      "Training:: Epoch 191, Iteration 70, Current loss 0.0737716166737606 Accuracy 87.17846097871566\n",
      "Training:: Epoch 191, Iteration 80, Current loss 0.10427245808609942 Accuracy 83.97043428607684\n",
      "Training:: Epoch 191, Iteration 90, Current loss 0.06839229805041706 Accuracy 84.29204491674913\n",
      "Training:: Epoch 191, Iteration 100, Current loss 0.08000183241490082 Accuracy 85.45680735503838\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 191, Probability Accuracy 44.15627459916311\n",
      "Starting Training\n",
      "Training:: Epoch 192, Iteration 0, Current loss 0.17923050594753132 Accuracy 86.31396957123098\n",
      "Training:: Epoch 192, Iteration 10, Current loss 0.04989112842777862 Accuracy 86.03384474658019\n",
      "Training:: Epoch 192, Iteration 20, Current loss 0.10923196559833766 Accuracy 83.99161132471164\n",
      "Training:: Epoch 192, Iteration 30, Current loss 0.05784071224381168 Accuracy 90.1085701085701\n",
      "Training:: Epoch 192, Iteration 40, Current loss 0.07188795487039776 Accuracy 87.48624484181568\n",
      "Training:: Epoch 192, Iteration 50, Current loss 0.06515876245500904 Accuracy 91.4386584289497\n",
      "Training:: Epoch 192, Iteration 60, Current loss 0.04759488012548948 Accuracy 89.55037951213859\n",
      "Training:: Epoch 192, Iteration 70, Current loss 0.19176127254754669 Accuracy 82.6937372235644\n",
      "Training:: Epoch 192, Iteration 80, Current loss 0.08724026414669957 Accuracy 86.68127053669222\n",
      "Training:: Epoch 192, Iteration 90, Current loss 0.1659585536669634 Accuracy 84.36607992388201\n",
      "Training:: Epoch 192, Iteration 100, Current loss 0.5718413791697455 Accuracy 70.79277178665112\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 192, Probability Accuracy 40.555164270621866\n",
      "Starting Training\n",
      "Training:: Epoch 193, Iteration 0, Current loss 0.0595700288276508 Accuracy 83.16988294843404\n",
      "Training:: Epoch 193, Iteration 10, Current loss 0.11914729718365756 Accuracy 83.62408350489399\n",
      "Training:: Epoch 193, Iteration 20, Current loss 0.26501510676249435 Accuracy 83.71265009773806\n",
      "Training:: Epoch 193, Iteration 30, Current loss 0.10231847755467681 Accuracy 86.78787878787878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 193, Iteration 40, Current loss 0.08542446541989346 Accuracy 87.82803306501101\n",
      "Training:: Epoch 193, Iteration 50, Current loss 0.07910002662059003 Accuracy 81.70340175318351\n",
      "Training:: Epoch 193, Iteration 60, Current loss 0.07813169512260065 Accuracy 83.30419292052915\n",
      "Training:: Epoch 193, Iteration 70, Current loss 0.07157557810229119 Accuracy 82.73813474941919\n",
      "Training:: Epoch 193, Iteration 80, Current loss 0.07848058794605763 Accuracy 89.61829102887037\n",
      "Training:: Epoch 193, Iteration 90, Current loss 0.13592164897214237 Accuracy 86.69490089784856\n",
      "Training:: Epoch 193, Iteration 100, Current loss 0.09246409869792499 Accuracy 81.57427580085532\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 193, Probability Accuracy 46.75663918465427\n",
      "Starting Training\n",
      "Training:: Epoch 194, Iteration 0, Current loss 0.09878059139723626 Accuracy 83.28334648776638\n",
      "Training:: Epoch 194, Iteration 10, Current loss 0.08455163589710352 Accuracy 82.29239054899236\n",
      "Training:: Epoch 194, Iteration 20, Current loss 0.10425041685449309 Accuracy 88.16397950256219\n",
      "Training:: Epoch 194, Iteration 30, Current loss 0.08749464774496717 Accuracy 82.92231579742437\n",
      "Training:: Epoch 194, Iteration 40, Current loss 0.08018910955236881 Accuracy 88.16781247411579\n",
      "Training:: Epoch 194, Iteration 50, Current loss 0.08084108872168189 Accuracy 82.9278101749447\n",
      "Training:: Epoch 194, Iteration 60, Current loss 0.06500756275919589 Accuracy 84.95080959212953\n",
      "Training:: Epoch 194, Iteration 70, Current loss 0.05079706178489456 Accuracy 88.04529539791187\n",
      "Training:: Epoch 194, Iteration 80, Current loss 0.09102253761372975 Accuracy 84.41997162353896\n",
      "Training:: Epoch 194, Iteration 90, Current loss 0.10485480318519816 Accuracy 84.02077151335311\n",
      "Training:: Epoch 194, Iteration 100, Current loss 0.1966390088822592 Accuracy 87.03683035714286\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 194, Probability Accuracy 45.268053196337576\n",
      "Starting Training\n",
      "Training:: Epoch 195, Iteration 0, Current loss 0.06090294522722304 Accuracy 88.76629889669007\n",
      "Training:: Epoch 195, Iteration 10, Current loss 0.0556234309271648 Accuracy 89.38108310456701\n",
      "Training:: Epoch 195, Iteration 20, Current loss 0.08915741976100126 Accuracy 82.79749199573105\n",
      "Training:: Epoch 195, Iteration 30, Current loss 0.08558007533247856 Accuracy 85.78226896187063\n",
      "Training:: Epoch 195, Iteration 40, Current loss 0.07562288055604553 Accuracy 88.43361396462501\n",
      "Training:: Epoch 195, Iteration 50, Current loss 0.06622402613839064 Accuracy 86.11324854026162\n",
      "Training:: Epoch 195, Iteration 60, Current loss 0.05490343347248721 Accuracy 82.04051075157011\n",
      "Training:: Epoch 195, Iteration 70, Current loss 0.05504170428709581 Accuracy 89.7748766828617\n",
      "Training:: Epoch 195, Iteration 80, Current loss 0.05983418188074628 Accuracy 85.87249670052218\n",
      "Training:: Epoch 195, Iteration 90, Current loss 0.06160046416682917 Accuracy 88.58654572940287\n",
      "Training:: Epoch 195, Iteration 100, Current loss 0.056524881738989737 Accuracy 83.90930239076208\n",
      "Calculating Expectation\n",
      "Epoch 195 iter 0\n",
      "Epoch 195 iter 10\n",
      "Epoch 195 iter 20\n",
      "Epoch 195 iter 30\n",
      "Epoch 195 iter 40\n",
      "Epoch 195 iter 50\n",
      "Epoch 195 iter 60\n",
      "Epoch 195 iter 70\n",
      "Epoch 195 iter 80\n",
      "Epoch 195 iter 90\n",
      "Epoch 195 iter 100\n",
      "Train Boundary avergage error = 108.461\n",
      "Train From boundary avergage accuracy = 85.738\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 195, Probability Accuracy 46.44373782988772\n",
      "Starting Training\n",
      "Training:: Epoch 196, Iteration 0, Current loss 0.05692576873788374 Accuracy 83.19498697916667\n",
      "Training:: Epoch 196, Iteration 10, Current loss 0.06125733719977037 Accuracy 88.21730669949565\n",
      "Training:: Epoch 196, Iteration 20, Current loss 0.0729949300119598 Accuracy 82.23945672724467\n",
      "Training:: Epoch 196, Iteration 30, Current loss 0.04837234912889536 Accuracy 85.69394096323148\n",
      "Training:: Epoch 196, Iteration 40, Current loss 0.10374865146099667 Accuracy 90.3944623553954\n",
      "Training:: Epoch 196, Iteration 50, Current loss 0.09637870087502791 Accuracy 88.96214896214896\n",
      "Training:: Epoch 196, Iteration 60, Current loss 0.09566753703837823 Accuracy 81.73033875540965\n",
      "Training:: Epoch 196, Iteration 70, Current loss 0.05803100300053356 Accuracy 85.69354326421345\n",
      "Training:: Epoch 196, Iteration 80, Current loss 0.0889014132365759 Accuracy 85.359477124183\n",
      "Training:: Epoch 196, Iteration 90, Current loss 0.05796833618755612 Accuracy 86.68024003429062\n",
      "Training:: Epoch 196, Iteration 100, Current loss 0.08126595933147616 Accuracy 83.19756180927423\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 196, Probability Accuracy 45.87966607283424\n",
      "Starting Training\n",
      "Training:: Epoch 197, Iteration 0, Current loss 0.06082639885004551 Accuracy 82.67342413683878\n",
      "Training:: Epoch 197, Iteration 10, Current loss 0.09076239388077362 Accuracy 84.77059812635119\n",
      "Training:: Epoch 197, Iteration 20, Current loss 0.09323926505840503 Accuracy 84.02384390388389\n",
      "Training:: Epoch 197, Iteration 30, Current loss 0.08158788423379358 Accuracy 87.38785934123501\n",
      "Training:: Epoch 197, Iteration 40, Current loss 0.07187691193030857 Accuracy 83.13564562370004\n",
      "Training:: Epoch 197, Iteration 50, Current loss 0.07635000452015098 Accuracy 84.19864559819413\n",
      "Training:: Epoch 197, Iteration 60, Current loss 0.07353312827114881 Accuracy 86.69299187241221\n",
      "Training:: Epoch 197, Iteration 70, Current loss 0.08854405079160399 Accuracy 85.24866372298396\n",
      "Training:: Epoch 197, Iteration 80, Current loss 0.07153892434489566 Accuracy 85.58427653716733\n",
      "Training:: Epoch 197, Iteration 90, Current loss 0.09961746499356285 Accuracy 80.86711247741894\n",
      "Training:: Epoch 197, Iteration 100, Current loss 0.05980297084109219 Accuracy 88.80916241203256\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 197, Probability Accuracy 45.612545055309276\n",
      "Starting Training\n",
      "Training:: Epoch 198, Iteration 0, Current loss 0.05416640808393271 Accuracy 77.9644661406596\n",
      "Training:: Epoch 198, Iteration 10, Current loss 0.04697046886486832 Accuracy 88.83756735950732\n",
      "Training:: Epoch 198, Iteration 20, Current loss 0.0653970693289565 Accuracy 84.15272284758139\n",
      "Training:: Epoch 198, Iteration 30, Current loss 0.04931553515139326 Accuracy 88.8543823326432\n",
      "Training:: Epoch 198, Iteration 40, Current loss 0.03684669922244543 Accuracy 86.56806189682041\n",
      "Training:: Epoch 198, Iteration 50, Current loss 0.08029864695681863 Accuracy 86.67027661206625\n",
      "Training:: Epoch 198, Iteration 60, Current loss 0.05645113087258643 Accuracy 91.55017996149661\n",
      "Training:: Epoch 198, Iteration 70, Current loss 0.05059354046534767 Accuracy 84.68006157975763\n",
      "Training:: Epoch 198, Iteration 80, Current loss 0.03792979864950759 Accuracy 86.18389934727084\n",
      "Training:: Epoch 198, Iteration 90, Current loss 0.07156112536721795 Accuracy 81.49879586343675\n",
      "Training:: Epoch 198, Iteration 100, Current loss 0.055799993891541146 Accuracy 86.06469808982523\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 198, Probability Accuracy 45.144176989683885\n",
      "Starting Training\n",
      "Training:: Epoch 199, Iteration 0, Current loss 0.03535807781665987 Accuracy 84.9950209121689\n",
      "Training:: Epoch 199, Iteration 10, Current loss 0.06203238509609956 Accuracy 83.92820791205808\n",
      "Training:: Epoch 199, Iteration 20, Current loss 0.05437548633653461 Accuracy 85.31815653215799\n",
      "Training:: Epoch 199, Iteration 30, Current loss 0.09412866231334835 Accuracy 84.58446778166731\n",
      "Training:: Epoch 199, Iteration 40, Current loss 0.03855657986790076 Accuracy 86.95322239687788\n",
      "Training:: Epoch 199, Iteration 50, Current loss 0.16414708630253652 Accuracy 77.61872482953866\n",
      "Training:: Epoch 199, Iteration 60, Current loss 0.05296486041932194 Accuracy 90.27588407647008\n",
      "Training:: Epoch 199, Iteration 70, Current loss 0.046961017403912224 Accuracy 88.87490723711807\n",
      "Training:: Epoch 199, Iteration 80, Current loss 0.044439401972825754 Accuracy 89.69434416365824\n",
      "Training:: Epoch 199, Iteration 90, Current loss 0.052138792509105 Accuracy 87.00752148997135\n",
      "Training:: Epoch 199, Iteration 100, Current loss 0.08562179881384023 Accuracy 82.78237650200268\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 199, Probability Accuracy 45.53859220284211\n",
      "Starting Training\n",
      "Training:: Epoch 200, Iteration 0, Current loss 0.03770606680721267 Accuracy 84.84642060936339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 200, Iteration 10, Current loss 0.07021467750892457 Accuracy 84.67328985951245\n",
      "Training:: Epoch 200, Iteration 20, Current loss 0.05111508268922559 Accuracy 84.48196513835356\n",
      "Training:: Epoch 200, Iteration 30, Current loss 0.05161236442488505 Accuracy 87.01298701298701\n",
      "Training:: Epoch 200, Iteration 40, Current loss 0.07807971626906787 Accuracy 79.68863142650254\n",
      "Training:: Epoch 200, Iteration 50, Current loss 0.05754822136855549 Accuracy 90.60293157676102\n",
      "Training:: Epoch 200, Iteration 60, Current loss 0.1014319360280186 Accuracy 83.82229673093043\n",
      "Training:: Epoch 200, Iteration 70, Current loss 0.09600482361827854 Accuracy 84.48791089377886\n",
      "Training:: Epoch 200, Iteration 80, Current loss 0.04723137761156785 Accuracy 89.06425102726934\n",
      "Training:: Epoch 200, Iteration 90, Current loss 0.06179567691043495 Accuracy 87.07005423074065\n",
      "Training:: Epoch 200, Iteration 100, Current loss 0.06924049805771848 Accuracy 85.249438978077\n",
      "Calculating Expectation\n",
      "Epoch 200 iter 0\n",
      "Epoch 200 iter 10\n",
      "Epoch 200 iter 20\n",
      "Epoch 200 iter 30\n",
      "Epoch 200 iter 40\n",
      "Epoch 200 iter 50\n",
      "Epoch 200 iter 60\n",
      "Epoch 200 iter 70\n",
      "Epoch 200 iter 80\n",
      "Epoch 200 iter 90\n",
      "Epoch 200 iter 100\n",
      "Train Boundary avergage error = 111.443\n",
      "Train From boundary avergage accuracy = 85.600\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 200, Probability Accuracy 44.71319965198658\n",
      "Starting Training\n",
      "Training:: Epoch 201, Iteration 0, Current loss 0.05224775463645539 Accuracy 76.84325505188421\n",
      "Training:: Epoch 201, Iteration 10, Current loss 0.05835874314271803 Accuracy 88.50678733031674\n",
      "Training:: Epoch 201, Iteration 20, Current loss 0.14074389785821556 Accuracy 87.28090072998721\n",
      "Training:: Epoch 201, Iteration 30, Current loss 0.06520349448155249 Accuracy 82.38963241082783\n",
      "Training:: Epoch 201, Iteration 40, Current loss 0.08872777409174022 Accuracy 83.1051716644937\n",
      "Training:: Epoch 201, Iteration 50, Current loss 0.0478238185005803 Accuracy 91.03363567649282\n",
      "Training:: Epoch 201, Iteration 60, Current loss 0.04924535650249987 Accuracy 85.75219197046609\n",
      "Training:: Epoch 201, Iteration 70, Current loss 0.06300205953101985 Accuracy 86.98792890409656\n",
      "Training:: Epoch 201, Iteration 80, Current loss 0.05373353730810314 Accuracy 86.25852033987987\n",
      "Training:: Epoch 201, Iteration 90, Current loss 0.05997928354803685 Accuracy 87.15162412539577\n",
      "Training:: Epoch 201, Iteration 100, Current loss 0.061030098900763226 Accuracy 84.02380259370655\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 201, Probability Accuracy 45.38695778265733\n",
      "Starting Training\n",
      "Training:: Epoch 202, Iteration 0, Current loss 0.10002858951155787 Accuracy 83.25134168157425\n",
      "Training:: Epoch 202, Iteration 10, Current loss 0.07616196742975145 Accuracy 82.59642978744102\n",
      "Training:: Epoch 202, Iteration 20, Current loss 0.04794604627973437 Accuracy 90.60434939324763\n",
      "Training:: Epoch 202, Iteration 30, Current loss 0.06747024050244876 Accuracy 78.562675070028\n",
      "Training:: Epoch 202, Iteration 40, Current loss 0.11782766826939878 Accuracy 86.38395415472779\n",
      "Training:: Epoch 202, Iteration 50, Current loss 0.0687932104317227 Accuracy 89.02789857496488\n",
      "Training:: Epoch 202, Iteration 60, Current loss 0.09119659482398733 Accuracy 89.69782370439957\n",
      "Training:: Epoch 202, Iteration 70, Current loss 0.052879862189336886 Accuracy 83.11417372613478\n",
      "Training:: Epoch 202, Iteration 80, Current loss 0.0645590180040982 Accuracy 81.07671070794716\n",
      "Training:: Epoch 202, Iteration 90, Current loss 0.10581312984657298 Accuracy 78.36010885787024\n",
      "Training:: Epoch 202, Iteration 100, Current loss 0.1635286339244914 Accuracy 86.41678274450213\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 202, Probability Accuracy 44.43561751667564\n",
      "Starting Training\n",
      "Training:: Epoch 203, Iteration 0, Current loss 0.05596795310388837 Accuracy 87.12300647402495\n",
      "Training:: Epoch 203, Iteration 10, Current loss 0.3252572499225593 Accuracy 81.86315219353673\n",
      "Training:: Epoch 203, Iteration 20, Current loss 0.3229782502972686 Accuracy 84.25133994754248\n",
      "Training:: Epoch 203, Iteration 30, Current loss 0.5992690867654219 Accuracy 74.84441636392356\n",
      "Training:: Epoch 203, Iteration 40, Current loss 0.3778910871717417 Accuracy 77.85485194301484\n",
      "Training:: Epoch 203, Iteration 50, Current loss 0.18671365941900772 Accuracy 80.61552042372202\n",
      "Training:: Epoch 203, Iteration 60, Current loss 0.2789223839322291 Accuracy 77.96055540815424\n",
      "Training:: Epoch 203, Iteration 70, Current loss 0.23958620888332433 Accuracy 79.26145096833692\n",
      "Training:: Epoch 203, Iteration 80, Current loss 0.12410478895561076 Accuracy 83.86309409538437\n",
      "Training:: Epoch 203, Iteration 90, Current loss 0.14532688500550822 Accuracy 78.75272707923163\n",
      "Training:: Epoch 203, Iteration 100, Current loss 0.19236312734265928 Accuracy 86.77048260381594\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 203, Probability Accuracy 43.06490036044247\n",
      "Starting Training\n",
      "Training:: Epoch 204, Iteration 0, Current loss 0.1250872267216739 Accuracy 71.02417582417583\n",
      "Training:: Epoch 204, Iteration 10, Current loss 0.09838220667548166 Accuracy 86.28716002530044\n",
      "Training:: Epoch 204, Iteration 20, Current loss 0.09105146722212229 Accuracy 86.02554901679345\n",
      "Training:: Epoch 204, Iteration 30, Current loss 0.10531913813266901 Accuracy 85.39546359379818\n",
      "Training:: Epoch 204, Iteration 40, Current loss 0.087447924930766 Accuracy 84.28860059549127\n",
      "Training:: Epoch 204, Iteration 50, Current loss 0.0771374350069212 Accuracy 84.2704313418983\n",
      "Training:: Epoch 204, Iteration 60, Current loss 0.07343729178449754 Accuracy 88.57732545726283\n",
      "Training:: Epoch 204, Iteration 70, Current loss 0.08290125059100101 Accuracy 89.8536773199846\n",
      "Training:: Epoch 204, Iteration 80, Current loss 0.10395135413236178 Accuracy 84.7366855102648\n",
      "Training:: Epoch 204, Iteration 90, Current loss 0.19055796411799472 Accuracy 79.70513374633818\n",
      "Training:: Epoch 204, Iteration 100, Current loss 0.08886211639310027 Accuracy 80.8571217275551\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 204, Probability Accuracy 45.051891287235364\n",
      "Starting Training\n",
      "Training:: Epoch 205, Iteration 0, Current loss 0.05098329596333876 Accuracy 88.98066523446181\n",
      "Training:: Epoch 205, Iteration 10, Current loss 0.12774931606708737 Accuracy 84.8211689444606\n",
      "Training:: Epoch 205, Iteration 20, Current loss 0.07542808647814819 Accuracy 85.79218418661011\n",
      "Training:: Epoch 205, Iteration 30, Current loss 0.08438672432194387 Accuracy 85.5282799800962\n",
      "Training:: Epoch 205, Iteration 40, Current loss 0.06860458155352077 Accuracy 88.1259842519685\n",
      "Training:: Epoch 205, Iteration 50, Current loss 0.05243081821160858 Accuracy 88.66177271666105\n",
      "Training:: Epoch 205, Iteration 60, Current loss 0.10763203777296444 Accuracy 85.68212878848782\n",
      "Training:: Epoch 205, Iteration 70, Current loss 0.06499055365705246 Accuracy 87.56900745087442\n",
      "Training:: Epoch 205, Iteration 80, Current loss 0.08319280099228481 Accuracy 77.51587334522969\n",
      "Training:: Epoch 205, Iteration 90, Current loss 0.401898864504754 Accuracy 82.31420507996238\n",
      "Training:: Epoch 205, Iteration 100, Current loss 0.11521663482474644 Accuracy 86.93554122009448\n",
      "Calculating Expectation\n",
      "Epoch 205 iter 0\n",
      "Epoch 205 iter 10\n",
      "Epoch 205 iter 20\n",
      "Epoch 205 iter 30\n",
      "Epoch 205 iter 40\n",
      "Epoch 205 iter 50\n",
      "Epoch 205 iter 60\n",
      "Epoch 205 iter 70\n",
      "Epoch 205 iter 80\n",
      "Epoch 205 iter 90\n",
      "Epoch 205 iter 100\n",
      "Train Boundary avergage error = 114.797\n",
      "Train From boundary avergage accuracy = 85.482\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 205, Probability Accuracy 44.11857314496416\n",
      "Starting Training\n",
      "Training:: Epoch 206, Iteration 0, Current loss 0.060677084635049616 Accuracy 80.30676605504587\n",
      "Training:: Epoch 206, Iteration 10, Current loss 0.07507697617876168 Accuracy 82.68749495438766\n",
      "Training:: Epoch 206, Iteration 20, Current loss 0.12341456522245053 Accuracy 85.95830920671685\n",
      "Training:: Epoch 206, Iteration 30, Current loss 0.07507020157475638 Accuracy 89.05069327268725\n",
      "Training:: Epoch 206, Iteration 40, Current loss 0.09788402590792981 Accuracy 84.584539986634\n",
      "Training:: Epoch 206, Iteration 50, Current loss 0.06657060041533391 Accuracy 82.24727980062002\n",
      "Training:: Epoch 206, Iteration 60, Current loss 0.10577261174892684 Accuracy 88.50249945663987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 206, Iteration 70, Current loss 0.0567016026297864 Accuracy 88.55637924307452\n",
      "Training:: Epoch 206, Iteration 80, Current loss 0.059000684906880804 Accuracy 85.63111318118304\n",
      "Training:: Epoch 206, Iteration 90, Current loss 0.08534256432791058 Accuracy 84.66237733151361\n",
      "Training:: Epoch 206, Iteration 100, Current loss 0.12125908856702466 Accuracy 86.01843367649437\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 206, Probability Accuracy 45.91115300161578\n",
      "Starting Training\n",
      "Training:: Epoch 207, Iteration 0, Current loss 0.05514160548478176 Accuracy 85.26613320593972\n",
      "Training:: Epoch 207, Iteration 10, Current loss 0.07878990247678566 Accuracy 90.74649515825986\n",
      "Training:: Epoch 207, Iteration 20, Current loss 0.07644940778638602 Accuracy 89.11937570998172\n",
      "Training:: Epoch 207, Iteration 30, Current loss 0.09161139163597297 Accuracy 84.43303816315336\n",
      "Training:: Epoch 207, Iteration 40, Current loss 0.06770887541740797 Accuracy 88.08111727568395\n",
      "Training:: Epoch 207, Iteration 50, Current loss 0.06390061181628287 Accuracy 90.50105543828464\n",
      "Training:: Epoch 207, Iteration 60, Current loss 0.07238615973615616 Accuracy 89.54657050790242\n",
      "Training:: Epoch 207, Iteration 70, Current loss 0.10635869712643381 Accuracy 79.04717546077583\n",
      "Training:: Epoch 207, Iteration 80, Current loss 0.08846558219155998 Accuracy 88.28245451742214\n",
      "Training:: Epoch 207, Iteration 90, Current loss 0.06265619370448489 Accuracy 88.9080459770115\n",
      "Training:: Epoch 207, Iteration 100, Current loss 0.08647373250192418 Accuracy 85.6336150959279\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 207, Probability Accuracy 45.95827981936446\n",
      "Starting Training\n",
      "Training:: Epoch 208, Iteration 0, Current loss 0.048984891006745086 Accuracy 82.67339402581258\n",
      "Training:: Epoch 208, Iteration 10, Current loss 0.11154306484856134 Accuracy 82.23593754774053\n",
      "Training:: Epoch 208, Iteration 20, Current loss 0.0648770603419303 Accuracy 79.45864661654136\n",
      "Training:: Epoch 208, Iteration 30, Current loss 0.051547543988406115 Accuracy 86.06707166493801\n",
      "Training:: Epoch 208, Iteration 40, Current loss 0.052052291010194804 Accuracy 85.44007277689333\n",
      "Training:: Epoch 208, Iteration 50, Current loss 0.05122205555356339 Accuracy 86.58130236100533\n",
      "Training:: Epoch 208, Iteration 60, Current loss 0.05210935555366643 Accuracy 87.55517067992625\n",
      "Training:: Epoch 208, Iteration 70, Current loss 0.041888583165480005 Accuracy 87.98970020231745\n",
      "Training:: Epoch 208, Iteration 80, Current loss 0.04648429687505001 Accuracy 80.4649788198207\n",
      "Training:: Epoch 208, Iteration 90, Current loss 0.16424971089142598 Accuracy 78.67768595041322\n",
      "Training:: Epoch 208, Iteration 100, Current loss 0.08993600872064633 Accuracy 85.49710424710425\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 208, Probability Accuracy 45.24454157517504\n",
      "Starting Training\n",
      "Training:: Epoch 209, Iteration 0, Current loss 0.05299779824079014 Accuracy 85.09903619943842\n",
      "Training:: Epoch 209, Iteration 10, Current loss 0.11042598005800752 Accuracy 88.07534558711833\n",
      "Training:: Epoch 209, Iteration 20, Current loss 0.05977274047877405 Accuracy 87.0355755220981\n",
      "Training:: Epoch 209, Iteration 30, Current loss 0.04590535858918395 Accuracy 79.10700600976082\n",
      "Training:: Epoch 209, Iteration 40, Current loss 0.08574737500274417 Accuracy 77.24047155774294\n",
      "Training:: Epoch 209, Iteration 50, Current loss 0.0905877894462306 Accuracy 81.79085148698024\n",
      "Training:: Epoch 209, Iteration 60, Current loss 0.06793666257501077 Accuracy 83.5513078470825\n",
      "Training:: Epoch 209, Iteration 70, Current loss 0.13895580807376717 Accuracy 85.46327031317651\n",
      "Training:: Epoch 209, Iteration 80, Current loss 0.08971779040780367 Accuracy 83.45826858030898\n",
      "Training:: Epoch 209, Iteration 90, Current loss 0.09933565572774808 Accuracy 90.16569120435248\n",
      "Training:: Epoch 209, Iteration 100, Current loss 0.09572364866253255 Accuracy 86.3356452972062\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 209, Probability Accuracy 44.33338857355927\n",
      "Starting Training\n",
      "Training:: Epoch 210, Iteration 0, Current loss 0.1690503841676211 Accuracy 83.07634042092734\n",
      "Training:: Epoch 210, Iteration 10, Current loss 0.08422508327056934 Accuracy 86.72888440472758\n",
      "Training:: Epoch 210, Iteration 20, Current loss 0.0665939840333741 Accuracy 87.80979827089337\n",
      "Training:: Epoch 210, Iteration 30, Current loss 0.2666829363332207 Accuracy 77.2001290634831\n",
      "Training:: Epoch 210, Iteration 40, Current loss 0.06816658726666823 Accuracy 90.31436755189362\n",
      "Training:: Epoch 210, Iteration 50, Current loss 0.07700150362144785 Accuracy 82.58779920839355\n",
      "Training:: Epoch 210, Iteration 60, Current loss 0.08844640175765016 Accuracy 84.651180979196\n",
      "Training:: Epoch 210, Iteration 70, Current loss 0.04500560761778352 Accuracy 91.41513132105057\n",
      "Training:: Epoch 210, Iteration 80, Current loss 0.10597819857127845 Accuracy 80.24134163143142\n",
      "Training:: Epoch 210, Iteration 90, Current loss 0.0649416712854648 Accuracy 90.58049396069947\n",
      "Training:: Epoch 210, Iteration 100, Current loss 0.062339245012671476 Accuracy 89.44940663395815\n",
      "Calculating Expectation\n",
      "Epoch 210 iter 0\n",
      "Epoch 210 iter 10\n",
      "Epoch 210 iter 20\n",
      "Epoch 210 iter 30\n",
      "Epoch 210 iter 40\n",
      "Epoch 210 iter 50\n",
      "Epoch 210 iter 60\n",
      "Epoch 210 iter 70\n",
      "Epoch 210 iter 80\n",
      "Epoch 210 iter 90\n",
      "Epoch 210 iter 100\n",
      "Train Boundary avergage error = 115.784\n",
      "Train From boundary avergage accuracy = 85.480\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 210, Probability Accuracy 44.00660811202718\n",
      "Starting Training\n",
      "Training:: Epoch 211, Iteration 0, Current loss 0.05103135335834773 Accuracy 89.3152144950706\n",
      "Training:: Epoch 211, Iteration 10, Current loss 0.06445401523913694 Accuracy 87.9688688630113\n",
      "Training:: Epoch 211, Iteration 20, Current loss 0.05420503314427609 Accuracy 86.90406693909321\n",
      "Training:: Epoch 211, Iteration 30, Current loss 0.0480340017714664 Accuracy 83.78565494322902\n",
      "Training:: Epoch 211, Iteration 40, Current loss 0.05601432645609002 Accuracy 85.96801859126874\n",
      "Training:: Epoch 211, Iteration 50, Current loss 0.04929829312074844 Accuracy 89.30792051042387\n",
      "Training:: Epoch 211, Iteration 60, Current loss 0.08217038159412386 Accuracy 85.52317621748485\n",
      "Training:: Epoch 211, Iteration 70, Current loss 0.08561260381181195 Accuracy 89.4328530814822\n",
      "Training:: Epoch 211, Iteration 80, Current loss 0.0696093649921303 Accuracy 88.1819797405367\n",
      "Training:: Epoch 211, Iteration 90, Current loss 0.043576157228740545 Accuracy 90.51263362487852\n",
      "Training:: Epoch 211, Iteration 100, Current loss 0.10281092466266192 Accuracy 82.71818841163672\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 211, Probability Accuracy 44.19501180759829\n",
      "Starting Training\n",
      "Training:: Epoch 212, Iteration 0, Current loss 0.04199657175853459 Accuracy 88.11155338338288\n",
      "Training:: Epoch 212, Iteration 10, Current loss 0.06408003154048238 Accuracy 86.49478600217516\n",
      "Training:: Epoch 212, Iteration 20, Current loss 0.06907703734180533 Accuracy 85.42857142857143\n",
      "Training:: Epoch 212, Iteration 30, Current loss 0.059707578080349884 Accuracy 86.35429378893956\n",
      "Training:: Epoch 212, Iteration 40, Current loss 0.09705819831527106 Accuracy 77.39651416122004\n",
      "Training:: Epoch 212, Iteration 50, Current loss 0.05168285488200795 Accuracy 87.09020188923874\n",
      "Training:: Epoch 212, Iteration 60, Current loss 0.08801739372237302 Accuracy 89.01465354153974\n",
      "Training:: Epoch 212, Iteration 70, Current loss 0.07198138046470891 Accuracy 83.93485242391803\n",
      "Training:: Epoch 212, Iteration 80, Current loss 0.0659046033106501 Accuracy 90.24316263080733\n",
      "Training:: Epoch 212, Iteration 90, Current loss 0.0378453894037964 Accuracy 85.77918302078791\n",
      "Training:: Epoch 212, Iteration 100, Current loss 0.14710248427104278 Accuracy 85.81918748807935\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 212, Probability Accuracy 44.588909143638396\n",
      "Starting Training\n",
      "Training:: Epoch 213, Iteration 0, Current loss 0.05303049325280779 Accuracy 84.57170714920707\n",
      "Training:: Epoch 213, Iteration 10, Current loss 0.04376966808356888 Accuracy 91.0594755915583\n",
      "Training:: Epoch 213, Iteration 20, Current loss 0.04566564399836695 Accuracy 83.45381526104417\n",
      "Training:: Epoch 213, Iteration 30, Current loss 0.14628321255275542 Accuracy 83.5401859905944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 213, Iteration 40, Current loss 0.05672260405518891 Accuracy 86.63678324161192\n",
      "Training:: Epoch 213, Iteration 50, Current loss 0.051838945224281306 Accuracy 87.39989128465302\n",
      "Training:: Epoch 213, Iteration 60, Current loss 0.05359360946090129 Accuracy 88.26666666666667\n",
      "Training:: Epoch 213, Iteration 70, Current loss 0.04173342976911887 Accuracy 86.65831712863782\n",
      "Training:: Epoch 213, Iteration 80, Current loss 0.048036726200600816 Accuracy 84.73297120607604\n",
      "Training:: Epoch 213, Iteration 90, Current loss 0.03935149758599345 Accuracy 88.64008974027095\n",
      "Training:: Epoch 213, Iteration 100, Current loss 0.15092498208499228 Accuracy 88.20933446262202\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 213, Probability Accuracy 45.16779218627004\n",
      "Starting Training\n",
      "Training:: Epoch 214, Iteration 0, Current loss 0.05594671597126979 Accuracy 90.05227998838222\n",
      "Training:: Epoch 214, Iteration 10, Current loss 0.15845096599773417 Accuracy 85.77527423658464\n",
      "Training:: Epoch 214, Iteration 20, Current loss 0.10745969015684959 Accuracy 86.65772149775673\n",
      "Training:: Epoch 214, Iteration 30, Current loss 0.08598492308255043 Accuracy 87.8364662417279\n",
      "Training:: Epoch 214, Iteration 40, Current loss 0.0881979508299602 Accuracy 87.10689294619159\n",
      "Training:: Epoch 214, Iteration 50, Current loss 0.08614304705885012 Accuracy 86.61151246795619\n",
      "Training:: Epoch 214, Iteration 60, Current loss 0.09151981424252165 Accuracy 86.78914795064419\n",
      "Training:: Epoch 214, Iteration 70, Current loss 0.14084700333786346 Accuracy 86.55675930287329\n",
      "Training:: Epoch 214, Iteration 80, Current loss 0.07722735512390837 Accuracy 81.02555423929599\n",
      "Training:: Epoch 214, Iteration 90, Current loss 0.045859736445733844 Accuracy 87.86307509777211\n",
      "Training:: Epoch 214, Iteration 100, Current loss 0.09351015329075309 Accuracy 83.04405369635091\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 214, Probability Accuracy 45.20922235571943\n",
      "Starting Training\n",
      "Training:: Epoch 215, Iteration 0, Current loss 0.05168425078314256 Accuracy 88.35119004817717\n",
      "Training:: Epoch 215, Iteration 10, Current loss 0.09520574596836505 Accuracy 84.38160771243236\n",
      "Training:: Epoch 215, Iteration 20, Current loss 0.20023208270805343 Accuracy 78.59618717504333\n",
      "Training:: Epoch 215, Iteration 30, Current loss 0.07370554771193732 Accuracy 84.92111244818825\n",
      "Training:: Epoch 215, Iteration 40, Current loss 0.18168512446614252 Accuracy 77.38896366083445\n",
      "Training:: Epoch 215, Iteration 50, Current loss 0.0736055734409447 Accuracy 88.74209710127639\n",
      "Training:: Epoch 215, Iteration 60, Current loss 0.12167811505132584 Accuracy 82.0919540229885\n",
      "Training:: Epoch 215, Iteration 70, Current loss 0.06272471164563792 Accuracy 87.5610238817786\n",
      "Training:: Epoch 215, Iteration 80, Current loss 0.05585098438516109 Accuracy 86.41311836847181\n",
      "Training:: Epoch 215, Iteration 90, Current loss 0.10605846019994201 Accuracy 75.6162143218722\n",
      "Training:: Epoch 215, Iteration 100, Current loss 0.06188905619043102 Accuracy 89.41470906091804\n",
      "Calculating Expectation\n",
      "Epoch 215 iter 0\n",
      "Epoch 215 iter 10\n",
      "Epoch 215 iter 20\n",
      "Epoch 215 iter 30\n",
      "Epoch 215 iter 40\n",
      "Epoch 215 iter 50\n",
      "Epoch 215 iter 60\n",
      "Epoch 215 iter 70\n",
      "Epoch 215 iter 80\n",
      "Epoch 215 iter 90\n",
      "Epoch 215 iter 100\n",
      "Train Boundary avergage error = 117.084\n",
      "Train From boundary avergage accuracy = 85.266\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 215, Probability Accuracy 45.14355553714215\n",
      "Starting Training\n",
      "Training:: Epoch 216, Iteration 0, Current loss 0.04717587382338907 Accuracy 87.14655249308714\n",
      "Training:: Epoch 216, Iteration 10, Current loss 0.06550324743898053 Accuracy 83.75846269575327\n",
      "Training:: Epoch 216, Iteration 20, Current loss 0.11947047223498689 Accuracy 85.10865502696353\n",
      "Training:: Epoch 216, Iteration 30, Current loss 0.1016992264253983 Accuracy 82.63222632226322\n",
      "Training:: Epoch 216, Iteration 40, Current loss 0.11429440077115309 Accuracy 79.86382805751741\n",
      "Training:: Epoch 216, Iteration 50, Current loss 0.11497119725402583 Accuracy 81.44925695414709\n",
      "Training:: Epoch 216, Iteration 60, Current loss 0.07187895137111298 Accuracy 86.46699473952147\n",
      "Training:: Epoch 216, Iteration 70, Current loss 0.18531343586532276 Accuracy 85.40317233641046\n",
      "Training:: Epoch 216, Iteration 80, Current loss 0.0760275188628702 Accuracy 87.97255710212033\n",
      "Training:: Epoch 216, Iteration 90, Current loss 0.074172081265019 Accuracy 84.69941453662716\n",
      "Training:: Epoch 216, Iteration 100, Current loss 0.13193478421990806 Accuracy 87.98052989656507\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 216, Probability Accuracy 43.94446285785309\n",
      "Starting Training\n",
      "Training:: Epoch 217, Iteration 0, Current loss 0.05668183047472223 Accuracy 88.3983947464429\n",
      "Training:: Epoch 217, Iteration 10, Current loss 0.0793105611090203 Accuracy 83.35699373695198\n",
      "Training:: Epoch 217, Iteration 20, Current loss 0.06079962204767249 Accuracy 80.48871653506518\n",
      "Training:: Epoch 217, Iteration 30, Current loss 0.09627177226816726 Accuracy 84.89836301493074\n",
      "Training:: Epoch 217, Iteration 40, Current loss 0.07089139754169294 Accuracy 88.89130908298846\n",
      "Training:: Epoch 217, Iteration 50, Current loss 0.062001087331519864 Accuracy 83.75551560712535\n",
      "Training:: Epoch 217, Iteration 60, Current loss 0.07655917151900055 Accuracy 82.76291562872306\n",
      "Training:: Epoch 217, Iteration 70, Current loss 0.043715066704335684 Accuracy 89.91348600508906\n",
      "Training:: Epoch 217, Iteration 80, Current loss 0.06327598481053054 Accuracy 88.10506833712984\n",
      "Training:: Epoch 217, Iteration 90, Current loss 0.05603058115624347 Accuracy 82.31650521609538\n",
      "Training:: Epoch 217, Iteration 100, Current loss 0.06233787306498621 Accuracy 92.04326373231383\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 217, Probability Accuracy 45.667647180676965\n",
      "Starting Training\n",
      "Training:: Epoch 218, Iteration 0, Current loss 0.04146699096229895 Accuracy 89.86040154892454\n",
      "Training:: Epoch 218, Iteration 10, Current loss 0.11680895107075093 Accuracy 81.29340632404609\n",
      "Training:: Epoch 218, Iteration 20, Current loss 0.052563677010127395 Accuracy 88.12235862346549\n",
      "Training:: Epoch 218, Iteration 30, Current loss 0.08166670498122897 Accuracy 79.76701168877169\n",
      "Training:: Epoch 218, Iteration 40, Current loss 0.05702675195942766 Accuracy 86.02574440760984\n",
      "Training:: Epoch 218, Iteration 50, Current loss 0.07731292251760147 Accuracy 86.34508889536578\n",
      "Training:: Epoch 218, Iteration 60, Current loss 0.047829262470283816 Accuracy 87.563822027717\n",
      "Training:: Epoch 218, Iteration 70, Current loss 0.11007621978600743 Accuracy 80.27000052529286\n",
      "Training:: Epoch 218, Iteration 80, Current loss 0.05533269929334952 Accuracy 88.6908100312475\n",
      "Training:: Epoch 218, Iteration 90, Current loss 0.11949405972617025 Accuracy 87.2313633310947\n",
      "Training:: Epoch 218, Iteration 100, Current loss 0.06040039860705227 Accuracy 81.27683109905449\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 218, Probability Accuracy 44.57606579110909\n",
      "Starting Training\n",
      "Training:: Epoch 219, Iteration 0, Current loss 0.06402320916672788 Accuracy 82.1670671783652\n",
      "Training:: Epoch 219, Iteration 10, Current loss 0.05356849290359829 Accuracy 82.91323577001906\n",
      "Training:: Epoch 219, Iteration 20, Current loss 0.06393729130386373 Accuracy 87.21993368833517\n",
      "Training:: Epoch 219, Iteration 30, Current loss 0.03548097654022508 Accuracy 90.20138719883184\n",
      "Training:: Epoch 219, Iteration 40, Current loss 0.06861041792594005 Accuracy 82.08386561778615\n",
      "Training:: Epoch 219, Iteration 50, Current loss 0.0659651280590244 Accuracy 86.85741739176854\n",
      "Training:: Epoch 219, Iteration 60, Current loss 0.040474607803199315 Accuracy 86.97169227305983\n",
      "Training:: Epoch 219, Iteration 70, Current loss 0.059713755518941335 Accuracy 90.27348394768133\n",
      "Training:: Epoch 219, Iteration 80, Current loss 0.049389491062193265 Accuracy 89.17053298630758\n",
      "Training:: Epoch 219, Iteration 90, Current loss 0.040887892405179066 Accuracy 88.5720700493938\n",
      "Training:: Epoch 219, Iteration 100, Current loss 0.053607722454147536 Accuracy 81.40975226110892\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 219, Probability Accuracy 45.04733396859593\n",
      "Starting Training\n",
      "Training:: Epoch 220, Iteration 0, Current loss 0.03910925898949647 Accuracy 83.30954397000122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 220, Iteration 10, Current loss 0.040461090866253505 Accuracy 82.59244154431757\n",
      "Training:: Epoch 220, Iteration 20, Current loss 0.05170503175364286 Accuracy 83.87852291357639\n",
      "Training:: Epoch 220, Iteration 30, Current loss 0.08002936985715428 Accuracy 82.55771787743261\n",
      "Training:: Epoch 220, Iteration 40, Current loss 0.04315155378204613 Accuracy 78.48898216159496\n",
      "Training:: Epoch 220, Iteration 50, Current loss 0.05592337498263071 Accuracy 78.80756835429878\n",
      "Training:: Epoch 220, Iteration 60, Current loss 0.04429067640622827 Accuracy 88.75263527758257\n",
      "Training:: Epoch 220, Iteration 70, Current loss 0.06616254241759947 Accuracy 84.2810575878305\n",
      "Training:: Epoch 220, Iteration 80, Current loss 0.05395666249053557 Accuracy 87.66756032171581\n",
      "Training:: Epoch 220, Iteration 90, Current loss 0.056997231270243566 Accuracy 88.3855331841909\n",
      "Training:: Epoch 220, Iteration 100, Current loss 0.05583502784637097 Accuracy 83.78780702347926\n",
      "Calculating Expectation\n",
      "Epoch 220 iter 0\n",
      "Epoch 220 iter 10\n",
      "Epoch 220 iter 20\n",
      "Epoch 220 iter 30\n",
      "Epoch 220 iter 40\n",
      "Epoch 220 iter 50\n",
      "Epoch 220 iter 60\n",
      "Epoch 220 iter 70\n",
      "Epoch 220 iter 80\n",
      "Epoch 220 iter 90\n",
      "Epoch 220 iter 100\n",
      "Train Boundary avergage error = 118.223\n",
      "Train From boundary avergage accuracy = 85.216\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 220, Probability Accuracy 45.12978000580022\n",
      "Starting Training\n",
      "Training:: Epoch 221, Iteration 0, Current loss 0.03196544647466332 Accuracy 88.18976279650437\n",
      "Training:: Epoch 221, Iteration 10, Current loss 0.03419418591663158 Accuracy 87.8192604170367\n",
      "Training:: Epoch 221, Iteration 20, Current loss 0.06603039787522386 Accuracy 75.1809560673505\n",
      "Training:: Epoch 221, Iteration 30, Current loss 0.12014164514800521 Accuracy 78.30779944289694\n",
      "Training:: Epoch 221, Iteration 40, Current loss 0.0832352281843798 Accuracy 87.56575801052128\n",
      "Training:: Epoch 221, Iteration 50, Current loss 0.09908228922084573 Accuracy 82.08157374244935\n",
      "Training:: Epoch 221, Iteration 60, Current loss 0.10737021482207049 Accuracy 82.78842540398347\n",
      "Training:: Epoch 221, Iteration 70, Current loss 0.04570401098099256 Accuracy 84.76634415762466\n",
      "Training:: Epoch 221, Iteration 80, Current loss 0.05853513015256626 Accuracy 89.22173767192217\n",
      "Training:: Epoch 221, Iteration 90, Current loss 0.07081376124197546 Accuracy 85.5536611216073\n",
      "Training:: Epoch 221, Iteration 100, Current loss 0.05102518954164258 Accuracy 83.56382978723404\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 221, Probability Accuracy 45.78934830343456\n",
      "Starting Training\n",
      "Training:: Epoch 222, Iteration 0, Current loss 0.038226053733787 Accuracy 89.97926469415424\n",
      "Training:: Epoch 222, Iteration 10, Current loss 0.0806863595277701 Accuracy 85.64659655610771\n",
      "Training:: Epoch 222, Iteration 20, Current loss 0.0660445073883667 Accuracy 80.85488758897738\n",
      "Training:: Epoch 222, Iteration 30, Current loss 0.06563986307898514 Accuracy 84.67456097953924\n",
      "Training:: Epoch 222, Iteration 40, Current loss 0.0460804670712289 Accuracy 88.71227741330834\n",
      "Training:: Epoch 222, Iteration 50, Current loss 0.04868776463515475 Accuracy 82.20816898653715\n",
      "Training:: Epoch 222, Iteration 60, Current loss 0.0872694055107892 Accuracy 82.38419477973795\n",
      "Training:: Epoch 222, Iteration 70, Current loss 0.05400472891488767 Accuracy 86.0827558443799\n",
      "Training:: Epoch 222, Iteration 80, Current loss 0.0872536343503566 Accuracy 84.54762446010457\n",
      "Training:: Epoch 222, Iteration 90, Current loss 0.06379425327136069 Accuracy 85.86060454659005\n",
      "Training:: Epoch 222, Iteration 100, Current loss 0.05514006878855695 Accuracy 86.82701202590194\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 222, Probability Accuracy 44.466897294609936\n",
      "Starting Training\n",
      "Training:: Epoch 223, Iteration 0, Current loss 0.05348120061242317 Accuracy 83.05997054791042\n",
      "Training:: Epoch 223, Iteration 10, Current loss 0.06371292253356654 Accuracy 85.12270607209798\n",
      "Training:: Epoch 223, Iteration 20, Current loss 0.06317817810461873 Accuracy 87.79893786151608\n",
      "Training:: Epoch 223, Iteration 30, Current loss 0.059326499336635544 Accuracy 85.94502910450638\n",
      "Training:: Epoch 223, Iteration 40, Current loss 0.06945287617779458 Accuracy 84.19146647543923\n",
      "Training:: Epoch 223, Iteration 50, Current loss 0.06807335730541235 Accuracy 85.50249789139039\n",
      "Training:: Epoch 223, Iteration 60, Current loss 0.0514442740776143 Accuracy 89.70672800460035\n",
      "Training:: Epoch 223, Iteration 70, Current loss 0.08167146585785315 Accuracy 83.92421943256541\n",
      "Training:: Epoch 223, Iteration 80, Current loss 0.09435046911643781 Accuracy 88.28518844522685\n",
      "Training:: Epoch 223, Iteration 90, Current loss 0.048590634812512884 Accuracy 85.85375036979265\n",
      "Training:: Epoch 223, Iteration 100, Current loss 0.14807966774988793 Accuracy 86.09664594882132\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 223, Probability Accuracy 44.900567593321455\n",
      "Starting Training\n",
      "Training:: Epoch 224, Iteration 0, Current loss 0.08123269645100066 Accuracy 86.68516560958422\n",
      "Training:: Epoch 224, Iteration 10, Current loss 0.10756399310182524 Accuracy 81.3029315960912\n",
      "Training:: Epoch 224, Iteration 20, Current loss 0.07159132747875359 Accuracy 89.3020263750402\n",
      "Training:: Epoch 224, Iteration 30, Current loss 0.3200169735287592 Accuracy 80.38555215836658\n",
      "Training:: Epoch 224, Iteration 40, Current loss 0.08303495711354204 Accuracy 86.32100396301189\n",
      "Training:: Epoch 224, Iteration 50, Current loss 0.17052826779585206 Accuracy 77.19390956831919\n",
      "Training:: Epoch 224, Iteration 60, Current loss 0.0617201670575351 Accuracy 83.21113403872327\n",
      "Training:: Epoch 224, Iteration 70, Current loss 0.11087929017129157 Accuracy 86.47396810506567\n",
      "Training:: Epoch 224, Iteration 80, Current loss 0.054068228628276475 Accuracy 87.5521387094136\n",
      "Training:: Epoch 224, Iteration 90, Current loss 0.06948867811975215 Accuracy 87.52788933511825\n",
      "Training:: Epoch 224, Iteration 100, Current loss 0.07357612606735557 Accuracy 85.35767703336944\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 224, Probability Accuracy 44.62671417326097\n",
      "Starting Training\n",
      "Training:: Epoch 225, Iteration 0, Current loss 0.043647050212991124 Accuracy 86.79579498632985\n",
      "Training:: Epoch 225, Iteration 10, Current loss 0.1037702361376236 Accuracy 85.8942065491184\n",
      "Training:: Epoch 225, Iteration 20, Current loss 0.06277700364417298 Accuracy 81.66212970376301\n",
      "Training:: Epoch 225, Iteration 30, Current loss 0.0592431432082753 Accuracy 82.81087016195443\n",
      "Training:: Epoch 225, Iteration 40, Current loss 0.16422928060573394 Accuracy 73.70300122726766\n",
      "Training:: Epoch 225, Iteration 50, Current loss 0.053716230733433463 Accuracy 86.72898791992145\n",
      "Training:: Epoch 225, Iteration 60, Current loss 0.08178469470008448 Accuracy 82.37970956300609\n",
      "Training:: Epoch 225, Iteration 70, Current loss 0.07854564444459755 Accuracy 86.04119926575567\n",
      "Training:: Epoch 225, Iteration 80, Current loss 0.08120825761190632 Accuracy 86.47922832401622\n",
      "Training:: Epoch 225, Iteration 90, Current loss 0.04185879561662386 Accuracy 88.3866160451336\n",
      "Training:: Epoch 225, Iteration 100, Current loss 0.05681120810447057 Accuracy 87.11072122313428\n",
      "Calculating Expectation\n",
      "Epoch 225 iter 0\n",
      "Epoch 225 iter 10\n",
      "Epoch 225 iter 20\n",
      "Epoch 225 iter 30\n",
      "Epoch 225 iter 40\n",
      "Epoch 225 iter 50\n",
      "Epoch 225 iter 60\n",
      "Epoch 225 iter 70\n",
      "Epoch 225 iter 80\n",
      "Epoch 225 iter 90\n",
      "Epoch 225 iter 100\n",
      "Train Boundary avergage error = 118.684\n",
      "Train From boundary avergage accuracy = 85.120\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 225, Probability Accuracy 45.908356465177945\n",
      "Starting Training\n",
      "Training:: Epoch 226, Iteration 0, Current loss 0.031180562650256632 Accuracy 86.30266328058866\n",
      "Training:: Epoch 226, Iteration 10, Current loss 0.0442782516275565 Accuracy 88.74302592714145\n",
      "Training:: Epoch 226, Iteration 20, Current loss 0.10689878565502091 Accuracy 88.72393247269116\n",
      "Training:: Epoch 226, Iteration 30, Current loss 0.09729717808089046 Accuracy 85.15314240254574\n",
      "Training:: Epoch 226, Iteration 40, Current loss 0.0676188175523814 Accuracy 83.65443991601828\n",
      "Training:: Epoch 226, Iteration 50, Current loss 0.09661036744269531 Accuracy 81.57678169748176\n",
      "Training:: Epoch 226, Iteration 60, Current loss 0.12651787560355116 Accuracy 85.56614914883198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 226, Iteration 70, Current loss 0.049176818822220664 Accuracy 87.89686693182611\n",
      "Training:: Epoch 226, Iteration 80, Current loss 0.04896457115971445 Accuracy 87.04912266989722\n",
      "Training:: Epoch 226, Iteration 90, Current loss 0.03722436018335911 Accuracy 86.8151029441352\n",
      "Training:: Epoch 226, Iteration 100, Current loss 0.14202880442926255 Accuracy 74.49086410354016\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 226, Probability Accuracy 44.80455317562249\n",
      "Starting Training\n",
      "Training:: Epoch 227, Iteration 0, Current loss 0.09060237397549827 Accuracy 78.2445671894529\n",
      "Training:: Epoch 227, Iteration 10, Current loss 0.07065448989031532 Accuracy 81.29953563073914\n",
      "Training:: Epoch 227, Iteration 20, Current loss 0.05236001907747759 Accuracy 83.86312640239342\n",
      "Training:: Epoch 227, Iteration 30, Current loss 0.06356378578003033 Accuracy 87.78313668073379\n",
      "Training:: Epoch 227, Iteration 40, Current loss 0.07370596479507548 Accuracy 87.22145886851067\n",
      "Training:: Epoch 227, Iteration 50, Current loss 0.06035446100161352 Accuracy 85.54051525169983\n",
      "Training:: Epoch 227, Iteration 60, Current loss 0.05547208072086598 Accuracy 83.65618566348493\n",
      "Training:: Epoch 227, Iteration 70, Current loss 0.05690013897944131 Accuracy 72.24459746221471\n",
      "Training:: Epoch 227, Iteration 80, Current loss 0.0472388627726082 Accuracy 81.34895751685217\n",
      "Training:: Epoch 227, Iteration 90, Current loss 0.04409445498783077 Accuracy 84.79755538579067\n",
      "Training:: Epoch 227, Iteration 100, Current loss 0.05368496421124978 Accuracy 84.9272259035273\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 227, Probability Accuracy 44.8685627874218\n",
      "Starting Training\n",
      "Training:: Epoch 228, Iteration 0, Current loss 0.03953396618057026 Accuracy 87.86534112727738\n",
      "Training:: Epoch 228, Iteration 10, Current loss 0.07194083662283968 Accuracy 85.28767761472164\n",
      "Training:: Epoch 228, Iteration 20, Current loss 0.06607944867839997 Accuracy 87.02029191883233\n",
      "Training:: Epoch 228, Iteration 30, Current loss 0.06202441598188873 Accuracy 88.39391544843936\n",
      "Training:: Epoch 228, Iteration 40, Current loss 0.08345015093592908 Accuracy 83.65964600019777\n",
      "Training:: Epoch 228, Iteration 50, Current loss 0.06191280635825439 Accuracy 88.31181207414113\n",
      "Training:: Epoch 228, Iteration 60, Current loss 0.05300231756684913 Accuracy 88.31521739130434\n",
      "Training:: Epoch 228, Iteration 70, Current loss 0.043471248916338114 Accuracy 89.02083511960902\n",
      "Training:: Epoch 228, Iteration 80, Current loss 0.04322416824941547 Accuracy 82.3060890278398\n",
      "Training:: Epoch 228, Iteration 90, Current loss 0.05169294031248155 Accuracy 85.01701594125022\n",
      "Training:: Epoch 228, Iteration 100, Current loss 0.05982693165324527 Accuracy 87.13037634408602\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 228, Probability Accuracy 45.18757509218213\n",
      "Starting Training\n",
      "Training:: Epoch 229, Iteration 0, Current loss 0.03633301819453761 Accuracy 84.90413421210306\n",
      "Training:: Epoch 229, Iteration 10, Current loss 0.08151934358059558 Accuracy 84.61786223715087\n",
      "Training:: Epoch 229, Iteration 20, Current loss 0.11055365655832443 Accuracy 81.42430278884463\n",
      "Training:: Epoch 229, Iteration 30, Current loss 0.08267848686579647 Accuracy 88.05357626048526\n",
      "Training:: Epoch 229, Iteration 40, Current loss 0.13196913278246658 Accuracy 84.48757396449705\n",
      "Training:: Epoch 229, Iteration 50, Current loss 0.05996804777267229 Accuracy 84.69047324848634\n",
      "Training:: Epoch 229, Iteration 60, Current loss 0.05727516011479679 Accuracy 78.94846465127351\n",
      "Training:: Epoch 229, Iteration 70, Current loss 0.06356986566142152 Accuracy 87.79345250915357\n",
      "Training:: Epoch 229, Iteration 80, Current loss 0.05241086014114539 Accuracy 85.25048288332717\n",
      "Training:: Epoch 229, Iteration 90, Current loss 0.09013720447747868 Accuracy 85.19390491138579\n",
      "Training:: Epoch 229, Iteration 100, Current loss 0.04694274252106102 Accuracy 89.09698198735536\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 229, Probability Accuracy 44.641111157144636\n",
      "Starting Training\n",
      "Training:: Epoch 230, Iteration 0, Current loss 0.04593603251997528 Accuracy 82.63344100907553\n",
      "Training:: Epoch 230, Iteration 10, Current loss 0.275962815938689 Accuracy 80.26628625772706\n",
      "Training:: Epoch 230, Iteration 20, Current loss 0.1639836813962668 Accuracy 86.33057914175836\n",
      "Training:: Epoch 230, Iteration 30, Current loss 0.21753321734936848 Accuracy 79.16286557552992\n",
      "Training:: Epoch 230, Iteration 40, Current loss 0.15856274877227325 Accuracy 85.06608529772448\n",
      "Training:: Epoch 230, Iteration 50, Current loss 0.20758773317199716 Accuracy 84.7090161170679\n",
      "Training:: Epoch 230, Iteration 60, Current loss 0.1402167022895048 Accuracy 81.5536338904003\n",
      "Training:: Epoch 230, Iteration 70, Current loss 0.11322383503962759 Accuracy 85.72779404587604\n",
      "Training:: Epoch 230, Iteration 80, Current loss 0.19646965829132404 Accuracy 84.10291752814152\n",
      "Training:: Epoch 230, Iteration 90, Current loss 0.13790159477770278 Accuracy 79.84896281426249\n",
      "Training:: Epoch 230, Iteration 100, Current loss 0.2946103639017928 Accuracy 76.95972001685084\n",
      "Calculating Expectation\n",
      "Epoch 230 iter 0\n",
      "Epoch 230 iter 10\n",
      "Epoch 230 iter 20\n",
      "Epoch 230 iter 30\n",
      "Epoch 230 iter 40\n",
      "Epoch 230 iter 50\n",
      "Epoch 230 iter 60\n",
      "Epoch 230 iter 70\n",
      "Epoch 230 iter 80\n",
      "Epoch 230 iter 90\n",
      "Epoch 230 iter 100\n",
      "Train Boundary avergage error = 119.637\n",
      "Train From boundary avergage accuracy = 84.808\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 230, Probability Accuracy 39.08491113228653\n",
      "Starting Training\n",
      "Training:: Epoch 231, Iteration 0, Current loss 0.1748873146944302 Accuracy 82.46615905245346\n",
      "Training:: Epoch 231, Iteration 10, Current loss 0.327931445395883 Accuracy 81.2805248450978\n",
      "Training:: Epoch 231, Iteration 20, Current loss 0.3872797545262652 Accuracy 75.28728530829112\n",
      "Training:: Epoch 231, Iteration 30, Current loss 0.12772798125858165 Accuracy 82.42416477145328\n",
      "Training:: Epoch 231, Iteration 40, Current loss 0.12171045691276025 Accuracy 86.35424328394802\n",
      "Training:: Epoch 231, Iteration 50, Current loss 0.18836689349019692 Accuracy 77.22744227772793\n",
      "Training:: Epoch 231, Iteration 60, Current loss 0.22261073053000516 Accuracy 74.93220975345209\n",
      "Training:: Epoch 231, Iteration 70, Current loss 0.10820571871503325 Accuracy 86.76945293917771\n",
      "Training:: Epoch 231, Iteration 80, Current loss 0.14976745014185455 Accuracy 78.41457057213606\n",
      "Training:: Epoch 231, Iteration 90, Current loss 0.1604702210724024 Accuracy 82.61199445800621\n",
      "Training:: Epoch 231, Iteration 100, Current loss 0.11163361506382084 Accuracy 84.0525712414019\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 231, Probability Accuracy 44.12385549156896\n",
      "Starting Training\n",
      "Training:: Epoch 232, Iteration 0, Current loss 0.0989099616743978 Accuracy 81.56970207982012\n",
      "Training:: Epoch 232, Iteration 10, Current loss 0.09923305314390705 Accuracy 86.95614150159605\n",
      "Training:: Epoch 232, Iteration 20, Current loss 0.11327238360995392 Accuracy 82.39598430485347\n",
      "Training:: Epoch 232, Iteration 30, Current loss 0.088870700487429 Accuracy 86.26829528862932\n",
      "Training:: Epoch 232, Iteration 40, Current loss 0.0895772710665175 Accuracy 80.91497044002698\n",
      "Training:: Epoch 232, Iteration 50, Current loss 0.11577137027890205 Accuracy 84.11112173663575\n",
      "Training:: Epoch 232, Iteration 60, Current loss 0.10922228198452127 Accuracy 87.65722039559522\n",
      "Training:: Epoch 232, Iteration 70, Current loss 0.20878290647911352 Accuracy 82.56584950422776\n",
      "Training:: Epoch 232, Iteration 80, Current loss 0.11480935082645108 Accuracy 76.07587474425218\n",
      "Training:: Epoch 232, Iteration 90, Current loss 0.10167572461653882 Accuracy 89.58342383041571\n",
      "Training:: Epoch 232, Iteration 100, Current loss 0.12054219539801445 Accuracy 90.31623621965737\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 232, Probability Accuracy 45.4485851597133\n",
      "Starting Training\n",
      "Training:: Epoch 233, Iteration 0, Current loss 0.09466509248346382 Accuracy 86.10159064944196\n",
      "Training:: Epoch 233, Iteration 10, Current loss 0.10634775147899679 Accuracy 80.63373273356952\n",
      "Training:: Epoch 233, Iteration 20, Current loss 0.1656868218289509 Accuracy 87.30937463057099\n",
      "Training:: Epoch 233, Iteration 30, Current loss 0.14317841087931119 Accuracy 84.4955319636447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 233, Iteration 40, Current loss 0.11446569114565301 Accuracy 85.171650797908\n",
      "Training:: Epoch 233, Iteration 50, Current loss 0.08161538339578861 Accuracy 82.6566401975364\n",
      "Training:: Epoch 233, Iteration 60, Current loss 0.09263158931919054 Accuracy 83.6385282612393\n",
      "Training:: Epoch 233, Iteration 70, Current loss 0.05923236164421536 Accuracy 86.29956591718285\n",
      "Training:: Epoch 233, Iteration 80, Current loss 0.1030015922677081 Accuracy 85.29250031724547\n",
      "Training:: Epoch 233, Iteration 90, Current loss 0.07664903562494929 Accuracy 89.1297195031669\n",
      "Training:: Epoch 233, Iteration 100, Current loss 0.11017264410936874 Accuracy 86.70891423743134\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 233, Probability Accuracy 44.64732568256204\n",
      "Starting Training\n",
      "Training:: Epoch 234, Iteration 0, Current loss 0.10692163790930748 Accuracy 79.85535705733034\n",
      "Training:: Epoch 234, Iteration 10, Current loss 0.08513014529911947 Accuracy 82.29915631104276\n",
      "Training:: Epoch 234, Iteration 20, Current loss 0.09494176557093928 Accuracy 84.61420085562156\n",
      "Training:: Epoch 234, Iteration 30, Current loss 0.06008735501256491 Accuracy 89.04388354613188\n",
      "Training:: Epoch 234, Iteration 40, Current loss 0.06432657604898065 Accuracy 79.67459448701364\n",
      "Training:: Epoch 234, Iteration 50, Current loss 0.06777866218926398 Accuracy 89.74930953898449\n",
      "Training:: Epoch 234, Iteration 60, Current loss 0.07890491147589228 Accuracy 86.40074644273385\n",
      "Training:: Epoch 234, Iteration 70, Current loss 0.07986922075685125 Accuracy 81.72030629282054\n",
      "Training:: Epoch 234, Iteration 80, Current loss 0.07469490822464345 Accuracy 87.17801287948483\n",
      "Training:: Epoch 234, Iteration 90, Current loss 0.06967453012033929 Accuracy 88.40579710144928\n",
      "Training:: Epoch 234, Iteration 100, Current loss 0.09015913117799881 Accuracy 84.9246412345857\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 234, Probability Accuracy 45.6113021502258\n",
      "Starting Training\n",
      "Training:: Epoch 235, Iteration 0, Current loss 0.08058717740129408 Accuracy 82.76052145632941\n",
      "Training:: Epoch 235, Iteration 10, Current loss 0.06840554874935469 Accuracy 85.69179013671176\n",
      "Training:: Epoch 235, Iteration 20, Current loss 0.11443658689789009 Accuracy 79.31852366076593\n",
      "Training:: Epoch 235, Iteration 30, Current loss 0.09586677806684366 Accuracy 68.62671951503847\n",
      "Training:: Epoch 235, Iteration 40, Current loss 0.12002570374620629 Accuracy 73.254701944533\n",
      "Training:: Epoch 235, Iteration 50, Current loss 0.11205779310258322 Accuracy 84.5061894510226\n",
      "Training:: Epoch 235, Iteration 60, Current loss 0.0510208337553307 Accuracy 88.60568095906686\n",
      "Training:: Epoch 235, Iteration 70, Current loss 0.08409571726977474 Accuracy 87.65689362733221\n",
      "Training:: Epoch 235, Iteration 80, Current loss 0.07493234117769686 Accuracy 88.21897448600446\n",
      "Training:: Epoch 235, Iteration 90, Current loss 0.07480425462560397 Accuracy 79.92950654582074\n",
      "Training:: Epoch 235, Iteration 100, Current loss 0.08355345271797777 Accuracy 85.78879984826213\n",
      "Calculating Expectation\n",
      "Epoch 235 iter 0\n",
      "Epoch 235 iter 10\n",
      "Epoch 235 iter 20\n",
      "Epoch 235 iter 30\n",
      "Epoch 235 iter 40\n",
      "Epoch 235 iter 50\n",
      "Epoch 235 iter 60\n",
      "Epoch 235 iter 70\n",
      "Epoch 235 iter 80\n",
      "Epoch 235 iter 90\n",
      "Epoch 235 iter 100\n",
      "Train Boundary avergage error = 120.099\n",
      "Train From boundary avergage accuracy = 84.780\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 235, Probability Accuracy 46.57341426026432\n",
      "Starting Training\n",
      "Training:: Epoch 236, Iteration 0, Current loss 0.04970458814255569 Accuracy 86.37791527843883\n",
      "Training:: Epoch 236, Iteration 10, Current loss 0.04608094354092601 Accuracy 77.33239914447968\n",
      "Training:: Epoch 236, Iteration 20, Current loss 0.06748312224166804 Accuracy 85.04412447747329\n",
      "Training:: Epoch 236, Iteration 30, Current loss 0.07765343878144852 Accuracy 84.96182318514786\n",
      "Training:: Epoch 236, Iteration 40, Current loss 0.052496610456857995 Accuracy 80.51433773327264\n",
      "Training:: Epoch 236, Iteration 50, Current loss 0.07234402314590484 Accuracy 87.91203703703704\n",
      "Training:: Epoch 236, Iteration 60, Current loss 0.07486514453375741 Accuracy 73.45027981058975\n",
      "Training:: Epoch 236, Iteration 70, Current loss 0.06104784799897146 Accuracy 86.75066246687666\n",
      "Training:: Epoch 236, Iteration 80, Current loss 0.08453311758026502 Accuracy 84.00317292437863\n",
      "Training:: Epoch 236, Iteration 90, Current loss 0.06717219832335747 Accuracy 84.34446020005366\n",
      "Training:: Epoch 236, Iteration 100, Current loss 0.05459986576713436 Accuracy 82.27308930983135\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 236, Probability Accuracy 45.614202262087254\n",
      "Starting Training\n",
      "Training:: Epoch 237, Iteration 0, Current loss 0.05675182495547588 Accuracy 83.51659336265494\n",
      "Training:: Epoch 237, Iteration 10, Current loss 0.07238279595742668 Accuracy 84.06568655555101\n",
      "Training:: Epoch 237, Iteration 20, Current loss 0.04347649164110919 Accuracy 86.72361569615083\n",
      "Training:: Epoch 237, Iteration 30, Current loss 0.047106276979209 Accuracy 87.0898143502258\n",
      "Training:: Epoch 237, Iteration 40, Current loss 0.04727098099559439 Accuracy 86.71574368412247\n",
      "Training:: Epoch 237, Iteration 50, Current loss 0.05443900965875835 Accuracy 89.60687851792758\n",
      "Training:: Epoch 237, Iteration 60, Current loss 0.08319866577083146 Accuracy 77.02281636345361\n",
      "Training:: Epoch 237, Iteration 70, Current loss 0.052879143979397907 Accuracy 83.1095406360424\n",
      "Training:: Epoch 237, Iteration 80, Current loss 0.052599248965632985 Accuracy 85.66672357862387\n",
      "Training:: Epoch 237, Iteration 90, Current loss 0.051788304095932505 Accuracy 88.47083593262633\n",
      "Training:: Epoch 237, Iteration 100, Current loss 0.0508820460492332 Accuracy 81.88494492044063\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 237, Probability Accuracy 44.54810042673075\n",
      "Starting Training\n",
      "Training:: Epoch 238, Iteration 0, Current loss 0.07356537723996544 Accuracy 90.09397528321318\n",
      "Training:: Epoch 238, Iteration 10, Current loss 0.07019661105915223 Accuracy 81.66650731427478\n",
      "Training:: Epoch 238, Iteration 20, Current loss 0.07015121576103558 Accuracy 84.0343278144355\n",
      "Training:: Epoch 238, Iteration 30, Current loss 0.05004019013499853 Accuracy 83.76165410947122\n",
      "Training:: Epoch 238, Iteration 40, Current loss 0.056774154600976554 Accuracy 86.88120337833672\n",
      "Training:: Epoch 238, Iteration 50, Current loss 0.049079412075085024 Accuracy 86.91984426178851\n",
      "Training:: Epoch 238, Iteration 60, Current loss 0.06126958312017876 Accuracy 70.83333333333333\n",
      "Training:: Epoch 238, Iteration 70, Current loss 0.06110586131732172 Accuracy 77.34331150608045\n",
      "Training:: Epoch 238, Iteration 80, Current loss 0.047894650332761225 Accuracy 87.50912778904666\n",
      "Training:: Epoch 238, Iteration 90, Current loss 0.05480348586026698 Accuracy 86.71905129193983\n",
      "Training:: Epoch 238, Iteration 100, Current loss 0.03882494358972362 Accuracy 77.652866506264\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 238, Probability Accuracy 45.003728715250446\n",
      "Starting Training\n",
      "Training:: Epoch 239, Iteration 0, Current loss 0.04006507113227864 Accuracy 87.09256517702556\n",
      "Training:: Epoch 239, Iteration 10, Current loss 0.11753226481474327 Accuracy 82.49210466952402\n",
      "Training:: Epoch 239, Iteration 20, Current loss 0.2665061675731162 Accuracy 84.53421766656618\n",
      "Training:: Epoch 239, Iteration 30, Current loss 0.17395228668821272 Accuracy 85.34183382573991\n",
      "Training:: Epoch 239, Iteration 40, Current loss 0.1657973117930713 Accuracy 79.29020664869722\n",
      "Training:: Epoch 239, Iteration 50, Current loss 0.10991976897519878 Accuracy 79.14448187185339\n",
      "Training:: Epoch 239, Iteration 60, Current loss 0.09132717825012668 Accuracy 88.03898555901414\n",
      "Training:: Epoch 239, Iteration 70, Current loss 0.08763293592723745 Accuracy 87.23265837196064\n",
      "Training:: Epoch 239, Iteration 80, Current loss 0.07036837860895336 Accuracy 86.05151835624717\n",
      "Training:: Epoch 239, Iteration 90, Current loss 0.06549774839021934 Accuracy 81.80524963289281\n",
      "Training:: Epoch 239, Iteration 100, Current loss 0.11218878081818365 Accuracy 84.4858198569989\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 239, Probability Accuracy 44.877781000124294\n",
      "Starting Training\n",
      "Training:: Epoch 240, Iteration 0, Current loss 0.07126789099840376 Accuracy 85.72643917194442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 240, Iteration 10, Current loss 0.06119979935244337 Accuracy 85.5579537278958\n",
      "Training:: Epoch 240, Iteration 20, Current loss 0.07628616480602633 Accuracy 86.50842454802607\n",
      "Training:: Epoch 240, Iteration 30, Current loss 0.09975321773282037 Accuracy 88.49496221662469\n",
      "Training:: Epoch 240, Iteration 40, Current loss 0.17901355247971945 Accuracy 80.82125603864735\n",
      "Training:: Epoch 240, Iteration 50, Current loss 0.3529569093455253 Accuracy 74.21396830720215\n",
      "Training:: Epoch 240, Iteration 60, Current loss 0.06829818525769193 Accuracy 88.10906033550867\n",
      "Training:: Epoch 240, Iteration 70, Current loss 0.056268705605415546 Accuracy 85.68153700604032\n",
      "Training:: Epoch 240, Iteration 80, Current loss 0.0690890280921879 Accuracy 87.63602376660658\n",
      "Training:: Epoch 240, Iteration 90, Current loss 0.07326764424946695 Accuracy 87.95539920100173\n",
      "Training:: Epoch 240, Iteration 100, Current loss 0.06150990278933674 Accuracy 85.23345564975325\n",
      "Calculating Expectation\n",
      "Epoch 240 iter 0\n",
      "Epoch 240 iter 10\n",
      "Epoch 240 iter 20\n",
      "Epoch 240 iter 30\n",
      "Epoch 240 iter 40\n",
      "Epoch 240 iter 50\n",
      "Epoch 240 iter 60\n",
      "Epoch 240 iter 70\n",
      "Epoch 240 iter 80\n",
      "Epoch 240 iter 90\n",
      "Epoch 240 iter 100\n",
      "Train Boundary avergage error = 120.388\n",
      "Train From boundary avergage accuracy = 84.717\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 240, Probability Accuracy 44.45975059037991\n",
      "Starting Training\n",
      "Training:: Epoch 241, Iteration 0, Current loss 0.05366252131133861 Accuracy 89.34694149823227\n",
      "Training:: Epoch 241, Iteration 10, Current loss 0.13694358147287114 Accuracy 86.60638598474145\n",
      "Training:: Epoch 241, Iteration 20, Current loss 0.07759922913123955 Accuracy 88.97027143472641\n",
      "Training:: Epoch 241, Iteration 30, Current loss 0.04976960816687617 Accuracy 84.35496481626271\n",
      "Training:: Epoch 241, Iteration 40, Current loss 0.0888898016154371 Accuracy 83.02736536773405\n",
      "Training:: Epoch 241, Iteration 50, Current loss 0.08884217836372156 Accuracy 82.61201318520327\n",
      "Training:: Epoch 241, Iteration 60, Current loss 0.07034405843714805 Accuracy 82.64100112974711\n",
      "Training:: Epoch 241, Iteration 70, Current loss 0.07942578258231957 Accuracy 86.68811336889685\n",
      "Training:: Epoch 241, Iteration 80, Current loss 0.06672184726758112 Accuracy 82.15706124810181\n",
      "Training:: Epoch 241, Iteration 90, Current loss 0.04660285812518068 Accuracy 80.14611467788355\n",
      "Training:: Epoch 241, Iteration 100, Current loss 0.10354602145603012 Accuracy 86.71151745155541\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 241, Probability Accuracy 44.20609437792601\n",
      "Starting Training\n",
      "Training:: Epoch 242, Iteration 0, Current loss 0.06049759889410037 Accuracy 87.08203189215847\n",
      "Training:: Epoch 242, Iteration 10, Current loss 0.06588824656814801 Accuracy 82.84036902432206\n",
      "Training:: Epoch 242, Iteration 20, Current loss 0.08875896376980272 Accuracy 87.51308967384364\n",
      "Training:: Epoch 242, Iteration 30, Current loss 0.0692388169397418 Accuracy 84.68100321806202\n",
      "Training:: Epoch 242, Iteration 40, Current loss 0.07482938871483776 Accuracy 83.60141836682256\n",
      "Training:: Epoch 242, Iteration 50, Current loss 0.05552007875159611 Accuracy 81.8009556763882\n",
      "Training:: Epoch 242, Iteration 60, Current loss 0.06603924042258125 Accuracy 77.73728886221384\n",
      "Training:: Epoch 242, Iteration 70, Current loss 0.11660320562487292 Accuracy 85.51852361933618\n",
      "Training:: Epoch 242, Iteration 80, Current loss 0.35720790110909384 Accuracy 80.82886538182872\n",
      "Training:: Epoch 242, Iteration 90, Current loss 0.05945701197305881 Accuracy 89.9093022195889\n",
      "Training:: Epoch 242, Iteration 100, Current loss 0.17753552965112776 Accuracy 82.13999108337049\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 242, Probability Accuracy 42.26301943074947\n",
      "Starting Training\n",
      "Training:: Epoch 243, Iteration 0, Current loss 0.10883960292093739 Accuracy 82.44897959183673\n",
      "Training:: Epoch 243, Iteration 10, Current loss 0.07712562419017177 Accuracy 86.4965145041601\n",
      "Training:: Epoch 243, Iteration 20, Current loss 0.08945709464265114 Accuracy 83.62301753318374\n",
      "Training:: Epoch 243, Iteration 30, Current loss 0.07821583984714131 Accuracy 88.01900863713621\n",
      "Training:: Epoch 243, Iteration 40, Current loss 0.06526883098654057 Accuracy 79.50953678474114\n",
      "Training:: Epoch 243, Iteration 50, Current loss 0.06679267183284023 Accuracy 84.28752285834857\n",
      "Training:: Epoch 243, Iteration 60, Current loss 0.09956607092453791 Accuracy 88.30253451172302\n",
      "Training:: Epoch 243, Iteration 70, Current loss 0.07370747237586216 Accuracy 86.85797074356218\n",
      "Training:: Epoch 243, Iteration 80, Current loss 0.05439975489877637 Accuracy 80.21372058773161\n",
      "Training:: Epoch 243, Iteration 90, Current loss 0.11072279644408783 Accuracy 82.54193722943722\n",
      "Training:: Epoch 243, Iteration 100, Current loss 0.061466516368818246 Accuracy 86.58626114315679\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 243, Probability Accuracy 42.094502216514066\n",
      "Starting Training\n",
      "Training:: Epoch 244, Iteration 0, Current loss 0.06365075541545331 Accuracy 83.60732738374824\n",
      "Training:: Epoch 244, Iteration 10, Current loss 0.06977399816930574 Accuracy 88.2461043684674\n",
      "Training:: Epoch 244, Iteration 20, Current loss 0.06593258973955965 Accuracy 85.4643188137164\n",
      "Training:: Epoch 244, Iteration 30, Current loss 0.04710091050308968 Accuracy 91.24049863180298\n",
      "Training:: Epoch 244, Iteration 40, Current loss 0.05809460605137143 Accuracy 86.95478369014835\n",
      "Training:: Epoch 244, Iteration 50, Current loss 0.06804893809700847 Accuracy 85.20631551014992\n",
      "Training:: Epoch 244, Iteration 60, Current loss 0.051325337788164775 Accuracy 84.21397482098563\n",
      "Training:: Epoch 244, Iteration 70, Current loss 0.04657249074390002 Accuracy 85.93214953930878\n",
      "Training:: Epoch 244, Iteration 80, Current loss 0.06243820125983725 Accuracy 83.80741236509662\n",
      "Training:: Epoch 244, Iteration 90, Current loss 0.05852122048689622 Accuracy 81.39899325671954\n",
      "Training:: Epoch 244, Iteration 100, Current loss 0.05129887136862748 Accuracy 85.42815674891146\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 244, Probability Accuracy 43.41394953805361\n",
      "Starting Training\n",
      "Training:: Epoch 245, Iteration 0, Current loss 0.04235916740753761 Accuracy 83.79238636845902\n",
      "Training:: Epoch 245, Iteration 10, Current loss 0.06857043326034883 Accuracy 83.2585516476094\n",
      "Training:: Epoch 245, Iteration 20, Current loss 0.05938497399316526 Accuracy 83.26382073928688\n",
      "Training:: Epoch 245, Iteration 30, Current loss 0.05396882444486692 Accuracy 85.42686056458511\n",
      "Training:: Epoch 245, Iteration 40, Current loss 0.07685041590806077 Accuracy 83.73400902292812\n",
      "Training:: Epoch 245, Iteration 50, Current loss 0.05175133126795588 Accuracy 84.19937075674781\n",
      "Training:: Epoch 245, Iteration 60, Current loss 0.05202011739581448 Accuracy 85.34010556717459\n",
      "Training:: Epoch 245, Iteration 70, Current loss 0.05226048008705142 Accuracy 85.77139098315033\n",
      "Training:: Epoch 245, Iteration 80, Current loss 0.034393704619410256 Accuracy 90.03631210698786\n",
      "Training:: Epoch 245, Iteration 90, Current loss 0.03864622240788186 Accuracy 85.65374888934454\n",
      "Training:: Epoch 245, Iteration 100, Current loss 0.039394684473016583 Accuracy 85.87771864392575\n",
      "Calculating Expectation\n",
      "Epoch 245 iter 0\n",
      "Epoch 245 iter 10\n",
      "Epoch 245 iter 20\n",
      "Epoch 245 iter 30\n",
      "Epoch 245 iter 40\n",
      "Epoch 245 iter 50\n",
      "Epoch 245 iter 60\n",
      "Epoch 245 iter 70\n",
      "Epoch 245 iter 80\n",
      "Epoch 245 iter 90\n",
      "Epoch 245 iter 100\n",
      "Train Boundary avergage error = 121.095\n",
      "Train From boundary avergage accuracy = 84.614\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 245, Probability Accuracy 44.797924348510584\n",
      "Starting Training\n",
      "Training:: Epoch 246, Iteration 0, Current loss 0.042203897789537724 Accuracy 82.47873402081453\n",
      "Training:: Epoch 246, Iteration 10, Current loss 0.04625547064043919 Accuracy 86.99614183354768\n",
      "Training:: Epoch 246, Iteration 20, Current loss 0.04770371909957004 Accuracy 88.25535289452816\n",
      "Training:: Epoch 246, Iteration 30, Current loss 0.0657701669102152 Accuracy 81.80378272617162\n",
      "Training:: Epoch 246, Iteration 40, Current loss 0.039005567486496286 Accuracy 85.40626936751802\n",
      "Training:: Epoch 246, Iteration 50, Current loss 0.05155579076587951 Accuracy 87.51041666666667\n",
      "Training:: Epoch 246, Iteration 60, Current loss 0.06776184551956321 Accuracy 79.98685027577893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 246, Iteration 70, Current loss 0.061092738767387876 Accuracy 85.39551209727968\n",
      "Training:: Epoch 246, Iteration 80, Current loss 0.08669690024567005 Accuracy 83.19260554584062\n",
      "Training:: Epoch 246, Iteration 90, Current loss 0.041330643256506046 Accuracy 91.08642972536349\n",
      "Training:: Epoch 246, Iteration 100, Current loss 0.046240593837740526 Accuracy 81.53427056477112\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 246, Probability Accuracy 44.39408377180263\n",
      "Starting Training\n",
      "Training:: Epoch 247, Iteration 0, Current loss 0.06417037326943453 Accuracy 83.50298974943053\n",
      "Training:: Epoch 247, Iteration 10, Current loss 0.05255600905856118 Accuracy 88.98587933247754\n",
      "Training:: Epoch 247, Iteration 20, Current loss 0.06993236285045458 Accuracy 78.64364811885274\n",
      "Training:: Epoch 247, Iteration 30, Current loss 0.04640503866958918 Accuracy 88.10238030308933\n",
      "Training:: Epoch 247, Iteration 40, Current loss 0.057108723184833184 Accuracy 86.86463765859733\n",
      "Training:: Epoch 247, Iteration 50, Current loss 0.08429117431928047 Accuracy 85.57491682174476\n",
      "Training:: Epoch 247, Iteration 60, Current loss 0.052464350668265644 Accuracy 85.83637570527713\n",
      "Training:: Epoch 247, Iteration 70, Current loss 0.04795695954424885 Accuracy 88.810612048524\n",
      "Training:: Epoch 247, Iteration 80, Current loss 0.049204771490207855 Accuracy 85.3517877739331\n",
      "Training:: Epoch 247, Iteration 90, Current loss 0.05476262522562107 Accuracy 78.3998646820027\n",
      "Training:: Epoch 247, Iteration 100, Current loss 0.0445180980545761 Accuracy 90.10638903112022\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 247, Probability Accuracy 45.03241910759415\n",
      "Starting Training\n",
      "Training:: Epoch 248, Iteration 0, Current loss 0.07260707856853352 Accuracy 79.96277554818822\n",
      "Training:: Epoch 248, Iteration 10, Current loss 0.0706269525575581 Accuracy 83.78293271753962\n",
      "Training:: Epoch 248, Iteration 20, Current loss 0.0991880470746054 Accuracy 86.94756429853126\n",
      "Training:: Epoch 248, Iteration 30, Current loss 0.053071792530611867 Accuracy 84.83492749151496\n",
      "Training:: Epoch 248, Iteration 40, Current loss 0.0383241758575981 Accuracy 87.75900417528388\n",
      "Training:: Epoch 248, Iteration 50, Current loss 0.054518638316255356 Accuracy 82.74966574102643\n",
      "Training:: Epoch 248, Iteration 60, Current loss 0.03861957448899976 Accuracy 84.59986550100874\n",
      "Training:: Epoch 248, Iteration 70, Current loss 0.04272864173804004 Accuracy 84.60881448582165\n",
      "Training:: Epoch 248, Iteration 80, Current loss 0.0429672584542631 Accuracy 86.0970366958391\n",
      "Training:: Epoch 248, Iteration 90, Current loss 0.06452428359847756 Accuracy 88.34799463247353\n",
      "Training:: Epoch 248, Iteration 100, Current loss 0.04376306831694804 Accuracy 86.06600740650308\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 248, Probability Accuracy 44.270311140572566\n",
      "Starting Training\n",
      "Training:: Epoch 249, Iteration 0, Current loss 0.06165340065642755 Accuracy 85.14817835780315\n",
      "Training:: Epoch 249, Iteration 10, Current loss 0.3468534468586501 Accuracy 81.09293976864778\n",
      "Training:: Epoch 249, Iteration 20, Current loss 0.3606392471544048 Accuracy 76.19424949060448\n",
      "Training:: Epoch 249, Iteration 30, Current loss 0.19307759739237945 Accuracy 84.70228435900637\n",
      "Training:: Epoch 249, Iteration 40, Current loss 0.18264215220340385 Accuracy 83.6258609197956\n",
      "Training:: Epoch 249, Iteration 50, Current loss 0.16630916469812507 Accuracy 85.1795039164491\n",
      "Training:: Epoch 249, Iteration 60, Current loss 0.18139491855283568 Accuracy 87.30660567738464\n",
      "Training:: Epoch 249, Iteration 70, Current loss 0.20461719009564097 Accuracy 82.29902749597706\n",
      "Training:: Epoch 249, Iteration 80, Current loss 0.18151442336014642 Accuracy 80.7549412988557\n",
      "Training:: Epoch 249, Iteration 90, Current loss 0.28502141067328873 Accuracy 72.06931291557525\n",
      "Training:: Epoch 249, Iteration 100, Current loss 0.3934843017023375 Accuracy 82.42227671861403\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 249, Probability Accuracy 42.75210258109956\n",
      "Starting Training\n",
      "Training:: Epoch 250, Iteration 0, Current loss 0.10649355097800778 Accuracy 82.42639062311193\n",
      "Training:: Epoch 250, Iteration 10, Current loss 0.11390186996115273 Accuracy 79.04637278154902\n",
      "Training:: Epoch 250, Iteration 20, Current loss 0.07794163401943811 Accuracy 84.81101587747646\n",
      "Training:: Epoch 250, Iteration 30, Current loss 0.05375011600188102 Accuracy 91.29944446764964\n",
      "Training:: Epoch 250, Iteration 40, Current loss 0.16476909239045082 Accuracy 84.1067576380876\n",
      "Training:: Epoch 250, Iteration 50, Current loss 0.07941891120904324 Accuracy 84.46131062569418\n",
      "Training:: Epoch 250, Iteration 60, Current loss 0.08658619412339986 Accuracy 85.14791881664947\n",
      "Training:: Epoch 250, Iteration 70, Current loss 0.09177075981529596 Accuracy 86.41801815488624\n",
      "Training:: Epoch 250, Iteration 80, Current loss 0.10163757909015358 Accuracy 82.08289860570994\n",
      "Training:: Epoch 250, Iteration 90, Current loss 0.05598437330944208 Accuracy 85.40381791483114\n",
      "Training:: Epoch 250, Iteration 100, Current loss 0.09057763387579985 Accuracy 83.71873678732885\n",
      "Calculating Expectation\n",
      "Epoch 250 iter 0\n",
      "Epoch 250 iter 10\n",
      "Epoch 250 iter 20\n",
      "Epoch 250 iter 30\n",
      "Epoch 250 iter 40\n",
      "Epoch 250 iter 50\n",
      "Epoch 250 iter 60\n",
      "Epoch 250 iter 70\n",
      "Epoch 250 iter 80\n",
      "Epoch 250 iter 90\n",
      "Epoch 250 iter 100\n",
      "Train Boundary avergage error = 122.312\n",
      "Train From boundary avergage accuracy = 84.675\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 250, Probability Accuracy 44.36114678709036\n",
      "Starting Training\n",
      "Training:: Epoch 251, Iteration 0, Current loss 0.07421275288790247 Accuracy 87.24688644688645\n",
      "Training:: Epoch 251, Iteration 10, Current loss 0.07672338895576203 Accuracy 87.3048582490321\n",
      "Training:: Epoch 251, Iteration 20, Current loss 0.10240973993656836 Accuracy 85.98240929794251\n",
      "Training:: Epoch 251, Iteration 30, Current loss 0.10342748370246514 Accuracy 86.91843431384038\n",
      "Training:: Epoch 251, Iteration 40, Current loss 0.0778994655187055 Accuracy 82.11661861806007\n",
      "Training:: Epoch 251, Iteration 50, Current loss 0.08787146337834247 Accuracy 90.24906956770684\n",
      "Training:: Epoch 251, Iteration 60, Current loss 0.05763011562455329 Accuracy 85.77370779995486\n",
      "Training:: Epoch 251, Iteration 70, Current loss 0.08378490650242887 Accuracy 88.37177808874644\n",
      "Training:: Epoch 251, Iteration 80, Current loss 0.06905828086225968 Accuracy 82.34831350181925\n",
      "Training:: Epoch 251, Iteration 90, Current loss 0.06341197900182638 Accuracy 87.76389155931551\n",
      "Training:: Epoch 251, Iteration 100, Current loss 0.07144024463921059 Accuracy 79.38679245283019\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 251, Probability Accuracy 45.0988109541368\n",
      "Starting Training\n",
      "Training:: Epoch 252, Iteration 0, Current loss 0.06881219086020383 Accuracy 85.09546694375894\n",
      "Training:: Epoch 252, Iteration 10, Current loss 0.131259615015783 Accuracy 85.10055865921788\n",
      "Training:: Epoch 252, Iteration 20, Current loss 0.05297053762278 Accuracy 88.12254516889239\n",
      "Training:: Epoch 252, Iteration 30, Current loss 0.04425042331125128 Accuracy 88.08726684080791\n",
      "Training:: Epoch 252, Iteration 40, Current loss 0.07944535181909579 Accuracy 77.41046831955923\n",
      "Training:: Epoch 252, Iteration 50, Current loss 0.08047847943096618 Accuracy 82.9764453961456\n",
      "Training:: Epoch 252, Iteration 60, Current loss 0.09256492178293893 Accuracy 85.37123781833844\n",
      "Training:: Epoch 252, Iteration 70, Current loss 0.06977825363154164 Accuracy 89.46619891912393\n",
      "Training:: Epoch 252, Iteration 80, Current loss 0.08659765143705182 Accuracy 85.35323143579251\n",
      "Training:: Epoch 252, Iteration 90, Current loss 0.06276729886631106 Accuracy 86.06282043726796\n",
      "Training:: Epoch 252, Iteration 100, Current loss 0.043412386021553256 Accuracy 84.77325685606739\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 252, Probability Accuracy 44.78559887309939\n",
      "Starting Training\n",
      "Training:: Epoch 253, Iteration 0, Current loss 0.04756526598225864 Accuracy 86.27298627298627\n",
      "Training:: Epoch 253, Iteration 10, Current loss 0.09737460290909734 Accuracy 88.71401301180413\n",
      "Training:: Epoch 253, Iteration 20, Current loss 0.06502607201722342 Accuracy 84.08749591903363\n",
      "Training:: Epoch 253, Iteration 30, Current loss 0.09683567444342595 Accuracy 75.64194520154928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 253, Iteration 40, Current loss 0.04388376584418589 Accuracy 87.44786582660396\n",
      "Training:: Epoch 253, Iteration 50, Current loss 0.08673056736375588 Accuracy 86.83697849659988\n",
      "Training:: Epoch 253, Iteration 60, Current loss 0.04981425660930528 Accuracy 90.55975794251134\n",
      "Training:: Epoch 253, Iteration 70, Current loss 0.0808410820272538 Accuracy 87.07095209217708\n",
      "Training:: Epoch 253, Iteration 80, Current loss 0.04882903424077304 Accuracy 83.48073542951204\n",
      "Training:: Epoch 253, Iteration 90, Current loss 0.07471194258974077 Accuracy 82.34195342659085\n",
      "Training:: Epoch 253, Iteration 100, Current loss 0.09341522342845048 Accuracy 86.64091899386017\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 253, Probability Accuracy 44.20785516012761\n",
      "Starting Training\n",
      "Training:: Epoch 254, Iteration 0, Current loss 0.05840093662250524 Accuracy 75.58362335275486\n",
      "Training:: Epoch 254, Iteration 10, Current loss 0.05775036187433599 Accuracy 87.13406206809901\n",
      "Training:: Epoch 254, Iteration 20, Current loss 0.04998380965278976 Accuracy 86.11552128222576\n",
      "Training:: Epoch 254, Iteration 30, Current loss 0.059733201460975854 Accuracy 82.17154088583145\n",
      "Training:: Epoch 254, Iteration 40, Current loss 0.06391728817089212 Accuracy 85.46501359895447\n",
      "Training:: Epoch 254, Iteration 50, Current loss 0.06525743550799552 Accuracy 81.70699681110486\n",
      "Training:: Epoch 254, Iteration 60, Current loss 0.04716876743689964 Accuracy 84.18970720169644\n",
      "Training:: Epoch 254, Iteration 70, Current loss 0.0686028825631937 Accuracy 76.69711556684229\n",
      "Training:: Epoch 254, Iteration 80, Current loss 0.05770979149975728 Accuracy 81.99645803636992\n",
      "Training:: Epoch 254, Iteration 90, Current loss 0.05310850960874296 Accuracy 89.01550148855354\n",
      "Training:: Epoch 254, Iteration 100, Current loss 0.04032012836074647 Accuracy 88.8094521372667\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 254, Probability Accuracy 44.675083896093135\n",
      "Starting Training\n",
      "Training:: Epoch 255, Iteration 0, Current loss 0.0409416748146135 Accuracy 89.10095429432447\n",
      "Training:: Epoch 255, Iteration 10, Current loss 0.24580869939121158 Accuracy 78.60303063820373\n",
      "Training:: Epoch 255, Iteration 20, Current loss 0.22760933514021864 Accuracy 80.3255772005772\n",
      "Training:: Epoch 255, Iteration 30, Current loss 0.23850105992151857 Accuracy 76.07797140303843\n",
      "Training:: Epoch 255, Iteration 40, Current loss 0.09426941441974412 Accuracy 86.6092758876349\n",
      "Training:: Epoch 255, Iteration 50, Current loss 0.10169725014897632 Accuracy 86.93204530313125\n",
      "Training:: Epoch 255, Iteration 60, Current loss 0.1027297212033259 Accuracy 85.62906173842404\n",
      "Training:: Epoch 255, Iteration 70, Current loss 0.08249064315717305 Accuracy 82.17989985693849\n",
      "Training:: Epoch 255, Iteration 80, Current loss 0.12449110851445401 Accuracy 79.73403725470665\n",
      "Training:: Epoch 255, Iteration 90, Current loss 0.09153923353340726 Accuracy 87.84264561379732\n",
      "Training:: Epoch 255, Iteration 100, Current loss 0.07146923009309977 Accuracy 87.18557650365919\n",
      "Calculating Expectation\n",
      "Epoch 255 iter 0\n",
      "Epoch 255 iter 10\n",
      "Epoch 255 iter 20\n",
      "Epoch 255 iter 30\n",
      "Epoch 255 iter 40\n",
      "Epoch 255 iter 50\n",
      "Epoch 255 iter 60\n",
      "Epoch 255 iter 70\n",
      "Epoch 255 iter 80\n",
      "Epoch 255 iter 90\n",
      "Epoch 255 iter 100\n",
      "Train Boundary avergage error = 122.826\n",
      "Train From boundary avergage accuracy = 84.693\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 255, Probability Accuracy 43.82462609272072\n",
      "Starting Training\n",
      "Training:: Epoch 256, Iteration 0, Current loss 0.15931475984912882 Accuracy 88.6356699562519\n",
      "Training:: Epoch 256, Iteration 10, Current loss 0.12695255540790698 Accuracy 84.3794362198337\n",
      "Training:: Epoch 256, Iteration 20, Current loss 0.07779368251308434 Accuracy 87.48356861176201\n",
      "Training:: Epoch 256, Iteration 30, Current loss 0.10677510836703452 Accuracy 82.40947014371206\n",
      "Training:: Epoch 256, Iteration 40, Current loss 0.1486585824643912 Accuracy 84.63513337104564\n",
      "Training:: Epoch 256, Iteration 50, Current loss 0.11178834280151714 Accuracy 77.99757998705574\n",
      "Training:: Epoch 256, Iteration 60, Current loss 0.05751830338589775 Accuracy 83.75032920726889\n",
      "Training:: Epoch 256, Iteration 70, Current loss 0.10313010249984918 Accuracy 82.4051324051324\n",
      "Training:: Epoch 256, Iteration 80, Current loss 0.10603386951604711 Accuracy 83.56392058215793\n",
      "Training:: Epoch 256, Iteration 90, Current loss 0.06031363577447111 Accuracy 85.24703825400175\n",
      "Training:: Epoch 256, Iteration 100, Current loss 0.06492043138835796 Accuracy 86.9863452785435\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 256, Probability Accuracy 44.87177362555413\n",
      "Starting Training\n",
      "Training:: Epoch 257, Iteration 0, Current loss 0.0420353110748886 Accuracy 81.87402068317142\n",
      "Training:: Epoch 257, Iteration 10, Current loss 0.08881017453912139 Accuracy 87.44666280356108\n",
      "Training:: Epoch 257, Iteration 20, Current loss 0.08794879956527453 Accuracy 80.92800528401585\n",
      "Training:: Epoch 257, Iteration 30, Current loss 0.07710648569052342 Accuracy 87.66982622432859\n",
      "Training:: Epoch 257, Iteration 40, Current loss 0.1015612737054514 Accuracy 83.62628894352501\n",
      "Training:: Epoch 257, Iteration 50, Current loss 0.06269405354786842 Accuracy 84.1276198104392\n",
      "Training:: Epoch 257, Iteration 60, Current loss 0.050609907057668053 Accuracy 87.1675396231549\n",
      "Training:: Epoch 257, Iteration 70, Current loss 0.13294917642608028 Accuracy 82.79039271171233\n",
      "Training:: Epoch 257, Iteration 80, Current loss 0.04632736052927074 Accuracy 83.2457012931647\n",
      "Training:: Epoch 257, Iteration 90, Current loss 0.05063583600689617 Accuracy 81.64033723500042\n",
      "Training:: Epoch 257, Iteration 100, Current loss 0.05487150099735981 Accuracy 89.1268338854709\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 257, Probability Accuracy 45.430563036002816\n",
      "Starting Training\n",
      "Training:: Epoch 258, Iteration 0, Current loss 0.0431568656012015 Accuracy 82.86538270295355\n",
      "Training:: Epoch 258, Iteration 10, Current loss 0.050809267396993804 Accuracy 88.51208139041967\n",
      "Training:: Epoch 258, Iteration 20, Current loss 0.05056096238284804 Accuracy 86.24788414519466\n",
      "Training:: Epoch 258, Iteration 30, Current loss 0.057005262441513965 Accuracy 82.6914498141264\n",
      "Training:: Epoch 258, Iteration 40, Current loss 0.057349761331603584 Accuracy 86.89828801611279\n",
      "Training:: Epoch 258, Iteration 50, Current loss 0.063870583725374 Accuracy 77.13382507903056\n",
      "Training:: Epoch 258, Iteration 60, Current loss 0.054010526699437275 Accuracy 84.5680546923555\n",
      "Training:: Epoch 258, Iteration 70, Current loss 0.04883013170406335 Accuracy 77.71898883009995\n",
      "Training:: Epoch 258, Iteration 80, Current loss 0.06981908790722702 Accuracy 86.88607085346216\n",
      "Training:: Epoch 258, Iteration 90, Current loss 0.06875292920120757 Accuracy 88.19630826532288\n",
      "Training:: Epoch 258, Iteration 100, Current loss 0.05919163209837611 Accuracy 85.36161077799986\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 258, Probability Accuracy 46.2676596097278\n",
      "Starting Training\n",
      "Training:: Epoch 259, Iteration 0, Current loss 0.04735623533282747 Accuracy 79.25467423904027\n",
      "Training:: Epoch 259, Iteration 10, Current loss 0.045170783037398306 Accuracy 83.93372182000118\n",
      "Training:: Epoch 259, Iteration 20, Current loss 0.057099182996462085 Accuracy 84.45789265313049\n",
      "Training:: Epoch 259, Iteration 30, Current loss 0.05842558015258214 Accuracy 88.59670925088234\n",
      "Training:: Epoch 259, Iteration 40, Current loss 0.08088195197798256 Accuracy 73.33333333333333\n",
      "Training:: Epoch 259, Iteration 50, Current loss 0.06497492497542248 Accuracy 81.92781280412981\n",
      "Training:: Epoch 259, Iteration 60, Current loss 0.048041624803575836 Accuracy 79.80919077121898\n",
      "Training:: Epoch 259, Iteration 70, Current loss 0.0876108879985751 Accuracy 80.69261060921937\n",
      "Training:: Epoch 259, Iteration 80, Current loss 0.07992604468516606 Accuracy 83.4278865361566\n",
      "Training:: Epoch 259, Iteration 90, Current loss 0.04779710161403354 Accuracy 85.16654121189312\n",
      "Training:: Epoch 259, Iteration 100, Current loss 0.03121748653624236 Accuracy 84.88293129410376\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 259, Probability Accuracy 46.60086174752455\n",
      "Starting Training\n",
      "Training:: Epoch 260, Iteration 0, Current loss 0.057053581560827085 Accuracy 86.49593289273004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 260, Iteration 10, Current loss 0.07201300484899767 Accuracy 83.20213946916944\n",
      "Training:: Epoch 260, Iteration 20, Current loss 0.054130239129574695 Accuracy 84.52232851443524\n",
      "Training:: Epoch 260, Iteration 30, Current loss 0.0743069376453191 Accuracy 84.35762937446175\n",
      "Training:: Epoch 260, Iteration 40, Current loss 0.051564636870231585 Accuracy 83.31079253448742\n",
      "Training:: Epoch 260, Iteration 50, Current loss 0.06545628090467355 Accuracy 79.93339676498573\n",
      "Training:: Epoch 260, Iteration 60, Current loss 0.04951904553327332 Accuracy 82.89951096387443\n",
      "Training:: Epoch 260, Iteration 70, Current loss 0.10409963278063282 Accuracy 83.26835607537362\n",
      "Training:: Epoch 260, Iteration 80, Current loss 0.050044488241883296 Accuracy 88.55606372820023\n",
      "Training:: Epoch 260, Iteration 90, Current loss 0.061264716013141464 Accuracy 81.82410423452768\n",
      "Training:: Epoch 260, Iteration 100, Current loss 0.04140085475581504 Accuracy 89.45741281089343\n",
      "Calculating Expectation\n",
      "Epoch 260 iter 0\n",
      "Epoch 260 iter 10\n",
      "Epoch 260 iter 20\n",
      "Epoch 260 iter 30\n",
      "Epoch 260 iter 40\n",
      "Epoch 260 iter 50\n",
      "Epoch 260 iter 60\n",
      "Epoch 260 iter 70\n",
      "Epoch 260 iter 80\n",
      "Epoch 260 iter 90\n",
      "Epoch 260 iter 100\n",
      "Train Boundary avergage error = 122.941\n",
      "Train From boundary avergage accuracy = 84.742\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 260, Probability Accuracy 45.23035174213862\n",
      "Starting Training\n",
      "Training:: Epoch 261, Iteration 0, Current loss 0.07315203729477969 Accuracy 82.64184901013546\n",
      "Training:: Epoch 261, Iteration 10, Current loss 0.2413154809983455 Accuracy 74.80434912479838\n",
      "Training:: Epoch 261, Iteration 20, Current loss 0.11206956446738524 Accuracy 85.97198843967351\n",
      "Training:: Epoch 261, Iteration 30, Current loss 0.10387584565617425 Accuracy 85.30432690655394\n",
      "Training:: Epoch 261, Iteration 40, Current loss 0.14119115604440305 Accuracy 89.48322009470867\n",
      "Training:: Epoch 261, Iteration 50, Current loss 0.1521101093349031 Accuracy 82.01551602538986\n",
      "Training:: Epoch 261, Iteration 60, Current loss 0.08960268751570552 Accuracy 82.19066200825809\n",
      "Training:: Epoch 261, Iteration 70, Current loss 0.07278047570907989 Accuracy 79.44583318663521\n",
      "Training:: Epoch 261, Iteration 80, Current loss 0.07469258173519534 Accuracy 83.71446507039727\n",
      "Training:: Epoch 261, Iteration 90, Current loss 0.0725894450018691 Accuracy 86.19950945387934\n",
      "Training:: Epoch 261, Iteration 100, Current loss 0.10769191413885672 Accuracy 85.33355966728908\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 261, Probability Accuracy 44.64991506815263\n",
      "Starting Training\n",
      "Training:: Epoch 262, Iteration 0, Current loss 0.0632729233708277 Accuracy 83.21913413914338\n",
      "Training:: Epoch 262, Iteration 10, Current loss 0.08494725085332268 Accuracy 82.21637914448792\n",
      "Training:: Epoch 262, Iteration 20, Current loss 0.07000947249071818 Accuracy 88.97444996451384\n",
      "Training:: Epoch 262, Iteration 30, Current loss 0.05234094756317416 Accuracy 87.91962016851679\n",
      "Training:: Epoch 262, Iteration 40, Current loss 0.08479041768904325 Accuracy 80.59707812830828\n",
      "Training:: Epoch 262, Iteration 50, Current loss 0.06995798611950499 Accuracy 86.58803785371312\n",
      "Training:: Epoch 262, Iteration 60, Current loss 0.1238854785531959 Accuracy 81.04098893949252\n",
      "Training:: Epoch 262, Iteration 70, Current loss 0.06925672789137224 Accuracy 85.70847357325147\n",
      "Training:: Epoch 262, Iteration 80, Current loss 0.08738786984508308 Accuracy 77.57155635062612\n",
      "Training:: Epoch 262, Iteration 90, Current loss 0.09052952870291349 Accuracy 84.52246130615326\n",
      "Training:: Epoch 262, Iteration 100, Current loss 0.08865495660475586 Accuracy 83.25141937123972\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 262, Probability Accuracy 44.49175539627957\n",
      "Starting Training\n",
      "Training:: Epoch 263, Iteration 0, Current loss 0.04532488404458272 Accuracy 90.02029769959404\n",
      "Training:: Epoch 263, Iteration 10, Current loss 0.051165036907103695 Accuracy 84.11095527505064\n",
      "Training:: Epoch 263, Iteration 20, Current loss 0.04923397786282565 Accuracy 83.47799879445449\n",
      "Training:: Epoch 263, Iteration 30, Current loss 0.06819077129190185 Accuracy 86.72151334922839\n",
      "Training:: Epoch 263, Iteration 40, Current loss 0.049860581042509663 Accuracy 86.6774251389636\n",
      "Training:: Epoch 263, Iteration 50, Current loss 0.0636798458576583 Accuracy 88.51305695774872\n",
      "Training:: Epoch 263, Iteration 60, Current loss 0.039180132385493116 Accuracy 81.2872405999589\n",
      "Training:: Epoch 263, Iteration 70, Current loss 0.04323695616693696 Accuracy 88.04017672235261\n",
      "Training:: Epoch 263, Iteration 80, Current loss 0.0863005822165786 Accuracy 83.20656843713203\n",
      "Training:: Epoch 263, Iteration 90, Current loss 0.06321607164594216 Accuracy 84.74051356064706\n",
      "Training:: Epoch 263, Iteration 100, Current loss 0.05986862470517382 Accuracy 77.64106435176448\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 263, Probability Accuracy 45.12045821767411\n",
      "Starting Training\n",
      "Training:: Epoch 264, Iteration 0, Current loss 0.0507035704751821 Accuracy 86.94365046957942\n",
      "Training:: Epoch 264, Iteration 10, Current loss 0.0865671897809007 Accuracy 84.02631191399539\n",
      "Training:: Epoch 264, Iteration 20, Current loss 0.04849363964172652 Accuracy 88.33098766454177\n",
      "Training:: Epoch 264, Iteration 30, Current loss 0.04713001743640986 Accuracy 85.17721978605145\n",
      "Training:: Epoch 264, Iteration 40, Current loss 0.046591075260998205 Accuracy 91.03207174342606\n",
      "Training:: Epoch 264, Iteration 50, Current loss 0.035922683315265345 Accuracy 86.94117647058823\n",
      "Training:: Epoch 264, Iteration 60, Current loss 0.046373426004060005 Accuracy 83.60725903098785\n",
      "Training:: Epoch 264, Iteration 70, Current loss 0.057775324598648745 Accuracy 88.72049304236599\n",
      "Training:: Epoch 264, Iteration 80, Current loss 0.04712889198395846 Accuracy 80.2076806015576\n",
      "Training:: Epoch 264, Iteration 90, Current loss 0.07711866046931028 Accuracy 82.26816505104229\n",
      "Training:: Epoch 264, Iteration 100, Current loss 0.08286966738716246 Accuracy 84.02370405842213\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 264, Probability Accuracy 43.88915358163815\n",
      "Starting Training\n",
      "Training:: Epoch 265, Iteration 0, Current loss 0.04686709129192001 Accuracy 82.56459681490661\n",
      "Training:: Epoch 265, Iteration 10, Current loss 0.31370335992951487 Accuracy 84.31601025402004\n",
      "Training:: Epoch 265, Iteration 20, Current loss 0.14407281338669944 Accuracy 82.729431825102\n",
      "Training:: Epoch 265, Iteration 30, Current loss 0.32709677396615444 Accuracy 74.84828680795444\n",
      "Training:: Epoch 265, Iteration 40, Current loss 0.1626146367532139 Accuracy 66.67530160609873\n",
      "Training:: Epoch 265, Iteration 50, Current loss 0.12873811088480938 Accuracy 86.31500505668814\n",
      "Training:: Epoch 265, Iteration 60, Current loss 0.11362888008735605 Accuracy 87.44534976152623\n",
      "Training:: Epoch 265, Iteration 70, Current loss 0.10306778610808226 Accuracy 83.7740059507709\n",
      "Training:: Epoch 265, Iteration 80, Current loss 0.08908334667331606 Accuracy 80.61195445920303\n",
      "Training:: Epoch 265, Iteration 90, Current loss 0.08162470822755145 Accuracy 83.23622570680014\n",
      "Training:: Epoch 265, Iteration 100, Current loss 0.0818559138141102 Accuracy 84.64143340906168\n",
      "Calculating Expectation\n",
      "Epoch 265 iter 0\n",
      "Epoch 265 iter 10\n",
      "Epoch 265 iter 20\n",
      "Epoch 265 iter 30\n",
      "Epoch 265 iter 40\n",
      "Epoch 265 iter 50\n",
      "Epoch 265 iter 60\n",
      "Epoch 265 iter 70\n",
      "Epoch 265 iter 80\n",
      "Epoch 265 iter 90\n",
      "Epoch 265 iter 100\n",
      "Train Boundary avergage error = 122.943\n",
      "Train From boundary avergage accuracy = 84.659\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 265, Probability Accuracy 45.02682603471848\n",
      "Starting Training\n",
      "Training:: Epoch 266, Iteration 0, Current loss 0.08290352853958322 Accuracy 85.37553766405647\n",
      "Training:: Epoch 266, Iteration 10, Current loss 0.09373867316609424 Accuracy 81.01704922872537\n",
      "Training:: Epoch 266, Iteration 20, Current loss 0.07928103731326838 Accuracy 85.59179628592356\n",
      "Training:: Epoch 266, Iteration 30, Current loss 0.07600501674074286 Accuracy 84.41674087266252\n",
      "Training:: Epoch 266, Iteration 40, Current loss 0.05406542300389027 Accuracy 86.16069812072274\n",
      "Training:: Epoch 266, Iteration 50, Current loss 0.06041563213558791 Accuracy 90.61683926159388\n",
      "Training:: Epoch 266, Iteration 60, Current loss 0.08164465582254676 Accuracy 83.8724584103512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 266, Iteration 70, Current loss 0.05646043924885337 Accuracy 88.90158063719437\n",
      "Training:: Epoch 266, Iteration 80, Current loss 0.07424570538016188 Accuracy 82.99510408491236\n",
      "Training:: Epoch 266, Iteration 90, Current loss 0.06718355299648922 Accuracy 81.13328764776426\n",
      "Training:: Epoch 266, Iteration 100, Current loss 0.05921026721409684 Accuracy 85.07694052705206\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 266, Probability Accuracy 42.80575465053652\n",
      "Starting Training\n",
      "Training:: Epoch 267, Iteration 0, Current loss 0.17769032801768475 Accuracy 77.17431877566256\n",
      "Training:: Epoch 267, Iteration 10, Current loss 0.1670176173600744 Accuracy 84.58362328926003\n",
      "Training:: Epoch 267, Iteration 20, Current loss 0.07913232933313274 Accuracy 85.45849802371542\n",
      "Training:: Epoch 267, Iteration 30, Current loss 0.1253031533549924 Accuracy 83.93832762292804\n",
      "Training:: Epoch 267, Iteration 40, Current loss 0.12253040362556564 Accuracy 81.23992795525642\n",
      "Training:: Epoch 267, Iteration 50, Current loss 0.07326695405177365 Accuracy 74.44540065021992\n",
      "Training:: Epoch 267, Iteration 60, Current loss 0.11306586293248458 Accuracy 75.70772403449568\n",
      "Training:: Epoch 267, Iteration 70, Current loss 0.08017757284161908 Accuracy 82.19204908936615\n",
      "Training:: Epoch 267, Iteration 80, Current loss 0.1377686228825096 Accuracy 83.98086854053305\n",
      "Training:: Epoch 267, Iteration 90, Current loss 0.09904800981848089 Accuracy 88.45961579661501\n",
      "Training:: Epoch 267, Iteration 100, Current loss 0.07554447207282611 Accuracy 83.61928658619287\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 267, Probability Accuracy 45.004971620333926\n",
      "Starting Training\n",
      "Training:: Epoch 268, Iteration 0, Current loss 0.06993751555456451 Accuracy 85.09932729052838\n",
      "Training:: Epoch 268, Iteration 10, Current loss 0.07500020090891427 Accuracy 88.11215129133933\n",
      "Training:: Epoch 268, Iteration 20, Current loss 0.0937079298143234 Accuracy 80.06436884847123\n",
      "Training:: Epoch 268, Iteration 30, Current loss 0.0675641341910674 Accuracy 85.46898673155502\n",
      "Training:: Epoch 268, Iteration 40, Current loss 0.04390246722115956 Accuracy 85.22082921500287\n",
      "Training:: Epoch 268, Iteration 50, Current loss 0.03861151714839141 Accuracy 85.76559277434038\n",
      "Training:: Epoch 268, Iteration 60, Current loss 0.05867042434870285 Accuracy 88.73376370848565\n",
      "Training:: Epoch 268, Iteration 70, Current loss 0.044337582351465045 Accuracy 87.05314429851265\n",
      "Training:: Epoch 268, Iteration 80, Current loss 0.04398184097590707 Accuracy 81.20478973394908\n",
      "Training:: Epoch 268, Iteration 90, Current loss 0.07779936360428757 Accuracy 77.0355669295451\n",
      "Training:: Epoch 268, Iteration 100, Current loss 0.05891943063256822 Accuracy 86.51016117729502\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 268, Probability Accuracy 45.54760326469735\n",
      "Starting Training\n",
      "Training:: Epoch 269, Iteration 0, Current loss 0.043283117946336574 Accuracy 85.40667878988262\n",
      "Training:: Epoch 269, Iteration 10, Current loss 0.05395989041100293 Accuracy 81.25554569653949\n",
      "Training:: Epoch 269, Iteration 20, Current loss 0.07573214487615924 Accuracy 87.30166362150207\n",
      "Training:: Epoch 269, Iteration 30, Current loss 0.04586655711147825 Accuracy 85.66408223330185\n",
      "Training:: Epoch 269, Iteration 40, Current loss 0.06571898501560544 Accuracy 86.23430006850879\n",
      "Training:: Epoch 269, Iteration 50, Current loss 0.0545367091943771 Accuracy 85.18045212179968\n",
      "Training:: Epoch 269, Iteration 60, Current loss 0.05152402567708635 Accuracy 87.17316484306775\n",
      "Training:: Epoch 269, Iteration 70, Current loss 0.07290787876152396 Accuracy 80.87771942985746\n",
      "Training:: Epoch 269, Iteration 80, Current loss 0.07083526688901746 Accuracy 87.38882821032922\n",
      "Training:: Epoch 269, Iteration 90, Current loss 0.1193171581226876 Accuracy 87.36842105263158\n",
      "Training:: Epoch 269, Iteration 100, Current loss 0.038758160628201185 Accuracy 87.41061851984269\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 269, Probability Accuracy 44.70346356216597\n",
      "Starting Training\n",
      "Training:: Epoch 270, Iteration 0, Current loss 0.06312374268210375 Accuracy 85.86978568048137\n",
      "Training:: Epoch 270, Iteration 10, Current loss 0.043439529396153205 Accuracy 85.93256307474653\n",
      "Training:: Epoch 270, Iteration 20, Current loss 0.03703955627455558 Accuracy 85.49945256649707\n",
      "Training:: Epoch 270, Iteration 30, Current loss 0.06407175393652811 Accuracy 78.88926694642925\n",
      "Training:: Epoch 270, Iteration 40, Current loss 0.035340249255183774 Accuracy 88.54306113342258\n",
      "Training:: Epoch 270, Iteration 50, Current loss 0.052845838037401976 Accuracy 85.84558178692953\n",
      "Training:: Epoch 270, Iteration 60, Current loss 0.04203910679071943 Accuracy 84.03814064362336\n",
      "Training:: Epoch 270, Iteration 70, Current loss 0.0706500183078021 Accuracy 82.11652794292509\n",
      "Training:: Epoch 270, Iteration 80, Current loss 0.06535395595081343 Accuracy 80.19520308123249\n",
      "Training:: Epoch 270, Iteration 90, Current loss 0.03354503445528295 Accuracy 89.03935567468686\n",
      "Training:: Epoch 270, Iteration 100, Current loss 0.06538089338186304 Accuracy 87.29121017277919\n",
      "Calculating Expectation\n",
      "Epoch 270 iter 0\n",
      "Epoch 270 iter 10\n",
      "Epoch 270 iter 20\n",
      "Epoch 270 iter 30\n",
      "Epoch 270 iter 40\n",
      "Epoch 270 iter 50\n",
      "Epoch 270 iter 60\n",
      "Epoch 270 iter 70\n",
      "Epoch 270 iter 80\n",
      "Epoch 270 iter 90\n",
      "Epoch 270 iter 100\n",
      "Train Boundary avergage error = 123.612\n",
      "Train From boundary avergage accuracy = 84.559\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 270, Probability Accuracy 44.88016323486763\n",
      "Starting Training\n",
      "Training:: Epoch 271, Iteration 0, Current loss 0.04415396951919758 Accuracy 86.38007562058195\n",
      "Training:: Epoch 271, Iteration 10, Current loss 0.06078566334981233 Accuracy 84.15562320916905\n",
      "Training:: Epoch 271, Iteration 20, Current loss 0.08802245709354498 Accuracy 75.48381749616675\n",
      "Training:: Epoch 271, Iteration 30, Current loss 0.045440694321006725 Accuracy 81.93237394884065\n",
      "Training:: Epoch 271, Iteration 40, Current loss 0.051556267789084974 Accuracy 79.18135526831179\n",
      "Training:: Epoch 271, Iteration 50, Current loss 0.04876105606060092 Accuracy 89.0661286024289\n",
      "Training:: Epoch 271, Iteration 60, Current loss 0.04857264287320926 Accuracy 88.57719328098791\n",
      "Training:: Epoch 271, Iteration 70, Current loss 0.03689384243978953 Accuracy 86.81697612732096\n",
      "Training:: Epoch 271, Iteration 80, Current loss 0.05002610404564628 Accuracy 83.92464678178963\n",
      "Training:: Epoch 271, Iteration 90, Current loss 0.08560018349260633 Accuracy 86.05621861344812\n",
      "Training:: Epoch 271, Iteration 100, Current loss 0.07793034391031617 Accuracy 83.87269609944278\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 271, Probability Accuracy 44.20236566267556\n",
      "Starting Training\n",
      "Training:: Epoch 272, Iteration 0, Current loss 0.052922578515402795 Accuracy 89.91995532390172\n",
      "Training:: Epoch 272, Iteration 10, Current loss 0.057013151014463324 Accuracy 87.2560136065441\n",
      "Training:: Epoch 272, Iteration 20, Current loss 0.13993162862991404 Accuracy 84.24953277050976\n",
      "Training:: Epoch 272, Iteration 30, Current loss 0.05672970308236139 Accuracy 88.07607525503666\n",
      "Training:: Epoch 272, Iteration 40, Current loss 0.08061573682694823 Accuracy 77.021113243762\n",
      "Training:: Epoch 272, Iteration 50, Current loss 0.05652848080868008 Accuracy 85.6617087933814\n",
      "Training:: Epoch 272, Iteration 60, Current loss 0.039397614661684836 Accuracy 87.05260629353832\n",
      "Training:: Epoch 272, Iteration 70, Current loss 0.20751550129862484 Accuracy 85.14923342102875\n",
      "Training:: Epoch 272, Iteration 80, Current loss 0.06135629288352277 Accuracy 83.5926222935044\n",
      "Training:: Epoch 272, Iteration 90, Current loss 0.0520680900363563 Accuracy 82.42919389978213\n",
      "Training:: Epoch 272, Iteration 100, Current loss 0.05144779337611608 Accuracy 84.20310296191819\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 272, Probability Accuracy 44.11846956954054\n",
      "Starting Training\n",
      "Training:: Epoch 273, Iteration 0, Current loss 0.057462988139293444 Accuracy 84.96620245948367\n",
      "Training:: Epoch 273, Iteration 10, Current loss 0.07095146107426523 Accuracy 80.52399481193255\n",
      "Training:: Epoch 273, Iteration 20, Current loss 0.04985333888067269 Accuracy 85.66205462757186\n",
      "Training:: Epoch 273, Iteration 30, Current loss 0.12262540948068479 Accuracy 85.36546694149358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 273, Iteration 40, Current loss 0.056058229629344786 Accuracy 87.88973638431688\n",
      "Training:: Epoch 273, Iteration 50, Current loss 0.06678937409500768 Accuracy 80.74817051327977\n",
      "Training:: Epoch 273, Iteration 60, Current loss 0.064020821571125 Accuracy 78.41110179752259\n",
      "Training:: Epoch 273, Iteration 70, Current loss 0.050466009035266705 Accuracy 85.60696065413376\n",
      "Training:: Epoch 273, Iteration 80, Current loss 0.04973831776496858 Accuracy 89.4576042534619\n",
      "Training:: Epoch 273, Iteration 90, Current loss 0.04331538112081657 Accuracy 87.87789556340793\n",
      "Training:: Epoch 273, Iteration 100, Current loss 0.06882494352518681 Accuracy 87.05494693598082\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 273, Probability Accuracy 44.52168869370676\n",
      "Starting Training\n",
      "Training:: Epoch 274, Iteration 0, Current loss 0.06258079952557676 Accuracy 80.29761053083416\n",
      "Training:: Epoch 274, Iteration 10, Current loss 0.134152669812698 Accuracy 87.10679611650485\n",
      "Training:: Epoch 274, Iteration 20, Current loss 0.09802770280653625 Accuracy 84.77230500891064\n",
      "Training:: Epoch 274, Iteration 30, Current loss 0.0680707120507783 Accuracy 88.31888648108018\n",
      "Training:: Epoch 274, Iteration 40, Current loss 0.05517472147499007 Accuracy 92.09535759096612\n",
      "Training:: Epoch 274, Iteration 50, Current loss 0.05152774566193594 Accuracy 82.03119535566903\n",
      "Training:: Epoch 274, Iteration 60, Current loss 0.04938190147073416 Accuracy 79.59996904982978\n",
      "Training:: Epoch 274, Iteration 70, Current loss 0.03993840380785875 Accuracy 88.46971528617283\n",
      "Training:: Epoch 274, Iteration 80, Current loss 0.11046155567853813 Accuracy 82.39508700102354\n",
      "Training:: Epoch 274, Iteration 90, Current loss 0.05880103884344054 Accuracy 86.36716325263811\n",
      "Training:: Epoch 274, Iteration 100, Current loss 0.06444489968263409 Accuracy 83.0528310298707\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 274, Probability Accuracy 43.767452458880555\n",
      "Starting Training\n",
      "Training:: Epoch 275, Iteration 0, Current loss 0.06631945913164268 Accuracy 88.07259780296123\n",
      "Training:: Epoch 275, Iteration 10, Current loss 0.05707759091950716 Accuracy 86.0525600714258\n",
      "Training:: Epoch 275, Iteration 20, Current loss 0.059725385500565185 Accuracy 83.91157011457156\n",
      "Training:: Epoch 275, Iteration 30, Current loss 0.05833143021065503 Accuracy 85.20071707535693\n",
      "Training:: Epoch 275, Iteration 40, Current loss 0.05959317636615883 Accuracy 87.61789500041733\n",
      "Training:: Epoch 275, Iteration 50, Current loss 0.05876044045061205 Accuracy 86.78460898886784\n",
      "Training:: Epoch 275, Iteration 60, Current loss 0.039510211614560684 Accuracy 84.81589221465207\n",
      "Training:: Epoch 275, Iteration 70, Current loss 0.04743804738589197 Accuracy 80.52495409781339\n",
      "Training:: Epoch 275, Iteration 80, Current loss 0.049038468245634285 Accuracy 80.27430810678423\n",
      "Training:: Epoch 275, Iteration 90, Current loss 0.03423104609642097 Accuracy 76.11645369516533\n",
      "Training:: Epoch 275, Iteration 100, Current loss 0.061998470178696634 Accuracy 86.76189445586697\n",
      "Calculating Expectation\n",
      "Epoch 275 iter 0\n",
      "Epoch 275 iter 10\n",
      "Epoch 275 iter 20\n",
      "Epoch 275 iter 30\n",
      "Epoch 275 iter 40\n",
      "Epoch 275 iter 50\n",
      "Epoch 275 iter 60\n",
      "Epoch 275 iter 70\n",
      "Epoch 275 iter 80\n",
      "Epoch 275 iter 90\n",
      "Epoch 275 iter 100\n",
      "Train Boundary avergage error = 124.858\n",
      "Train From boundary avergage accuracy = 84.261\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 275, Probability Accuracy 44.69818121556117\n",
      "Starting Training\n",
      "Training:: Epoch 276, Iteration 0, Current loss 0.06464929298271421 Accuracy 87.64500092064077\n",
      "Training:: Epoch 276, Iteration 10, Current loss 0.3112156201705511 Accuracy 75.06582290175066\n",
      "Training:: Epoch 276, Iteration 20, Current loss 0.22105210321199417 Accuracy 83.810318275154\n",
      "Training:: Epoch 276, Iteration 30, Current loss 0.10601568436842723 Accuracy 81.84679122017289\n",
      "Training:: Epoch 276, Iteration 40, Current loss 0.20046012902197255 Accuracy 85.3484406162384\n",
      "Training:: Epoch 276, Iteration 50, Current loss 0.1257573806822244 Accuracy 85.52466928852341\n",
      "Training:: Epoch 276, Iteration 60, Current loss 0.13484350361997355 Accuracy 81.82414124639455\n",
      "Training:: Epoch 276, Iteration 70, Current loss 0.1706443289901243 Accuracy 69.99764687426465\n",
      "Training:: Epoch 276, Iteration 80, Current loss 0.08883867448706108 Accuracy 87.48303848024783\n",
      "Training:: Epoch 276, Iteration 90, Current loss 0.13761490496978734 Accuracy 85.11559110331982\n",
      "Training:: Epoch 276, Iteration 100, Current loss 0.05668569930053642 Accuracy 86.77318135245902\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 276, Probability Accuracy 43.1554252806894\n",
      "Starting Training\n",
      "Training:: Epoch 277, Iteration 0, Current loss 0.11017820126388737 Accuracy 81.70369322201783\n",
      "Training:: Epoch 277, Iteration 10, Current loss 0.14348278749797905 Accuracy 87.00635801916361\n",
      "Training:: Epoch 277, Iteration 20, Current loss 0.09897609940359026 Accuracy 84.07544412476595\n",
      "Training:: Epoch 277, Iteration 30, Current loss 0.10998156427982285 Accuracy 78.93312717433321\n",
      "Training:: Epoch 277, Iteration 40, Current loss 0.12102964223948807 Accuracy 79.37115560790473\n",
      "Training:: Epoch 277, Iteration 50, Current loss 0.07592141298928476 Accuracy 85.86013695787508\n",
      "Training:: Epoch 277, Iteration 60, Current loss 0.06379458724785635 Accuracy 88.07607211787412\n",
      "Training:: Epoch 277, Iteration 70, Current loss 0.06848716412059415 Accuracy 84.87376863863918\n",
      "Training:: Epoch 277, Iteration 80, Current loss 0.059220252917591594 Accuracy 85.11825774724846\n",
      "Training:: Epoch 277, Iteration 90, Current loss 0.11549889229969505 Accuracy 84.08343077801648\n",
      "Training:: Epoch 277, Iteration 100, Current loss 0.055701465062196885 Accuracy 83.04705325240002\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 277, Probability Accuracy 44.95266603140407\n",
      "Starting Training\n",
      "Training:: Epoch 278, Iteration 0, Current loss 0.06507565659454029 Accuracy 77.62840719451141\n",
      "Training:: Epoch 278, Iteration 10, Current loss 0.06091989923182928 Accuracy 85.10076206604572\n",
      "Training:: Epoch 278, Iteration 20, Current loss 0.11647818852550115 Accuracy 78.48856929651991\n",
      "Training:: Epoch 278, Iteration 30, Current loss 0.11904335550252766 Accuracy 88.76691729323308\n",
      "Training:: Epoch 278, Iteration 40, Current loss 0.060401216751972624 Accuracy 84.55921725161856\n",
      "Training:: Epoch 278, Iteration 50, Current loss 0.07628242941396735 Accuracy 87.06429548563611\n",
      "Training:: Epoch 278, Iteration 60, Current loss 0.07395560046617329 Accuracy 76.43714971977582\n",
      "Training:: Epoch 278, Iteration 70, Current loss 0.07435698551244005 Accuracy 82.10553716856941\n",
      "Training:: Epoch 278, Iteration 80, Current loss 0.06458415345907673 Accuracy 81.32922579667694\n",
      "Training:: Epoch 278, Iteration 90, Current loss 0.06164462053996966 Accuracy 81.77218168980937\n",
      "Training:: Epoch 278, Iteration 100, Current loss 0.06022291849212134 Accuracy 89.50365726227795\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 278, Probability Accuracy 44.13110577122261\n",
      "Starting Training\n",
      "Training:: Epoch 279, Iteration 0, Current loss 0.05038829797664866 Accuracy 86.4275092936803\n",
      "Training:: Epoch 279, Iteration 10, Current loss 0.1274120063938801 Accuracy 83.85004292664314\n",
      "Training:: Epoch 279, Iteration 20, Current loss 0.06125868653731942 Accuracy 83.58267459332332\n",
      "Training:: Epoch 279, Iteration 30, Current loss 0.05281150509478051 Accuracy 84.00120031392825\n",
      "Training:: Epoch 279, Iteration 40, Current loss 0.09429757953924599 Accuracy 79.72936278059377\n",
      "Training:: Epoch 279, Iteration 50, Current loss 0.1016089265870357 Accuracy 83.2124842370744\n",
      "Training:: Epoch 279, Iteration 60, Current loss 0.07122872162963796 Accuracy 88.01368616459797\n",
      "Training:: Epoch 279, Iteration 70, Current loss 0.07479123099622904 Accuracy 81.35572740454326\n",
      "Training:: Epoch 279, Iteration 80, Current loss 0.06128831418443783 Accuracy 82.48837386597545\n",
      "Training:: Epoch 279, Iteration 90, Current loss 0.05636370380279831 Accuracy 77.29869355720034\n",
      "Training:: Epoch 279, Iteration 100, Current loss 0.056617195253645405 Accuracy 78.39838103498121\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 279, Probability Accuracy 44.42080623109749\n",
      "Starting Training\n",
      "Training:: Epoch 280, Iteration 0, Current loss 0.03685438681823069 Accuracy 84.98751083244125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 280, Iteration 10, Current loss 0.05929352853995367 Accuracy 82.54166466548196\n",
      "Training:: Epoch 280, Iteration 20, Current loss 0.0596652652444878 Accuracy 83.88929618768329\n",
      "Training:: Epoch 280, Iteration 30, Current loss 0.04223739722477591 Accuracy 82.57693917145357\n",
      "Training:: Epoch 280, Iteration 40, Current loss 0.505251823963212 Accuracy 81.56685646966424\n",
      "Training:: Epoch 280, Iteration 50, Current loss 0.09629218483183974 Accuracy 86.39383470635131\n",
      "Training:: Epoch 280, Iteration 60, Current loss 0.07393011468247573 Accuracy 82.71249165807195\n",
      "Training:: Epoch 280, Iteration 70, Current loss 0.04698031850836273 Accuracy 88.8698129748685\n",
      "Training:: Epoch 280, Iteration 80, Current loss 0.05600960300013536 Accuracy 82.80305320435308\n",
      "Training:: Epoch 280, Iteration 90, Current loss 0.060781275250061426 Accuracy 81.60207243358562\n",
      "Training:: Epoch 280, Iteration 100, Current loss 0.061188779985498466 Accuracy 85.86670871439084\n",
      "Calculating Expectation\n",
      "Epoch 280 iter 0\n",
      "Epoch 280 iter 10\n",
      "Epoch 280 iter 20\n",
      "Epoch 280 iter 30\n",
      "Epoch 280 iter 40\n",
      "Epoch 280 iter 50\n",
      "Epoch 280 iter 60\n",
      "Epoch 280 iter 70\n",
      "Epoch 280 iter 80\n",
      "Epoch 280 iter 90\n",
      "Epoch 280 iter 100\n",
      "Train Boundary avergage error = 125.175\n",
      "Train From boundary avergage accuracy = 84.262\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 280, Probability Accuracy 44.922525583129634\n",
      "Starting Training\n",
      "Training:: Epoch 281, Iteration 0, Current loss 0.036050566262500174 Accuracy 84.24292038081377\n",
      "Training:: Epoch 281, Iteration 10, Current loss 0.0599303134616686 Accuracy 82.9194509662272\n",
      "Training:: Epoch 281, Iteration 20, Current loss 0.05734648942242329 Accuracy 84.08049027650436\n",
      "Training:: Epoch 281, Iteration 30, Current loss 0.043121552740516156 Accuracy 85.38672517827756\n",
      "Training:: Epoch 281, Iteration 40, Current loss 0.06183123928890124 Accuracy 89.14867699803749\n",
      "Training:: Epoch 281, Iteration 50, Current loss 0.042573751087764296 Accuracy 86.10726395720904\n",
      "Training:: Epoch 281, Iteration 60, Current loss 0.056249214157566375 Accuracy 91.49749154124373\n",
      "Training:: Epoch 281, Iteration 70, Current loss 0.04903950229455194 Accuracy 83.90043742499903\n",
      "Training:: Epoch 281, Iteration 80, Current loss 0.05096730525925401 Accuracy 82.88815398965488\n",
      "Training:: Epoch 281, Iteration 90, Current loss 0.03315735832844677 Accuracy 89.76073280513806\n",
      "Training:: Epoch 281, Iteration 100, Current loss 0.04050022910763425 Accuracy 82.61673860642412\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 281, Probability Accuracy 45.74242863653313\n",
      "Starting Training\n",
      "Training:: Epoch 282, Iteration 0, Current loss 0.0641515496719872 Accuracy 81.2872260452206\n",
      "Training:: Epoch 282, Iteration 10, Current loss 0.04526356673688274 Accuracy 83.40603610313154\n",
      "Training:: Epoch 282, Iteration 20, Current loss 0.06218152814045526 Accuracy 83.51357316265725\n",
      "Training:: Epoch 282, Iteration 30, Current loss 0.0861094416748097 Accuracy 85.3955254881016\n",
      "Training:: Epoch 282, Iteration 40, Current loss 0.040436486955837576 Accuracy 83.01158301158301\n",
      "Training:: Epoch 282, Iteration 50, Current loss 0.05418878051413658 Accuracy 83.3394535840188\n",
      "Training:: Epoch 282, Iteration 60, Current loss 0.05027652343051576 Accuracy 76.14421416234887\n",
      "Training:: Epoch 282, Iteration 70, Current loss 0.06130444313518328 Accuracy 81.1699550017307\n",
      "Training:: Epoch 282, Iteration 80, Current loss 0.0745052742947361 Accuracy 81.87702265372168\n",
      "Training:: Epoch 282, Iteration 90, Current loss 0.039706602541265734 Accuracy 83.38010101819931\n",
      "Training:: Epoch 282, Iteration 100, Current loss 0.04501124624839464 Accuracy 77.87987355110643\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 282, Probability Accuracy 45.68390852218585\n",
      "Starting Training\n",
      "Training:: Epoch 283, Iteration 0, Current loss 0.054955531158104615 Accuracy 83.62284225382695\n",
      "Training:: Epoch 283, Iteration 10, Current loss 0.04587241566392654 Accuracy 83.12996478584883\n",
      "Training:: Epoch 283, Iteration 20, Current loss 0.05366098533207821 Accuracy 86.28098359040064\n",
      "Training:: Epoch 283, Iteration 30, Current loss 0.04428250781284396 Accuracy 86.27334235453316\n",
      "Training:: Epoch 283, Iteration 40, Current loss 0.05372486660620902 Accuracy 82.23021582733813\n",
      "Training:: Epoch 283, Iteration 50, Current loss 0.05651359054500681 Accuracy 82.6645547342485\n",
      "Training:: Epoch 283, Iteration 60, Current loss 0.05460953514534033 Accuracy 84.23310690025026\n",
      "Training:: Epoch 283, Iteration 70, Current loss 0.04250309502153458 Accuracy 86.30633645693887\n",
      "Training:: Epoch 283, Iteration 80, Current loss 0.057104330209751204 Accuracy 84.0097941567065\n",
      "Training:: Epoch 283, Iteration 90, Current loss 0.04884546243888128 Accuracy 82.31803469167566\n",
      "Training:: Epoch 283, Iteration 100, Current loss 0.03206161854598495 Accuracy 87.32670371453641\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 283, Probability Accuracy 44.292683432075236\n",
      "Starting Training\n",
      "Training:: Epoch 284, Iteration 0, Current loss 0.02793284989657152 Accuracy 83.49473486273035\n",
      "Training:: Epoch 284, Iteration 10, Current loss 0.040641934304771664 Accuracy 85.84580697688983\n",
      "Training:: Epoch 284, Iteration 20, Current loss 0.04100064407817966 Accuracy 90.21012912382733\n",
      "Training:: Epoch 284, Iteration 30, Current loss 0.04654118786793946 Accuracy 85.40493720225206\n",
      "Training:: Epoch 284, Iteration 40, Current loss 0.05855800300608469 Accuracy 72.83580462276494\n",
      "Training:: Epoch 284, Iteration 50, Current loss 0.038925472832647766 Accuracy 88.82637501314544\n",
      "Training:: Epoch 284, Iteration 60, Current loss 0.06472761690251774 Accuracy 77.71739130434783\n",
      "Training:: Epoch 284, Iteration 70, Current loss 0.06720455649329794 Accuracy 87.96299821081884\n",
      "Training:: Epoch 284, Iteration 80, Current loss 0.06981203818843282 Accuracy 84.09402546523016\n",
      "Training:: Epoch 284, Iteration 90, Current loss 0.0708825625865708 Accuracy 85.6325007421229\n",
      "Training:: Epoch 284, Iteration 100, Current loss 0.05765737518324476 Accuracy 85.09079324625677\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 284, Probability Accuracy 45.538385051994865\n",
      "Starting Training\n",
      "Training:: Epoch 285, Iteration 0, Current loss 0.038599566596320724 Accuracy 87.16341877577423\n",
      "Training:: Epoch 285, Iteration 10, Current loss 0.04680305708593037 Accuracy 88.30268935236005\n",
      "Training:: Epoch 285, Iteration 20, Current loss 0.0769651636162557 Accuracy 82.6219512195122\n",
      "Training:: Epoch 285, Iteration 30, Current loss 0.039336997109516283 Accuracy 89.2702878330339\n",
      "Training:: Epoch 285, Iteration 40, Current loss 0.05038116207897373 Accuracy 84.53407374933022\n",
      "Training:: Epoch 285, Iteration 50, Current loss 0.037984565744991226 Accuracy 88.26031475967673\n",
      "Training:: Epoch 285, Iteration 60, Current loss 0.05704699554181074 Accuracy 85.5186499698654\n",
      "Training:: Epoch 285, Iteration 70, Current loss 0.043961801209789333 Accuracy 90.85320761693474\n",
      "Training:: Epoch 285, Iteration 80, Current loss 0.06915092511241443 Accuracy 83.59157914566947\n",
      "Training:: Epoch 285, Iteration 90, Current loss 0.037206006324381015 Accuracy 86.07194442533874\n",
      "Training:: Epoch 285, Iteration 100, Current loss 0.052965234302714256 Accuracy 88.33655218742128\n",
      "Calculating Expectation\n",
      "Epoch 285 iter 0\n",
      "Epoch 285 iter 10\n",
      "Epoch 285 iter 20\n",
      "Epoch 285 iter 30\n",
      "Epoch 285 iter 40\n",
      "Epoch 285 iter 50\n",
      "Epoch 285 iter 60\n",
      "Epoch 285 iter 70\n",
      "Epoch 285 iter 80\n",
      "Epoch 285 iter 90\n",
      "Epoch 285 iter 100\n",
      "Train Boundary avergage error = 126.533\n",
      "Train From boundary avergage accuracy = 84.179\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 285, Probability Accuracy 45.26473878278162\n",
      "Starting Training\n",
      "Training:: Epoch 286, Iteration 0, Current loss 0.061403178825337426 Accuracy 89.33580189770886\n",
      "Training:: Epoch 286, Iteration 10, Current loss 0.050522564801683975 Accuracy 82.74323790163194\n",
      "Training:: Epoch 286, Iteration 20, Current loss 0.049965470835400094 Accuracy 81.89431877050596\n",
      "Training:: Epoch 286, Iteration 30, Current loss 0.05592333515064165 Accuracy 83.26351718253045\n",
      "Training:: Epoch 286, Iteration 40, Current loss 0.08113417962897031 Accuracy 78.14238042269189\n",
      "Training:: Epoch 286, Iteration 50, Current loss 0.05604013078605289 Accuracy 82.67263427109974\n",
      "Training:: Epoch 286, Iteration 60, Current loss 0.08313374663417927 Accuracy 85.87128076226107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 286, Iteration 70, Current loss 0.08631524141080457 Accuracy 87.15050565139798\n",
      "Training:: Epoch 286, Iteration 80, Current loss 0.07028844910508955 Accuracy 87.20305358846869\n",
      "Training:: Epoch 286, Iteration 90, Current loss 0.04767110232978127 Accuracy 81.96459562651857\n",
      "Training:: Epoch 286, Iteration 100, Current loss 0.04910608389626689 Accuracy 84.14536099051294\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 286, Probability Accuracy 44.28905829224841\n",
      "Starting Training\n",
      "Training:: Epoch 287, Iteration 0, Current loss 0.047797409157714243 Accuracy 86.44896628541933\n",
      "Training:: Epoch 287, Iteration 10, Current loss 0.0660482347362798 Accuracy 81.43366464995678\n",
      "Training:: Epoch 287, Iteration 20, Current loss 0.056073371956871884 Accuracy 79.16385379496576\n",
      "Training:: Epoch 287, Iteration 30, Current loss 0.06634223310802717 Accuracy 84.3194296484471\n",
      "Training:: Epoch 287, Iteration 40, Current loss 0.07819350025246187 Accuracy 83.85459238545924\n",
      "Training:: Epoch 287, Iteration 50, Current loss 0.049905009115774696 Accuracy 83.03085299455536\n",
      "Training:: Epoch 287, Iteration 60, Current loss 0.041217808192715547 Accuracy 83.78611293499672\n",
      "Training:: Epoch 287, Iteration 70, Current loss 0.0530885847031185 Accuracy 90.20340089191835\n",
      "Training:: Epoch 287, Iteration 80, Current loss 0.050711333391581376 Accuracy 86.75544007794738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:: Epoch 374, Iteration 50, Current loss 0.04307076376363107 Accuracy 84.21522741076852\n",
      "Training:: Epoch 374, Iteration 60, Current loss 0.0676386938986225 Accuracy 77.44174003107199\n",
      "Training:: Epoch 374, Iteration 70, Current loss 0.04962205686831937 Accuracy 73.4812794445822\n",
      "Training:: Epoch 374, Iteration 80, Current loss 0.03286055008133284 Accuracy 84.16679371855466\n",
      "Training:: Epoch 374, Iteration 90, Current loss 0.045579356748320775 Accuracy 79.33070139836919\n",
      "Training:: Epoch 374, Iteration 100, Current loss 0.026234406726585488 Accuracy 88.5280058830739\n",
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 374, Probability Accuracy 43.298359365289805\n",
      "Starting Training\n",
      "Training:: Epoch 375, Iteration 0, Current loss 0.03836542704710035 Accuracy 85.13078274448566\n",
      "Training:: Epoch 375, Iteration 10, Current loss 0.14550320231560593 Accuracy 80.87248322147651\n",
      "Training:: Epoch 375, Iteration 20, Current loss 0.08624575779645743 Accuracy 81.34861385739075\n",
      "Training:: Epoch 375, Iteration 30, Current loss 0.06931820237364628 Accuracy 84.75375263614936\n",
      "Training:: Epoch 375, Iteration 40, Current loss 0.050474114261613676 Accuracy 81.89954014856738\n",
      "Training:: Epoch 375, Iteration 50, Current loss 0.05902271748871851 Accuracy 81.76068376068376\n",
      "Training:: Epoch 375, Iteration 60, Current loss 0.04804756851445714 Accuracy 83.46720005000469\n",
      "Training:: Epoch 375, Iteration 70, Current loss 0.049551061640262434 Accuracy 73.73462077558013\n",
      "Training:: Epoch 375, Iteration 80, Current loss 0.0559818005541164 Accuracy 78.73684210526316\n",
      "Training:: Epoch 375, Iteration 90, Current loss 0.06263117559414515 Accuracy 89.72115910333515\n",
      "Training:: Epoch 375, Iteration 100, Current loss 0.05855129207240982 Accuracy 78.01163798440015\n",
      "Calculating Expectation\n",
      "Epoch 375 iter 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f9150d72a17a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mcalculate_element_probb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d65de1ea321d>\u001b[0m in \u001b[0;36mcalculate_element_probb\u001b[0;34m(data_feat, data_count, video_ids, labels_all)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mselected_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_vidid_selected_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_vidid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprob_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_vals_per_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_vid_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprob_video_each_segment_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_vidid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3ccc3c9dc0be>\u001b[0m in \u001b[0;36mprob_vals_per_segment\u001b[0;34m(selected_frames, cur_vid_feat, labels)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcur_boundary_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcur_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlen_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_possion_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_boundary_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cur_ele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mright_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fe31267f918d>\u001b[0m in \u001b[0;36mget_possion_prob\u001b[0;34m(cur_len, cur_class)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     factorials = torch.cumsum(torch.log(torch.arange(1, torch.max(cur_len)+1, 1).type(torch.float).to(device)), \n\u001b[0;32m----> 8\u001b[0;31m                               dim=0)[min(cur_len)-1:]\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfactorials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/video_r/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0massigned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRAPPER_ASSIGNMENTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massigned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initialize_epoch = 50\n",
    "expectation_cal_gap = 5\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(51, 1000):\n",
    "    print(\"Starting Training\")\n",
    "    model.train()\n",
    "    for i, item in enumerate(trainloader):\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y, y_list, x = model(item_0)\n",
    "        probs = torch.softmax(y, dim=1) # get_ensemble_out([y, y_list])\n",
    "#         logits = torch.log(probs + 1e-8)\n",
    "        \n",
    "        boundary_target_tensor = get_single_random(item_2, item[4])\n",
    "        \n",
    "        loss = 0\n",
    "        if epoch <= initialize_epoch:\n",
    "            loss += ce_criterion(y, boundary_target_tensor)\n",
    "            loss += 0.15 * torch.mean(torch.clamp(mse_criterion(F.log_softmax(y[:, :, 1:], dim=1), \n",
    "                                                                F.log_softmax(y.detach()[:, :, :-1], dim=1)), min=0,\n",
    "                                            max=16) * src_mask_mse[:, :, 1:])\n",
    "        else:\n",
    "            probs = probs.permute(0, 2, 1)\n",
    "            es_loss, _ = get_estimated_loss(probs, item_1, item[4], item_2)\n",
    "            loss += es_loss\n",
    "            probs = probs.permute(0, 2, 1)\n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                pred = torch.argmax(probs, dim=1)\n",
    "                correct = float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total = float(torch.sum(src_mask).item())\n",
    "                print(f\"Training:: Epoch {epoch}, Iteration {i}, Current loss {loss.item()}\" +\n",
    "                      f\" Accuracy {correct * 100.0 / total}\")\n",
    "                \n",
    "    # Calculating Expectation Step\n",
    "    model.eval()\n",
    "\n",
    "    if (epoch >= initialize_epoch) and (epoch % expectation_cal_gap == 0):\n",
    "        print(\"Calculating Expectation\")\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "        for i, item in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                item_0 = item[0].to(device)\n",
    "                item_1 = item[1].to(device)\n",
    "                item_2 = item[2].to(device)\n",
    "                src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "                src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "                y, y_list, x = model(item_0)\n",
    "                probs = torch.softmax(y, dim=1) # get_ensemble_out([y, y_list])\n",
    "                \n",
    "                probs = probs.permute(0, 2, 1)\n",
    "                calculate_element_probb(probs, item_1, item[4], item_2)\n",
    "                \n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Epoch {epoch} iter {i}\")\n",
    "                    \n",
    "        get_boundary_err()\n",
    "        \n",
    "    if epoch == initialize_epoch:\n",
    "        torch.save(model.state_dict(), config.output_dir + \"c2f-tcn-initial-15-epochs.wt\")\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        \n",
    "        print(\"Calculating Validation Data Accuracy\")\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for i, item in enumerate(testloader):\n",
    "            with torch.no_grad():\n",
    "                item_0 = item[0].to(device)\n",
    "                item_1 = item[1].to(device)\n",
    "                item_2 = item[2].to(device)\n",
    "                src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "                src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "                y, y_list, x = model(item_0)\n",
    "                probs = torch.softmax(y, dim=1) # get_ensemble_out([y, y_list])\n",
    "                \n",
    "                pred = torch.argmax(probs, dim=1)\n",
    "                correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "                total += float(torch.sum(src_mask).item())\n",
    "        val_acc = correct * 100.0 / total\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(model.state_dict(), config.output_dir + \"c2f-tcn-emmax-best-model.wt\")\n",
    "        torch.save(model.state_dict(), config.output_dir + \"c2f-tcn-emmax-last-model.wt\")\n",
    "        print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast//results/em-maximize-mstcn-speed/ms-tcn-em.wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/\"\n",
    "                            f\"/results/em-maximize-mstcn-speed/ms-tcn-em.wt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Validation Data Accuracy\n",
      "Validation:: Epoch 16, Probability Accuracy 59.57357998094212\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating Validation Data Accuracy\")\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "for i, item in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "\n",
    "        pred = torch.argmax(predictions[-1], dim=1)\n",
    "        correct += float(torch.sum((pred == item_2) * src_mask).item())\n",
    "        total += float(torch.sum(src_mask).item())\n",
    "\n",
    "print(f\"Validation:: Epoch {epoch}, Probability Accuracy {correct * 100.0 / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_labels(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            select_item = np.random.randint(start, i, 1)[0]\n",
    "            unique_ids.append(select_item)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    select_item = np.random.randint(start, len(labels_arr), 1)[0]\n",
    "    unique_ids.append(select_item)\n",
    "    return unique_ids\n",
    "# get_selected_labels(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(labels_arr):\n",
    "    unique_ids = []\n",
    "    \n",
    "    prev_ele = None\n",
    "    start = 0\n",
    "    for i, ele in enumerate(labels_arr):\n",
    "        if prev_ele is not None and prev_ele != ele:\n",
    "            unique_ids.append(i - 1)\n",
    "            start = i\n",
    "        prev_ele = ele\n",
    "    \n",
    "    unique_ids.append(len(labels_arr) - 1)\n",
    "    return unique_ids\n",
    "# get_boundary(np.array([2, 2, 2, 2, 3, 3, 4, 4, 4, 1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vidid_selected_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[5]\n",
    "    for i, count in enumerate(count_all):\n",
    "        video_id = video_ids[i]\n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_selected_labels(labels)\n",
    "\n",
    "        loaded_vidid_selected_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_boundary_frames = {}\n",
    "for batch in trainloader:\n",
    "    count_all = batch[1]\n",
    "    labels_all = batch[2]\n",
    "    video_ids = batch[4]\n",
    "    for i, count in enumerate(count_all):\n",
    "        \n",
    "        labels = labels_all[i][:count]\n",
    "        selected_ids = get_boundary(labels)\n",
    "        video_id = video_ids[i]\n",
    "        video_id_boundary_frames[video_id] = selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in video_id_boundary_frames.keys():\n",
    "    if len(video_id_boundary_frames[ele]) != len(loaded_vidid_selected_frames[ele + \".txt\"]):\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(video_id_boundary_frames, open(\"dump_dir/video_id_boundary_frames_dict.pkl\", \"wb\"))\n",
    "# pickle.dump(loaded_vidid_selected_frames, open(\"dump_dir/loaded_vidid_selected_frames_dict.pkl\", \"wb\"))\n",
    "pickle.dump(video_id_boundary_frames, open(\"dump_dir/chunk_1_video_id_boundary_frames_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_out(outp):\n",
    "    \n",
    "    weights = [1, 1, 1, 1, 0, 0]\n",
    "    ensemble_prob = F.softmax(outp[0], dim=1) * weights[0] / sum(weights)\n",
    "\n",
    "    for i, outp_ele in enumerate(outp[1]):\n",
    "        upped_logit = F.upsample(outp_ele, size=outp[0].shape[-1], mode='linear', align_corners=True)\n",
    "        ensemble_prob = ensemble_prob + F.softmax(upped_logit, dim=1) * weights[i + 1] / sum(weights)\n",
    "    \n",
    "    return ensemble_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/mnt/ssd/all_users/dipika/ms_tcn/data/breakfast/results/c2f-tcn-model/split2_c2ftcn_model.wt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iteration done\n",
      "11 iteration done\n",
      "21 iteration done\n",
      "31 iteration done\n",
      "41 iteration done\n",
      "51 iteration done\n",
      "61 iteration done\n",
      "71 iteration done\n",
      "81 iteration done\n",
      "91 iteration done\n",
      "101 iteration done\n",
      "111 iteration done\n",
      "121 iteration done\n",
      "131 iteration done\n",
      "141 iteration done\n",
      "151 iteration done\n",
      "Train Boundary avergage error = 107.269\n",
      "Train From boundary avergage accuracy = 87.407\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, item in enumerate(trainloader):\n",
    "    with torch.no_grad():\n",
    "        item_0 = item[0].to(device)\n",
    "        item_1 = item[1].to(device)\n",
    "        item_2 = item[2].to(device)\n",
    "        src_mask = torch.arange(item_2.shape[1], device=item_2.device)[None, :] < item_1[:, None]\n",
    "        src_mask_mse = src_mask.unsqueeze(1).to(torch.float32).to(device)\n",
    "\n",
    "        middle_pred, predictions = model(item_0, src_mask_mse)\n",
    "        prob = torch.softmax(predictions[-1], dim=1)\n",
    "        prob = prob.permute(0, 2, 1)\n",
    "        calculate_element_probb(prob, item_1, item[4], item_2)\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f'{i+1} iteration done')\n",
    "get_boundary_err()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    idx = 4\n",
    "\n",
    "    cur_vid_feat = torch.softmax(predictions[-1], dim=1).permute(0, 2, 1)[idx]\n",
    "    cur_vidid = item[4][idx]\n",
    "\n",
    "    selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "    \n",
    "    bound_list = video_id_boundary_frames[cur_vidid]\n",
    "    \n",
    "    cumsum_feat = torch.cumsum(cur_vid_feat, dim=0)\n",
    "    prob_each_segment = prob_vals_per_segment(selected_frames, cur_vid_feat, item_2[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min prob 0 = 7.953912266787591e-36\n",
      "Min prob 1 = 2.7495868628582206e-249\n",
      "Min prob 2 = 8.185175464823537e-201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 442)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "ymax = 1\n",
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "\n",
    "plt.plot([0, item[1][idx].item()], [0, 0])\n",
    "\n",
    "for pivot in loaded_vidid_selected_frames[cur_vidid + \".txt\"]:\n",
    "    plt.plot([pivot, pivot], [0, ymax], '--')\n",
    "    \n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, ymax])\n",
    "    \n",
    "all_prob = []\n",
    "for i, segment in enumerate(prob_each_segment):\n",
    "    segment = segment.cpu().numpy()\n",
    "    print(f\"Min prob {i} = {np.min(segment).item()}\")\n",
    "    xs = np.arange(loaded_vidid_selected_frames[cur_vidid + \".txt\"][i], \n",
    "                   loaded_vidid_selected_frames[cur_vidid + \".txt\"][i+1])\n",
    "    plt.plot(xs, segment, '*')\n",
    "\n",
    "    \n",
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "\n",
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))\n",
    "loss_arr = -loss_arr[0].detach().cpu().numpy()\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr/np.max(loss_arr), 'k+')\n",
    "\n",
    "plt.ylim([0, ymax])\n",
    "plt.xlim([0, item[1][idx].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1895, 48])\n"
     ]
    }
   ],
   "source": [
    "loss_prob = torch.softmax(cur_vid_feat.unsqueeze(0), dim=2)\n",
    "print(loss_prob.shape)\n",
    "es_loss, loss_arr = get_estimated_loss(loss_prob, torch.tensor([item_1[idx]]), [item[4][idx]], item_2[idx].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1387])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf5b8ecb00>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_frames = loaded_vidid_selected_frames[cur_vidid + \".txt\"]\n",
    "plt.plot(np.arange(selected_frames[0] - 1, selected_frames[-1] + 1), loss_arr[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3987, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([75, 345, 1305, 1415, 1440, 1519],\n",
       " [73, 282, 455, 1312, 1434, 1458],\n",
       " tensor(1520, device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid], loaded_vidid_selected_frames[cur_vidid + \".txt\"], item_1[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_arr = []\n",
    "for i in range(item[1][idx].item()):\n",
    "    plt_arr.append(cur_vid_feat[i,item[2][idx][i]].item()) \n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.plot(np.arange(item[1][idx].item()), plt_arr)\n",
    "\n",
    "for pivot in video_id_boundary_frames[cur_vidid]:\n",
    "    plt.plot([pivot, pivot], [0, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([334, 334])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_features_shortened = cur_vid_feat[:item[1][idx]]\n",
    "cur_features_shortened[:, item[2][idx][:item[1][idx]]].shape\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 47, 47, 47, 47, 47, 47,  4,  4,  4,  4,  4,  4,  4,  4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[2][idx][video_id_boundary_frames[cur_vidid][1]-6:video_id_boundary_frames[cur_vidid][1]+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_boundary_frames[cur_vidid][0]-6,video_id_boundary_frames[cur_vidid][0]+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0794, 13.3350],\n",
       "        [ 1.0831, 13.5102],\n",
       "        [ 1.1767, 13.7662],\n",
       "        [ 1.3750, 14.0177],\n",
       "        [ 1.7298, 14.1727],\n",
       "        [ 2.1706, 14.0367],\n",
       "        [ 2.7338, 13.4563],\n",
       "        [ 4.5600, 12.1745],\n",
       "        [ 7.8450, 10.3246],\n",
       "        [11.0151,  8.3563],\n",
       "        [13.4775,  6.6546],\n",
       "        [14.1836,  5.2898],\n",
       "        [11.2142,  4.2791],\n",
       "        [ 5.9829,  3.5694],\n",
       "        [ 2.3440,  2.9389],\n",
       "        [ 0.7406,  2.3515]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_vid_feat[video_id_boundary_frames[cur_vidid][1]-7:video_id_boundary_frames[cur_vidid][1]+9][:,[47,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frames_and_get_classification_value(model_output, count_list, video_ids_list, labels_all, p_criterion):\n",
    "    new_selected_frames = {}\n",
    "#     new_selected_frames_probs_values = {}\n",
    "#     new_selected_frames_labels = {}\n",
    "    \n",
    "    labels_arr = []\n",
    "    probs_arr = []\n",
    "    \n",
    "    for i, video_id in enumerate(video_ids_list):\n",
    "        current_boundary_frames = boundaries_dict[video_id]\n",
    "        current_video_logits = model_output[i].T\n",
    "        current_video_count = count_list[i]\n",
    "        current_selected_frames = loaded_vidid_selected_frames[video_id]\n",
    "        labels = labels_all[i]\n",
    "        cur_seg_start = 0\n",
    "        for seg_num, ele in enumerate(current_boundary_frames):\n",
    "            cur_seg_end = ele + 1\n",
    "            total_seg_len = cur_seg_end - cur_seg_start\n",
    "            per_frames_far_from_boundary = int(0.3 * total_seg_len)\n",
    "            new_frames_start = cur_seg_start + per_frames_far_from_boundary\n",
    "            new_frames_end = cur_seg_end - per_frames_far_from_boundary\n",
    "            \n",
    "            frames_added = list(range(new_frames_start, new_frames_end))\n",
    "            frames_added.append(current_selected_frames[seg_num])\n",
    "            frames_added = list(set(frames_added))\n",
    "            frames_added.sort()\n",
    "            new_selected_frames[video_id] = frames_added\n",
    "\n",
    "            \n",
    "            labels_seg = torch.stack([labels[current_selected_frames[seg_num]]] * len(frames_added))\n",
    "            frames_added = torch.tensor(np.array(frames_added), dtype=torch.long, device=current_video_logits.device)\n",
    "            prob_seg = current_video_logits[frames_added, :]\n",
    "            \n",
    "            labels_arr.append(labels_seg)\n",
    "            probs_arr.append(prob_seg)\n",
    "            \n",
    "            cur_seg_start = cur_seg_end\n",
    "            \n",
    "    labels_arr = torch.cat(labels_arr)\n",
    "    probs_arr = torch.cat(probs_arr)\n",
    "    loss = p_criterion(probs_arr, labels_arr)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
